[deleted],,2014-07-31 03:11:36
"No problem, hope it's useful!",randomsample,2014-07-31 06:32:05
"Ive been trying to get a grip on R and truly understand the type of projects I can do.

Bravo.. I am goimg to study your code and start my own project.. thank you for the inspiration",talentdmryanski,2014-07-31 21:09:27
[deleted],,2014-07-31 16:09:53
[information is beautiful]( http://infobeautiful3.s3.amazonaws.com/2013/03/iib_death_wellcome_collection_fullsize.png) ,Mil0Mammon,2014-07-31 18:23:19
It's hard to comprehend just how violent the 20th century was compared to previous centuries.,Caelum_Cantorus,2014-07-31 20:01:06
[was the 20th century really the most violent?](http://socialdemocracy21stcentury.blogspot.nl/2013/02/was-20th-century-really-most-violent.html?m=1) ,Mil0Mammon,2014-07-31 20:08:23
"It's not a relative statistic. It is factual that many more people died. Not relative to overall population, but that isn't what matters. In addition, those events are all spread out. With the world wars, you see tens of millions dead, then about twenty years later, we do it all over again.",Caelum_Cantorus,2014-07-31 20:11:25
"It does matter to an extent. How many people do you think Genghis Kahn would have killed, had Europe been as populated as it was during ww2? 

I'd say the most valid measurement would be the percentage of people that die a violent death. I'm pretty sure that has been going down for centuries, though not in a continuous line. ",Mil0Mammon,2014-07-31 20:19:46
"But he didn't. You see, it's not a statistical argument I'm making. A great number more people died in the twentieth century due to violence. That's a great number more individual lives snuffed out by   two avoidable wars. It's tragic. Yes, Ghengis Khan was violent, but he didn't have access to the mechanized weaponry or gas weapons, things which made the slaughter so much more expedient than the Great Khan's method, usually mass beheading. Whole the number of deaths may be statistically close due to population size, that doesn't mean that just as many died. ",Caelum_Cantorus,2014-07-31 20:27:11
A great number of people partied  during the 20th century as well. Probably more than ever. Does that make the 20th century the most cheerful? ,Mil0Mammon,2014-07-31 20:34:26
Clearly someone never saw men in tights,tell_me_im_wrong,2014-08-01 18:24:09
"My argument is that there were more people to party, and therefore more fun and joy to be had. It's more violent because more life was lost, and that is sad. Also, if you want to argue history with me, send me something from an historical paper, not a website about Keynesian economic theory. ",Caelum_Cantorus,2014-07-31 20:42:42
"The underlying source is ""The Better Angels of our Nature: Why Violence Has Declined"", seems quite academic to me, but nobody's going to read 800 pages these days (though I read somewhere he used so many to convince people like you ;). 

As long as you agree that they were the most violent *and* cheerful, we can stop arguing. ",Mil0Mammon,2014-08-01 01:57:29
Better to have that than to not know what the fuck Ebola is and wonder why everyone in my village is dead.,Caelum_Cantorus,2014-07-31 19:58:33
"Very cool post.

If you wanted to move to more realistic continuous time Markov processes, I recently published a tool that very rapidly solves the (usually intractable) master equation for SIR, SEIR and other such compartmental models. The paper is [open access here](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0036160). Using this tool, you could get the likelihood function exactly and numerically without the need for monte carlo. You can get our [source code here](http://cis.jhu.edu/~goutsias/CSS%20lab/software.html). The only problem I see is that a spatial model would vastly increase the size of the state space and might over-burden a numerical algorithm. It might be worth trying as a non-spatial algorithm first (e.g., assuming a well mixed single spatial compartment) and then moving on to more spatial detail as the computational burden would allow. Not sure if it interests you, but thought I would share. Most likely publishable at that point, since I am yet to see someone numerically solve the inverse problem in the continuos-time Markov process case.",DrGar,2014-07-31 10:58:05
"Thanks for the feedback, I'll check out the paper. 


I definitely appreciate the benefits of numerical methods - in the early stages of my current project I tried to re-cast the problem in terms of a GMRF to use with INLA, but couldn't do it efficiently. 


For now I've resorted to MCMC in order to keep the class of applicable models as flexible as possible, and am focusing on trying to improve sampling efficiency. The goal is to make tackling problems with much larger spatial domains feasible, but there is definitely a computational cost to going that route. ",randomsample,2014-07-31 11:12:43
"Yes, unfortunately in these things there are presently many modeling choices that eventually get limited by our computational capacity. Continuous time is clearly advantageous, but then you have to sacrifice other aspects of model complexity such as the spatial resolution. On the other hand, spatial resolution is clearly advantageous, but this often requires greater data resolution and quality and will limit you to more ad-hoc discrete time Markov chain models. I am hoping eventually numerical tools will improve upon the one I published so people can eventually move towards the continuous time models, even with a large number of spatial and/or disease state compartments. Then there will be less difficult modeling decisions for practitioners such as yourself :-)",DrGar,2014-07-31 11:25:08
"Yep, the problems get big really quickly! Even within the discrete time world there are models I'd like to explore which are currently computationally incompatible with the size of spatial problem I'd like to tackle. For example, relaxing the exponential time assumptions in the transitions using something like a [path specific](http://onlinelibrary.wiley.com/doi/10.1111/j.1541-0420.2012.01809.x/abstract) model. It's an exciting time though, and I'm glad we've got folks like you to work on the hard numerical problems - I get out of my depth in numerical analysis pretty quickly.",randomsample,2014-07-31 12:38:27
"I'm guessing it's the SEIR part that's problematic with INLA? Can't quite think of how to squeeze one of those into GMRF; I'd be interested to know what approach you tried!

Edit: I'll have to get back to this model in the morning for a proper read, but I can't see if you've used informative priors for the various rates? It seems to me that one of the reasons for this outbreak becoming so much bigger than the usual ones, is the uncommonly low mortality rate or ""R"". Surely this information needs to be incorporated in your model?
",GrynetMolvin,2014-07-31 16:16:21
I feel like I just stumbled into shop talk between two wizards who run the entire world as a hobby.  ,chakalakasp,2014-07-31 17:35:46
Thank god I wasn't the only one.   I kept looking for a curtain. ,caramelboy,2014-07-31 18:18:39
"It's really quite simple. The RNO of the HGKL is relative to the DDF, that is, if you take into account the EDHH once the AAHB is applied, but only if the VJPA value is compatible with the UBO properties. Hope this helps.",ClarifiedInsanity,2014-07-31 21:12:01
Thanks I was lost.,RossTheColonel,2014-07-31 21:45:55
"You got that from Vickers. ""Work in Essex County"", page 98, right?  Yeah, I read that too. Were you going to plagiarize the whole thing for us? Do you have any thoughts of your own on this matter? ",rox0r,2014-07-31 21:50:57
"Just gotta say, you made an overworked phd student smile for the day with this comment :-). ",GrynetMolvin,2014-08-01 11:57:17
"I'm headed to dinner, so I'll have to reply with details later. The general idea was to reparameterize all of the SEIR transition equations as constraints, and use normal approximations all over the place. It can technically be done, but it requires serious approximations (and didn't end up being computationally feasible anyway). 

Edit, I can't immediately find the stuff I worked on (it was a while ago), but I started [here](http://www.r-inla.org/faq#TOC-I-have-a-complicated-state-space-model-can-this-be-fitted-using-INLA-). 


With enough work, you can approximate SEIR processes in that framework, but (although their software and algorithmic approach is super awesome), it isn't efficient for this case. 


On the other hand, the guys behind INLA are so far beyond my numerical and computational abilities that I may have just missed something. ",randomsample,2014-07-31 16:21:52
"I did use informative priors on the transition rates, but haven't had a chance to do a real sensitivity analysis (in the works). I based them on what I read in news articles about the potential range of incubation and infectious times, but haven't really revisited them. There's a brief discussion in the code comments. 

I think that this point is actually one of the biggest weaknesses of the method I'm using: the required exponential assumption on compartment membership times. Real world infectious and incubation periods are highly variable, and that's not captured here. ",randomsample,2014-08-01 11:52:48
"I love this. Thank you for providing the code! This is an interest of mine and I'm learning it currently.

Just wanted to point out a typo (I assume):

&gt;While there has been much analysis and speculation about the factors at play in the spread of the virus, to our knowledge there aren't any specific **predicitons** about the expected duration and severity of this particular epidemic. In the (certainly temporary) absence of epidemic forecasts, this document explores a simple spatial SEIR model to make some initial predictions.",minorsecond,2014-07-31 06:59:08
"Good catch, I'll get that fixed. ",randomsample,2014-07-31 07:15:23
"I ended up in your link and I felt like a guy sneaking in a party where people speak a different language.Neverthless, I like how your voice sounds.Thank you for doing this.I also wanted to wish good luck to the man and his wife for her new job!Goodnight from Italy!",Adrasto,2014-07-31 15:52:39
"Yeah, every field has it's own language. I tried to keep it at a level where most folks could skim over the technical bits and still get the big picture, but it's a hard line to walk. ",randomsample,2014-07-31 15:54:02
RemindMe! 7 hours.,FalseFactsOrg,2014-07-31 16:56:22
"I'll message you on [**2014-08-01 06:56:26 UTC**](http://www.wolframalpha.com/input/?i=2014-08-01 06:56:26 UTC To Local Time) to remind you of this post.

[**Click Here**](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[http://www.reddit.com/r/statistics/comments/2c7aon/a_quick_spatial_seir_model_for_the_2014_ebola/cjdic18]%0ANOTE: MAKE SURE THE TIME OPTIONS ARE CORRECT.%0AEXAMPLE: RemindMe 48 hours/days/weeks/months etc%0A%0ARemindMe!  7 hours.) to also be reminded and to reduce spam.

_____
 ^(I will PM you a message so you don't forget about the comment or thread later on. Just use the **RemindMe!** command and optional date formats. Subsequent confirmations in this unique thread will be sent through PM to avoid spam. Default wait is a day.)

[^([PM Reminder])](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK HERE else default to FAQs]%0A%0ANOTE: Don't forget to add time options after RemindMe command!%0A%0ARemindMe!) ^| [^([FAQs])](http://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/) ^| [^([Time Options])](http://www.reddit.com/r/RemindMeBot/comments/2862bd/remindmebot_date_options/) ^| [^([Suggestions])](http://www.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Suggestion) ^| [^([Code])](https://github.com/SIlver--/remindmebot-reddit)",RemindMeBot,2014-07-31 16:56:31
RemindMe! 1 month,ItsTyrrellYo,2014-07-31 17:29:15
Pretty sure it only follows him around.,DuckTapeHero,2014-07-31 19:28:43
It pms you if you reply to it to avoid spam,ItsTyrrellYo,2014-07-31 19:32:50
So why not just use enhancement suite,DuckTapeHero,2014-07-31 19:39:42
on my phone :),ItsTyrrellYo,2014-07-31 19:49:12
IPhone or android?,DuckTapeHero,2014-07-31 19:55:47
windoze,ItsTyrrellYo,2014-07-31 20:15:54
RemindMe! 1 month,gasolinerainbow,2014-07-31 17:56:18
RemindMe! 2 hours.,paradajz_hr,2014-09-02 08:41:06
"Messaging you on [**2014-09-02 17:41:18 UTC**](http://www.wolframalpha.com/input/?i=2014-09-02 17:41:18 UTC To Local Time) to remind you of [**this comment.**](http://www.reddit.com/r/statistics/comments/2c7aon/a_quick_spatial_seir_model_for_the_2014_ebola/ck75380)

[**CLICK THIS LINK**](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[http://www.reddit.com/r/statistics/comments/2c7aon/a_quick_spatial_seir_model_for_the_2014_ebola/ck75380]%0A%0ARemindMe!  2 hours.) to send a PM to also be reminded and to reduce spam.

_____

[^([FAQs])](http://www.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/) ^| [^([Custom Reminder])](http://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!) ^| [^([Feedback])](http://www.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback) ^| [^([Code])](https://github.com/SIlver--/remindmebot-reddit)",RemindMeBot,2014-09-02 08:41:22
RemindMe! 3 hours.,ratorian,2014-09-19 07:34:04
[deleted],,2014-07-31 15:38:59
"No problem, thanks for reading!",randomsample,2014-07-31 15:41:02
[deleted],,2014-07-31 20:16:21
RemindMe! 9 hours.,norrob,2014-07-31 19:24:59
"A previous R implementation of the idea:
https://gist.github.com/rasmusab/8956852",MurrayBozinski,2015-01-26 02:03:56
This is actually hilarious thanks for sharing that :),The_Sap_Must_Flow,2015-01-26 09:25:30
I lost it at the subgroup analysis bit... oh epidemiology ,Case_Control,2015-01-26 06:33:19
"If all else fails, use p-values and hope no-one notices.",bluecoffee,2015-01-26 00:23:17
What exactly is the problem with p-values?,ravinginthestreets,2015-01-26 03:09:59
"The problem is the common misconception of ""lower the p-value, stronger the evidence for the alternative"". There is no such thing as ""highly significant"" or ""almost significant"". It's either significant or it isn't.

And then there is the ""nonsignificant results are completely meaningless"" mantra, which is a good rule of thumb for undergrads to follow, but a counterproductive criterion for publication.",rottenborough,2015-01-26 11:38:47
"Imagine a one sample t test of normal data with unknown mean and variance.

Test the hypothesis:

H0: mu &lt;= 0

H1: mu &gt; 0


Imagine one population of measures (E[X] = mu1), 10 different samples which had p-values of around 0.0000001 :

Another population of measures (E[Y] = mu2), 10 different samples which had p-values of around 0.02.

It sounds like you are saying that both of them would equally state that mu &gt; 0 (which I agree) but couldn't it be argued that mu1 &gt; mu2. When people use the idea of highly significant, I think they are trying to convey the the idea that population 1 has a higher effect size than population 2.

Even though people only run a test once, they have some mental reference points (similar to different populations) and by saying ""highly significant"" they are actually trying to say the sample effect size is large enough that were are almost, with certainty it's not 0.
",Corruptionss,2015-01-26 17:49:51
"The problem is a p-value is not an effect size, and saying highly significant suggests a large effect when it may actually be small (e.g., when you have a really large sample size).",MortalitySalient,2015-01-26 22:20:05
"You can make a casual observation that mu1 &gt; mu2 is likely, but you can't scientifically argue that is the case.


The reason we use p-value is for error control. When you set the rejection criterion to p &lt; .05, it carries the promise that if you do everything correctly, the Type I error rate of your test procedure, in the long run, is 5%.


When you use p-values from two different tests to make the argument that mu1 &gt; mu2, you throw the scientific process out of the window. You don't know what your Type I error rate is in the long run, not to mention you have no idea how much power that procedure has. There is no reason to use the p-value at that point. You're better off just comparing the observed means of the two samples.

There are some interesting (and unintuitive) implications of thinking that p-value means the strength of evidence. For example, when the null hypothesis is true, all p-values are equally likely. That means when you're wrong about H0 being false, you are equally likely to obtain 0.0001, the stronger evidence, and  0.0200, the weaker evidence. Where as stating that all p-values below .05 yield rejection gives you the straightforward 5% error rate in that scenario.

I strongly recommend doing more readings on the topic: http://www.perfendo.org/docs/BayesProbability/twelvePvaluemisconceptions.pdf",rottenborough,2015-01-26 18:42:14
"
Interesting read. I guess the point is that p-values by themselves do not directly correspond with their effect sizes (you may need sample size and make various assumptions on top of having p-values).

If however p-value had a 1-1 transformation to the effect size (so that if one test had a p-value of 0.001 and another had p-value of 0.001 then this would imply that the sufficient statistics were the same). Then I think it would be easier to argue that there is a relationship between p-value and the effect size.",Corruptionss,2015-01-26 19:09:24
"It is correct that, in very specific cases, p-value has a direct relationship to the observed effect size, but why use p-value instead of the observed effect size itself? It misleads people into thinking the Type I error rate of the analysis has been controlled for, when the report simply provides observed scores.

There is no reason (perhaps other than expediency) to use p-value for something that it is neither intended for nor generally applicable to.",rottenborough,2015-01-27 10:42:04
I was thinking more of a meta analytical perspective where sometimes the data (or even the complete sufficient statistics) are not readily available. ,Corruptionss,2015-01-27 11:15:06
"There are techniques for those types of complex analyses that try to control for error rates. I don't know much about that area of the field.

In general, researchers should make their raw data and analysis procedures readily available for others. With the information technologies available today, using advanced techniques to get around the lack of raw data access should be exceptions rather than the norm.",rottenborough,2015-01-27 14:27:12
"Strict assumptions which you never really adhere to, as well as having somewhat arbitrary rules. However, in many cases you just don't have that many better alternatives (as they all come with their own caveats).",KoentJ,2015-01-26 03:41:24
"Is it strictly the p-value that is problematic/has arbitrary rules, or is it the combination of say an F-statistic and it's p-value? I'm still trying to figure out how they are related. In my Bachelor's we'd always look at the p-value, whereas in my current course we're only ever looking at say the F-statistic compared to its critical value.",ravinginthestreets,2015-01-26 04:07:59
"There's no functional difference between the p-value or the F-statistic. p-value just normalises the likelihood of any given test statistic. 

The problem is that the p-value is taken as the be all and end all, without understanding the limitations or even the meaning of it. p-value says nothing about the magnitude of the effect, it's merely the likelihood that the observed difference happened due to random chance.",Azza_,2015-01-26 05:03:26
That is practically all you can hope for with nonparametric statistics. I did look up a limited way to calculate effect size for a stupid reviewer once though. Ended up not doing it and explaining how that would confuse readers and that the qualitative results should be the focus anyway.,DrHappyFunTime,2015-01-26 08:12:02
Somebody else already answered your question but I wanted to point out that confidence intervals around some parameter are generally better than p-values since they give you information on the magnitude of the effect.  I usually report both p-values and confidence intervals.,kevjohnson,2015-01-26 07:02:50
huh? confidence intervals are just the point estimate with standard errors. ,OhanianIsACreep,2015-01-26 16:19:22
"No, they incorporate sample size as well.",kevjohnson,2015-01-26 18:22:26
only through the standard error though.,OhanianIsACreep,2015-01-26 18:27:41
"Aside from the things people have mentioned about p-values specifically, frequentist stats as a whole is considered a bit old hat by many statisticians. The root of the problem is that frequentist stats looks at p(data|event) then uses it to make statements about p(event|data). This leads to a lot of handwavy-ness. Once upon a time that handwaving was acceptable, as the Bayesian approach of calculating p(event|data) directly can be computationally intensive, but nowadays there's not much excuse.

Unfortunately, statistical education in science is lagging very hard behind, so this usually comes as a surprise to anyone who isn't a statistician.",bluecoffee,2015-01-26 07:24:44
"The problem with the Bayesian approach is having to specify an a priori distribution. This isn't an issue when some parameters are known (due to prior research; or in hard sciences like physics where a hypothetical distribution can be calculated beforehand), but often parameters are unknown and the reason why the research is conducted.

In this case many novice bayesian statisticians will still assume a normal distribution for the population in which case baseyian inferences don't offer much more over frequentist inference (in fact, they will almost be identical).

Frequentist inferences have their place when done properly, and when people realize that like /u/Azza_ states [here](http://www.reddit.com/r/statistics/comments/2tp56l/xkcd_today_if_all_else_fails_use_significant_at/co17hjw), they interpret it correctly as merely stating the likelihood that the observed *difference/b-weight/whatever value you are testing*, would be observed if the *difference/b-weight/whatever the value* you are testing would be equal to zero.

Nothing against Bayesian statistics, it's a big step forward, I just wanted to state that frequentist statistics can still have a place in science when (i) done correctly, (ii) is applicable given the assumptions (which I have to agree, it often isn't), and (iii) there isn't a better alternative (for example when parameters cannot be given an a priori distribution).",KoentJ,2015-01-26 08:09:21
"&gt; The problem with the Bayesian approach is having to specify an a priori distribution.

This is a feature, not a bug.",OhanianIsACreep,2015-01-26 16:20:01
"Yes, yes, I know! But the feature also has its downsides, is what I meant.",KoentJ,2015-01-27 03:03:40
"&gt; The root of the problem is that frequentist stats looks at p(data|event) then uses it to make statements about p(event|data).

That's *not* what frequentist statistics does. If by ""event"" you mean parameter values (what we usually care about), then for a frequentist, P(event | data) doesn't even make sense, as parameters are not random variables.

&gt; This leads to a lot of handwavy-ness.

Because choosing priors for Bayesian analysis doesn't?",ivansml,2015-01-26 08:52:34
Interesting. So would you say a course on Bayesian is essential? I can take one next semester and was debating whether I should.,ravinginthestreets,2015-01-26 08:01:10
"If you'd like to understand statistical methods as well apply them, yes. It's a much more consistent, intuitive approach. Personally a whole pile of frequentist concepts only made sense after I'd worked through a [Bayesian-based machine learning textbook](http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/).",bluecoffee,2015-01-26 08:45:02
Isn't ironic that the frequentist approach borns as a way to evade calculation of a priori? Just look the first paper of Student about the probable error of the mean (better know as t distribution) and you find that the first analysis are made using a bayesian approach. ,clbustos,2015-01-26 08:44:49
"Those don't seem like the real criticisms.  The problem is that people always assume that a p-value is P(H_1 | Data), and in general don't know how to interpret them. 

Then when you start doing sequential testing or something similar p-values aren't easily interpreted when used correctly.",anonemouse2010,2015-01-26 07:39:06
"[Image](http://imgs.xkcd.com/comics/p_values.png)

**Title:** P-Values

**Title-text:** If all else fails, use ""signifcant at a p&gt;0.05 level"" and hope no one notices.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1478#Explanation)

**Stats:** This comic has been referenced 2 times, representing 0.0041% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_co11rft)",xkcd_transcriber,2015-01-25 22:24:37
"Just realized randall misspelled significant.
",deanzamo,2015-01-25 22:30:05
wonder if it is significant?,baslisks,2015-01-26 05:38:12
It must be - he fixed it.,deanzamo,2015-01-26 06:23:08
Probably hoped nobody would notice or it is a trick to help reviewers not notice...,DrHappyFunTime,2015-01-26 08:15:50
"This is my experience with management. They only care about p values, r squared, and percentage prediction accuracy (the more the better, regardless of overfitting).",,2015-01-26 10:10:16
"True but it ain't only management, sadly",arvi1000,2015-01-26 10:26:22
"Shouldn't the bit about p&gt;.10 read ""Hey, look at this interesting directional analysis""?",AllezCannes,2015-01-26 12:39:30
Can someone explain subgroup analysis for me please?,DVDV28,2015-01-26 15:43:36
"""Hey look, if we restrict our attention to a subgroup of our initial sample, chosen *post hoc*, we can find effects that aren't sustained over the whole sample!""

Or alternatively, http://www.xkcd.com/882/",conmanau,2015-01-26 16:18:55
"[Image](http://imgs.xkcd.com/comics/significant.png)

**Title:** Significant

**Title-text:** 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php/882#Explanation)

**Stats:** This comic has been referenced 166 times, representing 0.3360% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_co1u3s1)",xkcd_transcriber,2015-01-26 16:19:08
CNN needs to fire a statistician.[FTFY],,2012-10-26 19:06:42
touche!,freudian_nipple_slip,2012-10-26 21:17:59
"*Touché. ;)

BTW, your username is awesome. :D",Nolari,2012-10-27 02:09:00
"What's their population of polls?

There's an error with regard to the underlying population of people, but there is no sampling error of polls if they sample all of the polls they are reporting on in the report of the polls they are reporting on—by definition.

Let's say they were talking about 10 different polls, but they lost what one of them said. They they would have a sample error. Depending on why that one poll was lost, they might have a bias, too. However, if you are specifically interested in the polls (as opposed to the populations of people) and you have the exact number reported by every single poll you care about there's no error regarding what each of those polls *said*.",rz2000,2012-10-27 01:10:38
Is that because they just couldn't figure out how to quantify uncertainty in a meta-analysis?,samclifford,2012-10-26 18:34:14
"Yes, this is the point I'm trying to make below.  They don't even need to rely on standard meta-analysis techniques though, it's really nothing more than pooling variances. ",freudian_nipple_slip,2012-10-26 21:17:42
Apparently their poll of polls has NO sampling error!  ,freudian_nipple_slip,2012-10-26 17:41:58
"How could it?  You're not randomly sampling polls - you define your set of polls and take values from each one in the entire population.  It's misleading because  there is sampling error *within* each poll, but I don't think it's wrong.",tdyo,2012-10-26 18:46:56
"Except that their statement is that their poll of polls is made up of **at least three** polls that meet their criteria. That would indicate to me that the number of polls is not necessarily always all the polls that meet their criteria, in which case they are really only taking a sample of polls and there should be a sampling error.",,2012-10-27 01:02:49
"I don't know why this turned into a back-and-forth; tdyo's comment is 100% correct: The population is *defined* to be the set of other polls, so our sample is *equal* to the population, thus trivially implying zero sampling error for this so-called ""poll of polls"" (it should really be called ""a misleading average of polls"").",blindConjecture,2012-10-26 20:37:58
"Then why is it if the polling agencies were to conduct another poll immediately after the first one, the poll of polls would have a different average when there's no sampling error?",freudian_nipple_slip,2012-10-27 08:10:36
"Ok, let's make sure we're in agreement about the situation that's being discussed. This is what I'm saying is going on:

First we've got n≥3 many polls conducted using a ""CNN-approved methodology"". For simplicity here, let's just assume that exactly 3 such polls were conducted. Each of these polls has its own mean. Now it *is* true that the distribution from which these 3 respective means were obtained has nonzero variance, but that *not* what we care about here (and I think that where your confusion is arising). If we were to redo these three polls, then we *would* obtain different results as you say.

**However**, in addition to these three initial polls, we also have this ""poll of polls"". *This* is what we care about. This poll of polls defines its population to be the three earlier conducted polls. Therefore, when it averages over them, it is averaging over every single member of it's population, and thus there is zero sampling error, trivially.

You ask what would happen ""if the polling agencies were to conduct another poll immediately after the first one"". Well, which polls are we redoing? If we redo any one of the three original polls, and then subsequently perform the poll of polls on the *new* population, then the poll of polls will indeed be different, but only because we're measuring from a new population. **But** if we were to only redo the poll of polls (which is what we would do, since again, that's all we care about here), we would simply be taking exactly the same average over exactly the same three original polls, and nothing would change. i.e. zero sampling error

Is everything clear now?",blindConjecture,2012-10-27 11:22:22
"I understand what you're saying but I still think CNN is misleading.

What is sampling error? The average person couldn't explain what it means but it's the width of the confidence interval of your estimate.

If you're telling me the width of the confidence interval of the poll of polls is zero, you're flat out wrong",freudian_nipple_slip,2012-10-27 13:20:33
"&gt;What is sampling error? The average person couldn't explain what it means but it's the width of the confidence interval of your estimate.

I have yet to see *you* explain sampling error without reaching some point of childish frustration.  Case in point:

&gt;If you're telling me the width of the confidence interval of the poll of polls is zero, you're flat out wrong

I would really like to see what you're trying to get at.  Could you please explain what you mean?  I would suggest using Wikipedia links within your explanation for the big words that average people can't understand.

I believe you're right - I just want to understand the details.",tdyo,2012-10-28 07:55:39
"The more proper term would be margin of error, which is the half width of the confidence interval (i.e. when they say +/- 3%).  

When you use a poll of polls, you have two sources of variation, the within poll variation and the between poll variation.  By saying we're fixing the population of polls and essentially sampling all polls, there is no between poll variation but there is within poll variation that each contributes.  

The within poll variation is for a given poll, if we were to resample and estimate the proportion again, how much would it vary?  Each poll has this source of variation.  The between poll variation is how much do the polls themselves vary. Let's say our 3 polls are all quite close to one another, then this source of variation is small, but perhaps polls are diverging by as much as 10%, then this will be quite large.  When we say we sample the entire population of polls, then this source of variation is zero, since we sampled everything.

CNN is treating poll numbers as if they are fixed constants, not themselves a random variable.

Can we all agree that an individual poll has some variation?  Let's say +/- 3%, which is about twice the standard deviation of the poll estimate (actually +/- 1.96 SD)

Lets call these sample polls p1, p2, and p3.  

The poll of polls is (assuming equal weights) (p1 + p2 + p3) / 3

We would like to know the standard deviation of this estimate.

Well, var((p1 + p2 + p3) / 3) = 1/9 var(p1 + p2 + p3) by pulling out the 1/3.

Next by independence of polls 
= 1/9 (var (p1) + var(p2) + var(p3))

We said above the standard deviation is 1.5% or 0.015 (using +/-3% as our margin of error for each poll). The variance is the square of the standard deviation so,
= 1/9 ( 0.015^2 + 0.015^2 + 0.015^2) = 0.000075

This is the variance of the poll of polls, we need the standard deviation so take the square root 
sqrt(0.000075) = 0.00866.

The margin of error is about double the standard deviation (really 1.96) 
0.00866 * 2 = 0.01732

so the poll of polls has a margin of error of +/- 1.7%

One shortcut way you could have easily calculated this is take the margin of error divided by the square root of the number of polls

3% / sqrt(3) = 1.7%",freudian_nipple_slip,2012-10-28 09:03:59
"Thank you, that really helped, particularly the treatment of the polls as random variables.  Perhaps ignorantly, I had never considered a poll as such.",tdyo,2012-10-30 15:53:13
"The population of the polls of polls is the first set of polls, but the intent of any of these polls is to represent the population of people who vote; should CNN take this into account when they publish their sampling error?",thirdlip32,2012-11-02 09:32:13
"And how about when those polls are taken again the next week, i.e. with new samples?",freudian_nipple_slip,2012-10-26 18:49:02
The poll of polls is updated?  Sampling error doesn't refer to things changing over time - it refers to error associated with a randomly selected sub-population not accurately representing the population as a whole.,tdyo,2012-10-26 18:59:46
So if I take an average of averages...  isn't that effectively the same as pooling everyone together and treating that as like a super sample (adjusting for sampling weights and if sample sizes differ)? And you're saying that doesn't have any sampling variability? ,freudian_nipple_slip,2012-10-26 19:06:00
"No, I'm saying it doesn't have any sampling *error*.

Let's say you have a population of 10,000 people, and you want to find the average weight of those people.  You're not going to weigh 10,000 people, but you can take a random sample of 100 or 1,000 people and take the average weight of *that* sub-population.  Obviously, the more people you weigh, the more confidence you have in your estimation of the average weight of the 10,000 people.  The deviance from the true mean of the 10,000 people obtained by sampling fewer people is called sample error.

In your poll of polls, you have a population of at least 3 polls, and you are including every poll in your poll of polls.  Because you're not really sampling from the population of polls (you're incorporating the *entire* population), there isn't really any sampling error.",tdyo,2012-10-26 19:16:50
"Equivalently you can take one sample of 300 people, compute the average and get the same number.  The sample error in that case is sample SD / sqrt(300 - 1) which is very close to pooling the sampling errors from the 3 samples of sqrt( (sample SD_1^2 / 99 + sample SD_2^2 / 99 + sample SD_3^2 / 99).
 ",freudian_nipple_slip,2012-10-26 20:07:40
"I think the error is still expressed as a maximum bounds of possibility from the least confident sample size. no direct sample error from the poll population, but sample error from the original polled population.",LuminousP,2012-10-26 20:11:52
And how is that deviance calculated? Using the sample variance!,freudian_nipple_slip,2012-10-26 20:25:26
"&gt;And how is that deviance calculated? 

we look to see how many goats they offer to pazuzu, the demon of the west winds. 

but really, the 3 polls are the population, thus no sampling error. 


LOOKS LIKE OP NEEDS TO HIRE A STATISTICIAN",gloystertheoyster,2012-10-26 20:35:25
"I have a PhD in statistics.

3 polls have variation in them.  If they didn't, you could take another sample of those same 3 polls and get the exact same number if there was no sampling error.",freudian_nipple_slip,2012-10-26 21:16:22
"I have an MD in... err nothing.... 

but it's not really taking a sample of anything because what it's looking at is the population (the three polls) within the narrow context of the poll of polls.... how could their be a sampling error when what we are looking at is the entire population (the three polls)? that's what I don't get. 

sampling error comes from taking a sample. they aren't doing that. ",gloystertheoyster,2012-10-26 21:31:33
"Because the polls themselves are random variables.  It's more like a stratified sample where your strata are well defined which is why that's the population.

Another way to look at this, if there was truly no sampling error. Imagine the 3 polls: Rasmussen, Gallup and Ipsos.  That's your population of polls.

They go out and poll and get their sample proportions, let's say Rasmussen 48%, Gallup 45%, Ipsos 48% and each with a sampling error or +/-3.  

Suppose they go out again the same day and then get Rasmussen 47%, Gallup 46%, Ipsos 45%, all reasonable numbers and all within the margin of error from the first poll.  

First case poll of polls is 47%, second case poll of polls is 46%.  They could keep sampling new polls over and over the same day and the poll of polls number fluctuates.  If there was no sampling error, there would be no fluctuation. 

In the context of ANOVA, fixing what polls we have to say it's the population means that the between-variation of the polls is zero, however they each have a within-variation component that we need to factor in.  ",freudian_nipple_slip,2012-10-26 21:43:41
"awesome i enjoyed reading this too

also even though they measured the entire population (all the polls), there is also ""time"" right? That's what you're getting at right? ",bobthemagiccan,2012-11-01 23:33:26
"No, time is not accounted for at all.  What they are measuring is a snapshot at the current time of the true percentage of people voting for, e.g. Obama. This is why on [fivethirtyeight](http://fivethirtyeight.blogs.nytimes.com/) Nate Silver has both a Nov 6 Forecast and Now-cast (currently).  

I've thought about this since I originally posted and I think the best way to explain this (see my other post for an actual calculation if you're more mathematically inclined).

Within-poll variation: Which poll would you trust more for measuring the poll accurately?  a sample of 100 people or a sample of 1,000 people?  
Obviously, the answer is 1,000. The larger your sample size, the more precise (smaller variance), your estimate.  

That is for a single poll and that is precisely where the +/- comes from in an individual poll.  Now if you take a poll of polls, say 3 polls, wouldn't all 3 polls having a sample size of 1,000 be more precise than if all 3 polls had a sample size of 100?  Of course they would.  This is the point I'm railing on throughout this thread. 

Between poll variation: Suppose we're taking an average of 3 polls as our poll of polls estimate.  Which scenario would you trust more?
Scenario 1: Poll 1 Obama 45%, Poll 2 Obama 50%, Poll 3 Obama 55%

Scenario 2: Poll 1 Obama 49%, Poll 2 Obama 50%, Poll 3 Obama 51%.

Both cases lead to a poll of polls estimate of Obama 50% but the fact that Scenario 2 has estimates that are closer together, we would believe that one more.

CNN by saying we have sampled the entire population of polls, completely disregards this between-poll source of variation. I didn't voice a strong enough opinion about this at first but that is extremely flawed.  I focused more on the fact they discounted the within-poll variation which is also egregious.  

",freudian_nipple_slip,2012-11-01 23:46:23
thank you for writing this all up and educating me. you've given me a reason to stay up and read wikipedia. ,gloystertheoyster,2012-10-26 22:00:52
It's the poll of polls!  How dare you question their method!,HughManatee,2012-10-26 18:28:57
"So if I take random variable X with variance v1 and random variable Y with variance v2, and take their average. Boom! Magic!  Variance disappears!",freudian_nipple_slip,2012-10-26 18:31:16
"Bam.  Just like that.  CNN is like ""Variance?  Fuck that shit.  **We don't care about variance.**""",HughManatee,2012-10-26 18:34:04
Now I see your confusion.  Variance != sampling error.,tdyo,2012-10-26 19:35:06
"No. Sampling error is a function of variance. Sampling error = 1.96 * sqrt(sample variance / (n-1)) where I assume 1.96 because samples are large enough using the normal instead of the t is negligible. 

When they're using poll of polls, calling sampling error the sampling error of polls is either ignorant or malicious.  Each poll has error associated with it.  You have to take the ANOVA decomposition that has within-poll variation and between-poll variation.  ",freudian_nipple_slip,2012-10-26 20:04:35
They probably mean they don't know how to calculate it. Or they think that because they read the results from their 3 poll sample correctly that they made no errors. ,makemeking706,2012-10-26 19:07:27
"No, as tdyo explains above, they mean exactly what they say - no sampling error. If you were finding the average weight of ten people, you'd weigh them all, sum their weights and divide by ten. What woul your sampling error be?  None, right?  You've measured everyone. 

Same thing with the poll of polls. They averaged the results from all of the polls in the set of polls they used. Sure, things might change if there we're a new set of polls and a new average of polls - but that is true in the weight example above too. Also, the poll of polls will always be the same if it is done on the same population. ",electricfistula,2012-10-27 11:34:43
"It's a meaningless population though, as stated else where. ",makemeking706,2012-10-27 12:00:46
"or may be their statestician was under influnce of poll dancers.
",theindianguy,2012-10-27 09:02:43
Poll dancing leads to ballot *stuffing*.,luxliquidus,2012-10-27 10:38:34
"Hey, maybe CNN really knows exactly who is going to vote and sampled them all.",deanzamo,2012-10-26 21:49:56
Maybe they should hire some journalists too. ,zotquix,2012-10-27 23:00:06
CNN is poll smoking.,fulanitodetal,2012-11-01 15:01:22
Heheh election fraud might be happening and the top post in r/statistics is a joke.,irascible,2012-10-27 16:30:19
"That's not what I was expecting to read.

Sure, some journals have banned or limited significance testing. (The *American Journal of Public Health* did it for a while in the 80s, and *Epidemiology* has a strong reporting policy.) And *Psychological Science* recently announced their support for ""[the new statistics](http://pss.sagepub.com/content/25/1/7),"" meaning an emphasis on effect sizes and confidence intervals instead of *p* values.

But I haven't heard anyone seriously advocate tossing out confidence intervals as well, and then cast doubt on Bayesian statistics too. I don't see how working with solely descriptive statistics will make results more reliable or easier to interpret. Even if CIs are not perfect, surely they're better than providing descriptive point estimates alone?

edit: I skimmed the first author's previous paper on Bayesian statistics (Trafimow 2005). It argues that (a) we don't always know a good prior and (b) even then, a flat prior may not make sense, because we don't know if all events are really equally likely. That may be true, but with sufficient data, how does that really matter? Do we really need the prior distribution to be ""accurate"", whatever that means, or just not obviously stupid?",capnrefsmmat,2015-02-23 20:19:23
"Arguing against the flat prior, while at the same time demanding larger sample sizes—I doubt they know what they're doing. ",kimolas,2015-02-23 20:41:12
"&gt;Arguing against the flat prior, **while at the same time** demanding larger sample sizes

I am largely ignorant about Bayesian analyses and priors. Can you explain a little more why these two are conflicting ideas? ",sometimesynot,2015-02-24 10:58:53
"I believe the idea behind the use of a flat prior is that given enough data the prior will have a very small effect on the models predictions, and the data will in a sense overwhelm the prior and take a primary role in determining the models output. This is just what I recall as an argument for using them, I'm not well versed in bayesian models.",ampoth,2015-02-24 12:40:20
"What you are saying is valid for any prior. 
In fact, with enough training data, the parameters of the models will converge to the same result, and that independently of of the prior distribution given. So the fact of choosing uniform or another distribution will have no effect. 
However, uniform is usually chosen because it represents the state of maximum uncertainty.  ",mou7a,2015-02-24 13:37:22
"Not strictly. A uniform (as done as a beta(1, 1)) prior contains more information than a beta(0.5,0.5) (bathtub shape). Jeffreys Prior are about arrive at the least informative (wrt Fishers Information) prior and are often not uniform. ",iacobus42,2015-02-24 19:33:09
The clearest exception is priors on slopes--do you want uniformity with respect to the value of the slope or with respect to its arctangent?,everyday847,2015-03-16 11:12:28
Agreed man.,Jericho_Hill,2015-02-24 05:02:40
"That pretty much sums up my reaction to the announcement, too.  I was expecting ""new statistics"", and got ""use descriptives only"".  

I'm fine with dismissing their arguments, since the editors seem a combination of naive and militant.  However, I'm disturbed by how strongly they're legislating methodology.  ",Jofeshenry,2015-02-24 04:28:37
"What is ""new statistics"" anyway? First time I hear about it.",PlayMeWhile,2015-02-25 06:42:21
"Did he really cite Karl Popper as evidence of Bayesian stats being untrustworthy? Wow. Plus, they're asking for larger sample sizes and to report effect sizes, so as to ensure ""stability"". Isn't that kind of the point of null hypothesis testing, to check the likelihood against chance that the two sampling distributions were pulled from the same population? I totally get the idea that one test being "" more significant "" than another is kind of a meaningless comparison, but don't throw the baby out with the bath water.

It just sounds like a whole bunch of whining about problems without offering any solutions. If your solution is to remove methods without offering alternatives, you're not helping, you're hindering. I honestly can't see this go all that well.",Zoraxe,2015-02-23 23:15:22
"my feelings exactly. Here's the quote that gets me:

&gt; a 95% confidence interval does not indicate that the parameter of interest has a 95% probability of being within the interval. Rather, it means merely that if an infinite number of samples were taken and confidence intervals computed, 95% of the confidence
intervals would capture the population parameter.

Tomato/tomato. As far as I understand, the author might as well be saying ""I don't think samples are representative of populations"", at which point why bother doing statistics in the first place.",bacteriadude,2015-02-23 20:46:10
"Well, the difference in that quote is that Fisherian statistics doesn't treat the parameter of interest as a random variable. So you don't ask about the probability of the parameter being in a certain place. But either way, 95% of CIs capture the population parameter.

You could charitably interpret their point to say that using CIs to check if the parameter has a certain value -- e.g. checking if the CI overlaps zero -- has the same pitfalls as hypothesis testing, and you'd be right. Treating CIs as tests gains you nothing. But that's not the point of using them.",capnrefsmmat,2015-02-23 20:51:31
"What is the point, if i may ask?",recoveringpsychopath,2015-02-24 01:12:06
"With any non-arbitrary metric, CIs are informative. Imagine two drugs that increase the lifespan of cancer patients by 1 year. The first has a CI of 1 year, and the other of 1 month. The second one is much more reliable, and many people would prefer to take the ""guaranteed"" 11 extra months. 

With arbitrary metrics, such as marital happiness on a 1-7 scale, they're much less useful in my opinion. ",sometimesynot,2015-02-24 04:44:33
"Why? If there's an experiment where say, giving marriage counseling, is found to result in mean 6 SD 1 versus control group mean 6 SD 3, how does the arbitrariness change the usability of the information?",sharkinwolvesclothin,2015-02-24 06:19:59
"You kind of changed focus on me. To continue my previous example, what does it mean if you have one intervention that raises marital satisfaction by .5 with a confidence interval of .25 and another with a CI of .05? The first one is certainly less reliable, but by how much and is it meaningful or important?

For your question about the difference *between* groups, that can be expressed as an effect size. ",sometimesynot,2015-02-24 07:01:32
"My example was still zero effect size but yeah it was awkwardly worded. Still, i don't understand how we know people prefer less variability in life expectancy but don't know anything about marriage happiness just on how the measurement works. To me it seems like both require domain knowledge and personal judgment in the same way. Is it the unit or the measurement, i.e. if we quality-adjust the life years, is it still universal to prefer less variation at same mean? How about if we include something like years to divorce in a survival analysis?I'm not saying you're wrong, I just don't understand.",sharkinwolvesclothin,2015-02-24 08:02:06
"Mainly it's because most arbitrary scales like this are ordinal scales, not interval, and certainly not ratio. So there is no guarantee that the difference between 6 and 6.4 is the same as between 6.4 and 6.8. 

&gt;How about if we include something like years to divorce in a survival analysis?

If you switch to years to divorce, then you're back on a non-arbitrary (and ratio) scale so the means and confidence intervals have more meaning. 

&gt;is it still universal to prefer less variation at same mean?

I would say ""yes, but"".  ""Yes"" because there is more precision in what you can expect so you can be more confident in your decision to use the intervention. However, a wider CI (around the same mean) indicates that for some people, the intervention was more successful. If you can identify who those people are (e.g, those who don't have high blood pressure), then you can potentially have a treatment that is much more effective. ",sometimesynot,2015-02-24 09:26:40
"Ordinal versus ratio is a good point, though that's true for a mean as well, not just confidence interval of the mean. Edit.. And wouldn't be true for the CI of a scale-appropriate statistic, like rank-correlation or something.so it's not the CI, it's. The mean where the issue is.Thank you for your insight!",sharkinwolvesclothin,2015-02-24 12:32:47
Getting an effect size estimate and measure of uncertainty. Usually the plausible effect sizes are much more interesting than overall significance.,capnrefsmmat,2015-02-24 04:59:34
Pretty sure this is false. The naive interpretation of frequentist confidence intervals is wrong in much the same way that the naive interpretation of p-values (as the probability of the null) is wrong. ,59383405987,2015-02-25 18:18:01
Couldn't have said it better myself.,smgcamper,2015-02-23 22:38:10
"Some other recent articles by Trafimow:
http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00235/full
http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00180/full

He has an unusual take on statistical analysis.",jeremymiles,2015-02-23 20:55:28
"Unusual? More like clueless. With that approach, we could use that ""real"" standard deviation to calculate effect sizes of differences and obtain greater effects, for example. So, we could have infinite ES with 0% of reliability.... Weird.",clbustos,2015-02-24 08:49:22
"The author makes it sound like confidence intervals and hypothesis tests have no meaning and are not interpretable.

Yet, both have a firm basis in decision theory.",TerrySpeed,2015-02-23 20:51:46
"Let's see. Hypothesis tests don't take utility or prior information into account, and are not applicable to events that cannot happen an unlimited number of times. From what I know, decision theory doesn't have any such limitations. Maybe you can explain what the ""firm basis"" is.",midianite_rambler,2015-02-24 09:42:07
"Let l1 be the loss if the null hypothesis is rejected even though it is true, and l0 be the loss if the null hypothesis is not rejected even though it should have been.

The basic Neyman-Pearson approach to construct hypothesis tests is to bound l1*alpha (expected risk of the test if the null is true) while minimizing the expected risk if the null is false, l0 * (1 - B), where B is the power of the test under the alternative.

Basically, the Neyman-Pearson approach sets a bound on how much risk of a false rejection is acceptable, then find the test that reduce the risk of deciding to not reject a wrong null.",TerrySpeed,2015-02-25 00:00:41
"there are a lot of other assessments other than nhst and descriptives. check out Observation Oriented Modeling , and other randomization assessments. ",dpac007,2015-02-24 06:11:53
"What's the reasoning for the ""ban"" (?) in other journals?",brews,2015-02-24 08:41:00
"So I guess to summarize:

They don't want frequentist approaches because you don't get a posterior, and they don't want Bayesian approaches because you don't actually know the prior.",I_AM_PETER_AMA,2015-02-23 21:31:56
"I thought the same thing.  I'm having a hard time imagining what they'll be publishing.  Inferences from descriptive statistics from ""large"" samples?",Jofeshenry,2015-02-24 04:32:17
"Multiple regression is based on maximum likelihood, and the theory automatically implies and generates confidence intervals for coefficient estimates.  Will they ban regression too?  Or can you keep the estimates, but not report their significance?  I don't see how any of this adds rigor.",ctphoenix,2015-02-24 07:51:05
"This was my first reaction on reading as well. They don't want null hypothesis testing, they don't want CIs, they kinda don't want Bayesian right now although they sound like they might be OK with it soon... what quantitative methods are actually left at that point?

Putting this many restrictions on how quantitative research can be conducted and reported won't result in better submissions; it will simply result in less submissions.",Binary101010,2015-02-24 09:10:47
"This is great in some ways but a little drastic.  I'm an epidemiologist but I do a lot of school outcomes research as well.  

The obsession with p-values with disregard to effect sizes is ridiculous. In my experience, this is a much better prioritization.

For example, we did an analysis on overlapping student intervention programs and their influences on students' standardized test scores.  The p-value was significant, but the effect size was so small that it was not meaningful at all. The effect size, if I remember correctly, was something like for every hour of programming a student's score would go up 0.0004 points.  Meaning, their score maybe went from a 151 to a 152 which is not considerably meaningful considering scores can range from 120 to 180-- but with a large sample size it was significant.  Everyone was ecastic because it showed a significant relationship, but in the larger picture of everything it was not very meaningful.

I do agree with the idea that ""the p &lt; .05 bar is too easy to pass and sometimes serves as an excuse for lower quality research.""  However, I don't think we need to necessarily remove ALL p-values but perhaps need to just put a much larger emphasis on effect sizes.",palbrcht,2015-02-24 07:13:57
"I agree, except for your last sentence. I think the brunt of the issue is that the state you're comparing to, the null hypothesis, is often not a very likely case in the first place. Educational programs are perhaps an exception, since I know a disturbingly large number of studies find no effect or miniscule effect sizes like you describe. But in general, it really isn't interesting to ask if there is simply any detectible difference, no matter how small, so p-values don't have value. And if the observed effect size *was* practically significant, confidence intervals or even just standard error give a better picture of precision than a p-value, regardless of if the null effect is interesting or not. So, when *is* it still helpful to report p-values? If we should shift focus away from them, why keep them around?",Astro_Bull,2015-02-24 18:29:39
"I've taken the opportunity to sticky this post to the top of the subreddit, because it is very worthy of a discussion.",dearsomething,2015-02-23 20:31:17
"This is actually a good thing if you are privy to social psychological research. 2/5 papers I've read have hypotheses that... I dunno, let's just say seem a bit ""led"" or contrived after the fact. Like, the researchers collected data first, saw what patterns emerged, then said, ""Oh yeah, this was totally our hypothesis!""

I'm not knocking social psychology. Researching human behavior is difficult for sure, and researchers need to get published to stay afloat. Switching to effect sizes and confidence intervals is a great move when considering these points -- researchers can now be honest and just say ""Look, we weren't specifically looking for this hypothesis, but here's what we found, here are the effect sizes, we'll look into it further with an updated hypothesis."" They get to publish, the data is still useful, the charade is removed, and science goes on.",griffer00,2015-02-24 05:43:40
"&gt;Switching to effect sizes **and confidence intervals** is a great move

I agree, but they're giving a blanket ban on confidence intervals too, which feels problematic.",Astro_Bull,2015-02-24 18:09:34
"This action of rewriting hypothesis is actually sometimes enforced by the reviewers themselves. In a sense, they seem to prefer that you give a much deeper treatment of the theory and literature that supports the hypothesis also supported by the data than that you use space with a lot of information that, in the end, doesn't explain the data at all.

This is a recognized problem in the area of Social Psychology and is the big reason why some of us are starting to pre-register our experiments. This is a good approach because it also helps with dealing with the publication bias and improves the accuracy of meta-analysis of the literature.",Natrina,2015-03-06 14:05:00
It seems like this change only encourages people who were p-hacking to use some other sort of trickery instead.  The problem isn't frequentist methods but the incentives for the researchers themselves.,Luonnon,2015-02-24 13:10:06
"Are they really banning all mentions of p-values &amp; confidence intervals?

If so, this is insane.
",TerrySpeed,2015-02-23 20:38:23
"Question 3: Are any inferential statistics required? 
No, because the state of the art is uncertain. 

Oh, see? They want certainty. All inferential statistics can offer is probability. ",sparkadog,2015-02-24 05:15:58
"I think considering the likelihood that the observed effect size could have appeared by chance alone is an intrinsic part of thinking about study results; i.e. it was our natural thinking about results that led to their formalization through frequentist statistics rather than vice versa.

Removing this formalization won't change the way we think about results, but instead will just make our conclusions less consistent as we intuit or subconsciously estimate the p values for results we read.",fat_genius,2015-02-24 04:18:50
"This makes for an interesting experiment. No more bar charts with standard errors or CIs, but jittered scatterplots with every data point and some summary effect sizes. It seems like results sections will end up focusing on visual arguments and discriminations, rather than statistical ones.",jtth,2015-02-24 05:23:13
"This is even more stupid when you read their editorial statement.

They allow stats copy in the submission, the materials that the reviewers will review as they decide how to rate the article on its merits.

But if the paper is accepted, then the author has to strip out the stats copy that is relevant to hypothesis testing.

In other words, the reader will not have the same information that the reviewer used to decide if the study should be published.  

I've never seen someone try to create such an obvious blind spot, well, outside of politics.",wil_dogg,2015-02-24 09:26:13
I don't know how they do their research but my null is always that I've found nothing. If that can't be tested then how can you assert that you've found anything?  ,bickbastardly,2015-02-24 15:44:00
There are some discussions on this editorial on the ASA biometrics and biopharmaceutical mail list. ,quadrobust,2015-02-25 07:52:38
So this is the moment where it stops being science journal and starts being science-fiction magazine. Only way of evaluating your findings is going to be by philosophical debate. Quite a stepback for social sciences i must say.,macstat,2015-03-03 13:13:19
"PhD psychologist here, the better half is a social psychologist (I'm clinical / child developmental but have always focused on research methods and stats).

This is freaking insane.

Yes, you can edit the journal toward larger N studies and better stats copy, but what the editors have done here is taken a decent second-tier journal in social psychology (and second tier in psychology proper, this is a decent journal, historically) and moved it down to Psychological Reports (a vanity press that accepts just about everything and charges authors for reprints to defray the cost of publishing).

This is particularly odd in social psychology where much of the basic research starts with college student samples -- large N is easy when you are studying college students -- and uses self-report measures that often have a halo bias effect that skews effect sizes to be larger than what one would see using alternative methods.",wil_dogg,2015-02-24 09:16:53
Does anyone in the field know what kind of reputation weight this journal has?,cooked23,2015-02-24 05:26:05
Well their impact factor is 1. 128. So not terribly high at all.,charcoalslam,2015-02-24 06:53:58
"&gt; Well their impact factor is 1. 128. So not terribly high at all.

Compared to what otehr journals in the same field?

Impact factor means nothing out of context.",saijanai,2015-02-24 09:47:48
It is listed on the 90th place in this list: http://www.scimagojr.com/journalrank.php?category=3207,ROLeite,2015-02-24 15:26:04
I've always used impact factor as a quick and easy way to judge journals. I'm in the public health and biology fields so I don't know what the scale for this would be.,charcoalslam,2015-02-25 16:22:08
"Well, impact factor compares a journal's impact to ALL other journals, including _Nature_ and _Science_ and the like.

If the top journal for a given field is in an obscure field with 100 researchers total, the impact factor is going to be meaningless since it will have to be incredibly small because only 100 researchers might be reading it, citing it, etc.

In this case, there's 200+ journals in the field as counted by [SCImago](http://www.scimagojr.com/), and likely many thousands of researchers, so you can look at the journal ranking and get a better feel. This particular journal is in the [90th out of all 216 social  science journals sorted by impact factor](http://www.scimagojr.com/journalrank.php?category=3207&amp;area=0&amp;year=2013&amp;country=&amp;order=sjr&amp;min=0&amp;min_type=cd&amp;page=1) , which is more suggestive.

Than you can look at  how old the journal is: [Volume 1 was published in 1980](http://www.tandfonline.com/loi/hbas20#.VO7QeymZDNg)

Unfortunately, it's still hard to get a good feel of the value of a journal, just from those raw numbers. 

It ranks [48th ](http://www.scimagojr.com/journalrank.php?area=0&amp;category=3207&amp;country=all&amp;year=2013&amp;order=h&amp;min=0&amp;min_type=cd) in the [H-index](https://en.wikipedia.org/wiki/H-index) for the same 216 journals, which implies that the impact factor may not reflect how important the journal is, and of course, it may be one of the very top journals in its own little-sub-genre and still be obscure in the larger categories of ""applied psychology"" or ""social psychology.""

It's interesting though, the [journal's website](http://www.tandfonline.com/loi/hbas20#.VO7ZcymZDNi) says it's got a 1.128 SJR ranking, but SJR from 2013 says it has a 0.627. But the journal cites Thomas Reuters rather than SJR directly. Don't know what the discrepancy means.

.

The whole field of journal ranking is very confusing and controversial. A review article that is merely a summary of existing knowledge, might have many of dozens of authors, and eventually cited many hundreds or even thousands of times, and yet not contribute in any way to new science, is still counted in the various journal ranking schemes as every bit as important as a new paper with real-world importance equivalent to Einstein's paper on Special Relativity. Journals are accused of encouraging this kind of review article just so they can boost their impact factor ranking and authors are happy to have their name appended, just so they can be part of a ""1000+"" citation article.

.

If I recall correctly, there are actually science journals devoted specifically to the topic of journal impact factor and related subjects. I don't know what their journal impact factor is, however.
",saijanai,2015-02-26 00:36:15
"They are throwing away all of frequentist statistics because people might not understand the meaning of the results ?...

Talk about babies and bathwater",tpn86,2015-03-02 08:17:01
"Social psychologist here. The ""best"" part of this problem is that it will never be solved. This whole movement largely took off after several researchers were literally making up sexy data to cater to the top journals, who like cute counterintuitive findings. In theory, CIs do suffer the same plight as do p values, and likewise can be misinterpreted (terms like ""very"" or ""marginally"" significant, anyone?). Should no 0 fall  in CI, then the p &lt;= .05. Indeed, just 1 CI at hand may not be useful at all, but one can also make the case that bootstrapping the CIs can substitute the ""unlimited samples"" clause of CI's definition. Some stats packages have options now to bootstrap CIs ( STATA, for instance, has a command for it). 

Again in theory, p values are negatively related to sample size. The bigger the sample, the smaller the p (I've seen correlations of .005 have p &lt; .05 in a sample of over 7.25k). Given that we assume that the sample is from the population, the estimates of the sample should be inferred to the population without the need of p values; especially as same sizes go up. P values, thus, may be whether we can say the sample is from the population; just that (this is my possibly incorrect logic).

What I'm trying to convey here is that perhaps inference statistics and their p values aren't needed if we want to describe the population at hand (e.g., the mean of this sample, supposedly from the true population, is 4.5, which is thus the parameter given the representable sample; and the other sample's mean is 4.0, which we can also assume to be a parameter. Thus, based on the scale of measurement, can be interpreted both visually, and via an effect size. The keyword here is interpretation. Many just wait for the .05 and cry significance, without putting too much effort in interpreting the pattern. Is an increase of .5 hours of sleep good? Probably yes. What about an increase of .5 on a 10-pt scale? Perhaps not so much. Effect sizes, again in theory, should be independent of inferences and sample sizes, so the do serve as complements to facilitate the interpretation. So if the new therapy increased sleep by .5 hours, and g = .30, that tells that the increase is still relatively small. Still, an extra .5 hours of sleep is nice, but perhaps a new treatment can be better. 

Interestingly, the folks at data colada (datacolada.org) have argued against the usefulness of the effect size: http://datacolada.org/2015/02/09/33-the-effect-size-does-not-exist/

The most optimal of a solution as I see it (and I may very well be wrong here), is not the set of numbers you report per se, but how science is practiced. At least in my field, it's publish or perish. This pressure to obtain quantity, rather than quality of research has led to HARKing, p hacking, and data fabrication. It seems like we are too lazy to make our own decisions with our own minds, taking the route of having p values and number of publications do the deciding for us. This is also why you see smaller than larger samples more often—how can you produce 10 publications a year if you are too busy collecting large sample sizes for 5 studies, rather than small sizes for 10 studies? Large samples, unfortunately, aren't always feasible. 

We must not fear for our job if we see that &gt; .05 is a good part of science. A ""nonsignificant"" finding testing Theory X can be just as interesting as a ""significant"" one. We shouldn't be afraid to admit that (luckily, some journals are accepting null findings). Publication bias is a real thing, and that may have motivated T &amp; M to make that strong statement. ",NoSurfinMovie,2015-03-10 07:07:05
"I have a lot of problem understanding why NHSTP are so ""interesting"". It seems that nobody seems to care about central tendency and the distribution of the exposures, as if they don't exist!",Hedleypty,2015-03-15 03:22:50
"I think the problem is people still dont understand endogeneity. I think journals would rather keep 'boring science' away from their fantasy world. BTW there are heaps of ways of removing endogeneity, including the variable or panel or IV. Or simply theorize the likelihood of it changing the direction/magnitude of the effect. Idiots.  ",david1610,2015-02-26 00:10:37
"You're all missing the meta-funny part of this. The probability that he will have to pay off on the bet, given that the sun has exploded, is essentially zero. ",hacksoncode,2012-11-09 09:04:19
"Ah, but the *meta*-meta funny part is that our sun is not massive enough to go nova in the first place! It'll swell up many times its current size into a red giant in a few billion years when it runs out of hydrogen to fuse in its core, but no nova. ",Khiraji,2012-11-09 10:51:16
"Well if it can't go ANOVA, should it go Kruskal-Wallis instead then?",djaipel_samoylovich,2012-11-09 15:12:27
http://i.imgur.com/fY7Ca.gif,Khiraji,2012-11-09 18:42:48
"Assuming that we even accept the other aspects of the set-up, why on *earth* would a frequentist use alpha=0.05 in that situation?

The level would at least relate to the relative costs of the two error types.

I guess I'd say that I'm a Bayesian even when I'm not using a Bayesian method, and I do like to point out when there is silliness/oddness in frequentist approaches\* but I don't think there's a need for a straw man

\* on the other hand, I like seeing when there is silliness or oddness in Bayesian approaches, too. 

",efrique,2012-11-09 06:23:28
"Yeah, no.

Frequentists aren't idiots. They're aware of Bayes' theorem and the false positive paradox. The probability of the sun exploding is essentially nil, and so the p-value they come up with will be similarly small.

In the end, the Bayesian and the frequentist would bet you the same amount of money, anyway.",SaberTail,2012-11-08 23:35:27
"&gt;The probability of the sun exploding is essentially nil

Caught using an explicit prior!",MurrayBozinski,2012-11-09 00:05:09
"&gt; The probability of the sun exploding is essentially nil, and so the p-value they come up with will be similarly small.

A frequentist is not allowed to have a prior on the hypothesis.  In particular, the claim that ""The probability of the sun exploding is essentially nil"" is not allowed in frequentist analysis.

The cartoon is essentially correct, but there is a small misinterpretation of the frequentist positition.  To be fair, most frequentists don't understand how to interpret their own results.

The two hypotheses are:

- H1 The sun has not exploded

- H2 The sun has exploded

A frequentist will consider both hypotheses individually as follows:

- If H1 is true, then what is the probability of our test giving the correct result?

- If H2 is true, then what is the probability of our test giving the correct result?

In this case, the test has been constructed so that, in both cases, the probability of the test being correct is 35/36, and the probability of it being incorrect is 1/36.  (Late edit: you can use the word 'lie' as a synonym for 'incorrect'.  Regardless of the true state of the sun, the probability that the test will 'lie'/'be incorrect' is 1/36.)

P(incorrect | H1) = 1/36

P(incorrect | H2) = 1/36

As these two numbers are the same, we can summarize it and say that the probability of it being correct is 35/36.

P(incorrect) = 1/36

P(correct) = 35/36

The **critical** thing to understand is that the frequentist can make these claims only **before** the test has been activated.  In other words, the frequentist will make this claim at sundown.  This 'correctness' probability only applies to **future** experiments.  Also, interestingly, the Bayesian will entirely agree with the above analysis.

Bayesian (at sundown): ""Yes, we haven't activated the detector yet, and we both agree that the test has a 35/36 probability of being correct.""

The challenge is: how do we 'interpret' **past** results.  Given a Yes or No from the apparatus, does that tell us anything about the two hypotheses? The frequentist is not allowed to make any interpretation after the test has been performed.  As far as a true frequentist is concerned, the experiment is 'dead' after the data has been seen.  There is no 'probability that a *past* experiment, with a particular result, *was* correct'.

The two approaches do not lead to contradictory interpretations:

Bayesian: ""We've got the data, and I still think the sun is OK.""

Frequentist: ""No comment. We've got the data, and I (like any true frequentist) have nothing to say on the matter.""

It's easy to not be wrong if you make no comment.  They neither agree nor disagree, the frequentist simply makes no comment.

Finally, a true frequentist will say ""I'll bet you $50 that, if we run the test again, it will give the correct answer.  Obviously, we'll settle up in 12 hours time when we know the true state of the sun.""  (And the Bayesian won't take that bet.)

A frequentist is *not* allowed to bet on the correctness of experiments where the data has already been gathered and seen.  A frequentist is only allowed to bet on future experiments.",SkepticalEmpiricist,2012-11-09 01:54:27
"So, sure, frequentist *statisticians* only really talk about P(data|H0), but somebody is usually rejecting H0.  Sure, the rejection of H0 is a logical argument, not a statistical argument, but that logical argument, that a model for which the observed data is extremely unlikely is unlikely to be true, is just as valid in the case of the exploding sun.",inspired2apathy,2012-11-09 05:58:45
"&gt; but that logical argument, that a model for which the observed data is extremely unlikely is unlikely to be true, is just as valid in the case of the exploding sun.

It's not logical.  The interesting thing is that, in many models, the data that looks weird under the null hypothesis also looks pretty weird until all the alternates also!   Therefore, a low p-value really doesn't tell us that much, in general.

Given the following statement:

    The data is strange, or the null is false.

it is often the case that follow up experiments show that, in fact, the null still is true and the data was just weird.  There is no logical basis for jumping from the above to:

    The null is false.

unless, at the very least, you have shown that the data fits very nicely to the alternate hypothesis.

There is a lazy assumption (which I made for the first 31 years of my life!) that the weirdness-of-the-data-under-the-null-hypothesis must be negatively correlated with the weirdness-of-the-data-under-the-alternate-hypothesis.  But for some models, that is not true.  See http://dx.doi.org/10.2307/2289131 for more (Sorry, that's behind a paywall).",SkepticalEmpiricist,2012-11-09 08:21:19
"Right, that's an assumption that you should evaluate for the observed data.  In the specific scenario outlined in xkcd, the data is in fact much more likely under the alternative hypothesis.",inspired2apathy,2012-11-09 09:23:43
Awesome that was written by my professor!,babyduke1,2012-11-09 16:43:22
"&gt;The criticial thing to understand is that the frequentist can make these claims only before the test has been activated. In other words, the frequentist will make this claim at sundown. This 'correctness' probability only applies to future experiments. Also, interestingly, the Bayesian will entirely agree with the above analysis.


This is all misleading.

Entirely by coincidence, in this experiment, P(lie | H1)= P(lie|H2), and this is the unique circumstance in which a frequentist can compute P(lie).

If you break it down a bit more and look at P(sun reported as exploding |H1) and P(sun reported as exploding |H2), a frequentist can not compute P(sun reported as exploding). A Bayesian can, because they have a prior distribution over H. 

This means that, if we move back to your example.

&gt;Finally, a true frequentist will say ""I'll bet you $50 that, if we run the test again, it will give the correct answer. Obviously, we'll settle up in 12 hours time when we know the true state of the sun."" (And the Bayesian won't take that bet.)

A Bayesian could offer the counter bet. 

""I bet $50 that every time it says the sun has exploded tomorrow it's wrong.""

Even though the event happens in the future, a frequentist can't compute these odds without a prior distribution over H.",DoorsofPerceptron,2012-11-09 02:38:25
"Eh?  That is exactly the point I made.  I used 'incorrect' as synonym for 'lie'.  Everything you said is identical to what I've said.

(The only subtle difference is that I would not refer to the Bayesian bet at the end as a 'counter bet'.  It's not really the opposite bet, as it conditions on the test result.  Whereas the frequentist did not condition on the test result.)

At the end, you are talking about P(sun is gone | experiment says the sun is gone).  We both agree that this can't be calculated by the frequentist.  And that it can only be calculated with a prior.
",SkepticalEmpiricist,2012-11-09 02:48:47
"&gt;Eh? That is exactly the point I made. I used 'incorrect' as synonym for 'lie'. Everything you said is identical to what I've said.

Yes, I know.

I said it was misleading. In general a frequentist can not compute p(lie).

Consider the alternate scenario in which the machine only lies/is incorrect if the sun does not go out, and rerun your numbers.

You'll find under these conditions  a frequentist can not make this bet:

&gt;Finally, a true frequentist will say ""I'll bet you $50 that, if we run the test again, it will give the correct answer. 

It's just a big one off coincidence that they could compute P(Lie) and it doesn't hold in general.",DoorsofPerceptron,2012-11-09 02:57:34
"&gt; In general a frequentist can not compute p(lie).

Frequentists will compute a lower bound for P(lie) by taking the worse case.  The confidence is a classical example of this.  Consider:

    P(estimated interval will contain theta | theta)

This is the same as:

    P(not lie | theta)

It is a 95% confidence interval if, and only if,   P(not lie | theta) is *at least* 95% for *all* values of theta.

Imagine that we had, in our sun-nova experiment,

    P(lie | H1) = 97%
    P(lie | H2) = 95%

We can still say that P(lie) is at least 95%.",SkepticalEmpiricist,2012-11-09 08:15:01
"&gt; Frequentists aren't idiots.

I'm not sure where ideas like this come from. This isn't the first time I've heard this insinuation. It seems random to me.",rnmartingale,2012-11-09 00:26:48
"I think this stems from the unfortunate truth that a lot of applied scientists (in medical science, for instance) misinterpret/misuse frequentist statistics. But I agree with you, it's silly to call frequentist statisticians ""idiots"".

I had the pleasure of taking a philosophy course in probability theory a while ago. I can recommend this to anyone who wants to get more familiar with the philosophical issues with various notions of probability (objective, subjective, etc).

Also, for more on this topic I welcome everyone to /r/probabilitytheory&lt;/shamelessplug&gt;",,2012-11-09 02:59:13
"&gt; But I agree with you, it's silly to call frequentist statisticians ""idiots"".

That's not how I read it.",Bromskloss,2012-11-09 14:52:35
"Everyone knows that real frequentists don't bet because you can't assign a probability to a one off event.

But the point is valid, if you stop reading it as a personal attack.

 We've all read papers that propose some controversial theory and squeak through the review process with p&gt;0.05 . These papers have valid results according to their frequentist framework, and yet none of us would put money on them being right.

That said, reporting Bayesian posterior probabilities isn't the right answer. I don't care what your priors were, I just want to know what you learnt by running the experiment.",DoorsofPerceptron,2012-11-09 01:49:06
"This is why Bayes factors exist, because they integrate out the prior and report how much any observer would have learned from the data. You and I, with different priors, would report the same bayes factor, and done correctly we would be dodging the absolute ridiculousness of multiple testing we see all the time.",trousertitan,2012-11-09 06:49:35
"&gt; ... I just want to know what you learnt by running the experiment.

A frequentist does not learn anything from experiments.  Before the test was activated the frequentist would have said:

   ""I have no comment to make on whether the sun is still there or not.""

Also, before the test, the frequentist would have said:

  ""We have an apparatus which has a 35/36 chance of being correct.""

After the experiment, if asked whether the sun in still there, the frequentist will again say:

   ""I have no comment to make on whether the sun is still there or not.""

This is the same as their opinion before the test.  See the Yes or No from the apparatus hasn't changed anything.  Nothing has changed for them.  Therefore, the frequentist has learned nothing.

Frequentism is not about learning things from past experiments, but it is about writing down the probability of future experiments being correct.",SkepticalEmpiricist,2012-11-09 02:39:21
"&gt;Frequentism is not about learning things from past experiments, but it is about writing down the probability of future experiments being correct.

And how do you get these probabilities?

You learn them using frequentist techniques such as these:

http://en.wikipedia.org/wiki/Maximum_likelihood",DoorsofPerceptron,2012-11-09 02:41:06
"Use of a likelihood isn't a ""frequentist technique"". Besides, a Bayesian only uses ml in special cases (finding a MAP w/ uniform prior).

The whole ""one off"" event criteria is not well defined. On one hand many stars have exploded in the past, but if you're going to be rigid about repeatability, every event is unique.

&gt; ""I don't care what your priors were, I just want to know what you learnt by running the experiment.""

It's fine if you don't want to explicitly use a prior distribution, but don't think that just because you're not explicitly using a prior that it's possible to isolate ""what you learned from an experiment"" from prior beliefs. ",,2012-11-09 05:50:46
"I think you're misunderstanding me. I thought I was talking to someone trying to be a frequentist, that was claiming they didn't learn.

&gt;Use of a likelihood isn't a ""frequentist technique"".

It's a technique used by frequentists, that shows they are capable of learning. 

&gt;It's fine if you don't want to explicitly use a prior distribution, but don't think that just because you're not explicitly using a prior that it's possible to isolate ""what you learned from an experiment"" from prior beliefs.

Assume I am a Bayesian and I use priors. If I make use of your experiments I only want to update on the basis of your experimental results, and not on the basis of your initial priors. 

That's all I was trying to say.",DoorsofPerceptron,2012-11-09 06:09:49
"&gt; I only want to update on the basis of your experimental results, and not on the basis of your initial priors.

But frequentists do not ever 'update', on any basis whatsoever.  There is no learning, there is no updating.",SkepticalEmpiricist,2012-11-09 07:47:57
"&gt;But frequentists do not every 'update', on any basis whatsoever

Meta statistical studies can be formulated as a bunch of updates by sequentially merging likelihoods.

Edit: Are you using ""frequentist"" to only refer tests that reject or fail to reject the null hypothesis? If so that's a very narrow view of statistics.",DoorsofPerceptron,2012-11-09 08:01:30
"&gt; Are you using ""frequentist"" to only refer tests that reject or fail to reject the null hypothesis? 

No.

The likelihood is the likelihood, regardless of whether it came from one experiment, or merging data from many experiments.  Anyway, a (Neyman-Pearson) frequentist never has any opinion on the truth of any hypothesis nor on the value of any parameter, regardless of the amount of data analyzed.  At best, they might say ""before we did the analysis, we could prove that it had a 99.9999% chance of being correct.""  But they remain fully ignorant of the hypotheses/parameters after the analysis is done - i.e. they cannot apply the probability 'back in time' to an analysis that has been completed.

Of course, Fisher's p-values are used in a pseudo-updating manner.  But p-values are [nonsense](http://www.reddit.com/r/statistics/comments/12wei3/xkcd_frequentists_vs_bayesians/c6yu5lb).  Therefore, I don't really think somebody who uses p-values is really a frequentist - especially if they interpret a low p-value as showing that the null is false.

(Edit: 'can apply' -&gt; 'cannot apply')",SkepticalEmpiricist,2012-11-09 08:30:51
"Ok, well I'm not entirely sure how your definition of ""frequentist"" differs from mine, but otherwise fine, I think we agree about the mathematics and standard interpretation. It's just my vocabulary is slightly different from yours.

What I'm trying to argue is that: Given the lack of commitment by all good frequentists to whether or not they're right we would be better off disregarding their conclusion, and instead aggregating the statistics or likelihoods they used to come to their conclusion.

Even if an NP frequentist doesn't learn from their experiments, that doesn't mean that I can't.

",DoorsofPerceptron,2012-11-09 09:12:25
"Maximum likelihood is not 'owned' exclusively by either frequentists or Bayesians.  But it does tick a lot of Bayesian boxes ([likelihood principle](http://en.wikipedia.org/wiki/Likelihood_principle) for example) so it's very hard for a frequentist to say Maximum likelihood is theirs.

Anyway, there's a more important point here.  First of all, consider the concept of an *estimator*.  An estimator is simply a function that takes the observed data and spits out a number (or a set of numbers).  There is no such thing as a frequentist estimator.  Some estimators are based on multiplying a prior by a likelihood and taking some summary statistic of the posterior.  Therefore, those estimators can be called Bayesian estimators.

**But the non-Bayesian estimators are not 'frequentist estimators'.**

So, an estimator may have been constructed on the basis of Bayesian principles.  Or on the other hand, it may just be an arbitrary ('black box') function.

So, where does frequentism come into this?  A frequentist does **not** care what principles, if any, where used to construct the estimator in the first place.  A frequentist will take the estimator (regardless of whether it was a Bayesian estimator) and will do some tests to see how it performs.  Is it an [unbiased estimator](http://en.wikipedia.org/wiki/Bias_of_an_estimator)?  If it produces two numbers as output, the frequentist would ask if it is a [confidence interval](http://en.wikipedia.org/wiki/Confidence_interval).

It is often the case that the estimators that are favoured by the frequentist are the estimators that were created by the Bayesians!  For example, the MLE is a [consistent estimator](http://en.wikipedia.org/wiki/Consistent_estimator) and hence the frequentist might like it.  But it's based on an idea which is very recognizable to a Bayesian (using a uniform prior).

Note: Even if non-uniform priors are used, the mode of the posterior (the MAP) is a consistent estimator.  This is interesting as it emphasizes that a uniform prior is not any more frequentist than a non-uniform prior.  There are some pseudofrequentists who *think* they are frequentist because they use only uniform priors in their analysis.

On the other hand, the MLE is often a biased estimator and a fully-Bayesian estimator (using the mean, not the mode, of the posterior) might actually have less bias than the other estimators.  In my own research, it is well known that the MLE-based estimators are bad at model selection.  Frequentists will often use Bayesian estimators to improve the bias of an estimator.

So, in summary, a frequentist and Bayesian will often using exactly the same estimator, but they might give different reasons for liking the estimator.

A Bayesian will create an estimator from a well-known recipe (priors and likelihoods).  The frequentist doesn't care what recipe was used, but will instead perform quality assurance after to fact to see how the estimator performs.

(Edited for clarity. And I added more about the MAP.)",SkepticalEmpiricist,2012-11-09 08:03:00
"I'm not a statistician, I'm just curious ... but to me this sounds very emacs vs vim ish. surely they're both valid?",lahwran_,2012-11-09 00:55:36
"They don't actually disagree with each other.  To be more precise: given a low p-value, a frequentist will not conclude that the sun has gone nova.  In fact, the frequentist never has anything to say about the truth or otherwise of a hypothesis, before or after seeing any data.

This might seem like a surprising claim by me, please see my other [(very long) comment](http://www.reddit.com/r/statistics/comments/12wei3/xkcd_frequentists_vs_bayesians/c6yqn82).",SkepticalEmpiricist,2012-11-09 01:41:32
"There were some pretty savage fights over Bayesian vs Frequentist statistics in the 20th century which led to a sort of segregation of institutions. There are some statisticians who believe that the researcher's opinion should never come into the analysis, that the idea of a prior (your beliefs about your model parameters before the collection of data for your experiment) is subjective at best and close to heresy.

It's not just a case of ""your text editor doesn't work the way I like my text editor to work but we both end up writing the same files"" as much as ""your understanding of probability is a fundamentally flawed philosophy and will give the wrong answer which will lead to bad decisions"".

Edit: an example of where Bayesian statistics really holds up is when data is next to impossible to collect, such as figuring out the probability (and appropriate uncertainty estimates) of a mid-air plane collision happening before such an accident had ever occurred.",samclifford,2012-11-09 03:14:43
"Depending on how heated these debates got, I think you actually meant ""hearsay"" rather than ""heresy"".",histumness,2012-11-09 13:26:32
"No I actually mean heresy. There were members of the statistics community who saw the introduction as a prior as breaking the rules in such a big way that they campaigned against Bayesianism's acceptance as a legitimate statistical methodology. Even though flat priors would give the same parameter estimates as the Frequentist estimate, to accept a flat prior meant accepting the legitimacy of the concept of a prior itself.

These debates basically split the community for decades and led to people not being hired, articles being rejected outright, etc.",samclifford,2012-11-09 14:53:44
"Our department has both.  In fact, one Bayesian is married to a Frequentist.  I always say ""right tool, right problem"" and don't get wrapped up in the philosophy.",,2012-11-09 09:59:07
"The short version is that there's a handful of theorems that say that the axioms of probability are the Right Way to deal with subjective uncertainty, so one can meaningfully talk about probabilities as representations of subjective degrees of belief, and the probability of a hypothesis being true. That's the Bayesian view.

Frequentists say that probabilities have to be frequencies, that one can't meaningfully talk about the probability of a hypothesis, and also the whole issue with priors in Bayesian methods makes Bayesian stuff too subjective.

(Have I misrepresented either view?)",Psy-Kosh,2012-11-09 04:40:34
"I think that's about right.

I want to add that I think that much of the confusion isn't about the core issue that you have identified. Much of the confusion arises because there are two places at which you
could draw the boundary between *subjective* and *objective*. People flip-flop, without realising that they are vacillating, and make a terrible muddle.

I've tried to [highlight the issue](http://www.cawtech.freeserve.co.uk/prob.2.html). I've submitted this in various places without getting many upvotes and cannot work out what is wrong with my writing. Perhaps it is too long? Too dry? Tries to hard to be vivid and entertaining?",AlanCrowe,2012-11-09 05:26:53
"&gt; Frequentists say that probabilities have to be frequencies,

Frequentists say that probabilities represent long term frequencies of an event under repeated identical trials.  ",anonemouse2010,2012-11-09 12:17:37
"It's more like several hundred years of emacs/vim.

Amazingly though, it's not a pointless discussion, there's still new ideas on this divide from both sides in the last few years that have been enlightening.",,2012-11-09 05:16:59
"It's really not, though. Bayes' theorem wasn't a big deal as far as statistical-type analyses go  until the 1940s or later, iirc from my Bayesian class.",TeslaIsAdorable,2012-11-09 07:22:38
This is why it's important to clarify both the null and alternative hypotheses.,abstrusiosity,2012-11-09 06:36:51
I wonder if now they will finally make government data sources exist in standard databases instead of arcane and hard to interpret text files. ,Europa_Explorer,2015-02-19 14:38:03
You want a sql dump instead of a text file?  Or do you mean an API to connect to a live db?,flipstables,2015-02-19 22:33:03
A decent api is a good start. Having tables with proper column names is also good. A lot of files have an arcane numbering system to store information. Consistency is also important. No government agency maintains any consistent data reporting formats with another agency.,Europa_Explorer,2015-02-19 22:44:06
"At this point I would be happy to even have consistent coding of basic things like State. In some data sets its FIPS codes, in others its a numerical code thats ordered alphabetically, and then occasionally in some of the older systems its a text abbreviation.    ",Case_Control,2015-02-20 08:01:47
"Congratulations to DJ Patil! Pretty inspiring person and so fun, entertaining and educational to listen.",BigMakondo,2015-02-19 07:44:40
I think you a word.,homercles337,2015-02-19 15:03:41
Wow,xodkrm,2015-02-19 10:41:41
We really out here!,Ithm,2015-02-19 12:48:36
"""Data science"" is a bullshit marketing term, this move denotes ""jumping the shark"" for computational scientists.",homercles337,2015-02-19 15:03:12
I agree that we need a better name for this. What scientist *doesn't* work with data?,BlackbirdSinging,2015-02-19 20:34:14
How bout Shark Jumping Scientist,I-want-to-be-better,2015-02-19 22:57:05
Plots that do not label their axes are indeed terrifying.,DrGar,2014-11-01 10:19:32
Did you check the fit by carving the residual vs. predicted values into another pumpkin?,metagloria,2014-11-01 11:10:34
"Would have been way more terrifying if you fit a polynomial of say, degree 5. The horror. ",dhammack,2014-11-01 10:55:53
And then used that to [extrapolate](http://xkcd.com/605/) new points well beyond the observed dataset. Oh the humanity.,DrGar,2014-11-01 11:14:32
"[Image](http://imgs.xkcd.com/comics/extrapolating.png)

**Title:** Extrapolating

**Title-text:** By the third trimester, there will be hundreds of babies inside you.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=605#Explanation)

**Stats:** This comic has been referenced 308 times, representing 0.7889% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_clq40y5)",xkcd_transcriber,2014-11-01 11:14:49
God. This is worst than torture,InAlteredState,2014-11-01 16:51:23
"What are you talking about? Of course it was a good model, I had 0 MSE",Corruptionss,2014-11-01 12:27:39
Carve [this](http://math.boisestate.edu/~calhoun/teaching/matlab-tutorials/lab_11/html/lab_11_05.png) for next Halloween?,typesoshee,2014-11-01 21:50:42
"not quite as fear-inducing as p=0.06, but good nonetheless!",,2014-11-01 14:30:05
"You've got five data points. An assumption of a linear relationship is probably perfectly fine unless you've got a very good reason for choosing a specific form of non-linear function based on where the data comes from.

A few more holes would've made this much better. Get a drill.",samclifford,2014-11-01 18:12:51
It would be even more horrifying if you hadn't plotted the residuals.,beaverteeth92,2014-11-01 12:57:06
Should've carved Anscombe's Quartet into 4 pumpkins.,psychometry,2014-11-02 07:09:06
"That's a sexy R^2, not scary at all.
",papasweets,2014-11-02 20:47:44
"I hope he explains as much of his methodologies as possible, would be interesting to see how they can applied for any future political campaigns!

Some background reading on the role of data analysis in 2012 presidential campaign:
[On the role of data analysts for Obama's win](http://swampland.time.com/2012/11/07/inside-the-secret-world-of-quants-and-data-crunchers-who-helped-obama-win/)
",peppermint-Tea,2013-01-06 20:49:01
"The method was a basic [data mining](https://en.wikipedia.org/wiki/Data_mining) application. The open source software, R, has many good packages that can do such analyses. The more interesting part would be the variables that were significant predictors of the outcomes.",makemeking706,2013-01-06 21:02:25
"It's far subtler than that.

As I understand it, the way he makes forecasts is via a giant Monte Carlo simulation where the parameters of the individual races are set by a carefully weighted average of polling data -- weights are determined by recency, sample size, ""trustworthiness"" (past accuracy of a given polling firm's numbers), and some secret sauce. There's a fairly substantive rundown [here](http://fivethirtyeight.blogs.nytimes.com/methodology/).",dwf,2013-01-07 10:44:27
Really interesting. I wonder what the other parameters are. ,makemeking706,2013-01-07 10:50:40
do you know a good website or something to read up on Monte Carlo simulation?,FinBizaar,2013-01-10 10:31:45
"Depends on what aspect; there are entire theses written on the subject. An author of one such thesis, Dr. Iain Murray, has [slides (and some videos) for a bunch of tutorials he has given](http://homepages.inf.ed.ac.uk/imurray2/teaching/) on the subject of Monte Carlo.

I think the simulation methods in Silver's case were fairly simplistic, at least if he considered all races independent (which would probably be inaccurate but not entirely unreasonable). If so then the simulation aspect would have just been a bunch of independent coin-flips (or discrete k-ary draws if there are k &gt; 2 people running), performed thousands of times to get an idea of how often different scenarios played out. I could be mistaken (I haven't read his book yet) but it seems as though most of the work went into setting the parameters for the various races.",dwf,2013-01-10 10:54:56
"What you just said: the method was math and computers.

On a statistics sub, no less.",TIGGER_WARNING,2013-01-07 05:40:09
"Yeah, it's basically an interative model fitting process that looks for statistically significant relationships in the data without any a priori hypotheses about what the relationships might be. ",makemeking706,2013-01-07 05:49:16
"I'm just an onlooker here hoping to learn something from this sub - but, isn't that what machine learning is?",lahwran_,2013-01-07 07:09:12
"From a [recent r/math post](http://www.reddit.com/r/math/comments/15zup0/these_are_stepbyverifiablestep_notes_designed_to/): ""The algebraist, the topologist, the theoretical physicist, the applied mathematician and experimental physicist are artificial distinctions at the core. There is unity.""

Data mining is mostly a buzzword. You'll see all sorts of artificial distinctions separating disciplines if you get into ML or related data analysis. Good number of complaints about it in /r/machinelearning.

Anyway, there was no 'data mining' involved as far as I'm aware; Nate Silver's methodology relied on collating pollsters' reports and weighing them against each other.",TIGGER_WARNING,2013-01-07 15:04:36
"I thought machine learning is just a name for what you get when you do statistics with a program at the steering wheel, instead of with a human at the steering wheel",lahwran_,2013-01-07 15:06:26
"Wiki presents a [somewhat reductionist contrast](http://en.wikipedia.org/wiki/Machine_learning#Machine_learning.2C_knowledge_discovery_in_databases_.28KDD.29_and_data_mining) between machine learning and data mining that keeps it simple, but like wiki sort of suggests, terminology is more often related to what your goals are than what techniques you might be using.

It's all very arbitrary in the end no matter how you slice it. What makes a biophysicist a physicist but a physical chemist a chemist? How about the difference between a molecular biologist and a biochemist? Or the difference between a statistician and a data scientist?

All that really matters is the knowledge that the labels don't mean much, if anything. It's all just a changing of hats as you walk out the door.",TIGGER_WARNING,2013-01-07 15:40:23
I am sad we have to learn on like SAS Enterprise Data Miner or some crap rather than R in our data mining class next semester.,factory81,2013-01-07 06:57:19
"UoA ?

I did a project at uni with it, it's not bad and some workplaces they use SAS EDM, so having it on your CV can be a definite plus.",montrex,2013-01-07 17:51:08
"Nah.

",factory81,2013-01-07 18:14:40
I want to know if he was involved in *Rampart*?,nelsonha,2013-01-06 21:55:44
Any word on whether Woody Harrelson had anything to do with Rampart? I know he did an AMA. Did anybody use that opportunity to ask him?,jamestown112,2013-01-06 22:12:11
"While what he has done looks interesting, has he had much of a contribution to actual Statistics research? 

I am just looking around to see who this guy is and seemed overwhelmed by baseball and election related links. ",RSeafood,2013-01-07 10:31:59
Nope.  There's nothing wrong with that though.  I'm sure the members of /r/physics would like to know if Brian Cox was doing an AMA.,beaverteeth92,2013-01-07 17:08:06
"Yeah, I wasn't trying to imply there was something wrong with it. I was just curious as to what he did other than some work in something to do with baseball and election results. 

It would still be interesting to read a paper that went into the details though, if someone had a link that would be nice. ",RSeafood,2013-01-07 17:11:07
"I think he keeps his exact methods secret because the New York Times makes him, but don't quote me on it.  Either way he talks a lot about good model building and pitfalls in his book.",beaverteeth92,2013-01-07 17:14:45
"Awesome, I'm looking forward to it!",,2013-01-06 20:32:57
I've never been more excited about an IAmA!,Speed_of_Light,2013-01-07 08:06:45
"I wonder if he thinks its remarkable, or just he was the one to do it. ",thirdlip32,2013-01-07 11:52:26
"Nate is interesting enough and I've been reading his blog since 2008, but as far as statistics goes, there's nothing especially extraordinary about what he's doing. 

""Nate Silver will be doing an AMA"" is interesting. Claiming he's something more than he is takes the shine off it.
",efrique,2013-01-08 07:17:55
"Their Poll of Polls samples *polls* - the sample space is made up of polls, not voters. There is no error in their sampling of polls, so they're technically correct.

EDIT: Though I suppose you could also take that as they did a meta-analysis and didn't bother with coming up with any measure of the ""pooled"" sampling error of the polls they sampled. That's plain dishonest, though, not just technically correct.",,2012-11-01 13:47:18
"Yes, what ~~he~~ she said. There was already a *very* long back-and-forth about this here:
http://www.reddit.com/r/statistics/comments/125ih8/cnn_needs_to_hire_a_statistician/

Edit: Sorry about that",blindConjecture,2012-11-01 14:37:22
[deleted],,2012-11-01 14:41:52
"Thanks! ;)

I think I read those comments. The weirdest things get interesting conversations going here.",,2012-11-01 17:19:55
Who thought this was a good idea?,mrpopenfresh,2012-11-01 17:40:39
People who value presentation over accuracy. That's just what news channels do.,,2012-11-01 18:00:18
"That has gotta be a gag...

What's next? ""Technically correct and misleading"" shows up as CNN's new slogan?",rottenborough,2012-11-01 14:57:19
"It's better than wrong and misleading, which is how statistics are usually presented.",,2012-11-01 15:05:54
[deleted],,2012-11-01 13:18:30
"What happens here is that they define their population as all the polls that they are going go sample, so they are technically correct that they have no sampling error. But then they have to be very precise with their definition of the poll, which they are obviously not, as they interpret it as a prediction of the election outcome, whereas in reality these statistics would only be factually relevant for a research into election polls.

So in the way they interpret it, it is obviously just an indirect statistic based on the samples by the individual statistics which do, of course, have a sampling error. 

Another issue with this statistic, is that these individual polls can overlap, which makes them biased. However, I assume that if they even remotely thought their methods through, they have taken this into account.

So all in all, this CNN statistic will probably give a decent estimation of the real proportions, but their statement is obviously just some sort of publicity stunt to attract interest to their poll and very misleading.",Zonnegod,2012-11-01 16:59:50
[deleted],,2012-11-01 19:49:05
"Not necessarily. It is only wrong if you want to use it to draw conclusions about things outside the population, which is not what CNN does here. They just pose the statistic and let the (statistically illiterate) viewer draw their own (wrong) conclusions.

It is clearly done with bad intentions (namely: to deceive people to think that CNN's polls are perfect) so you could definitely blame CNN for that.",Zonnegod,2012-11-01 20:06:05
"&gt; Another issue with this statistic, is that these individual polls can overlap, which makes them *biased*.

Emphasis mine. No, that would not make them biased, it would make them correlated. ",freudian_nipple_slip,2012-11-01 23:51:17
You are correct! I meant to say that it makes the poll of polls biased as it may count some people's opinion multiple times.,Zonnegod,2012-11-02 04:39:42
"Still incorrect.  Bias has a very precise definition in statistics -- the expected value of your statistic does not equal the quantity in the population you are trying to measure.  

If some people appear in multiple polls, that just means they are correlated and thus the variance of the poll of polls is larger than we calculated.

Take an extreme example. Let's say every one of our polls samples the exact same people.  Taking the poll of polls would be equivalent to taking just one individual poll. The individual poll is not biased and thus the poll of polls is not.*

*Note: Individual polls can be biased if the poll is not sampled according to the proper sampling weights",freudian_nipple_slip,2012-11-02 06:40:03
"&gt;If some people appear in multiple polls, that just means they are correlated and thus the variance of the poll of polls is larger than we calculated.

But it also means that the votes will be counted multiple times. So if a specific region (nonrandom) participates in multiple polls, the mean will slightly shift towards their opinions, rendering the expected value of the statistic unequal to the quantity in the population that we are trying to measure.

&gt;Take an extreme example. Let's say every one of our polls samples the exact same people. Taking the poll of polls would be equivalent to taking just one individual poll. The individual poll is not biased and thus the poll of polls is not.

This is a very bad example. Because here the proportion of votes (which is what we are trying to measure) would still be **exactly** the same. 

Say, we are looking at an election between A and B and we split up our group into a region that votes for A and a region that votes for B. Now an extreme example would be that if we let Poll1 only ask the region A and one Poll2 ask both regions, then if we take the average of these two polls we would have it dramatically biased towards the opinion of A. (Their opinions count for 3/4rd of the final poll of polls!)

Of course, if the poll participation would be completely random, then there would be no extreme issue with the overlap.

",Zonnegod,2012-11-02 07:26:26
"&gt; But it also means that the votes will be counted multiple times. So if a specific region (nonrandom) participates in multiple polls, the mean will slightly shift towards their opinions, rendering the expected value of the statistic unequal to the quantity in the population that we are trying to measure.

False, we just double counted someone, that doesn't bias the results.  That person who was sampled twice would still be likely to vote Obama or Romney with the population percentage.  

&gt; Say, we are looking at an election between A and B and we split up our group into a region that votes for A and a region that votes for B. Now an extreme example would be that if we let Poll1 only ask the region A and one Poll2 ask both regions, then if we take the average of these two polls we would have it dramatically biased towards the opinion of A. (Their opinions count for 3/4rd of the final poll of polls!)


If the regions are equally representative it doesn't matter.  The expected value of your 3/4 A 1/4 B poll is still the true population proportion.  You just have an inflated margin of error.  

Also, it's not a bad example. It directly illustrates what should be in your case the most ""biased"" example since they are all the same, yet there is no bias at all.  

&gt; Of course, if the poll participation would be completely random, then there would be no extreme issue with the overlap.

Precisely. Expected value is still correct and it's unbiased, you just have an inflated variance.",freudian_nipple_slip,2012-11-02 07:35:28
"&gt;If the regions are equally representative it doesn't matter. The expected value of your 3/4 A 1/4 B poll is still the true population proportion. You just have an inflated margin of error.

Oops, should have been 2/3rd vs 1/3rd of course! But what if the regions are not equally representative (which is what I stated!). If the statistic that you want to find is the proportion for A and B, but the way you set it up is (indirectly by using the average of two polls) in fact counting every vote for A twice, you are using a biased estimator.

Say the regions are of exactly the same size: Then Poll2 that samples the 'entire' population of A and B would give (A+B)/2 = 50% of the votes for A. And Poll1 would sample only region A/1 = 100% of the votes for A. Now if we take these two proportions and take the mean we would have 1+0.5/2 = 75% of the votes for A. It's an extreme case, but we cannot exclude that something like this happens.",Zonnegod,2012-11-02 07:47:12
"&gt; Oops, should have been 2/3rd vs 1/3rd of course! But what if the regions are not equally representative (which is what I stated!). If the statistic that you want to find is the proportion for A and B, but the way you set it up is (indirectly by using the average of two polls) in fact counting every vote for A twice, you are using a biased estimator.

If the regions are not equally representative then your **sampling** is biased.  Your poll1 estimator is biased in your example.  It is biased by itself, whether or not it is included in a poll of polls. 

&gt; Say the regions are of exactly the same size: Then Poll2 that samples the 'entire' population of A and B would give (A+B)/2 = 50% of the votes for A. And Poll1 would sample only region A/1 = 100% of the votes for A. Now if we take these two proportions and take the mean we would have 1+0.5/2 = 75% of the votes for A. It's an extreme case, but we cannot exclude that something like this happens.

It depends if region A differs from the general population, if it does, then the sampling is biased and the poll1 that only samples region A is biased and any poll of polls that uses this poll is biased.

If region A does not differ from the general population, then poll1 is an unbiased estimate of the population, and poll2 sampling from the entire A and B population is unbiased and 

1/2 of an unbiased estimate + 1/2 of an unbiased estimate = unbiased estimate. 
Or if you want to do it on the A and B scale, where we said A does not differ from the population
2/3 A + 1/3 B gives 2/3 of an unbiased estimate + 1/3 of an unbiased estimate = unbiased estimate.
",freudian_nipple_slip,2012-11-02 08:02:32
"&gt;If the regions are not equally representative then your sampling is biased.

Depends. If you define the individual polls as being sampled from a population of all relevant polls, yes. But if you define your statistic to be the average of those polls (which they do! Hence the ""No sampling error""), you are not sampling the polls, but just indirectly sampling from the voting population through the individual polls. So as they are not sampling, you can basically ignore the choosing of the polls and act as if CNN is directly sampling from the voting population. And as they do this, they may ask certain people multiple times, but give them the same weights as any other person. Hence their statistic is biased, but the bias will probably be very small, unless they really do some stupid things, like adding the same poll twice to their set of polls.",Zonnegod,2012-11-02 08:20:07
No. Not biased. Every poll could sample one person and that could be the same person in every poll so your estimate is either 0% or 100% and it's still not biased. If you repeatedly sampled over and over calculating the sample proportion you would get closer and closer to the true population proportion -- unbiased,freudian_nipple_slip,2012-11-02 08:23:56
"I think I found the source of our problem. I believe those opinion polls are often things you subscribe to, so they will be asking a fixed set of people (which they try to spread over the population as well as possible) the same question every week, to see if they have changed and to see which people have changed. At least, that is how they are done in my country. So in this case if they would all consist of one person and you would keep asking that same person over and over again, we would obviously have a biased result.

But maybe the polls CNN are using are indeed polling a new random set of people every time.",Zonnegod,2012-11-02 08:27:36
Still not biased. Just not very precise. A sample of one person so long as he is selected according to proper sampling weights is unbiased,freudian_nipple_slip,2012-11-02 08:36:16
"Let me use a toy example:

Suppose the population consists of 10 people: 5 Obama supporters, 5 Romney supports so the true proportion of people supporting Obama is 50%.

Now suppose each of us is going to take a poll of 2 people from this population of 10, independently so there's a chance we have the same people in our polls. 

For each of our polls, there is a 
5/10 x 4/9 = 2/9 chance of sampling two Romney supports (0% Obama),
2 x 5/10 x 5/9 = 5/9 chance of sampling one Romney supporter one Obama supporter (50% Obama)
5/10 x 4/9 = 2/9 chance of sampling two Obama supporters (100% Obama).

Thus each of our sample polls follows this distribution
0%: 2/9
50%: 5/9
100% 2/9

which obviously has a true average of 50%, the true proportion of the population, and hence each of our estimates is unbiased.

Now let's take a poll of polls which averages our results. 

The only way we get 0% is if we both get 0% which has probability
2/9 x 2/9 = 4/81

We can get 25% if one of us has 0% and the other has 50% which happens with probability
2 x 2/9 x 5/9 = 20/81

We can get 50% in 3 ways, for you/me, 0%/100%, 50%/50%, 100%/0%
which has probabilities
2/9 * 2/9 + 5/9 * 5/9 + 2/9 * 2/9 = 33/81

We can get 75% in 2 ways, one of us 50%, the other 100% which has probability
2 x 5/9 * 2/9 = 20/81

We can get 100% in 1 way, both of us 100% which has probability 
2/9 * 2/9 = 4/81.

Now we know the distribution of our poll of polls:
0%: 4/81
25%: 20/81
50%: 33/81
75% 20/81
100%: 4/81

Now we can go through the calculation if you want but you should notice by the symmetry of the distribution, the true mean of the poll of polls is 50%, unbiased.

This even allows for the fact, suppose I choose my 2 people from the population, the only way you choose different people is if your poll selects from one of 8 people out of 10 people for the first person and 7 out of 9 for the second or 8/10 x 7/9 = 62.2%.
That means 37.8% of the time, our samples will have at least one person from the population in common, yet our poll of polls estimate is still unbiased.",freudian_nipple_slip,2012-11-02 09:19:16
"Here you are making two over simplifying assumptions:

1. People have fixed preferences
2. The people are randomly selected

Good polls always ensure that they have a fair distribution of the real population. So say we have one region that samples a Romney-minded region with a big majority of Romney voters and another region that includes this Romney region and also an Obama region. Now, even though these polls might be fine for their respective regions, if you add up the two voting proportions and divide by 2, you're not getting a fair outcome for the entire region.",Zonnegod,2012-11-02 10:43:54
"Then your **samples are biased**.  Each individual poll is biased. Pooling your polls for a poll of polls does not inject bias.  The polls themselves are biased because your sampling is biased.

From your initial comment
&gt; You are correct! I meant to say that it makes the poll of polls biased as it may count some people's opinion multiple times.

Counting someone's opinion twice does not make the poll biased if the way the polls are constructed is done in an unbiased way.",freudian_nipple_slip,2012-11-02 10:49:39
"Theoretically, a poll of polls could be the whole population if they sample 100% of the available polls, which I think is what we're looking at here. If they population is ""every individual poll CNN already made"" then the data would be right on hand for the entire population of things which CNN itself had already done.",featherfooted,2012-11-01 15:14:25
"Fine. I agree with your technical point. A better title exits. 

But the bigger point, to me, is  that CNN is claiming something that is completely implausible by anybody who is educated in the basics of statistics.  

This begs the question: what the hell is the motivation for somebody at CNN to place this on the screen?  Are they THAT ignorant/ unaware? That's a HUGE mistake to make.   Or are they intending to mislead intentionally?   
",stat_geek,2012-11-01 18:44:46
Call Italy!,takeorgive,2012-11-01 12:25:23
"Regardless of this particular situation, shouldn't they just report the posterior probability distribution? Then they wouldn't have to speak about sampling error at all. That error would already be reflected as a smearing-out of the distribution.",absump,2012-11-01 17:09:27
"Yeah, because that would go over easier than 'margin of error.'

Also, they are not doing anything Bayesian. If they were, they'd probably use some kind of time decay of past polls as a prior distribution. Nate Silver does this, along with combining polls and looking historically at past elections.",freudian_nipple_slip,2012-11-01 23:49:54
"This course has exceeded my expectations so far. Video 1.1 wasn't that good because it seemed all over the place (I'd like to see them focus on 2-3 examples in more detail), but starting from 1.2 it has gotten very interesting. The lecturers seem very humble and down-to-earth as well.

I've been having trouble learning R on my own, so I look forward to learning it through this course. I'd recommend it to people who aren't that knowledgeable about statistics but want to learn about it. You still need to know some basics, though, such as what regression is.

Edit: Video 2.2 shows a graphical representation of two-factor regression models. Never imagined it that way, but it makes a lot of sense and is beautiful.

Video 2.3 shows graphically what happens when we overfit the data. Beautiful. I was a bit taken aback in the early videos because the slides were walls of text, but as the course moves along about half of the slides are actually graphical displays.",Pandanleaves,2015-01-20 23:07:14
"Anyone looking for a study partner? Shoot me a message.

EDIT: 

There's also a subreddit dedicated to their more advanced book ""Elements of Statistical Learning"" It looks pretty out of use, maybe we could make it a course forum http://www.reddit.com/r/eosl/",EngineeredEdge,2015-01-21 10:02:04
I just subbed to that subreddit. I'll try and post things there that are relevant to the course. ,,2015-01-21 19:38:28
"Great idea! Is it OK if we use this sub though when this course is for the Intro material? I hope so, but I want to check.",datakittenMEOW,2015-01-22 15:27:09
I sent a message to the mod and haven't heard back. It seems appropriate enough,EngineeredEdge,2015-01-24 20:32:22
Thanks,datakittenMEOW,2015-01-26 12:46:04
maybe we could create a sub ourselves? I am also taking the course and I am highly interested in sharing common doubts/questions and do some team work :),jarandaf,2015-01-27 00:58:23
"anyone know how this compares to Andrew Ng's ""Machine Learning"" course?

thx",sulandra,2015-01-21 02:11:26
"This one seems a lot more statistically oriented.  Like it's more for statistics-minded people, whereas Ng's is more for computer science-oriented people. ",beaverteeth92,2015-01-21 09:37:43
"I was curious about this too, thank you for answering.",datakittenMEOW,2015-01-22 15:27:43
I'm doing them in parallel so i'll let you know in 10 weeks,Megaelectromonkey,2015-01-21 05:24:08
"Did this course last year, enjoyed it and thought it was very well organised and run.  The tasks are good and make you think about the material that you've covered.",enilkcals,2015-01-20 23:30:59
"Thanks for sharing, I signed up.",AllezCannes,2015-01-20 22:58:42
Took it last year. Highly recommend it. ,where_is_the_mustard,2015-01-21 01:24:22
"I took this class last year &amp; loved it so much that I bought ESL &amp; Into to Stat Learning to support them. Their presentation in the videos is fantastic, relatively easy to follow &amp; genuinely useful. I wish I could attend their courses.",lenwood,2015-01-21 08:00:16
"Thanks, I enrolled!",Leglipa,2015-01-21 09:12:14
"Well, I'm at chapter 2 and trying to answers the quizzes within that section, and I now feel like a complete dumbass...

That question regarding the hypercube (2.2 R2) put me at a complete loss... I can't find any explanation on how to run that calculation in the textbook, the video makes no mention of it, and I can't find an explanation elsewhere online... Is this something people are just supposed to know?",AllezCannes,2015-01-23 17:21:25
"I took a stats course today, and this was the result. I hope it's not wholly inappropriate for this sub. I hope especially I have all my stats concepts correct! I'm sure someone will let me know.",Sandino21,2013-01-16 19:21:35
I'd give you extra credit for this.,valen089,2013-01-16 19:47:43
If you continue taking this course (not sure if it's a short course or a school class) you should absolutely continue writing them. ,TeslaIsAdorable,2013-01-17 04:50:41
Clearly your professor was very engaging,shaggorama,2013-01-17 10:42:54
"Keep this up, this shit could be publishable if you get enough of them.",youdneverthink,2013-01-17 05:46:18
"&gt;Keep this up, this shit could be ~~publishable~~ **punishable** if you get enough of them.",hijh,2013-01-20 10:22:38
Don't care if it's inappropriate. I like it.,Benny_the_Donkey,2013-01-16 19:46:29
Well thank you!,Sandino21,2013-01-16 20:23:52
I would have paid large sums of money to hear my Korean (with an extremely thick accent) stats professor recite this.,,2013-01-16 20:51:33
This is awesome!,OutsideCenter,2013-01-17 04:34:42
Beautiful. Wholly appropriate. ,dulby,2013-01-17 09:21:20
Best post in this subreddit I can remember in a long time.,,2013-01-17 14:26:01
"A friend of mine just pointed out that the sample would, in fact, skew the mean, and that the median would remain relatively static. Which bugs me more than it should.",Sandino21,2013-01-17 17:06:44
"yeah, but then it wouldn't rhyme",frau_bluecher,2013-01-17 19:14:12
I hope that's not your professor's name. Or maybe I hope it is.,,2013-01-18 11:17:07
yes,fittel,2013-01-16 21:54:23
"It is yes, fuck off.",Stillbornchild,2013-01-17 10:26:32
"On a related topic, has anyone tried post-hoc styling with the [formatR](http://cran.r-project.org/web/packages/formatR/index.html) package?",Disparities,2014-09-06 22:14:06
"I changed my code style to follow this and well it is a million times more readable for anyone that ever wants to look at my code.

Before I didn't really have any rules and well that was even a pain for me and I figure a nightmare for someone else.",mtelesha,2014-09-06 18:57:42
The biggest difference is between having a coding style and not having a coding style.  Which style you ultimately choose doesn't matter nearly as much as just having one in the first place.,dasonk,2014-09-07 14:46:34
Since I have always been a self-learned programmer I had zero style for decades :( (Walks away quietly),mtelesha,2014-09-07 18:55:20
"I agree with these guidelines and just feel lucky that when I was first taught to code, most of these things were taught, so it all seems natural to me... It definitely helps to be consistent.",agent229,2014-09-06 19:08:17
Why is double spacing preferred over tabbing for indentation?,dclaz,2014-09-07 01:39:58
"Ensures a uniform view between people looking at the code.  The number of spaces in a tab can be redefined.  The redefinition of number of spaces in a tab can lead to things like different indentation for multi line statements depending on who views the code.

Using spaces instead of tabs is the simplest way of making sure that everyone sees code formatted the same way.",crazy4cheese,2014-09-07 02:05:09
makes sense. thanks.,dclaz,2014-09-07 10:05:03
"The only thing I don't like about this is the use of &lt;- for assignment. Every other language uses =, and R supports it. Plus, it's one less keystroke. Why use the arrow?",DavidFree,2014-09-06 20:34:34
"I love using &lt;- for assignment. I think it's better than = because it makes the direction of assignment explicit. This is great for my students, because that's actually something novice programmers will struggle with.",bacteriadude,2014-09-06 23:43:38
I actually love it. I code in a lot of different languages and I need cues to let my brain know what language I am working with. The arrow helps me think in R. ,,2014-09-06 21:29:17
"* Equal sign has lower precedence than arrow, which prevents below code from working:

    \&gt; x &lt;- y = 5

    Error in (x &lt;- y) = 5 : could not find function ""&lt;-&lt;-""

* Arrow is allowed in contexts in which equal sign is not, like in function call:

    \&gt; set.seed(123)

    \&gt; y = rnorm(10)

    \&gt; median(x = y)

    [1] -0.07983455

    \&gt; x

    Error: object 'x' not found

    \&gt; median(x &lt;- y)

    [1] -0.07983455

    \&gt; x

     [1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774  1.71506499

     [7]  0.46091621 -1.26506123 -0.68685285 -0.44566197

(You may think of equal sign in function call as assignment operator for function environment, but, according to [R language definition](http://cran.r-project.org/doc/manuals/r-release/R-lang.html), 10.4.2, equal sign ""is not strictly an operator"".)

* Left-headed arrow is accompanied by right-headed arrow; equal sign does not have equivalent operator

    \&gt; (2 + 5 = y)

    Error in 2 + 5 = y : target of assignment expands to non-language object

    \&gt; (2 + 5 -&gt; y)

    [1] 7

* Arrow is consistent with rest of language, namely superassignment operator (&lt;&lt;- and -&gt;&gt;) and functions (`names&lt;-`, `class&lt;-`, `dim&lt;-`, `levels&lt;-` etc.)

R supports `=`, but arrow is superior in few corner-cases. Less keystroke argument is not very convincing, because usually much more time is spend thinking, re-thinking and debugging code than actually writing it.",mzalewski,2014-09-07 01:07:54
I avoid the arrow notation because i do a lot of numerical programming and the arrow looks too much like an inequality.  There's no confusion with the equal sign. ,shaggorama,2014-09-07 09:10:14
"To avoid repeating myself, here's my comment on '=' versus '&lt;-':

http://www.reddit.com/r/statistics/comments/28ty53/why_r_javascript_php_etc_are_much_better/cierfno
",Cosi1125,2014-09-07 03:05:44
"that's the only one I won't concede. Besides the obvious, when I use &lt;-, it's for assignment within a function and it needs to stand out. ",quatch,2014-09-06 20:38:15
"Interesting. I've seen people use the opposite convention:

* = for assignment to variables
* &lt;- for assignment to functions

as in

    foo &lt;- function(x) {
      y = bar(x)
      return(y)
    }

though I can easily see the opposite:

    foo = function(x) {
      y &lt;- bar(x)
      return(y)
    }

Do you primarily use the latter?",featherfooted,2014-09-06 23:22:23
"I mean,

x=somefunction(y&lt;-5,foo,bar,7)

where you end up with the side effect of assigning y. = does not do this in that situation, but will reference a specific argument of somefunction by name.",quatch,2014-09-07 10:38:17
"Oh within the function call, not the function body. Gotcha.",featherfooted,2014-09-07 10:41:49
"yeah, &lt;&lt;- is for within the body when you want sideeffects :)",quatch,2014-09-07 11:10:38
"Keystrokes?

Emacs+ESS: _ (which is also useful for discouraging the hideous use of underscores in names for things)

I'm not aware of any other editors. 

Kidding aside, I personally think &lt;- looks nicer and Emacs/Rstudio both have quicker ways to input it. I'm guessing any other decent editor can as well. 

When I look at old code with = for assignment I cringe. But not for any rational reason.

",ntoronto,2014-09-07 18:35:23
"Been programming heavily in R since 2000. I have never used the shitty arrow. It's ugly, unnecessary, and slower.",,2014-09-07 03:11:08
"Why is it slower?
",Drmanifold,2014-09-07 11:20:44
I'm guessing they mean slower to type,dasonk,2014-09-07 21:21:53
"Relevant to R style. I've been writing code that calls specific sections of a data set. To do this I am current using something like

    subdata &lt;- mydata[,1:I(N+1)]

Is there a style preferred method for this? Should I set a local variable first? My only other idea was

    end.row &lt;- N + 1
    subdata &lt;- mydata[, 1:end.row]

However, I think in the long term this makes the script look clunky. I would love any suggestions.

Also, why are there so many tab haters? I almost exclusively use tabs, but am aware many people wag their finger at this.",Stevo15025,2014-09-07 00:13:50
"Um, you are using a variable called ""end.row"" to index columns.  I think this inconsistency demonstrates the usefullness of informative names over terse names.",crazy4cheese,2014-09-07 01:51:08
"!!! Caught me red handed !!! Sorry for the error. Then again, having this terse name allowed you to quickly spot my error",Stevo15025,2014-09-07 12:41:59
"It would be better to use column names, in case the ordering changes at any point.",secondsencha,2014-09-07 02:10:03
tabs on unix systems can include characters other than 'white space'. so it can mess with compiling of programs in some languages. not sure about R specifically.,PM_ME_YOUR_VERTCOINS,2014-09-07 13:12:26
"Instead of using 1:x to create sequences, consider using the seq() or seq_along() functions, I find these simpler for me to understand when I go back and look at my code.",datakittenMEOW,2014-09-15 13:03:39
"Can someone explain to me the reasoning behind the ""else"" statement being surrounded by curly braces? 

I have seen that style in other languages as well, but it always looks odd to me. It's also an exception to the previously defined style regarding the closing curly braces getting their own line.",scopegoa,2014-09-06 22:07:57
"Well, this one is kind of a mixed case, but at least in R it ensures correctness. This post has nothing to do with how other languages work and why they might suggest that in style. But, to answer your question, let's take the following code snippet:

    if (condition) {
      bar()
    }
    else {
      baz()
    }
    qwop()

In C, this will work exactly as you expect it. In R, well... let's try out the interpreter. I have directly copied this from my R shell:

    &gt; if (cond) {
    +     print(""1"")
    + }
    [1] ""1""
    &gt; else {
    Error: unexpected 'else' in ""else""
    &gt;     print(""2"")
    [1] ""2""
    &gt; }
    Error: unexpected '}' in ""}""
    &gt; print(""3"")
    [1] ""3""

I replaced bar, baz, and qwop with printing '1', '2', and '3'. You will notice that all three values were executed (`cond` was `TRUE`) and I also received some ugly errors. Now, why is that? It is because in C, it will chain the else to the previous if statement at compilation because that is logical and reasonable. R, however, is interpreted, and so when it sees the ending brace of

    &gt; if (cond) {
    +     print(""1"")
    + }

It says ""ending brace, nothing else on this line, I guess this control block is finished"" and executes that section. Then it gets to the next line (beginning with `else`) and it says ""the fuck? Why is there an else here? There was no if that I was already in..."" because it already ended the previous if.

Since R is interpreted, you will find that this is also reasonable and logical, given the context.

Thus, it is imperative to place else's on the same line as the preceding if brace.",featherfooted,2014-09-06 23:30:55
"That makes perfect sense, thanks for taking the time to explain this. I really appreciate it!",scopegoa,2014-09-07 03:07:39
"I prefer using braces to make blocks explicit. It makes code easier to read, which makes code better.",crazy4cheese,2014-09-07 01:53:13
Thanks for posting this,wookiecontrol,2014-09-06 23:32:07
thank you for this!,iEuphoria,2014-09-07 09:54:07
"I like this guidelines quite a bit, the only modifications I make being

1. Use underscore instead of period for word separating; use of period to seperate words is inconsistent with its use in multiple dispatch, and it is standard in other languages to use an underscore.

2. For mathematical functions, I make them lower case rather than UpperCamelCase. This is consistent with the built-in mathematical functions, so if I want an inverse logistic function I define `expit` rather than `Expit`, though I suppose I might also do `InverseLogit`. Another example is doing `sinc` rather than `Sinc`. 

",NOTWorthless,2014-09-07 13:46:03
Well you can set up a shortcut. Emacs with ESS had one by default,Drmanifold,2014-09-08 05:13:07
"Just saying that begining function names with captial letters is very uncommon in R, for example, like MyFunction. (Sure it exists, but it is nothing I would recommend).",rasmusab,2014-10-06 15:41:07
Great idea for a book.,dza76wutang,2014-02-27 04:06:37
Is it available as a pdf? So that one can read it offline?,sidthecoolkid,2014-02-27 07:54:49
"Message me; I can email it to you. (Yes, I'm the author.)

I'm hesitant to post a PDF version because I'm finishing up the book version, which will be much longer and more in-depth, with new examples and new screwups. The book will be available as paperback and ebook. (Sign up on the website if you want to know when it comes out.)

I do have to thank /r/statistics, though, since I posted an early version a year or two and got such a great response. It's been slowly growing ever since. And I'm definitely open to suggestions!",capnrefsmmat,2014-02-27 09:00:29
I didn't get too into it but please please please please please....please mention relative risk ratios in your book.,dza76wutang,2014-02-27 09:52:53
So I've heard of odds ratios being misinterpreted as though they were relative risk ratios. Is there another misconception you're thinking of?,capnrefsmmat,2014-02-27 10:15:41
"There's 2 that I see in health data:

1.) A CI that straddles 1.0, e.g. (.9, 1.4) which means no effect, and the opposite effect are within the CI but the interpretations always seem to forget this.

2.) Relative risk versus absolute risk. A RRR of 2.0 sounds like a slam dunk, like a sure thing, like whatever is causing the risk should be extirpated from your life...however in reality it reflects going from 1 in 1,000,000 to 2 in 1,000,000. 

3.) A bonus misinterpretation is trying to assert causative relationships from RRR's that are relatively low. 50+ to 1 is probably causative, 1.5 to 1 is probably not.",dza76wutang,2014-02-27 13:06:30
"1. Not sure I follow you. Are you talking about something like [figure 1 in this paper](http://www.biomedcentral.com/1471-2288/13/134), where the results are all consistent with each other but some CIs cover zero and some don't?
2. Yeah, I mention this briefly in my draft but maybe should do it more justice.
3. I'm going to devote a lot of space to estimating and interpreting effect sizes instead of just significance.",capnrefsmmat,2014-02-27 14:19:49
2006 and 2010 straddle 1.0 which would imply no effect is within the realm of plausible outcomes. It's analogous to when CIs straddle 0. The founder of this site (http://www.thennt.com/) does a great job explaining the issue as it pertains to healthcare.,dza76wutang,2014-02-27 16:13:18
"So you're saying people don't realize that a CI including 1.0 -- instead of 0.0, the usual bar for other tests -- fails to reject the null when you're using RRs?",capnrefsmmat,2014-02-27 17:40:55
"Yes, exactly. They'll get something like ""the RRR for the study was 1.1 (95% CI was .9 to 1.2) so we conclude such and such does increase risk"".",dza76wutang,2014-02-27 19:43:24
"That's... dumb. If you have any examples from published papers, I'd love to read them. That'd be hilarious.",capnrefsmmat,2014-02-28 05:30:43
"This is a post of mine from an anti-meat message thread:

From the Harvard study I was linked to this paper which seems pretty solid: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2946797/

It's about heart disease and red meat, not bowel cancer, but still a significant health risk. I'll go through the results to show how even a study as well done as this one can be misleading.

Table 1: I only skimmed this, but it's great at first blush, there's no obvious reported confounder to muddy the waters re: meat. For example, imagine a cohort that was 90% smokers.

Table 2: Relative risks. Here's where the study is being naughty. Relative risks are being reported as, for example, 1.29 = 29% more likely to have a heart attack (1.29 = multi-variable, red meat and processed, highest quintile for meat eating). This is some scary stuff. But it's not considering the base risk, more on this below. RR's should be 40-50 to 1 to cite a causal link, consider smoking vs nonsmoking and lung cancer. In a multi-decade study of ~84K women, around 3K had an episode of heart disease (roughly 2:1 non-fatal vs fatal). This would imply a base risk of ~4%, so a 1.29 to 1 risk multiplier puts the 4% to a knee-knockingly terrifying...5.16%. The MV value for RR for 1+ meat servings was 1.16 (bottom right corner). Which means the RR for a CHD episode is only 1.16 * 4% = 4.64%.

According to the study, consumption of 1+ servings of red meat every day increases your risk by ~.64%. For poultry the RR is .9 and for fish it is .81 leading to ~3.60% and 3.24%. Assuming the 4% observed in the study can be extrapolated broadly.

Another example of naughtiness is the RR confidence intervals for Red Meat excluding processed meats. Note how many of the CI's straddle 1.0, this means the possibility of no impact is in the CI, and any value below 1.0 implies it might reduce risk. Look how many CIs straddle 1.0.

Table 3: Food by food and we see CIs straddling 1.0.

In conclusion, while there may be an increased risk of CHD associated with red meat consumption, it's tiny. Given how complex CHD is, it's a stretch to claim a causal relationship, especially considering how low the RRs are in the grand scheme.

EDIT: FWIW, I have a masters in math and work in healthcare analytics so I'm not just some shlep who is white knighting steak.

EDIT II: Check this out http://www.worldlifeexpectancy.com/cause-of-death/coronary-heart-disease/by-country/ you'll see Argentina (arguably the highest beef eating country in the world) has one of the LOWEST per 100K incidence of death via coronary artery disease. Turmekenstan has the highest CAD death rate and a far lower meat consumption rate than Argentina (http://vegetarian.procon.org/view.resource.php?resourceID=004748). This is imprecise but it does offer a directional macro perspective on beef and heart disease.
",dza76wutang,2014-02-28 06:04:33
"&gt; RR's should be 40-50 to 1 to cite a causal link

Where do you get this rule from?

As for the RRs in table 2, I don't see them as ""naughty."" Naughty would be omitting those RRs because they're insignificant. The insignificant RRs are only for subsamples of the data, so I am not surprised.

This doesn't look like a problem specifically with the interpretation of RRs; it looks like you're expecting them to have significant results across all groups all the time. That's not plausible -- that would strike me as a sign of fraud.",capnrefsmmat,2014-02-28 07:36:14
"There is nothing wrong with showing insignificant RR's the issue is moreso when claims like ""meat causes illness"" or ""meat is bad for humans"" get extrapolated from studies like this when the RRs are small or insignificant. In hindsight my language was imprecise due to rushing the post out.

The RR's need to be 40-50+ thing is a heuristic I've picked up while exploring epidemiology and evidence based medicine, I don't have a specific citation.",dza76wutang,2014-02-28 10:53:21
Any idea when you're going to release your book? I'll definitely purchase it. ,,2014-02-27 13:50:53
"No firm date yet. The manuscript is due to the publisher ([No Starch Press](http://www.nostarch.com/)) later this year, but I don't know whether it'll make it to print before the end of 2014 or not. When I find out I'll email everyone who signed up on the website.",capnrefsmmat,2014-02-27 14:27:40
"Is it geared toward particular areas of science? Any social science? I know some things are universal but they do use different methods, have different conventions, etc. ",tangeloo,2014-02-27 09:17:44
"It's fairly easy to find examples of bad statistics in medicine, neuroscience, and biology. But the chapters on power and multiple comparisons, for instance, apply to psychology just as well -- lack of power was first recognized as a serious problem in psychology in the 60s, I think, and it's been a problem ever since.

I'm not writing in great technical detail, so most of the material applies equally well to any field that uses basic statistics. But I suppose that, say, econometrics has its own set of common problems.",capnrefsmmat,2014-02-27 09:31:13
"Very good read. So good, that I want to translate it to Spanish :)",clbustos,2014-02-27 13:30:10
Definitely going to read this. ,,2014-02-27 13:50:06
This looks great.  Good job!,AllenDowney,2014-02-27 16:01:35
neat thanks,TehZman,2014-05-29 18:32:53
"Wow, great! For how long have you been working on this?",rasmusab,2014-06-12 01:58:08
/r/datasets,shaggorama,2014-05-30 06:28:02
So fucking good!,kidpost,2014-05-30 05:45:21
aaaaannnddddd... I still don't get it but I'll pretend to laugh just to fit in,umib0zu,2013-07-10 17:45:27
not really that funny anyway so you're good :),lol0lulewl,2013-07-10 20:58:41
"Yeah, this is more midlyinteresting, rather than funny. Don't get me wrong, I really love this comic as part of the series, just pointing out that laughing at this one will get you found out and not the opposite.",,2013-07-10 23:31:18
"nice, there was a mistake earlier but he fixed it

some guy in /r/probabilitytheory found it",lol0lulewl,2013-07-10 10:20:59
What did it say?,moultano,2013-07-10 12:09:28
"he swapped P(I am near the ocean) and P(I picked up a seashell)

[the xkcd forum has the origional](http://forums.xkcd.com/viewtopic.php?t=103541&amp;p=3404251#p3404386)",REDDIT_CENSUS_BUREAU,2013-07-10 13:09:28
"Either it was a genuine mistake on his part, or he [nerd sniped](http://xkcd.com/356/) me like a boss.",ploika,2013-07-10 13:50:16
"Wow, this is both funny and stupid at the same time. 

Also Bayes' Theorem rocks.",hainako,2013-07-10 22:26:13
Did I just stumble into /r/statisticscirclejerk?,Troybatroy,2013-07-10 12:31:05
? How is this a circlejerk?,,2013-07-10 13:48:13
"This is casual bullshit rather than interesting.  ""ooo, a comic mentions a statistics theory!  I'm creaming myself!"". childish.",drunken_Mathter,2013-07-10 14:31:34
"casual? sure

bullshit? maybe

definitely not interesting from our perspective but hey, why not look at it from the perspective that it helps make people more aware of our field",lol0lulewl,2013-07-10 21:00:40
by posting it in /r/statistics?  That's my only problem here.,drunken_Mathter,2013-07-10 21:05:36
"Jeeeesus, people. Lighten up.",MrInRageous,2013-07-10 15:21:26
Casual,drunken_Mathter,2013-07-10 15:24:22
Lighten up *more* casually?,MrInRageous,2013-07-10 15:33:55
Causal?,freudian_nipple_slip,2013-07-10 18:19:07
http://tohno-chan.com/vg/src/131499676255.jpg,joke-away,2013-07-10 19:22:01
"Interesting. Can you explain this 24 hour shift business? It doesn't look like you sleep at all. 

* Is this normal? 

* Is that much driving not regulated at all (like truck drivers).

* Do you pull 24 hour shifts on non-weekends? 

* What city is this?


",Bored2001,2014-03-03 00:47:06
"1. Depends on what you mean by ""this."" 24 hour *cab rental* is done by a pretty decent amount of regular cab drivers for weekends, since the day and night shift of Friday and Saturday are both good money makers. The law doesn't allow driving for more than 14 hours, but the only way they really can check for that is by auditing how long drivers are booked into the system. Technically they tell you that you ""should"" take a few hour break after 14, but if you start at 4:30 in the morning in order to guarantee a good car, your break will be during the prime time of the shift change when night drivers haven't come in yet and day drivers are heading back in, people are going home from work and out to eat, etc. The way to go unseen by auditors is to book out, take a break, eat, piss, and book back in. 24 hours of driving a cab is much easier than it sounds. Compared to driving cross country in 24 hours, for example: the cab business requires you to get in and out of the car, find places you may have never been, meet people, carry on conversations, lots of little mental stimuli and a small amount of physical work that keeps your body awake but doesn't wear it out.

2. Oh, I guess I answered your second question in the above paragraph. Yes, regulated, but hardly provable. What cop is gonna pull a cab driver over to check? The only way they know is by looking for drivers booked in for more than 14 hours. Make a point to book out before that, or whenever it's slow, and you'll break the time period up into smaller pieces that go undetected.

3. Definitely not. I personally have no idea how night time drivers make any money other than fri-sat nights. I think they gotta be selling drugs or some other kind of hustle. My city isn't big and active enough on weeknights to require a lot of cab business. The only 24 hour shifts I've done are Friday morning to Saturday morning, though I might try Friday night to Saturday night one of these days.

4. This is Phoenix and the greater metropolitan area. In cities like Chicago and New York, the cab business is so lucrative that just the licenses alone are sold for ~~hundreds of thousands~~ apparently [millions of dollars](http://www.nytimes.com/2013/11/15/nyregion/1-million-medallions-stifling-the-dreams-of-cabdrivers.html) but here in Phoenix, we've got too much urban sprawl and not enough traffic or culture to make cabs a high enough demand yet.",Pie_Bowler,2014-03-03 01:27:01
"Thanks for answering all the questions, this thread was really interesting due to your responses! :) I hope you can just work 4 days a week!",alexgmcm,2014-03-03 05:30:58
"If you get a whole bunch of this data together, paste it into a post as csv over at r/datasets.",bigwisebeard,2014-03-03 06:31:38
I have no idea what csv is or how to do that.,Pie_Bowler,2014-03-03 07:32:48
"You can save as different file types in excel. One of them is a csv file. Basically, it saves the text using commas to separate cells in the table. 

You lose formatting but the file is smaller and easier to load into programs other than excel. 

File- save as - change type to csv.",cookie_partie,2014-03-03 08:08:13
"Oh this was created on Open Office, not sure if they have that or not. I'll check later when I get a chance.",Pie_Bowler,2014-03-03 10:14:54
OOo does support CSV.,dandeliondreamer,2014-03-03 10:55:18
CSV is a really simple common format.  Open Office should support it.,TehGogglesDoNothing,2014-03-03 10:53:36
"There is a lot of valuable data here. You can track this weekly and see revenue trends over time. Maybe even track routes or even customer behavior. Figure out interesting ways to optimize your trips, save gas, find higher paying customers or maybe lots of short trips. ",,2014-03-03 18:58:35
"I've definitely noticed that Mon-Wed-Fri daytime shifts are the best for a lot of voucher calls but Mon and Fri are definitely a good mix of cash calls and voucher calls. (The vouchers are paid for by various organizations and we get credit instead of cash, but it's not the full meter amount so we pay a smaller fee for taking those calls.) It's a lot of doctor appointments and airport runs. There isn't necessarily the data yet, but there is enough experience to avoid certain zones of the map and to spend more time in other zones. Some senior centers have a lot of time consuming runs with low-paying vouchers and you don't get away from the senior center before they send you another call to pick someone else up from the senior center, so you have to reject it or drive empty for a while to get to a better area. If I get stuck there for a while, I'll take all the calls I can get, but I try to stay away from the areas that have those senior centers so I don't get stuck there.",Pie_Bowler,2014-03-03 21:30:29
"Post it to /r/datasets!

Do you have Excel version of this? Maybe even more data?",jinnyjuice,2014-03-03 23:15:17
"I actually use a plain notepad to keep track of the call time, the amount, and the fee for each call. Occasionally throughout the day I will add them up and see if/when I've made enough to cover the lease. That's about as much data as I've kept track of. Other info might be available, such as when all of the calls come in and where they come from, but that isn't available to me if it does exist or even if it can be measured.",Pie_Bowler,2014-03-04 02:07:43
nice...  if you could make a chart /r/dataisbeautiful would like it too,pembo210,2014-03-03 00:28:53
"I almost submitted this there, but I figured it's just too plain and I'm not really sure how to make it beautiful. I'm definitely excited by it and I love to show it off, but clueless on what kind of chart would make it beautiful. You're welcome to the data if you want to make something for them, though. I won't get mad or claim credit for your effort.",Pie_Bowler,2014-03-03 01:28:58
Can you please explain those jobs where you make $0 income but still have to pay the fee?,Rockefellersweater,2014-03-03 01:13:17
"The particular company I work for offers a low cab rental cost but with that they charge for each call that they send us. The fee is basically for technology and other overhead. If the passenger cancels (X) then they have the obligation to not charge us. However, people began gaming the system by arriving at the scene to pick up a passenger and sending in a ""no show"" on the car computer, thus removing the fee, and then picking up the passenger anyway as a street pickup (SP) thus making themselves an extra couple bucks. Auditors started looking at driving reports and saw a shit ton of ""no shows"" back-to-back with street pickups, and figured out that they're getting scammed. So they decided, ""what the heck, if you're still making great money out there and most of your calls are good, a couple bucks here or there won't hurt you, and besides, we still have to pay the overhead."" The way that some cab drivers beat that now is by calling the passenger when they're a few minutes away, telling the passenger to call dispatch and cancel the call, and they'll pick them up anyway. The passenger has no incentive to do this, even if the driver offers to take a buck or two off the price (then he'd only be saving himself 45 cents.) So the drivers are essentially revealing that they're scandalous and the passengers may or may not go for it, and that's why I don't do it. If I call a passenger to let them know that I'm close and they tell me that they've cancelled, I'll ask them very politely but urgently to please call dispatch ASAP to cancel the call so that I don't lose money. I'll actually take a minute to explain that too. If the call doesn't cancel within a minute of me hanging up, I just turn the meter on and off, showing that I picked them up, so that I can clear the call and take another. If I'm going to pay for it whether I get them or not, I'm not going to wait five minutes while I send in a ""no show"" code and they take forever to clear it while there are other calls I can be taking in the meantime.",Pie_Bowler,2014-03-03 01:48:12
Thanks for responding. Great spreadsheet.,Rockefellersweater,2014-03-03 01:54:35
Why would you do anything else than driving cab if you can make up to 500$ a day like you said in another comment??,Beeni30,2014-03-17 21:11:05
Dude- you made $400 plus in 24 hours driving?  Good effin job. ,ellenad,2014-03-03 00:44:27
"My first 24 hour shift I took about an hour break and I made $350. It's as good as two really good 12 hour shifts. So I decided I'd do them every other weekend just to boost my finances as I'm trying to get myself out of some credit card debt. My second 24 hour shift I made $500, and this one was $490 (Not sure what tips I didn't count or if I forgot to write a fare down, but that's how much I had in my pocket when I left the station.) If I was confident that $400-500 was possible every friday on a 24 hour shift, I would only work one day a week and spend the rest of my time going back to school or finishing all of the books on my shelf and working out and just getting outside to live a bit more. This was only my third run at it, but if it keeps going this way, I just might schedule my life so that I only work 4 days a month. Can you imagine?!?!?",Pie_Bowler,2014-03-03 01:33:57
"You must be young. I remember being able to pull all-nighters preparing for exams.  You can do this when you have the energy but I would not recommend it as a career choice!  You will burn out.

You're a smart guy.  Why don't you put some of that energy into programming.  That can turn into a career that makes a lot more per hour and won't burn you out.  Or you could develop a useful software tool and sell it. Then you make money while you sleep! ",true_unbeliever,2014-03-03 14:30:07
I'm planning on going to school for psychology once I can get a grant or find an affordable program. I think actually the timing would be perfect because /r/asmr is starting to be more popular and hasn't been studied enough yet in the field of psychology.,Pie_Bowler,2014-03-03 21:04:15
How is this statistics?,Insight_,2014-03-03 02:58:09
"[define:statistics](https://www.google.com.au/search?&amp;q=define:statistics) - ""the practice or science of collecting and analysing numerical data in large quantities""

Seems like statistics to me.",thelatemail,2014-03-03 03:29:24
"Yeah, lots of collecting but I don't see much analysing.",Insight_,2014-03-03 03:40:57
"It's here for analysis. I already started to analyze the fluctuation of hourly pay based on each call, for example, and the two points at which my lease is met and my lease plus gas is paid for.",Pie_Bowler,2014-03-03 07:38:05
"This is /r/statistics, not /r/onlystatisticsreadyforpublicationinnature",imherejusttodownvote,2014-03-03 12:32:54
"Awww, no love for Cosma Shalizi eh?

http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/",daidoji70,2014-07-14 13:39:05
"Cosma is nearly always interesting. Thanks for the reminder, it's been a couple of months since I looked.
",efrique,2014-07-15 19:44:57
"Off topic, but tangentially related -- what is a suitable tool or combination of tools to compose blog posts for scientific / computing topics? Such a beast would need to combine text, formulas, plots, code snippets, and maybe tabular data. I have tried to cheese something together but I'm dissatisfied by what I have, and I'm wondering if there is something obvious I've missed. I'm hoping to find an all-in-one tool or package -- I don't want to write the text, make the plots separately, embed the graphics file names by hand into the text, etc etc. ",midianite_rambler,2014-07-14 12:18:02
"Off the top of my head, there's [iPython](http://ipython.org/notebook.html), and for R you could use KnitR in Rstudio. Depends on your workflow.",DavidFree,2014-07-14 13:08:38
"Thanks, I will take a look at those.",midianite_rambler,2014-07-14 13:25:53
I think I'm going to be taking Bayesian Data Analysis with Gelman next semester... *swoon*.,Pseudo_Scientist,2014-07-14 15:40:02
r/rstats would be an even better place for this. ,iconoclaus,2014-04-15 10:02:36
I actually posted it there first but it didn't get much love.,passingby,2014-04-15 10:04:13
:(  probably has been posted there before then. ,iconoclaus,2014-04-15 10:06:52
"Well, I hope not, I did write this tutorial just 3 days ago ;)",passingby,2014-04-15 11:35:20
"as an aside, that is a very beautiful github page you've got set up there. ",sn0wdizzle,2014-04-15 12:42:49
Thanks! It's a labor of love and the style has evolved over the last year or so.,passingby,2014-04-15 12:49:54
Differences from Sweave?,quatch,2014-04-15 10:37:19
"Look at [this reply](http://www.reddit.com/r/statistics/comments/233q7n/if_you_use_r_latex_a_lot_you_might_like_knitr_it/cgt6053) and then [this reply](http://www.reddit.com/r/LaTeX/comments/231van/ive_been_using_latex_a_lot_with_statistics_r_so_i/cgt4omm).

Basically I think knitr is far ""neater"" (which is partly why it is named knitr) and works a lot better. I also think it is more modern and maintained better.",passingby,2014-04-15 10:51:14
"Though it is still not well integrated into Rstudio, when sweave is. I expect it is like lme4 and nlme packages: currently in transition, but not quite there yet.",Bishops_Guest,2014-04-15 15:23:06
"you don't know what you're talking about. nlme isn't in transition, it hasn't been actively developed for years and all they do is maintenance. lme4 just had a major release last summer that's now fairly stable. knitr support is absolutely seamless in rstudio.

 ",Bayesbayer,2014-04-15 16:14:56
"RStudio fully supports knitr: https://www.rstudio.com/ide/docs/authoring/overview

In fact, the creator of knitr is currently working at RStudio so the support will only get better: http://yihui.name/

Not to mention knitr supports RMarkdown (and various other technologies) which is being pushed by RStudio as well.",passingby,2014-04-15 15:28:24
Still does not look like it is worth transitioning yet (for me). Maybe when I run into one of the sweave formatting problems I have not already worked out myself.,Bishops_Guest,2014-04-15 15:53:36
I honestly thought knitr was specifically an Rstudio product when I started using it. The integration is seamless as far as I can tell. What features of knitr-Rstudio integration do you want that don't currently exist?,biobonnie,2014-04-16 08:22:00
"Yihui wrote knitr because he had to TA a class that taught sweave, and it was a PITA (That class was also why he wrote tidyr). Knitr is much smoother, has way better caching, and is integrated with RStudio. It also lets you use pandoc to get files into word and HTML as well as LaTeX.",TeslaIsAdorable,2014-04-16 07:24:30
"Hah, I would hate to TA sweave, we learned it in a grad class of 6 and it was 'fun'.",quatch,2014-04-16 10:25:11
"Yeah, I was in the class, and I felt nothing but sympathy for him. As I recall, he actually had to teach the lesson on Sweave as well as grade the homework. It wasn't fun for anyone. 

Of course, it didn't drive me batty enough to write knitr, but I'm glad Yihui is a special kind of crazy :)",TeslaIsAdorable,2014-04-16 11:29:41
"Wow, great info. Was this an ISU course by chance?",passingby,2014-04-17 06:40:33
"Yep, Stat 579. ",TeslaIsAdorable,2014-04-17 06:47:45
"I like R studio and R Markdown: http://www.rstudio.com/ide/docs/authoring/using_markdown.
  
I also use Lyx with knitr a lot for my homework (which requires page numbers and headers/footers) : http://yihui.name/knitr/demo/lyx/",quieromas,2014-04-15 13:44:30
"The actual syntax looks a lot like Sweave, are there any reasons for using knitr instead of Sweave?
",LuckyNumberVilsen,2014-04-15 10:40:59
"Ah yes, good question. I've found knitr to not only be easier to use, but its defaults are wayyyy better than Sweave when it comes to output.

Plus I think knitr is just more up to date and better maintained.",passingby,2014-04-15 10:48:40
I completely agree.  Honestly if people want to use Sweave they can but I don't understand why anybody would choose Sweave over knitr.,dasonk,2014-04-15 14:17:27
"details here: http://yihui.name/knitr/

It is just sweave with different defaults, and already added extras like cache.

Seems to me if you already have sweave working there isn't much point. Might be worth it for the extra markup language support without learning another system.",quatch,2014-04-15 10:54:09
"knitr just seems to be easier to use than Sweave. It has good documentation for its [hooks](http://yihui.name/knitr/hooks) and its [various options](http://yihui.name/knitr/options).

I created an album of a Sweave example vs Knitr. Knitr looks wayyyy better: http://imgur.com/a/j8VuI",passingby,2014-04-15 10:57:46
"most of that is the latex. The R code is more or less the same, save syntax highlighting. Since I never put R code into my publications, it isn't a feature for me. Might have been nice for my homework though. 

The only useful bit from that particular example is the side by side figures, which is nicer than my current method (using the latex subfig package).

I'm not saying it's a bad or useless package, just that it doesn't appear to do a whole lot (if you already sweave), so it does need to be sold.",quatch,2014-04-15 11:07:38
"Ah I see, that does look a lot better. As a hardcore latex/R user just getting into Sweave I will definitely try it out.",LuckyNumberVilsen,2014-04-16 06:58:03
"It's basically Sweave 2.0. I find it works a little better and easier with a few more options. 

Also, knitr supports things like R markdown and is well-integrated with R Studio and the new products it's pushing.",giziti,2014-04-15 20:18:06
"I do my stat homework using knitr now! I have to say that knitr is probably only suited for ""demo"" paper or homework as such. It has to run R code every time you compile into pdf, so it takes very long each time --&gt; not practical if you have R code that takes more than 5 secs to run.",selectorate_theory,2014-04-15 14:48:15
"Add this to your chunk and be amazed. It caches it and only re-evaluates the R code when it is modified, not every time.

    &lt;&lt;cache=TRUE&gt;&gt;=
    # code here
    @",passingby,2014-04-15 14:59:28
"If you include

    opts_chunk$set(cache=TRUE)

in a code block it will set it so that all code chunks cache by default.  This is pretty handy so you don't have to remember to cache every block.  Then if you have blocks you don't want cached you just include cache=FALSE in the code block parameters.",dasonk,2014-04-15 18:49:55
"This is also worth highlighting as a reason to prefer knitr to Sweave. You can cache with Sweave, given some effort, whereas with knitr it is very straight forward.",NOTWorthless,2014-04-15 15:11:13
"oh man I will definitely try it!
",selectorate_theory,2014-04-15 16:19:00
"oh shit YOU are the guy! I googled ""how to do homework in latex"" the other day and came across your site. I cloned and am using your latex template now. I'm curious why you created this \homeworkProblem instead of just using section? Sometimes the problems come in different types of numbering you know.",selectorate_theory,2014-04-15 18:34:23
"Haha, awesome! I hope you've found it all useful.

Also, \homeworkProblem just fits how I do my homeworks. You are totally free to remove it and just use \section.

It is more of a personal choice.",passingby,2014-04-15 20:19:04
"Even with caching, I find that it is still a good practice to do most of the processing in a separate script, then save and load the objects that I wish to display: My files are neater and easier to read. I still get the adaptability granted by sweave and knitr. Much faster compile times. An easier debugging process.",Bishops_Guest,2014-04-15 15:20:54
You could also source those files using knitr. I've done that for some particularly complex analyses with lots of extraneous functions.,TeslaIsAdorable,2014-04-16 07:26:44
"How do you get this to work on texmaker (ubuntu/linux mint)?

",fabreeze,2014-04-18 19:15:06
Nicely done.,tpn86,2014-07-30 05:00:32
"Is there a name for a Markov chain where they current state depends on the previous state AND an external variable?

My research (bio) has a kind-of Markov process, in the sense that I could draw up a transition table and simulate, but the transition table itself should be modified by an external variable, which changes as a function of the integrated sum of the Markov processes' history.",forever_erratic,2014-07-30 06:29:43
Discrete time stochastic processes are the generalization of Markov chains that you get from dropping the Markov (memoryless) property.,Kibatsu,2014-07-30 07:03:32
"I'm not sure about what you described specifically, but it reminds me a bit of chain heating in MC^3.",SigmaStigma,2014-07-30 07:06:39
"&gt;which changes as a function of the integrated sum of the Markov processes' history.

Let's say the Markov chain you describe is X(n). Define a new variable Y(n)=\sum_{i=1 to n}X(i). Now note that Y(n)=Y(n-1)+X(n). Therefore both X(n) and Y(n) only depend on the previous state. Therefore you can model your system as a markov chain over the (higher dimensional) state space [X,Y]. 

You then simply modify the transitions so Y evolves deterministically (i.e., with probability 1) as Y(n)=Y(n-1)+X(n), and your X(n) evolves according to the transition matrix that depends upon the value of Y(n-1).",DrGar,2014-07-30 08:39:13
"Great way to look at it, thank you!",forever_erratic,2014-07-30 08:48:33
"No problem, happy to help a fellow bio-er. ",DrGar,2014-07-30 08:49:57
"I feel like I'd need to know more details before giving a solution, but it's definitely possible to have a Markov process where instead of p(1,1)= .3 and p(1,2)= .7 in a 2-state chain, you could have .3•x and .7•x, where X is an external variable.

Or perhaps the X itself follows its own Markov chain?

Traditional Markov chains by definition **only** rely on the present state to determine the future state (it carries no memory of previous states). However, there may be some altered structure that works for you.",byram,2014-07-30 07:06:51
How does the process of a Markov Chain Monte Carlo compare to what is explained in the article? What differentiates MCMC from another form of Markov Chain?,AllezCannes,2014-07-30 10:24:41
In MCMC you'd draw random samples from a particular probability distribution (-&gt; the additional monte carlo component),rasbt,2014-07-30 11:09:08
"Neat, thanks.",halifaxdatageek,2014-07-30 10:38:19
"https://en.wikipedia.org/wiki/Markov_chain

Thanks!",westurner,2014-07-30 10:45:01
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Markov chain**](https://en.wikipedia.org/wiki/Markov%20chain): [](#sfw) 

---

&gt;A __Markov chain__ (__discrete-time Markov chain__ or __DTMC__ ), named after [Andrey Markov](https://en.wikipedia.org/wiki/Andrey_Markov), is a mathematical system that undergoes transitions from one state to another on a [state space](https://en.wikipedia.org/wiki/State_space). It is a [random process](https://en.wikipedia.org/wiki/Stochastic_process) usually characterized as [memoryless](https://en.wikipedia.org/wiki/Memorylessness): the next state depends only on the current state and not on the sequence of events that preceded it. This specific kind of ""memorylessness"" is called the [Markov property](https://en.wikipedia.org/wiki/Markov_property). Markov chains have many applications as [statistical models](https://en.wikipedia.org/wiki/Statistical_modeling) of real-world processes.

&gt;====

&gt;[**Image**](https://i.imgur.com/xtknu8X.png) [^(i)](https://commons.wikimedia.org/wiki/File:Markovkate_01.svg) - *A simple two-state Markov chain*

---

^Interesting: [^Markov ^chain ^Monte ^Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) ^| [^Continuous-time ^Markov ^chain](https://en.wikipedia.org/wiki/Continuous-time_Markov_chain) ^| [^Absorbing ^Markov ^chain](https://en.wikipedia.org/wiki/Absorbing_Markov_chain) ^| [^Lempel–Ziv–Markov ^chain ^algorithm](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm) 

^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cjc6k57) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cjc6k57)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2014-07-30 10:45:21
are these used in time series analysis alot? ,watersign,2014-07-30 21:06:03
"Hard to believe Jaynes *Probability Theory: The Logic of Science* isn't on this list. For that matter, Karl Popper *Conjectures and Refutations: The Growth of Scientific Knowledge*",tekelili,2014-11-05 17:25:48
Always looking for good books on epistemology. Amazon'ed your Popper suggestion. Didn't Jaynes die before fully completing that book? Shame.,DrGar,2014-11-05 17:42:23
"He did die before completing, but the editor has done a very good job of making it feel complete. It's still rather influential and, at least for me and a few others, really lays the groundwork for a pragmatic philosophy of science.",tekelili,2014-11-05 19:49:49
Can't resist the urge to post this: https://www.youtube.com/watch?v=Q9aM9Ch97U8,sloby,2014-11-05 17:37:29
"Breiman's paper on parametric vs. nonparametric modeling.

**EDIT:  Here it is:  [Statistical Modeling: The Two Cultures](http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)**",beaverteeth92,2014-11-05 12:01:55
"This is a cool paper, but I don't like how he puts down data based models...and declares that it is seemingly wrestling with futility ",,2014-11-05 18:09:03
"I had the exact same thoughts. Thought provoking, but not something I agree with. Read the comments at the end by Cox and others, and this idea meets appropriate amounts of resistance. 

Thought provoking enough that I included it though.",DrGar,2014-11-05 21:03:48
Do you have a link or a more detailed reference? I am unfamiliar and google scholar didn't give the answer.,DrGar,2014-11-05 12:04:52
I'm on mobile so I didn't post it but I will later.,beaverteeth92,2014-11-05 13:01:28
"Elements of Statistical Learning by Hastie, Tibshirani, and Friedman (maybe on other?).

Some measure theoretic probability should be covered; maybe Billingsley?

Programming should also be covered somewhere, not sure what book. And nonparametrics.",NOTWorthless,2014-11-05 11:00:45
"I will second Billingsley for Measure Theory.

Elements of Statistical Learning is [available online for free](http://statweb.stanford.edu/~tibs/ElemStatLearn/). I must admit that I have not read it yet.

Programming is probably best learned outside a book, but I agree that everyone needs to be proficient in at least one language (preferably multiple). 

What book do you think is a *must read* in nonparametrics? I have not read one that I am in love with.",DrGar,2014-11-05 11:06:08
"I think for nonparametrics

*All of Nonparametric Statistics*, by Larry Wasserman

is reasonable; its sister *All of Statistics* is also good from what I've heard. AONS is  missing a lot of stuff - in particular classical (rank-based), semiparametric, and Bayesian methods -  but that is probably unavoidable for this topic. ",NOTWorthless,2014-11-05 11:30:04
"There's a chapter or two on Bayesian Nonparametrics in Wasserman's upcoming book on Statistical Machine Learning.

Should be a good book when it comes out",1337bruin,2014-11-06 09:07:09
"A more advanced book, which I haven't read but was recommended by Michael Jordan, is 

*Introduction to Nonparametric Estimation*, by Tsybakov

I also forgot about asymptotics:

*Asymptotic Statistics*, by Aad van der Vaart. ",NOTWorthless,2014-11-05 11:33:37
"Ya, in general I have just had trouble finding a non-parametrics book that really does-it-all. Probably the nature of the material as you note.

I have not read van der Vaary. I like this one for asymptotics:

Anirban DasGupta, Asymptotic Theory of Statistics and Probability

It is more of a high level survey, but it has a nice broad coverage of topics and an easy-to-read style.",DrGar,2014-11-05 11:51:50
"Serious question: I don't know anything about measure theory, and I've never felt like I've missed it.  What could I accomplish if I understood it better that I can't already do?",DavidJayHarris,2014-11-05 17:15:11
"It depends on what level you want to understand things on. You can use tools from, say, empirical process theory to construct intervals in survival analysis, but without measure theory you won't understand why it works. Which might be fine, but I think one should know why the tools they use work. ",NOTWorthless,2014-11-05 17:29:27
[A Conversation with George Box](http://www.jstor.org/stable/2245757),psychometry,2014-11-05 11:15:49
"[A free pdf link](http://projecteuclid.org/euclid.ss/1177013223). ~~Have not read it, so I am reading it right now!~~ edit: Definitely a worthwhile read.",DrGar,2014-11-05 11:20:54
"Shao's text should be listed under Mathematical Statistics.  It's certainly not a measure theory text.  I'm not sure I would consider it a must have for Math Stat, certainly not the first book I would buy for a first-year graduate course.

""The Statistical Analysis of Failure Time Data"" (Kalbfleish and Prentice) is a standard survival text. 

""Counting processes and survival analysis"" (Fleming and Harrington) is a standard reference for the survival proofs with martingales.

""Approximation theorems of mathematical statistics"" (Serfling) is another good asymptotic reference, although a bit dated in a few of the later chapters, where we would now do things with Hadamard differentiability, which hadn't been popularized when the book was written.

Probably a testament to the last two is they're both 35 years old, but still selling for about $100 on book sites, so no one is dumping their old copies.",,2014-11-05 18:32:04
"Shao is definitely not a first year book (not even a master's level book), but definitely a book I think all PhD's should own. It discusses many of the advanced topics overall, and does so in a very rigorous fashion. 

That being said, Serfling's book is freaking fantastic!",Jimmy_Goose,2014-11-05 19:17:41
"https://www.stat.tamu.edu/~suhasini/teaching613/expected_observed_information78.pdf

Also, everyone should read a computational stats book. I like the one by Givens and Hoeting

Measure theoretic probability if you are getting a PhD.


Edit: Change MTP with measure theoretic statistics, like Jun Shao.",Jimmy_Goose,2014-11-05 12:27:41
"I am unfamiliar with Givens and Hoeting. Is it a book you think everyone should read, or more of a luke-warm recommendation based on the fact that people should learn bootstrap/MCMC, etc?",DrGar,2014-11-05 12:51:29
"Here is the [website](http://www.stat.colostate.edu/computationalstatistics/) for the book.

I think it is a good book that covers most topics that need to be taught in a computational course (the only thing I think it should also include is numerical linear algebra.) Also the website for it is second to none.

But every statistician should be familiar with not just MCMC and bootstrap (which they should fucking learn if they get a degree in stats). Also basic numerical analysis should be required for all statisticians, such as optimization, smoothing and numeric integration. 


Here is also a [paper](http://www.math.ttu.edu/~atrindad/stat5371/Hoeting-Givens.pdf) by them on how to display results. ",Jimmy_Goose,2014-11-05 13:07:45
"One that *always* sits on my desk: Hair, Black, Babin &amp; Anderson ""Multivariate Data Analysis"". Focused on data prep, model diagnostics, interpretation. It's organized in a 'step-by-step' manner for all the major multivariate techniques. Pretty awesome. Easy to find online",nerdsarepeopletoo,2014-11-05 12:57:19
"Steep pricing makes it good that it is online. Looking through it, seems like a good reference.",DrGar,2014-11-05 13:03:49
"Agreed. I ordered it from China for about 60$ after shipping... The cover is in Chinese and the paper is almost translucently-thin newsprint, but, hey, 60$",nerdsarepeopletoo,2014-11-05 13:09:00
"I'm surprised to not see anything on experimental design or observational studies.  I don't have any great books for experimental design, but this one is pretty good for observational study methodology: [Design of Observational Studies](http://www.amazon.com/Design-Observational-Studies-Springer-Statistics/dp/1441912126/ref=sr_1_48?s=books&amp;ie=UTF8&amp;qid=1387251803&amp;sr=1-48) by Rosenbaum.",Luonnon,2014-11-05 18:08:18
"I have enjoyed a bunch of Rosenbaum's papers, and so I have this book but am yet to read it. Definitely on my to-read list.

Unfortunately, I do not have a ""must read"" suggestion for experimental design that only focuses on that. I really enjoyed Spall's ""Introduction to Stochastic Search and Optimization"" which does cover experimental design and a host of other topics. All around a fantastic book, but I do not think that every statistician has to read it.",DrGar,2014-11-05 21:21:43
"Not focused especially on statistics, but ""thinking fast and slow"" is a must read (http://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow) and demonstrates why and how statistical reasonning is not common sense!!",CalvaireEtLutin,2014-11-06 00:14:04
Looks interesting as a popular science book that covers aspects of statistics. Always looking for a good airplane book :-),DrGar,2014-11-06 10:50:00
"Maybe not as serious as some people prefer but I really liked Shmueli's book on data mining.

http://www.amazon.com/Data-Mining-Business-Intelligence-Applications/dp/0470526823/ref=sr_1_1?ie=UTF8&amp;qid=1415284973&amp;sr=8-1&amp;keywords=shmueli

I also enjoyed Innumeracy and How to Measure Anything - these are not rigorous at all but if you want to bring stats into the boardroom/conference room you better be able to translate things into digestible chunks for the lay audience. In my professional experience, the biggest challenge is not the math (that's the easy/fun part) it's explaining the math to people who think gut feelings are more useful.",dza76wutang,2014-11-06 06:45:37
"Ya probably not the staples of statistical analysis, but could be quite valuable for people who are in the real world and need to talk to non-academics :-)",DrGar,2014-11-06 11:23:12
All of Statistics by Larry Wasserman. ,rosenjcb,2014-11-06 08:41:17
"Do you think every professional statistician should read this, or is it a good introductory overview for early stage graduate students? I just have a hard time thinking it would go into the depth of the other specialty books on the list.",DrGar,2014-11-06 11:02:24
"It's a nice refresher for any statistician as it goes over modern statistics very well (not getting bogged down by earlier, historical statistical methods). It also takes the time to logically explain the statistical processes and discipline in general. ",rosenjcb,2014-11-06 11:14:29
"I'm little late to the party but what order (if any) would you recommend reading these for a beginning grad student? I'm interested in expanding what I know and am learning now, but probably won't have time until after the semester, after which I'll have only finished Methods I and Theory I. I won't have a ton of background knowledge. Also would any of these be good to read before taking methods II and theory II?",buttfoot,2014-11-06 10:19:50
"I think it depends on your interests and background. 

However, number one on your list should be a mathematical statistics book (I personally suggest B&amp;D others have suggested C&amp;B, which is less dense to work through independently). 

After you get through math-stat, you are going to be more knowledgable and able to work through any of the other books. You will also have a good idea about what areas of statistics are most important and interesting to you. Really the order you tackle the others should be dictated by the problems you want to solve. The top foundational items (which would make a natural next step after math-stat), I think, would be regression, classification, asymptotics, and bayesian.",DrGar,2014-11-06 10:57:49
"Kutner et al: *Applied Linear Statistical Models*

Resnick: *Adventures in Stochastic Processes*

Tabachnick &amp; Fidell: *Using Multivariate Statistics*

Freedman: *Statistical Models: Theory &amp; Practice*

Kuhn &amp; Johnson: *Applied Predictive Modeling*

Dobson &amp; Barnett: *An Introduction to Generalized Linear Models*

Thompson: *Sampling*

Box, Hunter &amp; Hunter: *Statistics for Experimenters*

Schumway &amp; Stoffer: *Time Series Analysis and Its Applications*

Hollander, Wolfe &amp; Chicken: *Nonparametric Statistical Methods*

Robert &amp; Casella: *Monte Carlo Statistical Methods*

EDIT: Author name spelling.",srkiboy83,2014-11-07 01:14:33
"Aside from Kuhn and Johnson, these all appear to be missing from the list. Do you consider all of these ""must reads"" for all professionals, or are there any that are just suggested reads?",DrGar,2014-11-07 01:55:16
"I'd say they are must reads, especially since the list missed books on sampling, experimental design, linear models, GLMs, time series, and stochastic processes. 

Also, *Applied Predictive Modeling* isn't purely a regression text, it deals with classification just as much (if not more), as well as special topics related to predictive modeling.",srkiboy83,2014-11-07 02:20:32
"Tfw I'm the most knowledgeable person about statistics I know and I have read 0 of these books. Time to get reading! Although I still want to go with *Doing Bayesian Data Analysis: A Tutorial with R and BUGS* over Gelman et al because I want to do all the work in R. [The book itself has 51 reviews on Amazon](http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855), 44 of which are 5 stars, for a mean of 4.8. That seems very good.

Saved this thead for future reference. :)",Deleetdk,2014-11-05 20:18:15
"You are in for a treat; the ones that I have read on that list are all fantastic :-)

I think you are being downvoted since it is going to be extremely difficult for any book to displace Gelman (at least for me, and perhaps that is what others are responding to...I didn't downvote). That said, I do not have this book and it does appear to be quite popular, but it seems to perhaps occupy a different niche where it also teaches you R along the way. 

Definitely good for the person who needs to learn bayesian methods and how to program statistically. Personally, I never liked learning a language from a book, and since I already knew other languages I just tinkered around with R until I got the swing of it.",DrGar,2014-11-05 21:35:14
"No, I'm being down-voted because I wrote ""Tfw I'm the most knowledgeable person about statistics I know"", at which point they stopped reading further to see the next part was ""and I have read 0 of these books"", which completely changes the thrust of the message: from self-cocky to pointing out there is widespread ignorance of these books among my friends (and me).

I already know R, but if I want to use the knowledge in applied statistics with R, I will have to familiarize myself with the relevant packages. A good book doing both sounds very good to me.

[You can find the book here](http://gen.lib.rus.ec/search.php?req=Doing+Bayesian+Data+Analysis%3A+A+Tutorial+with+R+and+BUGS&amp;open=0&amp;view=simple&amp;column=def).",Deleetdk,2014-11-05 22:11:08
"If you're in any way a moderately experienced statistician, definitely go with Gelman!  I haven't read Kruschke's book, but I've read excerpts on his blog, along with his other blog posts, and skimmed the table of contents just now. Kruschke is good for a hand-holding tutorial of how to run jags and similar high-level programs. If you know R, you'll learn that in a week or so by just playing with stuff, looking up examples online etc. 

Gelman is a tutorial as well, but it's also very much a reference book. It covers the details of the basic mcmc algorithms (and I've found understanding the algorithms crucial for debugging more advanced models than glm's when using JAGS/STAN etc. ). It covers all sorts of parametrization tricks, a good overview of basic theoretical results, *and* it's got R code to play with as well! 

(As an aside, if you're starting out, I'd try and use STAN instead of JAGS. Not because it's faster, though that's sometimes true, but because it's about a million times easier to debug when your model doesn't run. You'll save yourself tons of headache! )",GrynetMolvin,2014-11-19 14:34:04
I'd replace Bickel and Doksum with Casella and Berger.... I think there are too many issues with the consistency in notation and the general flow from chapter to chapter in B&amp;D. C&amp;B is straightforward and much easier to follow. ,Distance_Runner,2014-11-05 11:23:10
"I too like C&amp;B, but personally prefer B&amp;D. I agree that it feels more dense to get through, but I personally felt like it gave deeper insights after slugging through it (and re-reading, and re-reading). Sometimes I think it is best to not breeze through things, since it can be easy to feel like you understand something but then realize later that you should have been forced to confront your confusion.

But I know a lot of people would agree with you and say C&amp;B&gt;B&amp;D. I don't expect everyone will agree with a single list :-)",DrGar,2014-11-05 11:27:59
"My extended edit apparently didn't go through, but what I said was along the lines of what you said. 

B&amp;D is more rigorous and *does* give a bit more depth than C&amp;B, and is all around more advanced than C&amp;B. We use B&amp;D in my program's advanced theory class, but work through C&amp;B in it's entirety for our theory 1 and 2 courses. My problem with B&amp;D isn't how dense it is, but the fact I find it overly wordy and more confusing than it needs to be. I've been lost reading through it a number of times not because the material was beyond my comprehension, but because I didn't understand the notation they used for a particular example/problem, because it differed from what was previously used on earlier pages. There are also far more typos in the book that I notice... and to B&amp;D's credit, I used the second most recent and *not* the most recent edition, so a lot of this may have been fixed. 

With all that said, when I'm faced with a theory question, I turn to C&amp;B first and almost always will I find the concise answer I'm looking for. To be fair, this is possibly because I've gone through both and only need the general outline to jog my memory, rather than the in depth explanation. 

TL;DR - Both are great books and have their unique benefits, and both are worth having as resources",Distance_Runner,2014-11-05 14:51:50
"Ahh, yes I use the 2nd edition updated printing, and did not notice an inordinate number of typos...but there is probably a reason they made an updated printing. I could imagine typos making it much, much more difficult to slog through.",DrGar,2014-11-05 15:02:46
"Aside from computer-science inspired issues of algorithmic and architectural efficiency, what *didn't* statistics teach you about big data analysis?",quaternion,2014-05-22 10:07:40
"4 and 5 were always my first tips to students who I taught. Play around with the data--plot it, run basic frequencies, look at outlier cases--before running any automated analyses. You'll get more comfortable looking at big data sets this way and develop a feel for which analyses are most appropriate (and why).",TheShittyBeatles,2014-05-22 11:12:29
"I agree, but how does that fit in with hypothesis testing? Specifically, it is drilled that you determine your model before you begin the study? Or are the rules sort of different when you're looking at prospective studies versus large data sets for retrospective studies?",skevimc,2014-05-22 12:49:35
"You've hit on a tricky problem here, one that there are a lot of mixed messages on. In my practice, I always try to encourage students or researchers to pre-specify *at least* a primary hypothesis or (few) hypotheses. Or, to be more pragmatic about it (and this is something I see many successful PIs do), plan the potential papers you will get from a research project / dataset in advance, which will lead naturally to a handful of pre-specified ""main"" hypotheses.

Speaking more generally, and in my subjective opinion, there's a useful distinction between what you might call exploratory or hypothesis-generating studies on one hand. And hard and fast hypothesis-confirming studies on the other hand.

The former are common in academic biomedical research, in epidemiology (particularly nutrition, public health, many ""big science"" or ""omics"" areas). In these sorts of studies you are searching for relationships, p-values are less important than effect sizes, and interesting hypotheses are explored further following a confirmation-by-replication paradigm.

The latter type are where clinical trials fall. The gold standard for evidence. here you want a definitive yes/no decision on a point of critical (usually clinical) importance. And you go to every length possible to guarantee that nothing compromises your type I error rate, or your power.

Clearly there are gradations between these extremes, and not all studies will naturally fall into one or the other.

To answer your question about pro- versus retrospective studies, I think that's a little bit of a different argument. Clinical trials are prospective because that's the cleanest design choice, but if you are exploring hypotheses you could use either strategy.",red_concrete,2014-05-22 15:23:19
"Oh, sorry, I should have specified. Not hypothesis testing. Datamining and survey research.",TheShittyBeatles,2014-05-22 12:56:29
"My general feel, and I'm not sure how others feel about this, is that it is okay to use the data to refine a model if it tells you something you ""should have known"" a priori. That is, if the data suggests some interesting, unexpected, hypothesis then this shouldn't be analyzed beyond an exploratory setting usually, whereas if the data says the data is blatantly heteroskedastic and you had no reason to expect otherwise but assumed homoskedastic for simplicitly, then this is okay to correct. Sometimes you can almost justify this by saying ""if I had put an appropriate prior on the model space I'm certain based on the diagnostics I'm seeing that the posterior would concentrate on heteroskedastic models.""",NOTWorthless,2014-05-23 07:43:10
"WHen in industry, confounding variables will screw your happiness. It is embarrassing to stand in front of a VP showing your analysis only to have them point out potential confounding variables in your analysis. ",,2014-05-22 20:16:51
"One of the best points of this post is number 10. I have seen it happen so many times, and I have also found myself guilty of it.

It's always tempting to take a set of tools, methods, and ideas that you are comfortable with, and just apply them, as if they were your hammer and everything you saw were a nail.

I found it very important to take a step back, and really think whether the problem fits the tools, and not the other way around, and if it does not, then it's time to search for new tools, or create new ones.",derzelas,2014-05-22 21:23:57
"Nice article, and balanced.

It occurs to me that there is some danger of overdoing the backlash against Big Data, given the tone of some of the recent articles - in the popular press anyway. It has to be remembered that vast amounts of data usually do (when treated with respect) allow for new and interesting inference. A lot of what is now being called hyperbole, may well be accurate, and I think most statisticians would agree with ""data scientists"" on that. But it needs serious and cautious analysis.
",red_concrete,2014-05-07 09:39:50
Big data is just a method of systematic collection of information. It does not turn it into knowledge just by sitting there. The methods of analysis are not different just because there's more to do so statisticians are more relevant than ever.,canteloupy,2014-05-08 01:39:45
"&gt;Given the importance of statistical thinking why aren't statisticians involved in these initiatives?

Because a bunch of people who have no academic training in related areas are calling themselves ""data scientists"" and suffering from confirmation bias when they fail to properly account for said issues. 

There is huge value in all of these things, but too many people are wielding a powerful tool with no real training. ",EdwardRaff,2014-05-07 20:10:09
The table from the alumni magazine... lol. There's no way that was real.,shaggorama,2014-05-07 11:08:49
"Pardon me for being green, but I just got in my machine learning research program.  How the hell can you not do statistics and claim to have something valid? I just built a Stacked Auto encoder and testing the result kicked the living shit out of me.. And I have a BS in Mathematics... With a concentration in statistics!

EDIT: Whoops. I thought this was /r/MachineLearning .  Ive been up for a while doing what else... statistics. ",StannisBaratheon_AMA,2014-05-07 09:56:38
Could you please expand on how the testing kicked your butt? What kind of math/stats was involved?,reallyserious,2014-05-07 11:36:20
"Well, it was my first, out in the wild, on my own experimentation/validation. I inherited a project and code from a PhD graduate that contained zero documentation of experiment procedures and it was an giant mess. There were literally hundreds of measures/scores with no indication of what was being measured or observed.  So, I had to ""reverse engineer"" the tests (from code to theory rather than the other way around), make sure what I was doing even mattered There were tons of useless junk files (""file1.txt, file2.m), tons of useless variables (a, ba,c)  plus Im brand new to machine learning and it's pretty broad Then, as if to mock me, I was told I had to write a paper on my findings in a few weeks.

WHAT!?

So I broke the work into 3 spaces. Data-processing, computation, and testing. Bought an Mathematical Neural Network Analysis book and got busy. 

It's not that the statistics was the worst part, but I dont go around claiming this and that about some data set unless I know my methods were pure and my theory was sound. It was just that extra thorn in my side that wouldnt have been there if done right the first time. 

Oh boy. Im ranting now... Anyway, happy ending, they were impressed enough by my work to get me a year-round full fellowship.

",StannisBaratheon_AMA,2014-05-07 16:42:34
"My guess would be some kind of cross validation.

",Case_Control,2014-05-07 13:12:10
"Our B Sc program has a brand new quantitative methods class that features a brief introduction to data mining by looking at clustering (hierarchical clustering, k-means, etc.). We are very careful to point out that these are not strictly statistical techniques as there is no uncertainty associated with the outputs, and that statistics is the science of dealing with uncertainty.

I enjoy reading Leek's blog and am glad to see that the limitations of Big Data are getting some attention, even if only for the reason that I don't want to see statistics replaced by something with a foundation that doesn't respect uncertainty.",samclifford,2014-05-07 16:26:35
"This is an editorial by the ASA president from about a year ago in Amstat News. I think it addresses some of these issues, particularly why statisticians are routinely being left out of the development of ""data science.""

http://magazine.amstat.org/blog/2013/07/01/datascience/",CommentSense,2014-05-07 23:41:15
"I am by no means an expert, but I get the sense there is somewhat of a misunderstanding about what big data means, even by those batting around the term. My understanding is that big data is not big data sets that are then evaluated, but rather real time big data flows in which patterns are sought; the distinction being that the size of the former is still even far smaller than the latter. 

If anything I think statisticians should maybe be considering doing research into how to apply statistical concepts to big data or discover new methods altogether.",NetPotionNr9,2014-05-08 04:26:03
"Big Data's take off was the ability to scale accurate user account reconciliations out of the hundreds of millions into the billions.  Most everything now associated with ""Big Data""  is seduction into the brute force crunching of more numbers with the same old techniques on more data and eventually less understanding of whats needed to accurately assess the data.  I include myself in the lot of those who do not completely understand the implications of all applied techniques to the data.",3pence,2014-05-07 16:01:40
"Here's the thing. The problem isn't the lack of statisticians involved in these projects. It's that those publications with disproportionate claims are not studying the data, as the author says they aren't asking questions. One can find any number of statistical tests that aren't inappropriate, but which do not pick at and probe that unique particular data set. I believe those people who belong to the same field in which the data was generated are in an even better position to determine biases and loose ends. The problem is they aren't doing it, who knows why. It seems odd to me that the group closest to the work doesn't ask the right questions. I suspect in these large scale cases it is lab politics and desire for fame not ineptness that results in oversight. ",zayats,2014-05-07 18:14:25
R user here. There was not a statistic package with python before? ,antblazer,2014-10-08 13:19:58
"Indeed there was not. Most users relied on Numpy. However, according to the very interesting [PEP450](http://legacy.python.org/dev/peps/pep-0450/), this new module is not meant to replace Numpy:
&gt;Q: Is this supposed to replace numpy?
&gt;A: No. While it is likely to grow over the years (see open issues below) it is not aimed to replace, or even compete directly with, numpy. Numpy is a full-featured numeric library aimed at professionals, the nuclear reactor of numeric libraries in the Python ecosystem. This is just a battery, as in ""batteries included"", and is aimed at an intermediate level somewhere between ""use numpy"" and ""roll your own version"".",andrew771,2014-10-08 13:28:00
scipy has some statistical tools that are not available in numpy as well.,squidgyhead,2014-10-08 13:45:46
"Since we're on this topic, what functionality is in NumPy and SciPy, respectively? What is the difference? How do the two libraries relate to each other? Is there any overlap? Does one build upon the other?",Bromskloss,2014-10-08 15:08:40
"Numpy provides fast multidimensional arrays, masked arrays (like NAs in R), and a bunch of operations on them. It also provides some random number generators and some assorted math stuff -- FFTs, vectorized trig functions and the like, and basic sample statistics like the Python module.

Scipy provides optimization, interpolation, loads of statistics functions, integration, image processing, special functions, and a bunch of other stuff.

You can compare their library references to get a feel for their coverage:

http://docs.scipy.org/doc/numpy/reference/index.html  
http://docs.scipy.org/doc/scipy/reference/index.html",capnrefsmmat,2014-10-08 16:05:47
"Is it fair to say that SciPy is more high-level than NumPy, and perhaps builds upon it, or does that not paint the correct picture?",Bromskloss,2014-10-08 17:15:12
"Yeah, that's fair. Scipy extensively uses the multidimensional arrays provided by Numpy.",capnrefsmmat,2014-10-08 17:17:49
This is wrong. Pystatsmodels was there as was scipy stats.,,2014-10-08 19:18:38
"As opposed to `statistics` module, neither of them was part of standard library.

But I wouldn't make a big deal out of this, as standard library can never cover all use-cases and everyone has to install additional modules/plugins sooner or later. Installing one more for statistics don't really matter.",mzalewski,2014-10-09 01:16:35
"Plenty (particularly stats_models, and parts of scipy). The difference here, is that this stats package is included in the default installation of python.",motorcyclesarejets,2014-10-08 13:25:44
As an advanced R user who has somehow always been able to work everything I need out in R... Do I need to learn python now? Is this going to be a thing?,artisanalpotato,2014-10-08 17:34:42
"This module is not going to be a thing for next couple of releases, if ever.

From even the most favorable point of view this new module is a stub. It provides only ten functions and covers a fraction of good old Microsoft Excel functionality. It's not even meaningful to compare it with other Python modules providing statistical functions (Pystatsmodels, pynum, pysci) or R.

For me, this new release has much stronger symbolic meaning than practical one. It is a sign that core Python developers acknowledge importance of language in science- and statistics-related fields and are willing to work towards making language better suited for these particular use-cases. In future they may provide something that will make Python even more important/useful than it already is.

So, this new module certainly does not **force** you to learn Python in order to stay competitive in your field, whatever you are doing. But there are some benefits in learning another programming language in general and Python in particular, so learning some basic Python right now might be a good idea anyway.",mzalewski,2014-10-09 01:45:57
It's good to know a bit. At the very least you should be able to read Python code.,towerofterror,2014-10-08 19:00:08
Pretty much every R job I came across also wanted Python.,,2014-10-08 19:19:22
Where have you found R jobs?  All I've found is SAS.,beaverteeth92,2014-10-08 19:56:53
"Data Science is a popular title for a job that includes R and python.

Edit: I would do searches for terms such as high dimensional or things of that nature. SAS will likely not be as important for these jobs.",,2014-10-08 23:04:56
"Thanks!  Would I also be expected to know C, C++, and/or Fortran?",beaverteeth92,2014-10-09 04:48:11
Maybe C++,towerofterror,2014-10-09 05:47:10
I would say you are safe with R and python but I am no expert. Look at the jobs that you would like and see for yourself. ,,2014-10-09 09:30:57
Indeed has thousands.,,2014-10-08 19:59:36
Thanks!,beaverteeth92,2014-10-08 20:58:35
They go really well together.,,2014-10-08 21:57:37
"It's positive from the 'batteries' included perspective, but most data science/statistics professionals will continue to rely on numpy and assorted scipy/pandas for their work.

So by all means come and learn python it's awesome. But this won't matter much for that decision.",ramblerman,2014-10-08 22:22:07
This is completely unimportant on a practical level to people who use statistics in their work. All of these functions are slow because they work on Python objects (which are very high-level) instead of e.g. Numpy arrays which don't have the same overhead. ,typographicalerror,2014-10-09 09:59:17
I meant people using python rather than R or similar. ,artisanalpotato,2014-10-09 10:08:20
"Well sure, but my point was that from a statistics perspective, this shouldn't change anything in re: learning Python. This isn't the change that is going to push it over the edge for you.

OTOH, I do data science in Python daily and it is pretty spiffy, so it's already there for me.",typographicalerror,2014-10-09 11:13:46
I understand your position. ,artisanalpotato,2014-10-09 19:41:30
"Statistics noob here.  I can't figure out--what's the difference between the sample measure of variance/deviation and the population?  Aren't they just the same calculation on different sets of numbers?  (I.e., why two separate functions?)",jgo3,2014-10-09 05:53:49
the difference is dividing by n vs. dividing by n-1,1337bruin,2014-10-09 08:14:11
Aha!  Now I remember reading that somewhere.  Thank you.,jgo3,2014-10-09 08:18:15
"One of my classmates invented the MCMC Bar Crawl:

&gt; The MCMC Bar Crawl* (a.k.a. Barkov Chain Monte Crawlo) is simple:
&gt;
&gt; 1. We randomly propose a nearby bar to visit
&gt;
&gt; 2. We vote: how many people like that bar better than where we are now?
&gt;
&gt; 3. If it's not unanimous, roll a die to see whether we stay here or move there
&gt;
&gt; 4. Have a drink and repeat

&gt; \* Basically a [Metropolis sampler](https://en.wikipedia.org/wiki/Metropolis_sampling) from the multinomial distribution on our bar preferences.",capnrefsmmat,2014-05-23 10:29:31
"Wait, so under what outcome conditions of the die roll do you move or stay?",Neurokeen,2014-05-23 15:54:50
"Suppose you have 6 people, 4 want to stay, I presume you move if you roll 4 or lower. The idea is you move with a probability equal to vote percentage. 

EDIT: should be ""4 want to move...""",giziti,2014-05-23 16:08:12
"So you'd need a die with as many faces or some multiple of the number of people. Got it.

By the way, if 4 want to stay, move on a 4 or lower? It sounds like it should be the other way around, moving in proportion to the number of people that want to move, as opposed to moving in proportion to the number of people that want to stay, shouldn't it?",Neurokeen,2014-05-23 16:22:59
"Yes, it should be the other way around. I started my random walk early this evening, IFKWIM.",giziti,2014-05-23 16:30:35
"&gt; So you'd need a die with as many faces or some multiple of the number of people. Got it.

Or just simplify the vote ratio down to sixths.
",person9080,2014-05-23 18:10:48
Or get a die with many faces and re-roll if the die shows a number greater than the number of people in the party.,samclifford,2014-05-23 19:16:26
One more reason to keep a d20 on hand at all times.,Neurokeen,2014-05-23 19:26:21
"No need, my phone has Python.

    from random import random
    6*random() &lt; 4",mszegedy,2014-05-24 00:15:24
"If you have access to a good random number generator, just generate a uniform(0,1) and use it directly instead of turning it into a dice roll!",giziti,2014-05-24 09:34:28
"If you have a d20, it's better to just use it as an RNG with resolution of only .05. So if you have, say, 7 people and 4 want to jump, you really want a draw from a uniform(0,1) and move if you get less than 0.5714. So you move if you roll 11 or lower. You're off by 0.0214, which is probably good enough for a drinking game.",giziti,2014-05-24 09:33:48
Of course not formally right b/c the density of % preference is not in there (what if 30% like almost every bar vs. 30% like 30% of the bars) -- so you need the vomit-inducing burn-in period to then move to the crawl.,fredit,2014-05-24 12:24:30
"You should also run a few other chains in parallel so you can do convergence diagnostics. 

Or be a heretic and subscribe to the ""one long chain, no burn-in"" philosophy. I think a little heterodoxy is in order for this application.",giziti,2014-05-24 14:07:33
This is golden.,not_really_redditing,2014-05-23 12:37:43
This thread has made me realize there is an association between drinking and people of statistics,Corruptionss,2014-05-23 12:41:55
"correlation =/= intoxication

But in this case, there's a n=95% chance it does!",wildncrazyguy,2014-05-23 23:48:35
I like your style,Corruptionss,2014-05-24 00:17:19
"Strange, out of academic fields I've only seen this preconception for geologists.",mszegedy,2014-05-24 00:17:45
And psychologist,Corruptionss,2014-05-24 00:22:57
Physicists drink a lot,1337bruin,2014-05-24 11:04:29
"I would advise against ever doing a bootstrap drinking game.

Edit: A drinking game based on the exponential distribution. Keep drinking until you obtain the memoryless property.",Jimmy_Goose,2014-05-24 06:59:33
"Actually though, does anyone have any real statistics drinking games?",CHAINSAW_VASECTOMY,2014-05-23 09:22:55
Choose a reasonable lambda and take shots at intervals sampled from a Poisson distribution? ,WallyMetropolis,2014-05-23 09:57:53
I think you need some kind of mixing distribution to make that interesting.,giziti,2014-05-23 12:13:13
"Sometimes, I like to do statistics and then go home and drink. It's a game because there is a finite supply of beer in my fridge, and it eventually all moves from the bottles to my stomach, which is zero-sum. A few colleagues have argued that it is not truly zero-sum because there are a variety of ways to calculate beer utility, but eventually I have all the beer and the bottles are all empty, so it counts.",Cammorak,2014-05-23 11:06:32
"We could invent some. How about a bartender challenge. freehand pour five shots in five seconds. The one whose average pour is farthest away from the ideal 1oz has to drink a shot.

Or, you could watch the news and every time statistics were used inappropriately you take a shot. Should get drunk pretty quick that way.",radiantthought,2014-05-23 09:28:49
"&gt;Should get alcohol poisoning pretty quick that way.
",not_really_redditing,2014-05-23 12:36:52
The mental image of progressively more intoxicated people attempting to measure shots in fractions of ounces has me chuckling.,Ragnar32,2014-05-23 19:24:47
This ones for all the college students out there. Take a shot every time during lecture you wonder if your Stat professor is playing as well. ,StannisBaratheon_AMA,2014-05-23 10:23:38
"I've only ever played an evolution drinking game wherein you flip to a random page in Darwin's Origin of Species, put your finger on a page, and if you can't read the sentence your finger lands on in one breath, you have to take a shot. There is, naturally, some variation among people in their breath capacity, so some people get pretty trashed and others are fairly sober. Still pretty fun though.",ScienceIsMetal,2014-05-23 10:56:43
This is what you get when you don't use an appropriate loss function when making decisions..,Coffee2theorems,2014-05-23 12:48:02
"Apparently, statisticians are lightweights.",BoothTime,2014-05-23 22:06:31
"Check your hetero(skadisty) privilege

Edit:

Also, check your assumptions!
",chaoticneutral,2014-06-12 04:52:25
Sorry in advance for this: *hetero(skedasticity)*  ,isarl,2014-06-12 10:04:23
[deleted],,2014-06-12 10:50:20
I dont think OP's mistake was just one letter.,honeyplease,2014-06-12 14:44:52
"A more detailed statistical walk through with output and R code found here:

http://rpubs.com/oharar/19171",Adamworks,2014-06-12 07:37:16
"&gt; the authors have data, and they were kind enough to make that data publicly accessible to the unwashed masses (even though the paper itself is paywalled).

Well, la di da. Look who has enough grant funding to publish open access.",log_2,2014-06-12 01:47:43
"or the institution has an OA fund, which is not that uncommon.",rottenborough,2014-06-12 11:44:48
"I don't think their initial residuals are that bad, nor are their new residuals that much of an improvement.  Certainly not enough for the snark displayed here.",FullSharkAlligator,2014-06-12 09:46:07
"The idea though, and the causal model *are* deserving of snark.",Icamehereto__,2014-06-12 11:10:28
"You may very well persuade yourself to ignore the assumption violation if you have a very clear and strong effect.

In this case, however, the result was .073 ""almost significant."" It's takes a lot of faith to accept the model as anything but meaningless.",rottenborough,2014-06-12 12:00:28
[I love it when researchers do that](http://mchankins.wordpress.com/2013/04/21/still-not-significant-2/).,beaverteeth92,2014-06-14 20:30:04
I have to agree.  Their initial residuals aren't terrible and seem to be skewed by three outlier points.,beaverteeth92,2014-06-14 20:29:15
ELI5? What's the fitted model?,altCognito,2014-06-12 05:31:04
"Basically, the residuals were never checked. When you fit a model, you will always get some error (residuals), no model is perfect. 

What you like to see in your residuals are a consistent level of incorrectness. Predicting your value of 1 death vs. 1,000 deaths, should on average have the same level of error in order to be a properly fitted model. If you see that your model does better at smaller estimates and gets worse at large estimates then you know you are not accounting for something.

The original researcher's model was effectively trying to draw a straight line through a curved dataset. It over estimated in some spots, under estimated in other. When people actually accounted for the non-linear behavior of the data, the relationship between gender of the storm name and damage disappeared.

",Adamworks,2014-06-12 08:59:23
This helps too thank you.,altCognito,2014-06-12 10:29:16
"Search for regression analysis, it is basically a model that tries to ""fit' the data as good as possible, reducing the (sum of squared) errors from each datapoint to (usually) a line.",honeyplease,2014-06-12 08:12:25
"Thanks, I guess that will do. I didn't realize that umn, questions were off topic here. :)",altCognito,2014-06-12 08:18:56
"You're welcome, although to be fair, your question is like asking ""what is a stove"" in a cooking forum. Good luck.",honeyplease,2014-06-12 09:01:15
"Very fair. My defense is that this particular article seems written to a general audience (it's on the Guardian, albeit in the Science section) and regression analysis might be common within STEM graduates it is not within the general population. It seemed like the article was speaking to the general audience for 50% and the other 50% it made fairly significant assumptions about what people knew about regression analysis and how to build a model. It's a weak defense, but that's what I'm going on here. ",altCognito,2014-06-12 10:27:19
"The title should be ""herricanes"" and ""himmicanes"". Hurricane is gender neutral. ",FakeBabyAlpaca,2014-06-12 06:09:07
"""Hurricane"" is pronounced more or less like ""her-icane"". I would pronounce the first part of ""herricane"" like that of ""herring"".",standard_error,2014-06-12 07:23:28
I got the pun right off the bat. I'm smert!,vriemeister,2014-06-12 09:59:52
[another critique](http://www.slate.com/blogs/future_tense/2014/06/03/are_hurricanes_named_after_women_more_dangerous_not_so_fast.html),dmlane,2014-06-13 06:52:20
"Very interesting stuff. Even though the psychology part was more intriguing than the statistics aspect;  there wasn't much of the latter.

Thanks for posting.",CommentSense,2012-02-16 21:42:31
"I thought that's what all companies with a loyalty program would do. They give away cashback/points to customers everytime they use their loyalty card. From this, the companies can analyse the customers' spending habits.

If you don't want to be tracked, I think the best way is just to pay cash. Even credit cards track their customer's spending.",randombabble,2012-02-17 03:57:40
But does it only work if the woman knows that she pregnant? I mean it would be more impressive it can tell something before it's known to the person.,picu,2012-02-17 07:18:32
I've heard that credit card companies can predict that people will divorce before they know it themselves.,paulginz,2012-02-18 03:40:22
"I an see this unfold, a marriage registry is the first lead...",picu,2012-02-20 07:59:23
"They can tell way more then just the easy stuff like that. Grocery stores can tell what kind of social segment you fit into easily with your grocery purchases, they can extrapolate from there to your income, distance to store (and thus relative address), health status etc.

When you sign up for their rewards program they pretty much tell you they are going to use it for this kind of stuff. Read the contracts and ask for information if you want. 

",icelandica,2012-02-17 14:43:19
"Meanwhile, my grocery store definitely can't figure out this kind of information. 

The week my baby was born, I bought a thing of powdered formula from the grocery store. I swiped my ""bonus"" card during the purchase. It's the only time I've ever bought any baby stuff at that store (we usually order from amazon). 

Since then, every time I'm in that store and I swipe my card, I get a coupon for baby forumula. The thing is--my daughter is almost 1.5 years old at this point! Stop giving me formula coupons!!!",AstroZombieDC,2012-02-21 06:50:55
This activity needs to be regulated in my opinion.  ,jmdugan,2012-02-16 22:33:23
How do you regulate the way a company can look at its own data?,autoencoder,2012-02-17 01:25:41
"We can regulate the way companies use their data - I worked on predictive analytics for credit cards (we had tonnns of data) and we were very regulated in what we could do. I think too much regulation is bad, and I think most companies agree they'd rather avoid it coming to that. In this case, it seems like Target realized they were getting close to a line and pulled back. ",rm999,2012-02-17 13:10:11
"You can, although if you should is another issue.

They do however speak a lot about the buying information on people from other sources, reselling of information should definately be regulated, they are in a sense distributing my intellectual property without my explicit consent in a lot of cases.",nebbugvrok,2012-02-17 23:17:16
"Well I think it's not ""their data"", really.  What I want is transparency, that if they catalog people and build statistical inference on their behaviors, then it needs to be made public.  

I'm not saying they can't, or that they must not have the data or even that they shouldn't do it--none of that.  The techniques are just so powerful and so invasive that people need to know it's happening.",jmdugan,2012-02-17 05:53:11
Where is the line though? What if I don't work for Target and I just sit by the register and take a list of what everyone is buying? Should that be regulated?,Drunken_Economist,2012-02-17 06:34:05
I think a less strict version of HIPAA and whatever the financial correlate is will eventually be put into place on the retail end. Businesses and the US Government have a lot of experience with data &amp; privacy regulations ,TheFrigginArchitect,2012-02-17 11:38:19
"You might do well to read about what goes on behind the scenes with profiling and human subject data. It's all very controlled and there are watchdogs who are eager to tackle anybody for multimillion dollar lawsuits at the slightest violation of these laws. I recommend starting with reading HIPAA. Of course you're being profiled, but the *worst* profiling happens on the net.",,2012-02-17 12:25:24
"Legal and moral arguments aside, let's look at the pragmatic aspect of your suggestion. How many companies collect such data? How much data is that per company? My company, for instance, has 1,300 employees and we process tens of thousands of transactions per day. We maintain *farms* of servers to store and manipulate that data. And that's just one mid-sized company out of millions.

Who collects the data? Who disseminates it? How much is kept anonymous? Who pays for the collection and dissemination? To whom do you provide access? Think about the implications of what you're suggesting! The whole exercise just sounds implausibly expensive and technocratic.",firstcity_thirdcoast,2012-02-17 07:25:05
"Im not saying a company would have to do that, in my vision they would just need to publish in clear detail what personal data they keep (in dattabase terms, the schema and what each column means), and what they do with people's personal data.

Yes, it's work, but frankly companies do a lot of work now and **it's mostly all selfish profiteering**.  REMEMBER, it's a privaledge for the state to use its guns to allow your company, so if it's doing things that can fuck people over, we regulate that behavior: pollution, fraud, and in this case broad privacy erosion.",jmdugan,2012-02-17 10:22:29
Why? ,monkeyslikebananas,2012-02-17 05:40:32
Answered above,jmdugan,2012-02-17 05:53:40
At first I was like. [And then I was like](http://gyazo.com/aadc24fdfd764a80f2bfae8c089a9ae1.png).,,2011-06-30 12:29:13
"The most important question about this is also the unanswered one: are the paper's results genuine, with the plagiarism limited only to ghostwritten phrases to wrap them up (in which case, the practice is dodgy but not obviously harmful) or are the results faked?

The article mentions that the literature looks real, which is a start in the right direction, but it would be easy to dredge up a few convincing references for whatever subject the analysis is meant to be about, without properly evaluating the body of research.",baetawolf,2014-10-07 00:42:04
"Hi, I am the author of the post. I agree that this is the most important question. It is difficult to answer it with high confidence. I checked that the cited literature exists and that the claims are real. However, I did not check whether they omitted some literature in order to boost the p-values. This is fairly easy to do for them, and hard to check for us. Another difficulty, is that they could be testing every predictor/condition pair systematically and report any significant association. They must find such correlations given sufficiently large n, but they would not necessarily hold up after multiple testing correction. Without more insight about how the people work, it is difficult to say whether the conclusions are valid.",gui11aume,2014-10-07 02:20:32
"I think that would be for an organism like the cochrane collaboration to check. Given the size of the problem (32 papers!) I dare say it would not be a waste of time for someone to check.

I am actually unemployed right now and have to find a project for a sponsored internship, maybe I should suggest something like that.",canteloupy,2014-10-07 06:21:33
It seems like they are rewriting the same paper over and over again,yen223,2014-10-07 01:24:39
It seems like they are writing the same paper over and over again. ,akimboslices,2014-10-07 02:20:28
"It seems like they are writing the same paper over and over and over again

",,2014-10-07 14:07:47
"See my answer to baetawolf, certain parts of the paper are always the same, but the results actually correspond to real literature.",gui11aume,2014-10-07 02:21:26
"http://www.reddit.com/r/MachineLearning/comments/2ijzaz/massive_scientific_plagiarism_detected_by/

A link to the repost at /r/machinelearning where the community managed to point out weaknesses in the approach and conclusions.",maxToTheJ,2014-10-07 15:11:41
"I can't see any such thing in this thread, just discussion of whether or not it's actually what you'd term plagiarism.",baetawolf,2014-10-07 15:42:39
"There was also a brief discussion of whether Wikipedia or the article came first, which was deleted because the comments were too 'heated'. The Wikipedia sentence was introduced in 2008, the papers were written in 2013.",gui11aume,2014-10-07 15:58:55
"&gt; just discussion of whether or not it's actually what you'd term plagiarism.

A crucial part of experimental design is figuring out if you are measuring the right thing. That is why the /r/machinelearning discussion is important.",maxToTheJ,2014-10-07 18:02:35
"Cool post, thanks for sharing.",sowenga,2014-10-07 07:53:34
"Reading this article, I really feel like the scientific papers are fraudulent in the sense that they give a totally misleading narrative of the processes of thought that go into the making of scientific discoveries.",ATG77,2014-10-07 05:25:33
I see what you did there,rmlrn,2014-10-07 09:12:07
"Chinese academics have resorted to [faking publications](http://www.nature.com/news/2010/100112/full/463142a.html) in the past, this comes as no surprise.",,2014-10-07 05:22:32
"I'm not surprised.  There is a website in China where you can latch yourself on as 1st, or other author to a high quality journal for a fee.  Everything in China was a scam.",anonemouse2010,2014-10-07 16:28:51
Thanks for this!,harDCore182,2015-03-08 20:53:20
Awesome!,mguzmann,2015-03-09 02:07:41
Great resource!  Saving this!,teknodukk,2015-03-09 06:01:48
And the solutions to the exercises. Is this post basically a Masters in machine learning? What would be missing?,DavidFree,2014-09-24 09:30:50
"It's probably the equivalent of 25-50% of a decent one. Getting my masters in machine learning involved 4 machine learning classes, an algorithms class, and some basic CS stuff I probably could have skipped. Plus ~6 months of original research and a masters thesis. I'd say this book covers the more practical and introductory aspects of those classes, but not the deeper knowledge nor the ability to read academic papers nor the ability to conduct original research. ",rm999,2014-09-24 16:49:05
Theory and courses in data structures and algorithms.,beaverteeth92,2014-09-24 09:37:54
"And MIT OCW has some great classes on that, too!",brouwjon,2014-09-24 15:09:40
"Prerequisites from the course's web page are pretty simple: ""First level statistics, linear algebra and computing.""

There's not a lot of barriers to entry ",brouwjon,2014-09-24 15:17:46
"Linear regression, probability theory, and mathematical statistics should be fine.",beaverteeth92,2014-09-24 15:20:29
Did you go through the lectures?  This looks awesome. ,yhatter,2014-11-06 07:53:17
My current go to for a cheat sheet: http://cran.r-project.org/doc/contrib/Short-refcard.pdf,aftersox,2013-10-29 17:47:01
I like this one. :),kinross_19,2013-10-29 22:15:42
"Not sure if you're the author, but there's a typo where it says ""shaprio.test"". It should read ""shapiro.test"".",aguyfromucdavis,2013-10-29 13:51:50
"Woops, thanks.  Will change immediately.",16bitsquare,2013-10-29 15:05:30
"Awesome, thanks. Stuck on my cubicle",kimetric,2013-10-30 13:48:47
"Good Stuff, thanks for sharing!",Gymrat777,2013-10-29 11:38:38
What software did you use to produce this?,naught101,2013-10-29 23:13:51
Adobe Indesign,16bitsquare,2013-10-30 07:44:54
"One other helpful suggestion was sent over from a reader, so the cheat sheet has been updated a bit further again. ",16bitsquare,2013-10-30 14:26:12
"This is a famous data set used by Bortkiewicz in the 19th century to investigate the Poisson distribution.
http://en.wikipedia.org/wiki/Ladislaus_Bortkiewicz",2718282,2011-09-19 13:40:39
"This might seem random, but it was one of the most famous applications of the Poisson probability model in statistics. Just like the German Tank problem, it's worth knowing from an historical perspective: statistics was used for ""real-life"" problem solving and potentially saved hundreds of lives. (While this model wasn't used to *prevent* deaths from kicking, the military strategy is to dispatch enough soldiers to overwhelm the opponent's army, accounting for how many die *en route* from such causes).",,2011-09-19 12:24:16
"As soon as I start lecturing statistics one day, I will remember to use this specific dataset as an example of some technique whenever possible!",generaal_kosie,2011-09-19 14:49:40
I'm using this example when I teach the Poisson Distribution next week. ,Fordperfect42,2011-09-19 19:41:16
"Ha, I remember having to go through a tutorial using that dataset. ",,2011-09-19 11:29:51
From the pscl package.,mathguymike,2011-09-19 11:28:44
"The most random, nice one.",,2011-09-20 08:01:52
"You, me, and STAT 1010",,2011-09-19 17:17:31
"also referenced in agresti's categorical data analysis, problem 1.9. i had a giggle as well.",greensmurf30,2011-09-20 00:02:53
"You should code for intoxicants (alcohol, marijuana, etc.) and look for interactions.",Degovx,2014-12-03 22:35:08
Distribution of tip as fraction of subtotal?,eigma,2014-12-03 21:18:12
"Distribution of the ""Tip %"" column. Unclear if ""Order total"" includes tax or not. http://i.imgur.com/YPMD4n5.png

EDIT:

* Mean: 17.47%
* Median: 16.05%",shaggorama,2014-12-03 22:06:05
"Hmmm, there is something wrong with the lower tail. Are you sure you're not forgetting to count the time you had to pay your customers for the delivery? /s",Alhoshka,2014-12-04 00:16:09
here's the [spreadsheet](https://docs.google.com/spreadsheet/ccc?key=0AhK2XqilMNZbdGpXVV9jVEs5aml1dWNkTUY3eHpEWWc#gid=0) if you're interested in other information. ,timmylace,2014-12-03 21:26:14
"You asked everyone their age? And they gave it to you?

PS: This really would have been more appropriate in /r/dataisbeautiful",shaggorama,2014-12-03 21:58:48
I wrote in the graph it was an age estimation. I made an educated guess for each and every person,timmylace,2014-12-03 22:01:49
It's weird how precise some of your guesses are. 48? 31?,shaggorama,2014-12-03 22:15:44
I made my best guess possible for each person.,timmylace,2014-12-03 23:42:03
dat log-normal.,made_this_up_quick,2014-12-04 00:42:29
I was - temporarily - very impressed that you had kept records for more than 1000 pizzas that you had ordered.,epostma,2014-12-03 21:26:24
Received,timmylace,2014-12-03 21:27:47
"If you had geocoded the deliveries as well, you could have estimated and correlated socioeconomic factors to tipping.",AllanBz,2014-12-04 00:57:36
"I am recording similar data from my delivery job. I recorded addresses of each customer to know the distance from the restaurant and frequency with which they order, but it would really be a pain to have to individually map out the distance for each address. Do you happen to know an easier way I could do that for when I eventually submit it here?",predicate_logic,2014-12-04 19:53:23
"Sorry about the delay. Do you know how to program? I believe both Yahoo! And Google have an API for geographically locating street addresses, and perhaps estimating routing information for shortest drive. If you can't get route distances, just take the coordinates and get the distance as the crow flies. Alternatively, if your routes have common segments, you could get the segment lengths, categorize each address by its closest segment traversal or sets of traversals and fudge something for the leaf nodes.

Edit: added some thoughts.",AllanBz,2014-12-06 01:57:14
"Since you seem to be much more disciplined in recording your data than when I was a delivery driver, I have a hypothesis to suggest to you.  If you work the during the day, record the weather for each delivery, particularly if it is sunny, overcast, or raining.  You assume that people will tip you more when it is raining because of the extra inconvenience, but there is research that suggests that helping behaviors increase with sunlight (people sitting near windows during the day tip more; experiment in hotel with dark tinted windows showed bellboys got bigger tips if they said it was sunny).  I had this idea a decade ago, so maybe someone has already looked at it, but if not, I think it would be a nice social psychology observation. ",manova,2014-12-04 05:45:14
Nice. Maybe you should also add the datetime (preferably order time and delivery time) to your data collection and check for a connection between tip and time / weekday.,schwar2ss,2014-12-03 23:19:50
[I did this table out of the data.](http://i.imgur.com/CwS8bbb.png),finnurtg,2014-12-04 06:27:23
I guess now you have to report this on your taxes,msdrahcir,2014-12-04 07:57:05
I like this because I never know what to tip delivery guys based on custom.,FullSharkAlligator,2014-12-03 22:17:39
"I work for a pizza company where a large cheese is like $17 or something. So these felt like small tips but the percentage of total looks more like mine does. 

I also wanted to say that 5 orders with a tip of $11.01 or larger felt so wrong to me, but I also do my best to pick up 10-2 shifts that deliver mostly to large companies. So a $300 order can get me a $15 tip, which is technically *only* 5%.

Anyway, thought it was interesting that the end data (tip amount) is so different but tip percentage is so similar. ",poopascoopa69,2014-12-03 22:54:24
Thanks for this.,efrique,2014-12-04 00:50:08
You could also do this as a stacked bar chart to include age distribution as well or male/female. I would also submit this to r/dataisbeautiful - it's some pretty slick work and they'll love it over there! ,Gold_Sticker,2014-12-04 04:36:34
What did you use to make this chart?,Adamworks,2014-12-04 06:26:24
"Did you track the total order amount?

Can you correlate tip amount with total $ amount of order?

I feel like you are missing one of the most important factors in tip amount (ie % of order amount).",Philosopherknight,2014-12-04 11:08:29
"&gt;Most of the people who can usefully receive it either already understand stats well enough that they don’t need it or already understand stats badly enough that every time someone posts a paper they smugly assert “Ah, but correlation doesn’t imply causation” and then pat themselves on the back and say “Well done, me. You did a science. Good job. You deserve your cookie” 

Exactly the reason I don't read the comments over in /r/science any more, they're full of this.


",DrunkDylanThomas,2014-05-14 03:42:59
"Maybe true.  But at least they understand it. It's fundamentally the worst mistake you can make.  To suggest otherwise, even as click bait, is always wrong.

What's critical to grasp next is that someone could set up a model which implies causation.  It's done all the time here, you pick a dependent variable and an independent variable and regress them, and woah, look at that huge beta!  But they have used correlation to prove causation.

Probability and statistics is an applied mathematics, it requires an interpretation.  There are people on this very subreddit who post their own blog posts as content who get upvoted who have tried to present cases using the title that correlation is evidence of causation.  That's never true.  Correlation is a measure of *coincidence*, not causation.  It's a measure of how frequently things happen at the same time.  There are no modifications that can be made to correlation which can change the interpretation of correlation.

Even this title is click bait in my opinion.  The article is trite trash for sure, expositionally written by someone who thinks the reader is an idiot.  But more factually correct then other things written here.

[edit]
Christ, case in point, look at [this comment in this thread](http://www.reddit.com/r/statistics/comments/25ivql/why_correlation_is_not_causation_is_the_wrong/chhosjf)

Another self published blog author who has not fucking idea what he's talking about.
[/edit]",drunken_Mathter,2014-05-14 07:37:58
"&gt; There are no modifications that can be made to correlation which can change the interpretation of correlation.

I don't know about that… What about randomized clinical trials? [Granger causality](http://en.wikipedia.org/wiki/Granger_causality)?",Jrizzler,2014-05-14 17:41:54
"They are *more than* correlation.  Correlation is not enough.  this is the point people are completely missing.  Causation does not pop out of correlation.  You can't say ""p=.98, I have causality!""",drunken_Mathter,2014-05-15 02:09:29
[deleted],,2014-05-14 21:46:21
"&gt; Everytime I see a comment along those lines on some science article on reddit, I die a little inside.

Its probably something else, after all, correlation doesn't imply causation.",zubinmadon,2014-05-15 19:05:09
[deleted],,2014-05-14 09:19:10
ELI5? please,illioneus,2014-05-14 11:32:52
"Stationary time series are time series that are mean returning and have constant variance and covariance. In other words, the series returns to the mean over time and always to the same mean (this need not be zero). Additionally, the variance between a point at time t and t + k (say today and next Wednesday) has to depend only on k. If the covariance between Monday and next Monday is different than the covariance between Tuesday and next Tuesday or next Monday and next next Monday, this is a problem. 

Almost all time series modeling and analysis assumes/requires a stationary series. The mean returning is simple to get (generally fit either a linear or a semi-parametric model and subtract it out). The constant covariance is harder but generally possible with log and other transformations. 

Unit root is a similar concept to stationary. ",iacobus42,2014-05-14 12:03:33
"Ok, but why does that prevent someone from calculating a correlation? Don't we have models/methods that don't assume a stationary series?",OhanianIsACreep,2014-05-14 13:10:29
"There's a rich body of knowledge about how to handle unit roots and non-stationary time series if the time series are cointegrated. 

 ",markgraydk,2014-05-14 13:18:31
"This is definitely the case, pre-whitening is something that I commonly do when calculating correlation of time series. There are also state space models and a ton of other interesting work. 

However, most ""classical"" methods and commonly used methods assume stationary series. They are somewhat robust but if you have issues with a trend that isn't dealt with, you are just going to have a case of GIGO.  ",iacobus42,2014-05-14 14:32:20
"Oh my god, where are your parents! ",WallyMetropolis,2014-05-14 12:22:49
I said explain LIKE I'm five. I just graduated 1st grade dude hop off my dick. ,illioneus,2014-05-14 12:29:23
[It's a reference.](http://xkcd.com/1364/),noelsusman,2014-05-14 15:57:18
"[Image](http://imgs.xkcd.com/comics/like_im_five.png)

**Title:** Like I'm Five

**Title-text:** 'Am I taking care of you? I have a thesis to write!' 'My parents are at their house; you visited last--' 'No, no, explain like you're five.'

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=1364#Explanation)

**Stats:** This comic has been referenced 28 time(s), representing 0.1399% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)",xkcd_transcriber,2014-05-14 15:57:35
Thanks,illioneus,2014-05-14 16:15:12
"&gt; “Correlation doesn’t imply causation” has more or less reached saturation, and that saturation point is far larger than is actually useful.

As both an educator and a researcher, I cannot disagree more strongly. My experience has been that most adults who make little or no contact with academia in their daily lives are still completely won over by spurious correlation if it even remotely agrees with an existing prejudice.

The author's point about multiple tests is perfectly appropriate. It reflects a deeper understanding of statistical inference, and the more people who understand it, the better. It certainly hasn't yet achieved anything close to saturation either. My favorite distillation of this point (for a general audience) is Lawrence Krauss' line: ""The universe is huge and old and rare things happen all the time.""

Nevertheless, however annoying MacIver finds the smugness of those who have a shallow understanding of spurious correlation, I'm quite confident that their reflexive skepticism is vastly healthier than the far more dangerous acceptance of flimsy correlational evidence by *most* registered voters.",belarius,2014-05-14 06:21:25
"Sure. I just also don't think they're going to be the ones receiving this message, or that just repeatedly telling them that correlation does not imply causation will produce anything useful.

I'd love it if people had better statistics education, I just don't think learning contentless mantras like this will help.",DRMacIver,2014-05-14 06:37:20
"The slogan ""correlation is not causation"" understates the problem. Correlation is often actively misleading about causal structure. 

Example one: minor medical complaints. The human body has great powers of recovery, and the human mind is very impatient. So there is a natural tendency to take a remedy quickly, before the body has had time to recover on its own. This generates correlations between remedies and recovery. Take the remedy, recover a week later. This lead people to believe in harmful remedies such as blood letting.

Example two: Social problems and moral panics. Many social problems have the form: something is fun to begin with, later it becomes a difficult problem. They often self limit as young people notice those a little older experiencing difficult problems. Often it is a limit *cycle* powered by generational forgetting. Generation X get caught. Generation Y are forewarned and don't get caught. Generation Z are not forewarned by the experience of Generation Y, because the Y were not getting caught in sufficient numbers. Generation Z are not forewarned by the experience of Generation X because that is ""ancient history"". So generation Z repeat the mistakes of Generation X. Think about amphetamine use cycling in and out of fashion.

To complete example two, stick a moral panic on top of limit cycle. The human tendency is to introduce rash public policy at the top of the cycle. Every public policy introduced at the top of the cycle (provided it is not so hugely harmful as to stop the cycle) appears to work. *Every* policy, wise or foolish. Thus the human tendency to panic and introduce bad policy at the top of the cycle is an engine for generating correlations that point in the opposite direction to the underlying causal effects.",AlanCrowe,2014-05-14 12:40:22
"""Correlation does not imply causation"" and ""correlation does not necessarily not imply causation"" are both fairly vapid content-free statements.

The thing is, showing causation is fucking hard.  And the publication of a peer-reviewed journal article is not the equivalent of proof.  If people want to discuss evidence of casuality, sure that's great.  But simply mocking ""correlation does not imply causation"" is pretty vapid meta-snark.",redditleopard,2014-05-14 08:44:46
"I'm not mocking the general notion of correlation not imply causation, or indeed all of the extremely complicated issues around there. It's an important issue which it's important to have a clear and nuanced understanding of.

What I am mocking is the tendency of scientifically and statistically illiterate internet commentariat who want to feel clever about themselves to post ""Ah but correlation does not imply causation!"" on every fucking science article without having read it.

Edit: I mean, I'm also saying that it's the wrong conclusion here, but that's because it's the wrong conclusion here, not because of mockery. Correlation does not imply causation *even when things are genuinely correlated*. What the site is showing is just a statistically invalid way of inferring correlation.",DRMacIver,2014-05-14 08:47:50
"In an internet discussion between people smugly saying ""Correlation does not imply causation"" and people smugly saying ""Correlation does not not imply casuation"" in which no one has actually read the paper or understood the key arguments, yeah, everyone's a fucking idiot.  Why pick sides?  Quite frankly, there should always be a fairly high bar for demonstrating causality, and lots of popular clickbait articles fall short of this.  So actually if I am going to choose sides I choose the idiots saying ""correlation does not imply causation.""

Regarding the blog post you're linking to, it mainly talks about spurious correlations due to small sample sizes.  This is actually one element of why ""correlation does not imply causation"" -- correlation may not actually be real or sustainable or have any predictive value.  I don't see how ""correlation is not causation"" is the ""wrong"" conclusion to draw.

Fuck it, you want to make arguments that a purported causal effect is real?  Make the god damned arguments.  That seems more productive than smugly trying to out-smug other smug people by talking about ""saturation"" and cookies.",redditleopard,2014-05-14 08:59:48
"On another tangent this is particularly frustrating when the person chirping ""correlation does not imply causation"" proposes an alternative cause that was addressed specifically in the article, or even in the abstract. ",NOTWorthless,2014-05-14 11:00:19
Ehh it's just a fun website for people to kill time on.  I'm appreciative people are learning anything statistical from it.,FullSharkAlligator,2014-05-14 09:06:32
"As an aside: what really needs to be taught and understood are Type I and Type II errors when examining results. This would help people decide on their own if there is, indeed, enough representation of the correlation at hand and, therefore, conclusive evidence

In the case examples of the link, both are Type I errors simply because there are too few years of data points (false positive)",piv0t,2014-05-14 13:22:30
"The type I error rate is somewhat inflated from the sparse data, but the bigger problem is multiple comparisons; the examples were generated from a huge set of possible examples of pairwise comparisons, and the examples given were drawn out on the basis of strong pairwise correlation.",Neurokeen,2014-05-14 14:00:19
"I really like this post.  Its especially relevant given the ""Big Data"" craze.  The point the post makes is that the conclusion should not be that ""correlation is not causation"" but that they aren't even truly correlated in the first place!  This is an issue of multiple testing.  With enough data we can easily find spurious correlations.  IMO this idea may be nearly as important in scientific research  as ""correlation is not causation""-- it's closely related to the ""file drawer effect""/publication bias and questions surrounding reproducibility of research.

We collect data before even posing questions, and thus looking for patterns ex post facto can be dangerous.  The question is, with an essentially unbounded number of possible comparisons, how do we decide which ones are truly relevant and which ones are spurious.    ",rabtinyrhedlites,2014-05-14 14:00:32
"&gt; suspiciously convenient anonymous blog commeter

I know that guy.",BillWeld,2014-05-14 11:12:16
TL;dr:  It's easy to overfit and fool yourself with data.,BillWeld,2014-05-14 11:13:45
"That's funny! When I saw that website, I thought, ""What a good example of the look-elsewhere effect,"" and then stopped thinking about it. Different statistical phenomenon, same negative psychological procedure.",mszegedy,2014-05-14 14:11:25
"Correlation implies causation.  I would rely on correlation over those polynomial ""fits"" of data.",homercles337,2014-05-14 02:45:06
"Correlation doesn't imply causation, but it does increase the likelihood that a causal link exists. This is something that people miss out when they repeat that infamous phrase.",yen223,2014-05-14 06:10:39
"To quote [xkcd](http://xkcd.com/552/):

&gt; Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'.",capnrefsmmat,2014-05-14 06:52:42
"[Image](http://imgs.xkcd.com/comics/correlation.png)

**Title:** Correlation

**Title-text:** Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=552#Explanation)

**Stats:** This comic has been referenced 143 time(s), representing 0.7165% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcdcomic/)/[kerfuffle](http://www.reddit.com/r/self/comments/1xdwba/the_history_of_the_rxkcd_kerfuffle/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me)",xkcd_transcriber,2014-05-14 06:52:53
Saying correlation does not imply causation is not implying that there may not be causation,Corruptionss,2014-05-14 15:27:11
"It very emphatically *does not* imply any such thing, either directly or indirectly. Furthermore, those are not polynomial fits in the traditional sense, but rather interpolated splines.",belarius,2014-05-14 05:57:39
"It kinda does. Correlation implies some sort of causal link exists, it just may be indirect (either a common cause or conditioning on a common consequence)",DRMacIver,2014-05-14 06:02:19
"""imply"" in this sense means the mathematical =&gt;. That is, 'imply' means 'necessitates.' 

We're not talking about the colloquial 'hints at' meaning of 'imply'. ",WallyMetropolis,2014-05-14 12:29:04
"This is horrendously wrong.  Correlation does not imply a causal link.  A causal link implies correlation, not the other way around.

Correlation is only a measure of *coincidence*.  You are 100% incorrect here.",drunken_Mathter,2014-05-14 07:43:24
"&gt; A causal link implies correlation, not the other way around.

It's actually messier than that. It's entirely possible to have a causal link from X -&gt; Z with corr(X,Z)=0.

And while correlation doesn't imply causation in the sense of corr(A,B)!=0 does not necessarily imply that A-&gt;B or B-&gt;A, *nonspurious* correlation implies an *unresolved causal diagram* - in other words, the presence of a correlation not attributable to sampling variability or the stochastic nature of whatever generating process does mean that there is a path in a causal network between X and Y.",Neurokeen,2014-05-14 08:07:15
"Excellent points.

A caveat of nonspurious correlation is that if you know a priori it's nonspurious, you must have a priori information about causation. I'm not aware of a test for spuriousness which doesn't make assumptions about causation.

My main point here is that correlation cannot tell you anything on it's own about causation.  Causation must come from somewhere else.

",drunken_Mathter,2014-05-14 08:13:54
"&gt;I'm not aware of a test for spuriousness which doesn't make assumptions about causation.

Even in a controlled experiment, you get a probabilistic answer when you try to rule out random error by a basic NHST. I don't see how this situation is qualitatively different - determining spurious correlation is equivalent to a NHST in the most basic of cases. And I don't see the argument that experiments are worthless or don't count as evidence of causal relations unless one is trying to argue against the existence of cause entirely.


------
Edit after thinking about this more during lunch: I don't think you can reasonably make the argument that there are no tests for spuriousness unless you dismiss the *entire enterprise of applied statistics* as rubbish. This goes a lot further and a lot deeper than simply hypothesis testing procedures as I had originally suggested.

One of the larger goals of statistics as it has been used in recent history has been to classify different types of random variability, and to recognize non-random variability from the noise of random variability. While there might not be tests that return simple and 100% reliable RANDOM or NONRANDOM results (because even in theory that much is impossible), the discipline as a whole still produces largely helpful results for guiding one in distinguishing between the two.",Neurokeen,2014-05-14 08:26:59
"Can you give an example of two variables that are correlated (and I mean really correlated, not variables that seem correlated because we made a million comparisons between totally unrelated time series and got a biased *estimate* of the correlation as a result) but don't have some latent causal association? For example, ice cream sales and umbrella useage are linked together in a causal fashion by the weather, although one does not cause the other.

I can't think of any myself, although I might be missing something obvious",NOTWorthless,2014-05-14 07:57:44
"You are missing the point completely.  The causal link doesn't come from measuring a correlation.  you're asking for a very specific situation which is very unlikely to occur.  That doesn't mean that correlation can be used as evidence of causation.

Precisely the point that you need to hunt through the physical system for the causation.  You cannot look at just the scalar which is correlation ad say ""a ha, causation"".

A causal system will produce a correlation.  A correlation does not imply that A caused B or that B caused A.  It only indicated they are coincident.

It's a measure of Mutual Information, it can never be used as anything more than that.

If you regress X on Y or Y on X, you get the same relationship.  You cannot use the size of the beta to confirm either model is the right one.  You need to infer causality elsewhere.",drunken_Mathter,2014-05-14 08:05:52
"But the point OP is trying to make, and that you called him a crank for, is that if two variables are correlated (in the relevant population) then *somewhere, something* out there is causing something else to happen. Whether it is A causing B, or B causing A, a latent C causing both, or something else. 

This isn't an insight from the mathematics, but a point about the world. ",NOTWorthless,2014-05-14 08:10:30
"Isn't the point that there is a possibility of two completely unrelated variables correlate to some degree? You make it sound as if that probability is negliable in the real world and it is just a matter of us discovering the causal chain leading to the correlation? Or am I missing something? 

I would think it could be possible to find to completely unrelated variables that correlate to a high degree. Is that different from if I drew to samples of data from the same process (e.g. gaussian) and they correlated highly? (I think you made an argument like that a few days ago when this issues was discussed as well)

I did enjoy the discussion about this in the thread in /r/dataisbeautiful  a few days ago but since we are discussing something at detailed level I would hope for a more detailed explanation :).

",markgraydk,2014-05-14 13:03:34
"You can find data in the real world which are ""correlated"" but are totally causally disconnected in the sense that data vectors X and Y have a high value of R = [(X - Xbar) dot (Y - Ybar)] / ||X - Xbar|| ||Y - Ybar||. This is what is in the original spurious correlation post. For example, it is highly unlikely IMO that murders in Missisppi and ice cream sales in India are causally associated, but we might by chance calculate the correlation of the two time series and the R we calculate might be big just due to sampling error. If we look for a big R in enough places we are bound to find it, whether there is a causal link or not.

That's fine, but it isn't the correlation that I'm talking about and it's not the correlation that we ought to be interested in. The population correlation is something that is intrinsic to the processes themselves, not something that can be measured directly from the data, although it can be estimated. And I'm arguing that *this* sort of correlation occurs due to some causal mechanism. I'm not saying we can assess the directional of the causation, but I can't think of any process that occurs where there isn't some causal link when the correlation is real at the population level. I admit I might be missing an obvious example, but I haven't seen one to my satisfaction.",NOTWorthless,2014-05-14 16:08:54
"You asked for a very specific thing.  Look at all the medical research that's been invalidated over time.  It's incredibly easy to find cases where causation was assumed from high correlation and it turned out to be wrong.

Thinking your way is not helpful because everything in the universe is related by physical laws.  With your way of thinking, you can find causation with the big bang.

It's not the correct way to think about it, and it's not accepted at the academic level.",drunken_Mathter,2014-05-14 08:16:09
"Welll technically we can never make the causation conclusion. 

&gt;What we do in science is to make the jump from correlation (or more accurately dependence since correlation is a type of association) to causation as small as possible. We never really know if it is causation. In practice no one thinks like this, and rightly so.

One way we go about this is doing designed experiments. But at the end of the day its all correlation/dependence data.
",fanofDK,2014-05-14 08:56:13
"&gt; You asked for a very specific thing. Look at all the medical research that's been invalidated over time. It's incredibly easy to find cases where causation was assumed from high correlation and it turned out to be wrong.

Right, but the problem - assuming that the research did not make a Type I error - was not that there was ""no causation"" happening, it was that multiple different causal associations were possible and they happened to pick the wrong one. For example, they might have concluded A causes B when in reality C causes A and B. 

&gt; It's not the correct way to think about it, and it's not accepted at the academic level.

I'm really trying hard to not sound like a jerk or ""pull rank"", so I'll just leave it at this: it's kind of dangerous to assume you know what someone's background is.",NOTWorthless,2014-05-14 08:36:19
"Pull rank?  That's dangerously close to an appeal to authority.

In systems which acquire significant quadratic variation, and do not have known physical laws dictating their behavior, we see significant but spurious correlation pop up all the time.  In fact part of the problem is that there's no reason to reject the correlation even when causality is dubious.  People run economic policy based on this all the time.

I think many people are focusing on systems driven by natural processes here and are losing the forest for the trees.  

Correlation is a scalar, which you get from mapping R^n x R^n -&gt;R.  Without talking about specific X and Y, if all we have is observations about X and Y, we can compute the correlation, but that does not give any information about causality.  It does not imply *any* causal link.  

Even if you find a lagged correlation, that's not a causation, it's a coincidence in the sense that the two things coincide, or happen close together in some space, in this case one offset by time.  That's all that can be inferred from correlation alone.

Any other information about causation comes externally from correlation.

More succinctly, reporting a correlation number alone , let's say .98, doesn't really tell me anything about two processes X and Y.  If I tell you that X and Y have a .98 correlation, you cannot infer that they are causally related.  Even if I tell you the number of observations are 1 million or anything else.  It does not come out of the correlation equation.


Fundamentally, what I have never seen proven is that the correlation mapping indicates any kind of causal link.  It only measures the (normalized) average product of ordered displacements.  It cannot tell you which displacement is driving the other, it cannot tell you that there is some hidden variable driving both, etc.  Nothing comes out of that equation other than a scalar indicating approximately how much the processes move together.  There is no why.

With just two processes X and Y and the scalar p, there is no information about causation.  Thinking that causation comes out of correlation in any way is fundamentally missing where in the model that causation is implied.  If someone conjectures that X ~ Y, or Y ~ X, or X~a+bZ and Y~d+eZ, that's an assumption about causation.  But correlation has no assumption about causation and cannot yield information about causation.  Causation must come from somewhere else.

As a note which is clearly my opinion, if we go through life assuming that all statistics works like it does in physics or chemistry, then we end up thinking that economists and sociologists are just bad physicists, and ignore that the underlying systems are driven by higher stochastic moments and are fundamentally different.  When experiments are not repeatable, and knowledge of the system changes the system itself, statistics is the only tool which can be used to even begin to analyze the system, and most likely the system cannot be fully analyzed.  There are systems which impact the daily lives of people in the society in which they live which they are incapable of thinking about correctly without grasping this fundamental truth about the math being used to describe the system.

",drunken_Mathter,2014-05-14 09:15:52
"&gt; In systems which acquire significant quadratic variation, and do not have known physical laws dictating their behavior, we see significant but spurious correlation pop up all the time. In fact part of the problem is that there's no reason to reject the correlation even when causality is dubious. People run economic policy based on this all the time.

You use the word ""significant"" here. Do you mean ""statistically significant"", and if so, significant as evidence for what? We find *sample* correlations which are large all the time, but these sample correlations may be weak evidence for the existence of a true (population level) correlation. 

&gt; Correlation is a scalar, which you get from mapping Rn x Rn -&gt;R. Without talking about specific X and Y, if all we have is observations about X and Y, we can compute the correlation, but that does not give any information about causality. It does not imply any causal link.

No, I've repeated stated that I'm referring to a population level correlation, and a population level correlation is not a mapping from R^n x R^n to R. When someone says they are talking about the population it is implicit that they are talking about the mapping from L^2 (P) x L^2 (P) to R, where P is a probability measure and L^2 (P) is the space of square-integrable random variables defined on whatever space P is a probability measure on.

&gt; Fundamentally, what I have never seen proven is that the correlation mapping indicates any kind of causal link. It only measures the (normalized) average product of ordered displacements. It cannot tell you which displacement is driving the other, it cannot tell you that there is some hidden variable driving both, etc. Nothing comes out of that equation other than a scalar indicating approximately how much the processes move together. There is no why.

There is no ""why"" in the mathematics at this stage, because you haven't formalized causation mathematically (although you can, using Judea Pearl's graphical models or the Rubin Causal model). But the overall point is that *in real life* there is typically a latent causation behind correlation. This isn't something that you can get at mathematically, it is a philosophical statement, and you haven't given an example to my satisfaction of a real-life *population level* association that doesn't have a lurking, potentially latent, causal association.

&gt; With just two processes X and Y and the scalar p, there is no information about causation. Thinking that causation comes out of correlation in any way is fundamentally missing where in the model that causation is implied. If someone conjectures that X ~ Y, or Y ~ X, or X~a+bZ and Y~d+eZ, that's an assumption about causation. But correlation has no assumption about causation and cannot yield information about causation. Causation must come from somewhere else.

Assuming that (X ~ a + bZ) is supposed to be in line with the R notation for such things, that notation is not typically interpreted to be causal. It is a specification of the conditional distribution (X|Z) but it is typically not assumed to be causal. ",NOTWorthless,2014-05-14 09:44:37
"&gt;When someone says they are talking about the population it is implicit that they are talking about the mapping from L2 (P) x L2 (P) to R, where P is a probability measure and L2 (P) is the space of square-integrable random variables defined on whatever space P is a probability measure on.

I'm glad I saw this again after talking with you a couple of days ago about spurious/nonspurious correlations. It makes a lot more sense seeing it the second time around.",Neurokeen,2014-05-14 10:26:23
"I'm confused, which parts of your argument here show that correlation imply causation?  

Much of what you've said is factually true, though just redefining what I've said.  I'm not going to argue tangential points, but if you want me to clarify specific ones, I'm more than happy to.  I'll come back to this later.

p = Corr(X,Y) shows no implication, measure, whatever of causation.  I have made no assumptions about sample vs. true, I have used different but equivalent notation for the mapping, I have called it a scalar which it is and so have you.

So please, direct the focus towards showing that Corr(X,Y) has causation embedded.

""In real life,"", as you say, the model or other information implies the causation. Corr(X,Y) does not.",drunken_Mathter,2014-05-14 09:51:55
"&gt; I have used different but equivalent notation for the mapping, I have called it a scalar which it is and so have you.

I don't want to belabor the point, but they are not equivalent. Saying they are equivalent is like saying the population mean and the sample mean are equivalent. I don't think this is tangential because I think it's likely that any real-life counterexample you propose will be conflating these two things.

&gt; So please, direct the focus towards showing that Corr(X,Y) has causation embedded.
&gt;
&gt;""In real life,"", as you say, the model or other information implies the causation. Corr(X,Y) does not.

The correlation does not have causation embedded as a concept mathematically. It's just a cosine, and there's nothing causal, statistical, or random about cosines. Without couching them in the real world, random variables are just functions from a probability space to R; without giving them an interpretation there isn't even anything ""random"" about them. But once we interpret them that doesn't mean we can't look at the world and observe that when two things are correlated in their relevant population there is a *reason*. In the real world, when things correlate we always ask why, we never say ""maybe they are just correlated for no reason."" There is something latent that, if we intervened on it, would lead to both quantities changing (in a probabilistic sense). And that underlying reason is as far as I can tell a cause. ",NOTWorthless,2014-05-14 10:25:26
"&gt; I don't want to belabor the point, but they are not equivalent...

Fair point.  I think they are equivalent for the purposes I was using the definition of the mapping, which is only to say that your additional definition, while finer grained, does not attach a property of causality to the function. Your definition certainly is more informative about the spaces and measures, but it doesn't attach the property of causality.  That's all I meant by equivalence.  The additional information didn't seem to promote correlation implies causality.

&gt; In real life...

My point, very specifically, is that the function cannot do anything to determine causality.  Causality comes from elsewhere, and this is extremely important.  The causality comes out of the assumptions that we are in ""real life"", and what that means when producing the statistics.

This is what is ridiculous about OP's post:  It attributes causality to the correlation function.  No one can tell me where that property gets attached to the scalar p by mapping X and Y to it.  They are just yelling at me that it most certainly does because we're in the real world (not just you, others in the thread).  My point is that causality comes from this extra information, not from Corr.

The causal property is not ""in"" or ""from"" the correlation.  It's external to it.

Going back to my example, if you have X and Y, you can compute p with no other context. You don't know if p is large, small, and you can't hypothesize that there is some latent Z because you don't have any of that information.  You only have X, Y and Corr(.,.).  You can still compute p *without* the context.  Causality is attached by something else. How can correlation indicate causality here, but it's the same number you'd get for any X and Y in the real world?  The correlation could be spurious, etc.,.  

In my opinion, I've shown that:

1)  Without knowledge about causality, you can compute a correlation for any data

Here causality may be known or not, may exist or not, may be X-&gt;Y or Y-&gt;X

2)  Given a computed correlation, gain no insight into causality from the computation

If causality isn't externally imposed or known, the correlation value merely shows coincidence, a value indicating how the two processes move together.


With those two statements, causality itself does not imply correlation, and correlation does not imply causality.  Correlation is just a mapping to a scalar value.  Causation comes externally.


So, if you agree with the above I cannot see why anyone would want to take a short cut and say that correlation implies causation.  The causation is implied by the external information available about the natural world, the experiment design, your gut feeling, whatever.  It's not attached to the scalar by the mapping Corr.  

When someone attaches causation to the scalar p, it removes causation from the natural system being studied and attaches it by application of the mapping.   The causation is there or not a priori, and that won't change the value of correlation for collected ata.  If you have causation and measure a low correlation, that's meangingful, something is wrong in the experiment or theory.  If you don't have causation and measure a high correlation, that's meaningful as well.  But none of it comes from just computing the value of correlation.
",drunken_Mathter,2014-05-15 02:30:56
"&gt; This is what is ridiculous about OP's post: It attributes causality to the correlation function. No one can tell me where that property gets attached to the scalar p by mapping X and Y to it. They are just yelling at me that it most certainly does because we're in the real world (not just you, others in the thread). My point is that causality comes from this extra information, not from Corr.

I don't think anyone is yelling at you.

OPs point had to do with real life to begin with, you are the one attributing his statements as mathematical statements about correlation as a mapping. The reason people say correlation does not imply causation is because it doesn't establish the directionality or account for confounders, which is an excellent point, but OP was talking about something different - that if two things are genuinely correlated then *something* is going on, it's just a matter of what. For example, if we rigorously determined by repeatedly testing it that whether it is raining in Hong Kong is highly predictive of whether the Yankees win their next baseball game (the time series OP referenced not meeting this standard), maybe you would have an example of two things which are correlated with no causal link. You could debunk his statement just by providing a suitable counterexample of some data realizations X and Y which are correlated in their respective populations but for which their is no (potentially latent) causal link.

&gt; and that won't change the value of correlation for collected ata.

Sorry to go on this tangent again, but you keep implicitly suggesting that correlation is a property of the data itself. This is only true if by correlation we mean sample correlation.",NOTWorthless,2014-05-15 07:20:23
"It's not tangential if we need to discuss it, I just wanted to be sure we needed to discuss everything you posted.  It's actually easier to do it one at a time anyways.

It's not necessarily a sample population, there's a lot philosophy behind that, but claiming that historical data series is a sample rather than true is a leap that not everyone is ready to make.  For example, in financial time series, the price P is considered to be true, it cannot be anything other than what it is.  The problem is (potentially) that it's moments are stochastic as well, including correlations.  In many cases it's clear that we are dealing with sample populations, e.g. when dealing with political data or medical data, it's absolutely clear that there is a sample population; it's just not possible to test everyone (esp when many are dead).  But not all processes are like this.


OP is using real world data, so he's measuring a property of the data and implying causation from it.  I'm stating that you can't do that, all you get is a number which implies a kind of similarity.  This similarity is called dependence in the literature, not because X causes Y or Y causes X, but because the set X and Y share information.  I call this coincidence: things are happening locally close at the same time.  This intersection of sets definitely occurs, even in the true process, but it's not the same thing as, say, sharing a physical forcing.

The leap to causality comes from elsewhere.  The probability is low that we'd ever see a large amount of data which reports a correlation where there is no underlying causality.  But it's still not correct to attribute the causality to the correlation.  Especially when looking at processes with stochastic moments.  This is exactly why LTCM fell, they assumed that historical correlations ran long enough (some for decades) to trade on them.  Then they changed drastically.

I agreed with OP's blog post, but then he said later than correlation means causality.  That's just not the case outside of controlled environments like medical and political testing or physical systems like chemistry and physics.

The point is, if you have controlled experiments or whatever, you have more information than just correlation, and it's the extra information which gives you causality.  Correlation itself does not.

There's a wide range of studies on sample data, but it's not the only kind of stochastic processes that are studied.  The other kind are much much harder, and generally has resulted in failures, but they do exist.",drunken_Mathter,2014-05-15 10:03:25
"&gt; It's not necessarily a sample population, there's a lot philosophy behind that, but claiming that historical data series is a sample rather than true is a leap that not everyone is ready to make. For example, in financial time series, the price P is considered to be true, it cannot be anything other than what it is. The problem is (potentially) that it's moments are stochastic as well, including correlations.

If we hope to generalize to the future then the old prices had better be a realized sample from a stochastic process; otherwise there is no philosophical basis for extrapolating. Without an underlying probability measure the data are just numbers and don't provide any information about the future. If you don't regard the price P as a realization of a random variable then it is just a number and you can't use it to learn anything. 

On another note, how can the moments and correlations be stochastic if the historical price is an a priori fixed number? We are apparently regarding the future prices as random if we are claiming the moments are stochastic - why wouldn't we regard the old prices as realizations from the same process, and do forecasts using conditional probability?

&gt; This is exactly why LTCM fell, they assumed that historical correlations ran long enough (some for decades) to trade on them. Then they changed drastically.

I'm sure you're not arguing that there is no causal association between the price of options and the price of stocks in the stock market. LTCM was a victim of some combination of the non-stationarity of the world and an underestimation of kurtosis (IIRC, I've heard either Merton or Scholes claim it was kurtosis), not a victim of causality. 

&gt; The point is, if you have controlled experiments or whatever, you have more information than just correlation, and it's the extra information which gives you causality. Correlation itself does not.

No one, anywhere, is claiming that correlation allows you to infer the *direction* of the causality. This is clearly unidentifiable. OP is only saying that *something* drives the correlation. 

&gt; The probability is low that we'd ever see a large amount of data which reports a correlation where there is no underlying causality.

Correlation, in the sense OP is talking about it, is, again, not a property of the data. ",NOTWorthless,2014-05-15 10:48:23
"Kurtosis in the sense that the probability of a supposedly rare event was not so rare as a normal distribution would lead you to believe.  The event was the breakdown of correlation.  

&gt; how can the moments and correlations be stochastic if the historical price is an a priori fixed number?

Who said the *a priori* historical price was deterministic?  What i said was (rephrased) that not all processes are the evolution of a sample draw.  The historical price (returns, actually) is not a sample of some unknown distribution.  The historical prices are the final realization up to time t, fully adapted.  They are the true process.  What's unknown is everything in the future.  This is unlike sampling from physical processes where the laws of physics are assumed not to change.


OP is directly saying that causality can be inferred.  It cannot be inferred from correlation.  You are implicitly stating that something is driving the correlation.  Yes, of course, in that case, there is causality.  *but you require the assumption that something is driving the correlation to get there*.  Correlation alone does not get you there.  

As I have said repeatedly, correlation is not enough, more assumptions are required.",drunken_Mathter,2014-05-15 11:31:10
"&gt; Who said the a priori historical price was deterministic? What i said was (rephrased) that not all processes are the evolution of a sample draw. The historical price (returns, actually) is not a sample of some unknown distribution. The historical prices are the final realization up to time t, fully adapted. They are the true process.
&gt; What i said was (rephrased) that not all processes are the evolution of a sample draw. The historical price (returns, actually) is not a sample of some unknown distribution. The historical prices are the final realization up to time t, fully adapted.

Insofar as the time series is regarded as a stochastic process (and it must be regarded as a stochastic process if the future is going to be treated as unknown and random) the past must be treated as the realization of some random variables. You cannot conduct inference (either frequentist or Bayesian) otherwise. You also cannot meaningfully conduct inference if the time series is thought to be highly non-stationary, because the past will not inform about the future, but that is a separate issue and it is unrelated to the issue of causation as far as I can tell. 

FWIW I'm not saying the past is subject to subject to *measurement* error - of course, for stocks we know with certainty what the historical price is. But this is completely tangential.

&gt; As I have said repeatedly, correlation is not enough, more assumptions are required.

To which /u/DRMacIver response was

&gt; As to where it's the case, the short answer is that it's a philosophical assumption that's used to help make sense of the very notion of causation. It's an axiom that dependence doesn't exist without causal relations, and it's a relatively fundamental one in the modern study of causality.

So he has already acknowledged that there is an assumption that isn't built into the mathematics. ",NOTWorthless,2014-05-15 12:16:27
"I am 100% correct here. Correlation implies a causal link. As per the parenthetical comment, this causal link can take several forms: It could be that one causes the other, it could be that there is a common cause, it could be that there is a common consequence that you are conditioning on, but *something* is going on.",DRMacIver,2014-05-14 08:18:37
"I think you're getting your definitions a bit messed up. What you're referring to is something alone these lnies : http://en.wikipedia.org/wiki/Confounding.

When we say a casual link we usual mean one causes the other. A very specific relationship.",fanofDK,2014-05-14 08:53:34
"Note that I didn't say there was some sort of causal link between the two variables (though it's unclear to me if I had said that it would be wrong and that the meaning is as specific as you say. I don't think the term is used only that specifically). I said that correlation was evidence that a causal link exists.

Still, if it causes confusion I'm happy to avoid it. Or clarify. By e.g. explaining exactly the sort of links I meant right next to saying them.

Also confounding is a bit more specific: It's about the effect on statistics that sort of indirect link has, rather than the behaviour itself.",DRMacIver,2014-05-14 09:02:54
"Say Z is the thing that cases X and Y. For example:

X=Z+1 and Y=Z+2.

However X does not cause Y and Y does not cause X. Z is the confounding variable that causes both of them and has a direct effect on how they both behave and any statistical behaviour coming from this. Correlation between X and Y is 1. 

Here i used very simple relationships between X and Z and Y and Z. I can make the statistics whatever I want by choosing a different relationship (note the correlation might also change from 1 despite there being a explicit relationship between X and Y). 

Correlation is not the be all and end all and can be very inadequate for variables that are closely related depending on the relationship. Correlation is only a measure of linear association.",fanofDK,2014-05-14 09:16:15
"Nope, sorry.  You and other cranks like to think this, but it's not true.

Show me where in the math this is the case, then.  Write a paper and get it published.  It's never true.",drunken_Mathter,2014-05-14 08:19:40
"Here, have an [entire book](http://www.amazon.co.uk/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628).",DRMacIver,2014-05-14 08:20:49
a priori causation is not correlation implying causation.,drunken_Mathter,2014-05-14 08:22:02
"Let me repeat for the third time what I originally said. Maybe you'll read it this time if I itemize it:

  1. Correlation (more generally, dependence) implies that there is some sort of causal connection between the two variables
  2. A causal connection in this context does not necessarily mean that one causes the other
  3. Examples (I think the only other examples) of other forms of causal links are the existence of a common cause, or that you are conditioning on a common consequence.",DRMacIver,2014-05-14 08:25:38
"Probably half the time the ""common cause"" causally linking two correlated variables is time, which is not what people typically mean by a causal connection.",redditleopard,2014-05-14 09:02:18
Show me in the correlation equation where that is the case.,drunken_Mathter,2014-05-14 08:27:43
"It's not in the correlation equation. It's in the notion of dependence. Independence implies uncorrelated, therefore correlated implies dependence.

As to where it's the case, the short answer is that it's a philosophical assumption that's used to help make sense of the very notion of causation. It's an axiom that dependence doesn't exist without causal relations, and it's a relatively fundamental one in the modern study of causality.",DRMacIver,2014-05-14 08:31:35
"Correlation is not a subset of dependence.  So if your 1st statement is false, what does that imply for the rest of your statements?",drunken_Mathter,2014-05-14 09:17:31
"Um. OK. I'm done with you now. Non-zero correlation very definitely does imply dependence, because independence implies zero correlation (See your first course in probability for details) and well, that's how contrapositives work. At this point you're either deliberately trolling or too confused for me to fix.",DRMacIver,2014-05-14 12:34:01
"Wow, referring me to a 1st course?  So no, it doesn't.  It indicated coincidence.  Not dependence.  1st courses typically gloss over details, so i guess you haven't progressed beyond it.",drunken_Mathter,2014-05-15 02:10:20
I'm referring you to a first course because you don't seem to have the knowledge you would have obtained from it. Please go away and obtain that and stop bothering the adults.,DRMacIver,2014-05-15 02:54:21
"your argumentation style shows me you are punching well above your weight on this.  Not surprising given the horrendous content you produce.

Can you even try to answer basic questions I've proposed?",drunken_Mathter,2014-05-15 03:24:38
"Ok, in response to this post, I think I'm going to be done as well - you're either a troll or something is very wrong. Independent random variables are necessarily uncorrelated because Cov(X,Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0 with the second equality being due to independence. The contrapositve of this is that correlated implies dependent. Independence and dependence are mathematical concepts and this is a consequence of the definition. I find it odd that you know ""what is accepted in academics"", and that you think OP is a crank who is ""punching well above his weight"" when this isn't as obvious to you as it is to him. ",NOTWorthless,2014-05-15 08:15:33
Dependence in this sense is that the sets have an intersection.  It's not the same thing as saying X causes Y or Y causes X.  Are you serious with this post?,drunken_Mathter,2014-05-15 10:04:09
"/u/DRMacIver  was clearly referring to the mathematical definition of independence and dependence when he claimed that correlation of random variables implies dependence. See, for example, [Theorem 4.2 here](http://www.maths.qmul.ac.uk/~pjc/notes/prob.pdf). I don't understand why this wouldn't be obvious. It has nothing to do with intersecting sets. ",NOTWorthless,2014-05-15 10:30:42
because we were talking about causation which is not that kind of independence.  Causation dependence:  X causes Y to happen.  Math dependence:  Intersection of a set is non-empty. (or linear dependence or whatever).  This is beyond mind boggling if people think they are the same thing.,drunken_Mathter,2014-05-15 10:33:07
"No one is claiming they are the same thing, but it was clearly what OP was referring to when he said correlation implies dependence (which he did **not** claim was the same as causation, he called correlation a special case of mathematical dependence). OP makes two statements

1. Correlation only occurs in the presence of causal forces.
2. Correlation implies dependence.

But they are **entirely different claims**. Number 1 was intended to be a philosophical statement, number 2 is a mathematical truth, and he never claimed 1 and 2 are equivalent.",NOTWorthless,2014-05-15 10:50:42
"This is about the issue known as ""no correlation without causation"".

See here for a scientific reference (sorry about the ugly link): http://books.google.fi/books?id=f4nuexsNVZIC&amp;pg=PA30&amp;lpg=PA30&amp;dq=%22no+correlation+without+causation%22&amp;source=bl&amp;ots=y1BNTtuCtj&amp;sig=tctp0FOYExW7WeBVSaiWsrzlH4c&amp;hl=en&amp;sa=X&amp;ei=6o1zU7STAYPsywPF8YHoBQ&amp;ved=0CC8Q6AEwAQ#v=onepage&amp;q=%22no%20correlation%20without%20causation%22&amp;f=false",grrrrv,2014-05-14 08:40:39
But correlation is correlated with causation.,chilloutdamnit,2014-05-14 12:05:41
"No statistical method ever implies a causal link. They all just show a relationship or an effect. That _could_ imply a causal nature between them, or a causal nature of a latent (or not measured) variable, or it may just show that there is an effect or a correlation -- no causation.


Causation is only shown via experimental design and requires direct manipulation by the experimenter to show that the change _at least in part causes_ an effect. ",dearsomething,2014-05-14 10:49:54
"There's actually a way to set up a method if you're careful, but it takes at least two variables that are presumed independent of each other and hold relationships that can be observed with your presumed cause and effect.

Following Pearl's most simple example, if you have factors A and B, along side presumed Cause and presumed Effect, you have to find A and B such that:

1) A and B are independent.

2) Given the presumed Cause, A and B are dependent

3) Given the presumed Cause, the relationship between A and presumed Effect is independent.

4) Given the presumed Cause, the relationship between B and presumed Effect is independent.

5) A and the presumed Cause are dependent.

6) B and the presumed Cause are dependent.

7) The presumed Effect and presumed Cause are dependent.

If these hold, then the only possible DAG structures that remain are those which have the presumed Cause as a parent to the presumed Effect.

In practice, this is pretty rough going when you can't follow a randomization procedure that you, as an investigator, can't enforce on the study, but it's not always impossible to find an A and B.",Neurokeen,2014-05-14 20:47:28
"Again, that's experimental design and not any particular statistic or analysis or measure.


While the field of ""statistics"" intrinsically includes experiments, math, probability, and interpretation, causation in the statistical, probabilistic,  and experimentation sense _requires_ designs intended to manipulate an effect.



Causation cannot exist, nor should even be discussed, without proper experimental design. The adages, and counter adages, of correlation ans causation are equally wrong.  ",dearsomething,2014-05-14 21:51:14
"The point is that this methodology allows an entirely observational 'experiment' without any direct manipulation of the experimenter to demonstrate a causal relationship, and also to determine the direction of that relationship. In other words, the last part of your post preceding the explanation is not correct.",Neurokeen,2014-05-15 05:18:57
"That's not a statistical methodology or approach. You've described _design_ methodology, not _statistical_ methodology. 

Causation cannot be inferred without proper experimental design -- regardless of the math you throw at your problem.",dearsomething,2014-05-15 08:54:15
"In a needlessly pedantic sense, ""cause"" isn't even in the language of statistics, but that example shows that given a set of purely statistical relationships, causation can be inferred only from those statistical relationships if we make the commonsense assumption that causes have certain properties that can be modeled as DAGs.",Neurokeen,2014-05-15 09:47:11
"No, it still doesn't. Likewise, ""commonsense assumption"" isn't in the language of statistics, either. 

You cannot show causation this way, you just show that another variable is related to two variables. Causation can only be inferred by direct manipulation in an experiment. You have to elicit the effect to infer cause.",dearsomething,2014-05-15 09:56:03
"&gt;Causation can only be inferred by direct manipulation in an experiment.

No, that's still incorrect.

Those seven factors rule out *every possible DAG* in which presumed Cause is not a parent of the node presumed Effect. As in, it exhausts the space for every situation but where Cause is a parent in the graph of Effect. This does not require intervention of the experimenter at all, but relies only on natural perturbations.

I would highly recommend reading up on Shipley, Pearl, or Rubin if you're going to continue this line.",Neurokeen,2014-05-15 10:11:46
"Even with the DAG approach it requires probabilistic assumptions where it reduces to the _probability_ of something having a causal effect. And a number of the assumptions in this approach must be _[very strong](http://jmlr.org/proceedings/papers/v6/dawid10a/dawid10a.pdf)_.

The only way to ensure a causal effect is through direct manipulation in an experiment. Otherwise: it's all still correlational and probabilistic. While those correlations could imply a causal nature, that causal nature cannot be confirmed without that direct manipulation. ",dearsomething,2014-05-15 10:23:00
"What we do in science is to make the jump from correlation (or more accurately dependence since correlation is a type of association) to causation as small as possible. We never really know if it is causation, in practice no one thinks like this, and rightly so (cause we would never get anything done).",fanofDK,2014-05-14 06:22:57
Or as the article itself says it could be pure chance. Roll a pair of dice enough and eventually both will give you a string of heads at the same time. That's not causality.,mniejiki,2014-05-14 06:15:47
"It depends on what you are comparing. It is not evidence of Die 1 (D1) being causally associated with D2. But there is a causal association between the *stopping value* of Die 1 (S1) and S2. They are causally linked by D1, D2, and our stopping rule.",NOTWorthless,2014-05-14 09:08:59
"Dice? Heads? I get what you are saying, but in either case, the calculated correlation would actually be close to zero. ",mhermher,2014-05-14 07:58:50
"No it won't be. Roll two dice. Wait until they both show the same value for 10 rolls in a row. Now plot those 10 data points.

Perfect correlation and no causality.",mniejiki,2014-05-14 08:04:45
"As NOTWorthless points out, if you follow the procedure you just described, you have a causal path from D1 to D2 that traces through the stopping rule here.

You've got causality.",Neurokeen,2014-05-14 13:11:11
It's also not really correlation. I'm talking about actual confirmed correlation between two random variables.,DRMacIver,2014-05-14 06:35:31
How do you tell one from the other?,mniejiki,2014-05-14 06:42:21
"Perfectly: with difficulty! In practice: Once you suspect it's there, run lots of experiments until you're pretty damn sure that what you're seeing isn't by chance. ",DRMacIver,2014-05-14 06:47:15
"Please don't listen to him. He's wrong. A casual link will yield a correlation in a model, but correlation never implies a causation is present where one was not already assumed.  In practice, people have an idea that, say the time of day drives temperature.  Then they say ""ah this correlation proves it"".  Technically the correlation measures how often two things occur together.  It never describes why.   

For example, when one regresses the dependent variable on the independent variable, that's an opinion or theory about causality:  the independent variable *causes* the changes in the dependent variable.

This guy is just another crank.",drunken_Mathter,2014-05-14 07:55:39
"This is patently false. If you have an experiment, you can infer causation from a correlation. I show a [cute little proof here](http://www.reddit.com/r/econometrics/comments/21rkw0/does_regression_assume_data_is_observational/cgg8eay). I didn't assume causation; I assumed *random assignment*. This allowed me to measure a causal effect *in expectation* (ie E[Y_1 - Y_0], the ""average treatment effect"" where we imagine that just prior to intervention, there existed two ""potential outcomes"", Y_1 under treatment and Y_0 under control, which could have been realized had the randomization procedure played out differently).

The nice thing about this causal framework is that your assumptions come into play in *how you design* a study. If you have treatment assignment in hand, then assuming random assignment of treatment is trivial. Similarly, in an observational study, you have to argue for some sort of *conditional randomization* -- for two units that look alike over a vector of covariates, it was only *random chance* that caused their treatment assignment to differ. Thinking about things like this in terms of randomization is a very powerful way of thinking about statistics and causality.",hadhubhi,2014-05-15 00:12:42
"I just read your proof. That doesn't prove causality from correlation.  

So to make it absolutely clear:

Give X, Y and Corr(.,.), where does Corr(X,Y) attach the property of causality to X and/or Y?  You only have knowledge of X, Y, and the function Corr(.,.), (and anything Corr uses, E[.], Var(.), etc).  

The question is where the property of causality is attached, and if it is attached by application of Corr(.,.) to X and Y.  

If you can show that Corr(X,Y) *with no other context* attaches a property of causality, only then can a statement like ""Correlation implies Casaution"" make any kind of sense.  

The causation comes from somewhere else, not the scalar value p.  You can compute p with no context, and a p given without any context cannot imply causation.

Causation is not the size of a statistic value.  It's inherent to the system being studied.  It's part of the non-statistic information.  Attempting to show attachment via computation of correlation is the same as *removing* causation from the system definition and attributing it to the scalar value of Corr(X,Y).  I don't know anyone who would want to do that. ",drunken_Mathter,2014-05-15 03:43:47
"I think you may be confused as to what that proof shows. I never claimed to ""prove"" causality from correlation. I proved that it is possible to identify an average causal effect given an assumption of randomization of treatment assignment. That's quite a different thing. If we don't have randomization, we don't necessarily have equality between p(1-p)(E(Y|X=1) - E(Y|X=0)) and p(1-p)E(Y_1 - Y_0). In other words, the group that was assigned treatment may not be equivalent to the group that wasn't. Under randomization, they will be (in expectation -- not necessarily in finite samples).

In any event, we're basically in agreement that inferring causation from correlation requires some ""special sauce"". I would agree that causation flows from the ""system definition"", not from correlation. My point was merely that it isn't necessary to pre-assume a causal relationship. Knowledge of the assignment process can be enough. 

Although I should note that sampling theory and the potential outcomes framework imply that knowing the treatment assignment process actually *is* a statistic of the data -- you just have to include the sampling process in ""the data"". A sufficient statistic for a sampling procedure is a vector of unit labels and sampling probabilities. And in this potential outcomes model, assignment probabilities are just sampling probabilities for particular potential outcomes.",hadhubhi,2014-05-15 09:55:51
"&gt; My point was merely that it isn't necessary to pre-assume a causal relationship. Knowledge of the assignment process can be enough.

Ah, I see, in that case, it makes perfect sense.  Thanks for clarifying.",drunken_Mathter,2014-05-15 10:05:40
Splines are even worse.,homercles337,2014-05-14 10:55:04
"A few of my favorites: 

* http://accidental-art.tumblr.com/post/67479068168/i-was-trying-to-draw-a-3d-maze-using-processing

* http://accidental-art.tumblr.com/post/66894502343/phylogenetic-tree-of-teosinte-that-accidentally

* http://accidental-art.tumblr.com/post/66791202482/ggfail-final-product-via-sam-weiss

* http://accidental-art.tumblr.com/post/66482319023/i-tried-to-plot-predicted-votes-by-party-instead

* http://accidental-art.tumblr.com/post/66392845108/this-is-my-desktop-background-i-created-it-in

* http://accidental-art.tumblr.com/post/66114245606/first-try-at-making-a-choropleth-using-ggplot2-at",ThisIsDave,2013-12-14 15:08:35
"Oh my goodness, I think I'm in love.",biobonnie,2013-12-14 18:16:00
"This is brilliant. As a statistician, I have my own share of accidental visual arts, but never thought about sharing it with others.  ",quadrobust,2013-12-15 04:35:52
Would be nice if you could provide the plotting command for the rio-plot.,Stat_apprentice,2013-12-15 07:27:56
"Every time you upload, Edward Tufte kills a kitten. ",Quixodion,2013-12-14 19:48:22
There is nothing unique here.,homercles337,2013-12-14 19:15:13
"It is not Benford's law. There is no scale invariance in ATM PINs.

There is something else afoot here.",ROBZY,2012-05-18 18:19:21
"If people are choosing their pins based on other numbers then it might apply, at least to some degree.",,2012-05-18 18:29:12
"Remember though, people are not only typing their pins on those keypads. They also may be pushing 1-3 for ""Yes"" ""No"" responses, or cashback, or some other reason independent of PIN. ",Palmsiepoo,2012-05-18 20:13:41
And getting a quick 10 or 20 dollars.. and hardly ever 90 or 70. ,Calpa,2012-05-19 04:34:54
Ohh... good point.,ROBZY,2012-05-18 18:31:24
"Now that I think about it, you're right, Benford's Law describes the leading digit, which we can't deduce from the ATM PIN wear patterns.  For example, if everybody had the same PIN ""9111"" then the PIN pad would show 3x the wear on the 1 as the 9.",sintaur,2012-05-18 19:38:31
"I would say this pattern is mostly due to dates being picked as a pin or part of a pin. Another factor might be the amount people are withdrawing/depositing etc, which may be why 3 is less used than 6-9.",Polycephal_Lee,2012-05-18 19:57:47
"10-19 and 19XX-20XX are frequent in dates, 31 and 31 or even 30XX not. ",shele,2012-05-19 02:30:31
"Precisely. ATM PINs are at a nominal level of measurement (i.e. they are equivalent to names; cannot be ranked or ordered), so Benford's Law shouldn't apply.

Edit: Just saw GrynetMolvin's post below and he's right; withdrawal amounts are sometimes entered using the keypad. If that's how that ATM is programmed, then Benfold's law may partially explain that pattern.",schotastic,2012-05-19 09:26:47
"Judging by the pattern, you have to wonder whether something else is involved here. It wouldn't surprise me if there were a strong tendency for people to choose numbers at the top left of the keypad because of writing (but what about 0? I don't know). It would be interesting to look at keypads in the Middle East to see if they show the opposite.",amemut,2012-05-18 17:04:00
"Perhaps peoples birthdays, the day of the month usually starts with 1 or 2. The 7 and 8 perhaps the year of birth for the 70s and 80s most often.",zsakuL,2012-05-18 22:57:33
"You usually enter the cash amount on the keypad as well, which do follow benfords law. ",GrynetMolvin,2012-05-19 00:21:57
"It shouldn't, no.",Drunken_Economist,2012-05-19 13:18:29
"This is really neat. Mine was automatically generated for me though, which makes me wonder whether they are generating them truly randomly. ",mikitronz,2012-05-18 17:57:01
"&gt;**truly** randomly.

&amp;#3232;\_&amp;#3232;",myfourthHIGHaccount,2012-05-18 20:17:13
"Yeah, totally acknowledged. The expectation from pin users it's that it comes from more than 4 digits, not that if you compared all pins they would be indistinguishable from random. But you're right, I didn't really mean random.",mikitronz,2012-05-18 21:56:44
"Well they could be using a hardware random number generator... Probably aren't, though.",Neurokeen,2012-05-20 10:06:18
"l guess l should stop including the ''Enter"" key in my PINS... ",jmwotw,2012-05-18 18:51:25
"Well except for the digit 3.  Stupid outlier.
",sintaur,2012-05-18 16:02:09
Hahahaha,jredwards,2012-05-18 16:41:23
"With only minor differences in the sensitivity this has been demonstrated multiple times from the 1970s and onwards. What is typically ignored in these articles is how much the mode of representation, the formatting, matters. If it's presented as '100 out of 10,000 women' rather than '1% of women' etc., they typically do up to three times better (46% in a 1995 study).

Basically, it all boils down to a neglect of base rates, which is also the cause of a number of other statistical biases. Statistical sophistication alleviates the problem greatly, and should be highlighted much more in the relevant professions. However, formatting is a promising debiasing intervention, especially when attempting to debias others. If people intuitively grasp some representations of data better than others, we should use that to our advantage.",Congruence,2014-07-08 11:25:45
"But how is it reported in the literature we read? Rarely as X out of Y unless prevalence is &lt;1 in 10,000. 

This is a simple calculation of PPV. There is no reason most doctors shouldn't be able to do it. While statistics education is part of our education (a very small part), you could miss every statistics question on every board exam and still make it through.",medstudent22,2014-07-08 15:08:04
"I dunno about your medical school, but in mine the stats were something of a joke. Not because of the instructor, either (he was very good, and the phd students would pack his optional talks until they were standing room only). This was at a ""top US medical school"" too, one many people apply to as a reach school. 

I just don't think medical students care about statistics until they're well past their classroom years and it's too late for hours of proper instruction. They have to learn on the fly and as a result, they learn some improper things.

I think we need to do away with the ""a /(a + c)"" method of teaching biostats. It doesn't teach concepts, and thus students don't really *learn* it any more than they need to for board exams. ",BillyBuckets,2014-07-09 03:55:16
"I completely agree. My school had one week of self education which was an absolute joke. I'm not surprised we have these problems. I would like to see statistics treated as a block of education like biochemistry or an organ system and I would want to see it tested like a statistics course where you have to do things out instead of answer multiple choice questions. 

Requiring one undergrad stats course for entry would be also be a start, but only a small one.",medstudent22,2014-07-09 15:02:15
Doctors hate it! This one weird trick can lead to more plausible data analysis!,GratefulTony,2014-07-08 10:40:22
Can't believe /r/statistics is the first sub I'm subscribed to to start using Buzzfeed-style ledes...,ctornync,2014-07-08 14:17:48
You won't believe what subreddit started using Buzzfeed-style titles!,VivaLaPandaReddit,2014-07-08 15:58:46
"Which statistical mistake are you? 

Top 27 statistical fallacies your doctor will make! 

Only 90s kids will rely on these heuristics!",BillyBuckets,2014-07-09 03:48:06
"Scary. Something is seriously wrong with our medical establishment and training. 

I have met doctors that didn't even know the structure of dna or what exactly it did. ",,2014-07-08 10:09:59
"&gt; Something is seriously wrong with our medical establishment and training. 

What needs to happen is to teach students how to use probability as a calculus for reasoning about uncertain hypotheses. Unfortunately the stuff they learn in cookbook statistics classes is way off the mark -- worse than useless. ",midianite_rambler,2014-07-08 23:18:26
"Why would I care if my family doctor knew the structure of DNA? How will that help him treat any issue I might have? Your comment reminds me of a study which showed radiologists reviewing altered imagery failed to notice a cartoon gorilla inserted into the scene, but since lesions don't look like cartoon gorillas, this isn't the problem it was made out to be. I'd rather my doctor was able to interpret statistics and guide my healthcare choices than tell me irrelevant factoids.",isarl,2014-07-08 14:10:40
"Obviously it depends on whether we're talking about a specialist, a general practitioner, or a family doctor, but the basic structure and functionalities of DNA is high school biology...",rottenborough,2014-07-08 14:59:43
"DNA structure is not an Irrelevant factoid. Understanding it is crucial to know the causes of cancer, how genetic medicine works, understanding modern pedigrees for inherited diseases, etc.",,2014-07-08 16:51:21
"Uh if your doctor doesn't know the structure of DNA than run as fast as you can. I don't even know how you could get into Medical school with that ignorance, but I wouldn't stick around to ask.",sagacioussage,2014-07-21 07:42:22
"Sorry, but OP's title is a misrepresentation of what the article says, and is wrong:

""**In one session**, almost half the group of 160 gynaecologists responded that the woman's chance of having cancer was nine in 10. **Only 21% said that the figure was one in 10 - which is the correct answer**. That's a worse result than if the doctors had been answering at random.""

That's **21% of doctors in one session**, chief, not **""21% of doctors.""**  Those statements are *very* different.",yakattackpronto,2014-07-08 11:21:20
"While you are completely correct and it is an important distinction, it is a very real problem with a long pedigree from other studies like it. The normal number cited is around 15% correct answers, going back to the 70s.

I'm just mentioning it so that people won't feel the problem doesn't exist. And again, you are completely correct in this instance!",Congruence,2014-07-08 12:06:36
"You're totally right, thanks for pointing that out. Although I wasn't trying to misrepresent anything, it should have said ""*these* doctors,"" not ""doctors""


Incidentally, you've also just answered my second question. Sometimes people take the result of an experiment as absolutes (accidentally or otherwise). Realistically, the sample may sometimes not represent the population. Anything else to add?",CHAINSAW_VASECTOMY,2014-07-08 13:20:00
"Sorry, I responded hastily and threw in ""chief"" because I was annoyed - **I was being a dick.**

The questions you're asking are good questions that most people gloss over all the time. /u/homercles337 points out that the sample was random, and that my point is invalid.  Your second question gets to why I think my point is still valid - a single finding isn't good enough. It needs to be replicated, etc., before broad generalizations can be made as to the significance and applicability of the findings.

Anyway, like I said, I responded to you in a dickish way - *I'm sorry.*  I just finished a book that really hits on what happens when people suck at math, specifically statistics and probability called ""Innumeracy,"" by John Allen Paulos.  As someone who works in the applied area of stats and is interested in how stats and research is misused/misunderstood, I found the book to be mostly an enjoyable, casual read.  You might enjoy it.",yakattackpronto,2014-07-08 14:38:50
"&gt; Your second question gets to why I think my point is still valid - a single finding isn't good enough. It needs to be replicated, etc., before broad generalizations can be made as to the significance and applicability of the findings.

Based on /u/Congruence's [answer](http://www.reddit.com/r/statistics/comments/2a5lhu/only_21_of_doctors_could_answer_this_statistics/cirtu1u?context=2), it seems like this is a well known phenomenon. 

I would personally be concerned about the fact that this study only sampled gynecologists, which would seem to me to be a highly non-representative sample of medical professionals (which is what the article, at least, seems to be generalizing as the population). ",TeslaIsAdorable,2014-07-09 07:46:38
"&gt; As someone who works in the applied area of stats ...

I cant imagine that someone so clueless works in stats.  I suspect your tenure in stats will be short.",homercles337,2014-07-08 19:24:36
"&gt; That's 21% of doctors in one session, chief, not ""21% of doctors."" Those statements are very different.

Randomly sampled.  Sorry ""chief"" but you are wrong.",homercles337,2014-07-08 13:33:26
"The point isn't to do something only randomly, its to do it representatively as well. Those two do not always coincide.",drotoriouz,2014-07-08 15:49:34
"Uh, clueless you are, no?",homercles337,2014-07-08 17:10:46
Randomly sampling usually is representative but not always. Obviously it depends on the purpose of the study but the point of sampling is to collect the sample that's representative of your study population so you can make a proper inference.,drotoriouz,2014-07-08 17:23:53
Could you give me an example of random sampling from the population that does not lead to a representative sample? (non-troll trying to learn here),selectorate_theory,2014-07-08 19:16:16
"The biggest issue is that its random. If you make a small sample, there is a significant probability that you over-represent one class or another of the subpopulations of interest.

Say you sample 10 people. A representative sample would have 5 men and 5 women. The probability of getting 0-3 men or 0-3 women (very lopsided / not representative) is greater than the probability of getting 4-6 men (a representative sample).

If you can sample many thousands of people, the issue starts going away some - but in real life it can be almost impossible to get a truly unbiased random sample, so we have to resort to other sampling methods.",EdwardRaff,2014-07-08 21:16:06
One that isn't very big.,Deleetdk,2014-07-08 20:05:16
"Uh, still clueless.  So you have had one statistics class?  Good for you.",homercles337,2014-07-08 17:37:54
"I respectfully disagree that random sampling makes my point any less valid.  However, I respectfully agree that my use of ""chief"" was unnecessary and dickish.",yakattackpronto,2014-07-08 14:40:20
Still wrong.  Random sampling is everything.  If you had one stats class you would know this.  Why are you even posting in /r/statistics if you know nothing about the topic?,homercles337,2014-07-08 17:09:23
"Is this a serious comment?  Random sampling is *not* everything, and is simply a key component with respect to interpreting studies and performing analyses.  You're right to point out that this is important, but you're missing the point of my comment and incorrect in that it is ""everything.""  When it comes to applying the results of a study/analysis to groups of people or real world situations (in this case, making generalizations about doctors everywhere), one study's findings are not enough.  One randomized sample is not enough. ",yakattackpronto,2014-07-08 18:03:30
It'd be great if you could elaborate what else we need to consider in order to generalize the result?,selectorate_theory,2014-07-08 19:17:11
He cant because he is wrong.,homercles337,2014-07-08 19:21:01
"I mean, I can be charitable and consider that he may be thinking about whether random sample of US doctors generalize to Indian doctors for example. In any case, I think it'd be better to hear him out.",selectorate_theory,2014-07-08 19:32:48
"As i have said in other replies, this character is utilizing flawed logic.  I am really not sure why he is defending this flawed logic. He even claims to be a statistician. Sad.",homercles337,2014-07-08 19:41:48
"You are making no sense.  I hope you do not consider yourself a statistician because, to be frank, from what i can figure out from your convoluted post you are wrong.  I really despise your attitude because you are clearly wrong and yet you use flawed logic to defend yourself.  Randomly sampling doctors is perfectly valid, i have no clue how you think it is not.  ",homercles337,2014-07-08 19:19:49
"You're getting downvoted to oblivion, but you're right. It was a poor comment. Upvoted in this sub of all places. Anyway, its probably cuz you're being kind of a dick about it. ",mhermher,2014-07-08 23:14:52
"Why do I get 0.091743 and not 1 in 10. Whats wrong with my approach?


I’ve said. 

P( + | Cancer) = 0.09,  P(Cancer) = 0.01, P ( + | Not Cancer) = 0.09, P(Not Cancer) = 0.99


So by Bayes Theorem.  

P(Cancer | +) = P(+|Cancer)*P(Cancer)/[ P(+|Cancer)*P(Cancer) + P(+|Not Cancer)*P(Not Cancer)]


= 0.9*0.01/[0.9*0.01 + 0.09*0.99 ] = 0.091743
",testaccount99999,2014-07-09 08:13:07
[More on this specific example of a lack of Bayesian reasoning.](http://yudkowsky.net/rational/bayes),KeasbeyMornings,2014-07-08 23:02:34
Nice,ilovemuppets,2012-05-26 11:07:58
"I like everything about this project--the 2-minute gimmick &amp; name is pretty clever and the guy actually sounds like he is having fun making these videos.

This should be great for people who want to learn R but feel intimidated by the learning curve.",mmmmatt,2012-05-26 11:48:10
I'm in that exact boat. I know I should learn R but don't have the time at work to set aside...this is now my summer project. ,glen_ford,2012-05-26 18:24:49
This is awesome - sure beats the hell out of reading a boring tutorial book!,socialsciencegeek,2012-05-26 15:59:00
"Good stuff for basic R programming, but that guy is annoying to the point of being unwatchable. ",Eist,2012-05-26 11:08:13
"Yeah it sounds like the Moviefone guy is giving me an R tutorial. 

It's actually kinda hilarious.",misplaced_my_pants,2012-05-26 18:22:45
"Some of them are actually kinda handy, but I end up wishing there were text analogues. ",therealprotonk,2012-05-26 14:57:52
"At first I thought your comment was too harsh, but yeah after a few videos his mannerisms become very tiring.  ",BillyBuckets,2012-05-27 17:12:43
Commenting just to save this,snosrep,2012-05-26 21:21:55
nice R tutorials,bdobba,2012-05-27 07:08:21
"Thanks so much for that link! Needed to learn it, but there's a lot of different stuff all at once.",,2012-05-27 08:54:28
No I feel dumb because I thought I was good at statistics after taking a couple classes.,Astrocytic,2014-07-31 18:08:38
"I think it is an amazing experience. People have different interests in statistics and it is a nice place where all that information is compiled together.

While I feel most responses are good, I have seen some responses that were very bad and sloppy. Sometimes I point out mistakes and am told that theoretical statisticians have no concept of practical applications.

I feel the standard normal approximation to some statistics like binomial data is used too frequently when people ask for advice. 

My Masters program did extremely well on hammering in details and accuracy and not being sloppy. So it's hard for me to not notice small things.



",Corruptionss,2014-08-01 15:29:56
We are on average quite impressive.,blank964,2014-07-31 20:29:19
I think there might be a few outliers bringing the average up. ,Racingram,2014-07-31 20:38:08
"Don't worry, I'm singlehandedly bringing it back down.",AllezCannes,2014-07-31 20:44:53
Mean checking in.,tafsonworks,2014-07-31 22:21:06
We are right 95% of the time!,octocoral,2014-08-01 07:28:31
"Agreed!! I'm getting a PhD in social sciences and working towards an applied masters in statistics.  Its been tough to learn, especially since I haven't had any heavy math in about 8 years.  Anyway, to me, this is one of the best places to learn and ask questions, which I did more of under a different user ID (lost my password and had to create a new one).  I particularly like the blog 'advertisements' that are posted here; for me, its difficult to find high quality material on the Internet. Also, I like seeing 'real' statisticians discuss strategy in comments section. I would say to everyone, ""Keep it up! People are watching/reading/learning even if they're not up voting.""",_Widows_Peak,2014-07-31 20:13:14
"I'm in the same exact situation as you! I don't contribute a lot, but I soak in from the sidelines.",sjgw137,2014-08-01 07:50:30
"if you think this is something, you should hang out on crossvalidated",shaggorama,2014-08-01 10:48:49
"Proportion of threads I could reasonably contribute to here: maybe 1 in 5.

Proportion of threads I could reasonably contribute to on crossvalidated: maybe 1 in 20.

That being said - a lot of the tough questions on crossvalidated go unanswered, since they're just that - tough. To those who haven't been there yet check it out at the link below. 

http://stats.stackexchange.com/",statisticalhornist,2014-08-01 11:54:16
[deleted],,2014-08-01 17:16:00
I don't think you need any rep to submit an answer,shaggorama,2014-08-01 19:30:08
I just graduated with an undergrad in math/stats. I feel like I know nothing after venturing into this sub. It is encouraging though as well.  ,sir_punsworth,2014-07-31 22:15:00
"Well that's comforting. I'm almost done mine and I feel the same way :P

It's nice to see what I don't know so I have some direction to venture into when trying to learn new things.",EPYJAO,2014-08-01 06:10:32
"Same econ/stats, I feel ridiculously clueless",getonmyhype,2014-08-01 15:10:07
Psychology here. I don't understand most of what's going on. But it looks like I might one day.,HungryAuryn,2014-08-04 11:30:54
"It is very impressive and also very intimidating at the same time. The further I progress in my applied stats program, the more comfortable I am posting here. 

However, I do feel that many in the subreddit do not participate for fear of being wrong. Statisticians here, can get VERY fiery. That being said, the quickest way to get an answer on the internet is to post the wrong answer :D.



",Adamworks,2014-08-01 06:25:41
"&gt; The quickest way to get an answer on the internet is to post the wrong answer.

That is now one of my favourite quotes.

&gt; However, I do feel that many in the subreddit do not participate for fear of being wrong.

This certainly applies to me, i've only been in the industry 2 years after graduating from my master's and am often worried about posting here incase it's wrong. It might make me look like i'm big-headed answering something i don't know.",kurokabau,2014-08-01 08:09:59
"Agreed. Statistics, its theory and applications, really amazes me. The main reason that I'm here mostly is to look at the problems and data that people have especially the unusual ones.",boogahwoogah,2014-07-31 23:04:17
"It also shows me how much I have to learn and the learning is a lifetime process, not just get a degree and call it a day.",,2014-07-31 19:51:50
"Completely get what you're saying here, and I've thought the same thing! Comparatively speaking to other ""academic"" type sub-reddits, the topics asked and talked about on here get pretty specific and advanced! 

Keep in mind, no single person is answering everything, but the collective knowledge as whole is quite impressive! I'm a biostatistics PhD student myself, and I frequent this sub on a regular basis because I'm always reading and learning new things! Often when I see something posted about a subject I know little about on here, I'll go to my textbook and/or other sources just to learn more about it! 

",Distance_Runner,2014-08-01 08:44:51
"Statistics is a massive subject, and no one can come close to mastering most of it. In many subjects (I'm a psychologist) I can bullshit and say something that might be sensible, whether you're talking about neurons or bystander effects or cognitive behavioral therapy.  In stats (and technical subjects) you just can't, 'cos it will be obvious you're talking nonsense. 

A friend of a friend moved from nuclear physics to statistics. He said that nuclear physics is harder than stats. But only a little.",jeremymiles,2014-08-01 08:57:25
"Yeah, I come here to learn how to think like a statistician, which is most important to me.",dbzgtfan4ever,2014-08-01 08:58:18
"It is to be expected from such a self-selected group. Most people don't like statistics and there are popular memes against it ""lies, damned lies, statistics"".",Deleetdk,2014-08-01 10:28:17
"Perhaps not a traditional joke, but Biostatistics Ryan Gosling cracks me up pretty good.

http://biostatisticsryangoslingreturns.tumblr.com",khanable_,2013-05-06 19:44:28
glorious,dclaz,2013-05-07 06:28:57
I always give people the fake compliment of being on top of the bell curve!,mdknights_23,2013-05-06 22:26:03
I like it. I'm going to remember that one.,vmsmith,2013-05-07 07:41:00
And further bolster their egos by admitting to be in the bottom on the Far right side,Icanflyplanes,2013-06-29 14:12:36
"Not really a joke, but it makes me laugh hysterically:

Two random variables were talking in a bar. They thought they were being discrete but I heard their chatter continuously.
",armchairdetective,2013-05-07 02:36:32
That absolutely qualifies as a joke. Especially when it involves two people at a bar.,TheRealDJ,2013-05-10 10:35:52
"A guy is flying in a hot air balloon and he's lost. So he lowers himself over a field and shouts to a guy on the ground:

""Can you tell me where I am, and which way I'm headed?"" ""Sure! You're at 43 degrees, 12 minutes, 21.2 seconds north; 123 degrees, 8 minutes, 12.8 seconds west. You're at 212 meters above sea level. Right now, you're hovering, but on your way in here you were at a speed of 1.83 meters per second at 1.929 radians""

""Thanks! By the way, are you a statistician?"" ""I am! But how did you know?""

""Everything you've told me is completely accurate; you gave me more detail than I needed, and you told me in such a way that it's no use to me at all!""

""Dang! By the way, are you a principal investigator?""

""Geeze! How'd you know that????""

""You don't know where you are, you don't know where you're going. You got where you are by blowing hot air, you start asking questions after you get into trouble, and you're in exactly the same spot you were a few minutes ago, but now, somehow, it's my fault!",SickSalamander,2013-05-07 09:21:32
"This is really a math joke, but I've always liked it...

A Mathematician and an Engineer were part of a psychology experiment. Each was told to walk into a room, assess the situation and do what they felt necessary.

In phase I, the Engineer walked into the room, and saw a fire in a waste basked in one corner. In the opposite corner there was a fire extinguisher, and a table was in the middle of the room. The Engineer took the fire extinguisher and put out the fire.

The Mathematician walked into the room, encountered the exact same situation, and took the exact same actions.

Both were congratulated and told to take a break for lunch while the rooms were set up for phase II.

After lunch, in phase II, the Engineer walked into the room. This time the waste basket was on the table, and there was still a fire in it. Plus a fire extinguisher in the corner. He took the fire extinguisher and put out the fire.

The Mathematician walked into the room and encountered the exact same situation. He took the waste basket with the fire, removed it from the table, and put it in the corner opposite the fire extinguisher, thereby reducing the second problem to the first problem, which he had already solved.

",vmsmith,2013-05-07 07:49:40
"I know a similar joke. An engineer is at his desk and a small fire erupts in the hall. He runs out of the room, sees the the sandbox uses the sand to put out the fire. A physicist works at her desk, she notices the fire, runs out of the room, sees the box with sand, makes a circle of sand around the fire to isolate it, and sits down to watch the fire. A mathematician is in his office. There's a fire in the hallway. He runs out of his room, sees the fire, sees the fire extinguisher, realizes that the problem has a solution, loses interest and gets back to his desk. ",lazylifelonglearner,2013-05-07 11:38:54
There was a statistician that drowned crossing a river... It was 3 feet deep on average. ,anatiferous_outlaw,2013-05-06 20:21:57
"Three statisticians go out hunting together. After a while they spot a solitary rabbit. The first statistician takes aim and overshoots. The second aims and undershoots. The third shouts out ""We got him!""",chilloutdamnit,2013-05-06 21:08:44
"I’ve heard this one with a physicist, an engineer, and a statistician.

The physicist assumes a vacuum and undershoots.

The engineer adds a fudge factor for air resistance and overshoots.

The statistician yells “We got him!”",BrowsOfSteel,2013-05-07 02:49:43
yours is better. more stereotypes per line.,Bayesbayer,2013-05-07 16:12:09
20% more stereotypes per stereotype!,Surufka,2013-09-09 23:45:13
"People call us stats nerds, but the reality is we're sum of the least squares.",casualfactors,2013-05-07 00:08:29
Can you explain please? Something to do with being square...,anananananana,2013-05-07 06:45:43
Minimizing the squared sum of the error is the basic point of OLS and that sum is more or less what the residuals are.,casualfactors,2013-05-07 08:18:25
http://en.wikipedia.org/wiki/Ordinary_least_squares,SParadise,2013-05-07 06:56:20
"Yea, thanks, I knew this...So the point is we are not exactly nerds, but approximately? Or what?",anananananana,2013-05-07 07:03:47
"Square is 'uncool'.  Least squares, least uncool.",anonemouse2010,2013-05-07 07:28:34
"This is a Statistics insult. Great for using on people who don't understand it.

""Wow, you really are at the top of the bell curve!""


EDIT: Another joke!

Think about how stupid the average person is. Then realise that half the world is even more stupid.",Odd-One-Out,2013-05-07 03:19:40
the second is from George Carlin.,DonDriver,2013-05-07 07:44:48
"Except the second one isn't a joke, it's a sad, scary reality. (I'm totally using the first one though!)",,2013-05-07 07:18:33
"Except it's probably inappropriate both as a stats joke and as an expression of reality, unless you want to qualify it with an assumption that mean = median.",mail124,2013-05-07 08:08:09
"Average can be interpreted as mean, median, mode, or another central value.",thderrick,2013-05-07 08:38:23
Don't overthink it man.,,2013-05-07 08:16:13
"While technically correct, the nature of the bell curves suggests that the vast majority of people are relatively average. Extremely stupid people are as rare as extremely smart people. ",SickSalamander,2013-05-07 18:02:43
"That's not the point. The point is the average person is pretty dumb, and they're smarter than half the other peoples!",,2013-05-07 18:29:03
"Say what you will about &lt;insert recent government educational policy&gt;, but at least it hasn't made things worse in that regard.",atcoyou,2013-05-07 08:44:25
"I know I'm pretty weird. I'm still young, though. Once my age is greater than 30 I'll be approximately Normal.",beck1670,2013-05-07 05:11:31
This describes me to a t,jerenept,2013-05-07 18:22:05
"What do you call two coin flips on their honeymoon?

Bernoulliweds",2bds,2013-05-07 08:42:55
"In spanish, the  mathematical expectation is called ""Esperanza matemática "" (mathematic 'hope'). So:

A woman ask to the physician, after her husband operation: ""And, doc, how the operation went?"". The doctor, which knows statistics, said: ""Like a Couchy distribution"".

At least my colleagues laugh when heard it first time :P",clbustos,2013-05-06 21:34:18
"3 Americans are on a train through Scotland: a statastician, a physicist and a mathematician. They all see a brown cow out the window.

The statastician says ""Oh, cows in Scotland must be brown!""

The physicist says ""Well, we know there's *a* brown cow in Scotland.""

The mathematician says ""Not quite! We know there is at least one cow in Scotland, and at least half of it is brown!""",gman2093,2013-05-07 10:16:19
How do you tell an introverted statistician from an extroverted one?  The extroverted one looks at the *other* guy's shoes.  ,misanthrope237,2013-05-07 11:29:02
"My favorite xkcd sketch:
I don't understand why they keep teaching the null hypothesis.  I saw a study a few years back that disproved it. ",sjgw137,2013-05-07 12:37:35
Here's the [xkcd comic](http://xkcd.com/892/),thebrownkid,2013-05-07 15:48:40
"This is more of a fact than a joke:
The majority of men have an above-average number of legs.

----------

Another one:

    I! /(Pikachu! * (I - Pikachu)!)
",Alhoshka,2013-05-07 15:04:42
[This one from xkcd.](http://xkcd.com/1132/),AxiomL,2013-05-06 23:07:46
[Another](http://xkcd.com/552/),SickSalamander,2013-05-07 09:16:30
that one is my personal favorite.,gman2093,2013-05-07 10:09:19
"Not wholly statistical in nature, but this amused me...

An infinite number of mathematicians walk into a bar.

 The first one orders a pint of beer.

 The second one orders a half pint and the third one orders a quarter pint.

 The fourth one orders an eighth of a pint.

 Before the fifth﻿ one can order, the bar tender says ""You're all idiots."" and pours two beers.",enilkcals,2013-05-07 05:09:10
"Whenever I heard the joke, the bartender said, ""You guy need to learn your limits.""",sevencorvina,2013-05-07 05:39:07
I lost my faith in humanity once I realized that the average person is also the meanest.,alwaysonesmaller,2013-05-07 05:40:30
"My favorite is ""Do you understand statistics?"" ""Maybe""",bearcat89,2013-05-07 08:14:06
"A statistician makes a move on a girl, but she says she just wants to fool around. The statistician has an idea - he gets a hand job and a foot job. On average he had sex, but he was fairly deviant!

What's the most fun quantity to calculate? The standard error of course! All statisticians like SE(X)!

Woah! 2*N(0,1)! It's almost a - a - a TRIPLE gaussian! What does this mu? (To be said in the double rainbow guy voice)",beck1670,2013-05-07 05:10:27
"You'd really want &amp;lambda;N(-1,1) + (1-&amp;lambda;)N(1,1) for a double Gaussian. Or you could go for the double exponential.",samclifford,2013-05-07 06:54:22
"It's not exactly a statistics joke but it's one of my favorite simpsons quotes:

Kent: Mr. Simpson, how do you respond to the charges that petty
       vandalism such as graffiti is down eighty percent, while heavy
       sack-beatings are up a shocking nine hundred percent?

Homer: Aw, people can come up with statistics to prove anything, Kent.
       Forty percent of all people know that.",failurepile,2013-05-07 07:46:01
"A statistician goes to a job interview is asked a single question, “What is 2+2?” He goes into a room, and emerges ten hours later. Sweating and panting, he comes back to the interviewer and whimpers, “...what do you want it to be?""",greensmurf30,2013-05-07 08:34:11
"I heard a much longer version of that one, but the punch line (identical) is attributed to a marketer.",bobbyfiend,2013-05-07 18:53:18
75.6% of all statistics are made up on spot.,,2013-05-06 21:40:23
And the other 30% contain some kind of basic mathematical error.,randomsnark,2013-05-06 23:54:38
You might want to check your work on that one.,alwaysonesmaller,2013-05-07 05:41:08
*whoosh*,99trumpets,2013-05-07 08:04:12
"You know what they say, there are 3 types of people in this world.

Those who can count, and those who can't.",atcoyou,2013-05-07 08:49:17
"that is only two types of people  
did you forget one",randomsnark,2013-05-07 18:37:50
"You must be thinking of the 10 types of people.  Those who can understand binary, and those who can't.",atcoyou,2013-05-08 06:20:06
"60% of the time, it works every time.",snapetom,2013-05-07 07:56:36
"Clever one, my friend ;)",clbustos,2013-05-06 21:47:07
Haha. My friend used to use this in debate in high school all the time... Different number every time. ,agent229,2013-05-07 06:23:51
"What did the statisticians discuss while going to the  bathroom? Their p-value.

Statisticians: We do it with confidence, frequency, and variation!

Edit - Another joke:

What do you call a baby eigen sheep? A lamb, duh",GreenWolfie,2013-05-07 11:10:04
"I don't know any great stats jokes... I once heard one about bell curves and Pavlov's dogs...  I'm a psych major after all.  I guess with stats one of my favorite subjects is prediction.  It just seems so powerful to be able to predict values on one variable by understanding it's relationship to another variable, and modeling that relationship....  Ah, but I regress.",ForScale,2013-05-07 06:22:48
[deleted],,2013-05-06 20:46:00
http://i.imgur.com/3Gpey.jpg,WhyCantWeHavePeace,2013-05-06 20:54:05
[deleted],,2013-05-06 20:57:40
"chill out, damnit.",FDBluth,2013-05-06 21:34:05
"I agree, he was a bad statistician. Probably a terrorist too.",anananananana,2013-05-07 06:50:04
"That's why he's a professor. Those who can do, those who can't teach.",tacojohn48,2013-05-07 15:36:17
I once made a t-shirt for my grad professor that said Statisticians Do It Correlated. That's about as funny as stats got for me.,dweebcentric,2013-05-07 06:57:06
"How to statisticians write inductive proofs?

With a Bayes case!",jschow3,2013-05-07 14:24:10
"I came up with this one in class yesterday:

Mean centred sounds like a good name for a cologne... *the smell of average!*",,2013-05-11 00:55:59
"Here's a good list, several reddit reference:

http://www.businessinsider.com/13-math-jokes-that-every-mathematician-finds-absolutely-hilarious-2013-5#ixzz2U3zcd7mF",amartinz,2013-05-23 13:07:34
"Once you understand it it's so simple. I once spent ages trying to convince someone of this fact. They never believed me.

A simple sentence that can help is.

""You are obviously more likely to be friends with someone with a lot friends, than someone who doesn't have many friends""

or

""Nobody is friends with a person with no friends""",Supersnazz,2012-10-21 18:20:07
"Like you say, this ""paradox"" (not really a paradox - more of a counter intuitive fact) is actually very simple.  I think it can be simple to explain too, I usually use:

""Imagine one guy has ten friends.  Each of his ten friends has only one friend, him.  The average friend count (11 people in the system, 10 of them have 1 friend, 1 has 10 friends, 21 friends / 11 people = 1.9 avg friends per person) is greater than one, so ""on average"" most people are below the average friend count.  Real life works out like like this too.  A few people have *a ton* of friends and the rest don't, and that sets up this little fact.""",electricfistula,2012-10-21 20:40:42
"Simpson's paradox is more of a mind blower. 

Imagine 2 drugs, A and B.

* Drug A works better than B on women. 
* Drug A works better than B on men. 
* Overall, Drug B works better on men and women combined.

I still can't wrap my head around it, but numbers don't lie.

http://en.wikipedia.org/wiki/Simpson's_paradox",Supersnazz,2012-10-21 20:47:24
"It's not really that hard, I work in a tipped profession and come across this all the time, when debating whether I want to work a long slow shift or a short busy shift.

The basic idea is that we assume just because we are comparing percentages we are comparing equal measures, but because the sample sizes are split differently, we aren't.

Look at it this way. You and I are going to the pub this Tuesday and Wednesday and we are going to play a game where we throw darts and try and hit the bulls eye.

On Tuesday you only throw the dart once, but you hit it. You now have 100% for that night. I throw the dart 99 times and hit the bulls eye 98 times. That would give me right around 99% accuracy. Looking just at those percentages without knowing how many times we both tried, it looks like you did better.

Now we come back Wednesday, this time though we switch, I throw the dart only once and I miss, leaving me with 0% accuracy on the night. You then throw 99 times, and hit the bulls eye 10 times, which gives you right around 10% accuracy on Wednesday. Again you seem to have won.

The trick is you really haven't. The data was just split weird, making it misleading. Really, over the course of two days, I hit the bulls eye 98 times out of 100, and you got only 11 out of 100.",Drugba,2012-10-22 00:47:06
"So, which shift do you want to work?

Great explanation btw.",floppydrive,2012-10-24 15:34:37
"The more I think about it, it isn't exactly simpson's paradox, that comes up at my work, but it is close. You make more money on a longer shift, because you get a higher number of tips over the course of a shift, but on a shorter busier shift, your pay per hour is higher because you get tipped more frequently. Basically if I work a long shift, I tend to make about $12-$15 an hour after wages, but it's very likely I will earn into triple digits. On a short shift, I can make between $25-$35 an hour after wages, but the shifts are only three hours long.

That being said, I have almost exclusively short shifts because I make enough money to cover my expenses and to me my free time is worth much more that the extra money I could be making.",Drugba,2012-10-24 18:15:12
"Well, we certainly at /r/depthhub certainly appreciate you using up some of your free time to enlighten us. ",somanyroads,2012-11-04 13:50:43
"If you only care about money, you want to work long busy shifts.  Working 4-close Fridays and open-close Saturdays at my restaurant came very close to covering all expenses while I was in college (tuition/books, rent, food, car and insurance, gas, internet, utilities).  Adding on the rest of the nights I worked gave me plenty of savings and spending money.  Long busy shifts are draining work, though and most people prefer to take the lower earnings of a shorter shift.  People fought to not close and I always took it.

I did not work in a super high end restaurant.  Guest check average was $15-$20.

Waiting tables is about finding your balance of volume and level of service.  Too few tables, and you just can't earn big numbers.  Too many tables, and quality of service drops, impacting tips and turn times, thus lowering earnings rate.",ahuggingkissingfiend,2012-10-24 18:02:04
The fact that ~20 hours a week at a restaurant paid your expenses is honestly mindblowing. ,roundtree,2012-10-24 20:30:47
"Averaged out I earned about $15-$18/hr over the course of a week.  On weekends it's usually &gt;$20/hr.  This is tips plus wage.  I keep very good track of income and spending.  I can tell you with confidence I was the best server at my restaurant, but the earning rate differential was not large between other servers and me (everyone compares tips).  My total earnings were higher because I closed most every shift I worked, but $/hr was very close with the other servers.

I worked 35 hours/week through college (while taking the maximum number of courses allowed at my school, 24 credits/semester), and graduated with a GPA unimpacted by my work and a healthy amount in savings.  I don't say this to brag, but to give context to my next statement.

I do not understand people who say you can't work through school anymore.  I had almost nothing for financial aid due to my parents continuing to claim me as a dependent, even though I was paying for everything on my own (also missed out on .  A half ride at any top tier school would be sufficient for me to have been able to afford any school in the country.  I've run the numbers (including cost of living in various high expense areas).  I went to a cheap state school and so came out ahead upon graduation.

Now, everyone in school can't wait tables, but there are plenty of jobs that you can work in school to earn money.  Minimum wage at a state school out of a big city is enough to cover expenses (I spent less than $14K/yr on all my essential expenses, inclusive of car, internet, etc....), but that would suck.  Luckily most work study jobs I've seen pay in the $8.5-$10/hr range, and it's not hard to break $10/hr as a manager in most any franchise (retail, food service, and the like), an easily achievable goal for most college students.

So yeah, wait tables if you need extra money.",ahuggingkissingfiend,2012-10-24 21:04:16
"For a good technical degree, you expect a workload of 35-40 hours per week, not including any ""crunchtime"". What you're saying amounts to ""I can't understand why everyone isn't working a 80 hr work week"".",helm,2012-10-25 02:21:56
"I maxed out on credits while doing Econ and Math.  I would go to school around 830-900 and live in the library til ~1600.  I worked Wednesday-Friday nights only and Saturday lunch and night, sometimes picking up a Tuesday night.

Yes it's a lot of time, but if people complain that school is unaffordable and you can't work your way through, I disagree.

You'll also note that my expenses were covered with about half of my work load.  That is, I over-earned (and over-worked) by a large amount if my goal was to cover school costs.

You may argue that I was fortunate to attend a state school with such a low tuition cost.  Well, financial aid has increased a drastic amount in recent times.

[Here's a review of just a few schools that have made no/low-loan pledges for providing financial aid to students from families with low incomes](http://projectonstudentdebt.org/files/pub/Pledges_Analysis.pdf).  The average tuition for a year at an income of $20K/yr listed there is ~$3300/yr.  I paid more than that per semester.  For an income of $40K, the average tuition is about $5100/yr.  That's still less than what I paid.

So I say that it is not unreasonable at all for an 18 year old who is not claimed as a dependent to work their way through school, regardless of major.

Ninja edit: [The returns to a college education have increased more than sticker price of tuition over time](http://www.brookings.edu/blogs/jobs/posts/2012/10/05-jobs-greenstone-looney).  College is a better deal than ever.  I don't see it as unreasonable for something so valuable to require hard work to achieve, and if someone doesn't want to graduate with the [average student debt of $26,600](http://projectonstudentdebt.org/state_by_state-data.php), then it is perfectly reasonable to work your way through school.  I don't think it's silly to say an average college student cannot earn $30K in 4 years (that would cover tuition, books, and various other school related expenses).",ahuggingkissingfiend,2012-10-25 08:06:01
Math and Econ? Please give your major as it is totally pertinent to the argument you are trying to make,,2012-10-25 21:20:01
"Math and Econ.  I was double majoring.  I'll give you a full rundown of my college career:

Freshman year:  Top flight private school $50K/yr.  Half ride because parents earn pretty well.  ~$20K in loans for that.  Undeclared major -- 18 credits per semester, but dropped philosophy first semester because it was awful.  Transferred out because cost, lack of theater, and reminded me of my high school.  Wasn't working this year.

Sophomore year:  Community college because I decided in August I wasn't going back.  Got a job waiting tables at a shitty chain restaurant.  Paid for community college myself, don't remember tuition -- it was low -- and a car $4400 after a summer and one semester of work.  18 then 20 credits - bouncing between Theater/Psych/Econ majors, but spent most of that on Theater.

Junior year+: State university in town $3500/semester.  18 credits first semester because of limit on first semester students, but took one full credit course as independent study for half credit thanks to a really chill professor (also counted as a 400 level course, so I guess double win). After first semester 24 credit/semester -- double major Math and Econ (I had no math credit coming in, so started at Calc I; I had two courses covered for Econ, I had a lot of gen eds covered).  I completed Econ requirements senior year, and was going to finish math in 5th year, but decided to leave after 4.5 (4.5 was a full load of upper level math courses and TA for Econ -- still 24 credits).  Graduated in a December B.A. Economics. 

Switched restaurants while I was there (move from $12-$15/hr to $15-$18/hr).  Worked summers at a camp because it's fun, but earned far less than I would waiting tables -- camps pay poorly.

I already covered my typical week, but I'll recap for you:

School ~830-1600 weekdays.  Work Wednesday-Friday nights ~1700-close, and Saturday open-close, with an occasional Tuesday night in there.

I covered all tuition and books plus the following expenses:

That $20K worth of student loans for my freshman year -- feelsgoodman.jpg

Rent -- living solo, so paid a premium; could have halved this by sharing a house

Car -- insurance, gas, maintenance

Food

Internet

Utilities

Going out/having fun -- I did this a lot; Sundays were always free and bars are open far later than my restaurant's dining room

One vacation/year either with whole family or just with sister (combination expense and lack of earnings -- she and I always covered our own costs for these)

Savings

Emergency fund

Retirement account

If you're observant you'll note I didn't pay for my cell phone.  I will also disclose that my father bought a pair of glasses and one year's worth of contacts for me (I don't always wear the contacts so it's much more than a one year supply).

I can also tell you my sister did the same.  She's now in law school and has decided not to work during the school year.  She has a half ride and is using loans to cover tuition.  She is living off of her savings from undergrad (larger than mine -- people liked her more as a server and she's obsessively thrifty), and working minimally in the summers.

Questions?",ahuggingkissingfiend,2012-10-25 23:03:17
"I don't know how hard an Econ major is, I know as a chemical engineer I would have gone insane working 40 hours per week ",,2012-10-26 06:17:47
"Well you were taking one STEM major, and I was taking two.  Econ is a mix of courses that are math heavy, and courses that require a lots of technical writing, or at least that's what I chose.  You could pretty easily wimp out on Econ if you so choose, but those are boring classes. I assume as an engineering student you're familiar with the work load from major-track Math courses.

I rarely worked 40 hours a week unless it was a very busy time of year (and this would usually coincide with breaks from school -- christmas holidays, graduation).  In fact, as I've mentioned several times, I worked far more than I had to.  I also received close to 0 financial aid at my state university, thanks to still being a dependent of parents who earn rather more than the average household.  This also precluded me from taking the sizable deduction for school expenses on my tax returns.  I got to take that only after graduating for my last year of school (they stopped claiming me after I graduated), and it was a very nice amount.

But hey, maybe someone like you wouldn't want to go to my state university for engineering.  I hear MIT has a pretty decent engineering department.  Let's [look at the link I already posted](http://projectonstudentdebt.org/files/pub/Pledges_Analysis.pdf).  Well, let's say you aren't being claimed as a dependent (that is, you are your own household).  Now let's assume you, as an intelligent college student, can find gainful employment at $8/hr.  Would you like to know how long it would take you to pay for MIT?  &lt;55 hours total.  The chart only lists tuition for a household income of $20K, so I have to make the (probably false assumption) that if you earn less than that annually you would still pay the same.  That's right, if your household income is $20K, your net cost of attending MIT is $440.  Don't forget your EITC for working but earning so little, and your tuition deduction on your tax returns.

Now I did cherry-pick a little bit there.  I chose what is commonly ranked the number one technology school.  If we take an average cost of attendance for households (and remember if you are not a dependent you are your own household) that earn $20K at the institutions in that link ([like I already did](http://www.reddit.com/r/statistics/comments/11v31a/everyones_facebook_friends_have_more_friends_they/c6rkjk1)), we see that it's $3300.  Again, if we assume you can earn $8/hr then you'd have to work 412.5 hours.  You get about four months combined of christmas/summer break.  Think you could fit that in?  It's only 11 weeks of full time work at $8/hr, or about 26 hours/week if you work the full four months.

I don't really recommend that people work as much as I did during school, or that they max out on credits every semester -- I guess I'm a hypocrite; forgive me?  I do claim that it is silly to say no one can work their way through college anymore, and that it's silly to say you need to work full time through school to do so.  I worked full time through school and earned nearly double what I needed.",ahuggingkissingfiend,2012-10-26 12:46:28
"I really think the server position has spoiled your mind set, those are hard jobs to get, especially if you're a male and nothing to look at. There are very few part time jobs that are flexible with school other than minimum wage restaurant/fastfood jobs. Yes at $18 an hour, half of which is under the table and untaxed, you can work your way through college.

And as for math heavy work loads, all the math classes we take are cake compared to the engineering courses.",,2012-10-26 16:41:52
"&gt;Now let's assume you, as an intelligent college student, can find gainful employment at $8/hr. Would you like to know how long it would take you to pay for MIT? &lt;55 hours total. The chart only lists tuition for a household income of $20K, so I have to make the (probably false assumption) that if you earn less than that annually you would still pay the same. That's right, if your household income is $20K, your net cost of attending MIT is $440. Don't forget your EITC for working but earning so little, and your tuition deduction on your tax returns.

&gt;Now I did cherry-pick a little bit there. I chose what is commonly ranked the number one technology school. If we take an average cost of attendance for households (and remember if you are not a dependent you are your own household) that earn $20K at the institutions in that link (like I already did), we see that it's $3300. Again, if we assume you can earn $8/hr then you'd have to work 412.5 hours. You get about four months combined of christmas/summer break. Think you could fit that in? It's only 11 weeks of full time work at $8/hr, or about 26 hours/week if you work the full four months.

But sure, let's focus on my specific job and experience.  As we know anecdotes make the best arguments.

",ahuggingkissingfiend,2012-10-26 17:16:07
"Its doable, but not everyone has the opportunity to even take the four years out as a wage earner for themselves and completely dedicate their life to working for just enough to survive through school.  You have to have no other responsibilities.  If youre already with a family or have to help your parents out, it's not doable.  The time value cost of not being able to earn a liveable wage for four years is why a lot of people cant work their way through college.  Young adults with no other responsibilities, sure, but not everyone trying to work their way through college is a bright, healthy, pink 18 year old with a solid family background.  Even if your family declared you as a dependent, they at least were able to bring you up in a way that you had the time and basic abilities required to work your way through school.  That doesn't mean it's not sometimes impossible (or at least extremely unlikely) for everyone.",sandiegoite,2012-10-30 10:57:05
The average college student had significant support from home too.,helm,2012-10-25 08:29:16
"It's important to consider that picking a shift is going to have more nuances (e.g., time of day, day of week, current events, patrons' generosity) than throwing darts, but I think the ""right"" answer is short and busy.",obsa,2012-10-24 18:07:54
"Truth be told, it really depends on the person. One of the guys at my restaurant loves taking the 7 hour (10am-5pm) shift. Other than the lunch rush, its pretty slow the whole shift, but he gets along really well with the cooks so basically he gets paid to fuck around in the back of the store all day.

I on the other hand can only be social for so long. After a few hours I've said everything I can say to my coworkers and at that point a long slow shift becomes like pulling teeth.",Drugba,2012-10-24 20:20:24
"Great example, also, isn't this also why point estimates have lower inference value than confidence intervals?",,2012-10-22 08:20:45
"Confidence intervals just give you a measure of variability in the data. What is ""lower inference value?""",pokie6,2012-10-24 15:27:35
"My reasoning is that confidence intervals take into account sample size and like you said variability, whereas if you were simply given a point estimate, not knowing anything about the sample size you would treat a sample size of 2 and 1000 the same.",,2012-10-24 19:52:37
"True. Basically, any measure of center (like a point estimate of a population mean) is almost completely useless without a measure of variability. Confidence interval includes that implicitly.",pokie6,2012-10-24 20:08:17
"The tl;dr on Simpson's paradox that helps me remeber it is ""the average of averages is not the average"".  ",EyeOfTheWitch,2012-10-24 18:03:12
"Nitpick! It's an average, but not a very useful one. And most likely not at all the one you want.",NegativeK,2012-10-24 19:10:57
"This was linked in /r/DepthHub.

Now that I've read this explanation, I finally have a name to put to the phenomenon discussed in the Car Talk Puzzler a few weeks ago: [question](http://www.cartalk.com/content/take-ray-out-ball-game?question), [answer](http://www.cartalk.com/content/take-ray-out-ball-game?answer).",McGravin,2012-10-24 15:02:40
Excellent explanation. ,Supersnazz,2012-10-22 01:08:11
"Did you take this metaphor from a book, or come up with it yourself?  I can't for the life of me remember what book it was in, but a very similar metaphor was used in something I recently read.  It might have been in something about complexity?

In any case, your example should be drilled into schoolteachers.  It always blew me away when I was in junior high/high school and scored perfect on every test, but missing a couple homework assignments was enough to drop me to a B or a C.  It always struck me as profoundly useless in terms of actually evaluating knowledge of a topic.",otakucode,2012-10-24 17:37:27
"That's because school (particularly secondary school) is about learning how to please your superiors, not about educating yourself. Took me too long to realize this as I was in the same boat.

Same thing applies at work. Being the smartest employee isn't nearly as beneficial as being the most well liked or most willing to take on tasks.",Oo0o8o0oO,2012-10-24 19:06:08
I came up with it myself : ),Drugba,2012-10-24 18:05:53
This can be easily explained mathematically. It's called a weighted average.,,2012-10-24 19:22:23
But.. But.. I'm good at darts! I swear!,ZestyOne,2012-10-24 19:26:13
"So it's kinda like how the two party system and gerrymandering work here in the US, where how you divide the data cause the outcomes to appear totally different.",Fatalstryke,2012-10-24 19:42:29
"Yes, but now connect it to Chelsea.",ScreamingGerman,2012-10-24 19:56:06
"Deal.

So, this Sunday and Wednesday Chelsea are playing Man U. In the lead up to the game Mata and Hazard make a bet to see who can humiliate the red's defenders more. On Sunday Mata nutmegs Ferdinand 6 out of the 7 times he attempts it, whereas Hazard wins 3 penalties and successfully Rabonas all three in because he has balls of steel. Then Mikel gets a hat trick, no one give a shit about math anymore and the Blues clobber Manure 8-0 and 6-0 respectively.",Drugba,2012-10-24 20:31:41
I understood some of those words!,ScreamingGerman,2012-10-24 20:56:47
"&gt;Imagine 2 drugs, A and B.

&gt;Imagine drugs, A, B

&gt;Drugs, A, B

&gt;Drug, B, A

&gt;Drugba

This might be the most obscure novelty account ever.

Or you think Didier abuses illegal substances.",link090909,2012-10-24 21:15:47
"About 3 years ago I was watching Sky Sports News and my roommate walked in right as they were talking about an injury Didier had. The reporter said something along the lines of, ""he is expected to be in rehab for the next two weeks."" to which my roommate, in passing, said, ""Drogba's in rehab? More like Didier Drugba then.""

A few weeks later I started playing an iPhone game called Underworld: Traffic that involves selling drugs. I named my character Drugba and since then it has been one of a couple go-to user names that I have.",Drugba,2012-10-24 22:22:18
"It's not *how it comes about* that bugs me with Simpson's paradox. It's the idea that if we didn't know which category a situation is in, then we sometimes are better off using the treatment prescribed by the aggregate rather than the treatment that would be preferable in either category. That's the part I always have trouble wrapping my head around.",Neurokeen,2012-10-22 09:15:49
"Am I the only one that finds this explanation misleading/unhelpful? The oversimplified way he describes it doesn't work because there has to be sample size variation. The groups ""men"" and ""women"" can't have any, drugs A and B are working on the exact same sample population (men or women). This phenomenon with the drugs would have to essentially occur in a drug trial. And this would absolutely be a case of the numbers lying. ",nubwithachub,2012-10-24 17:28:41
"Simpson's paradox is more of a mind blower. 

&gt;Imagine 2 drugs, A and B.
&gt;
&gt;* Drug A works better than B on women. 
&gt;* Drug A works better than B on men. 
&gt;* Overall, Drug B works better on men and women combined.

This is actually not possible. No wonder you find it so mind-blowing.",TheEquivocator,2012-10-31 23:11:28
Yes it is. That's the point of the paradox. If you have sample sizes that are different between the two groups (but still large enough to be statistically significant) it can occur.,Supersnazz,2012-11-01 02:23:57
"&gt;If you have sample sizes that are different between the **two groups**

You implied (or I took you to imply) that you were dealing with the same sets of groups in both cases (""women"" and ""men""). If drug A works better than drug B on all women and drug A works better than B on all men, then simple math shows that drug A works better than drug B on all women + men.

Maybe you meant that drug A worked better on *the [women/men] it was tested on* than drug B did on *the [women/men] it was tested on*, but you didn't make that clear, imo.",TheEquivocator,2012-11-01 04:37:45
"Title doesn't make sense... it is talking about ""everyone"" making a universal statement and then adds on ""on average"". It should be either ""the majority of people's Facebook friends have more friends then they do"" or ""on average, a person's Facebook friends will have more friends than they do"". Both still capture the essence of the paradox, without being misleading/non-nonsensical. 

That said, very interesting read. ",gddc33,2012-10-21 18:22:01
"Or ""Your facebook friends are probably more popular than you"" if you want to stick with the depressing tone.",MatrixFrog,2012-10-21 20:29:34
"True, I wrote it in about three seconds and didn't really think about it. Good call - I'll be more rigorous with my submission titles next time :)",DrunkPanda,2012-10-22 00:56:28
"&gt; non-nonsensical

I think you mean _non-non-nonsensical_.",Bromskloss,2012-10-22 06:05:07
I blame autocorrect,gddc33,2012-10-22 16:03:13
Sometimes people are astonished when you tell them almost everyone has more than the average number of legs... ,efrique,2012-10-21 22:55:17
Lecture 0 of [Udacity Stats 101](http://www.udacity.com/overview/Course/st101/CourseRev/1) course shows this exact paradox with simple expectation analysis in a short video format.,m1kael,2012-10-21 19:03:58
"That took a *really* long time for me to rationalise, but it does make sense! Nice post.",Eist,2012-10-21 17:48:17
"A lot of similar incarnations of this phenomenon have been pointed out here. Another example: 

About 15 years ago, the German railway company published the relative frequency of trains that were running on time. The proportion of trains running late seemed rather on the low side given the experience of a seasoned traveller.

Of course, the trains that tend to be late are the ones that tend to be more crowded.
",derwisch,2012-10-22 14:19:48
"I wrote a column about this a couple of years ago:

http://allendowney.blogspot.com/2011/01/are-you-popular-hint-no.html
",AllenDowney,2012-10-24 08:27:27
"The ""on average"" is what makes this non-paradoxical. Pretend you have 9 friends. Then if they are randomly assigned a friend count, there is a 10% chance that you have more friends and if you do, then all of your friends have a friend with more friends. So there is at least 9 people you know of, with a friend with more friends. So on average, people have less Facebook friends than at least one of their friends. This logic fails if they have 0 friends or 1 friend.

The way that you phrased that is terrible by the way. When talking about an average, you cannot make a general statement about everyone.",,2012-10-22 07:05:00
"So ... if I apply the friends function recursively, and calculate the mean number of friends, it will increase with every recursion step? What will be the limit?
",derwisch,2012-10-22 14:23:57
"Here's an alternative way of understanding the phenomenon.

Imagine a map (of Germany say) that has every town/city of more than 50,000 people marked, along with every road between these cities.  This is more or less what is meant by a ""graph"" in Graph Theory.  Now, there are a few very large cities in Germany (with say, more than 1 million people), and many more smaller cities.  But at the same time the smaller towns have a few roads connecting them to neighbors and the large cities have many roads connected to them (as the saying goes, all roads lead to Berlin).  Now pick a random town/city (A) on the map and pick a random road leading out of there to the next town/city (B).  Would you expect A or B to have a more roads connected to it?  Well, there are lots of small towns to choose from with few roads, so A probably has relatively few roads.  On the other hand, the roads emanating from A are probably connected to some big cities because big cities are exactly those things with roads to lots of other places.  On average, B will have more roads (and people) than A.

This (contrived) example is exactly the same as the Facebook case, and provides some intuition for what's going on.  (Most of) You are city A, because there are many more A's than B's. You have friends on Facebook that are like city B.

My guess is that there is some technical statement of this result in Graph Theory (maybe in terms of the stationary distributions of certain types of Markov chains), but I don't know about it.",a785236,2012-10-24 17:35:48
"Biologist here. I've heard plenty of times that p-values shouldn't be relied upon, but they never seem to mention what a good alternative should be. Sure a perfect experiment would have such a large effect size that you don't need stats at all, but sometimes you're looking in noisy data. Bayesian statistics is almost always mentioned, but no specifics. 

If I was planning on running an ANOVA, what should I use instead so I don't fall into the p-value trap?

I've tired to use AIC/BIC in my more recent analyses but my adviser finds them confusing and just wants to talk about the p-values.

EDIT: spelling",Apiphilia,2015-02-10 21:24:42
"This is an interesting point to follow up on:

&gt; a perfect experiment would have such a large effect size
&gt; that you don't need stats at all

A perfect experiment would have (among other things I guess)
almost no noise so that we'd accurately see whatever effect
is really there.

Given the accurate measurement of the effect, how do we know
if it's large or small ?

The answer to that requires some kind of non-statistical
knowledge.

For example, suppose drug A reduces headache 70% of the time
and drug B reduces headache 71% of the time, and those
measurements are so accurate that we can neglect the
uncertainty.

Is there a difference in the drugs ?
Of course there is.

But would we care ?
It depends.

Suppose drug B was twice the price ?
Would we buy drug B ?

Now suppose ""drug A"" was a placebo and ""drug B"" was some new
drug.  Would we say the new drug is ""effective"" ?

Suppose the ""uncertainties"" on both those measurements are 0.1%, 
so we have.

    |   | effectiveness | uncertainty |
    |---+---------------+-------------|
    | A | 70 %          | 0.1 %       |
    | B | 71 %          | 0.1 %       |

We could do a formal test (with a null hypothesis that there
is no difference).  We could calculate the relevant p-value
and it would be very small - ""significant"".

But does that mean B is a good drug ?

The p-value is small because the ""difference"" is large
**relative to** the uncertainty.

So a large p-value might just mean that the sample size is
large enough to make the statistical uncertainly even less
than an inconsequential effect size.

An insignificant p-value might, rightly, caution us against
being to excited by a particular numerical measurement.

But a significant p-value, on its own, is not necessarily
that interesting.  Having noted that is significant, we should look
at the numerical values of the original measurements and
think about those.

Edit:

Suppose there were 3 drugs

    |   | effectiveness | uncertainty |
    |---+---------------+-------------|
    | A | 70   %        | 0.1 %       |
    | B | 71   %        | 0.1 %       |
    | C | 70.5 %        | 0.1 %       |

and we do a ANOVA with the null hypothesis that there is no
difference between the 3.

We reject the null, which is nice.  But so what?  It is not
particularly surprising that there is some difference.  If
we measure anything accurately enough, we'll find some
difference.  The important question is whether the size of
the difference is interesting.
",dunwadda,2015-02-10 22:44:32
"Yes, I'm well aware that the effect size is most important. Thanks for the explanation though. I teach a freshmen-level biology class that spends about half a class on stats (just t-test and some descriptive stats in excel). We always try to impress upon them that the p-value just tell us if the means are different, but the effect size tells us whether or not we should care.

Thanks for you're explanation, it makes me feel confident that I'm going about things in at least an okay fashion. I took a stats minor during undergrad which has definitely made me more comfortable with stats than my peers (biology grad school).",Apiphilia,2015-02-11 05:02:35
"P-values are what get data published, unfortunately.  Bayesian methods are probably too high-level (you have to understand the ""frequentist"" stuff first) for anyone that isn't already at least quantitatively oriented.

I think an important point is thaf p-values ought not play a large role in *exploratory* analyses, where effect size should be the main focus.  In exploratory work, your tests are likely to be underpowered (because who gets more than 5 mice per group to ""explore"" with anyway).  An interesting consequence of applying a ""statistical significance filter"" to your data is that it will lead you to systematically overestimate true effect sizes, leading to disappointment when you try to follow up the experiment.  See the work of John Ionnadis for more on this, especially ""Why Most Published Research Findings Are False"".

That said, there's nothing wrong with p-values when you've carefully set up your test and you know the minimum effect size (call it X) you're interested in finding.  Once you know that, you can do your power calculations and figure out approximately what sample size you need to detect an effect size of X (although you'll probably have to estimate your population variance from the data you have) with probability Y.  P-values are fine, as long as you've done the work to design your experiment in the statistical sense.",bakersbark,2015-02-10 22:38:54
"&gt; Bayesian methods are probably too high-level (you have to understand the ""frequentist"" stuff first) for anyone that isn't already at least quantitatively oriented.

I completely disagree with the ""too high-level"" idea. Bayesian methods are considered to be more intuitive than frequentist methods. The problems are that there is a lack of moderately priced and easy-to-use tools, and that biologists are more familiar with the frequentist-based Stats 101 they took at uni all those years ago.",log_2,2015-02-10 23:52:51
"Okay. So here I am, ready to learn more about Bayesian methods. I'm quite competent in R and a beginner in Python. Are there any good resources out there that can get me started? Are there equivalent Bayesian tests to the p-value centric tests I'm used to (like there are ""equivalent"" parametric and non-parametric tests)?",Apiphilia,2015-02-11 05:12:07
"Can I recommend my own book, Think Bayes?",AllenDowney,2015-02-11 08:47:19
I second this recommendation.  Allen Downey's books are all excellent.,bakersbark,2015-02-11 10:46:03
"Try googling for the paper ""bayesian estimation supersedes the t test"".",log_2,2015-02-11 14:15:55
"It's all about how the article is reviewed and published. ALL ABOUT THE REVIEW. I don't see a great way to correct the problem either. As a student or postdoc you are hopefully taught by statisticians the ""correct"" way to set up your experiments and what stats/model to pick. Then you go to your lab and you do what the guy paying you does. Many suggestions to change are overlooked or ignored because the student or postdoc can't explain it in a way that their PI can understand and believe. So they go along with the PI with plans to set their lab up differently. 

So jump to their first faculty gig. They have a good publication record and now write their first independent grant. They set up their experiments and stats like they should be. But because of the data they don't use a familiar model, but are assured by a statistician that it is appropriate. They send it in. The reviewers are colleagues of the former PI and don't understand the stats described and request an ANOVA be performed instead. Desperate for funding the new PI does what is asked and moves forward because they have 4 other papers they are trying to publish, 2 courses to teach and a new grad student starting in the lab.

This is a rough example but is also very real. Or at least was my experience through grad school and postdoc. I am not going forward with becoming a faculty, in part, because of things like this. Academic research does not allow for great science anymore. Or rather,, it is extremely difficult to do great science because of the system.

",skevimc,2015-02-11 06:32:54
"Thanks for your input. I don't really do exploratory analyses, but more controlled experiments (with bees so my sample size is usually decent; insects are way more cooperative than mammals). Even though I've known that effect size is very important, your post makes me realize that I haven't been deciding on what a ""good"" effect size would be before I start my work. It makes complete sense that if I have to determine a critical p-value, I should also determine some sort of critical effect size, as well. I think I'll do that in the future.",Apiphilia,2015-02-11 05:07:16
Excellent idea!  The great thing about this is that when you determine a critical effect size you can use your expertise in your field (along with your advisor's) to determine what sort of effect size is really worth studying.,bakersbark,2015-02-11 11:02:21
"If I was supreme dictator of science, I would say that a particularly interesting method would be to run a series of experiments that test a hypothesis under slightly varying circumstances and/or between different labs. Then, do a meta-analysis of the pooled data, reporting average effect size under the slightly different conditions, then average effect size obtained in each lab. This would be a modest control for testing conditions and ""geography."" ",griffer00,2015-02-11 06:59:21
"I assume you are talking about a hypothetical ""ideal world"" situation, as this would be beyond onerous and extremely expensive! It would also have limited applicability depending on the field of study. Many things cannot be measured in the lab as they are in the field of course.",Gastronomicus,2015-02-14 08:28:37
"One of the ways you fall into the p-value trap is by asking yes/no questions.  Flip a coin 100 times, is it biased?  You calculate a p-value.

An alternative is to report measurements, with uncertainty levels.  For the case of the coin, you would instead say we measured the probability of heads to be 0.51 with a 95% confidence interval of 0.02.

In the biomedical sciences, people would say ""you can't publish that, you saw no effect.  You need to have a p=0.95."" If you did get that significance level, you'd go around saying ""the coin is biased"".  

In physics, you can publish that (if it is an interesting measurement).  It is particularly respected, if you can reduce the uncertainty level over previous measurements.  So, your next paper might be ""we measured the probability of heads to be 0.504 with a 95% confidence interval of 0.005.""

In physics, you wouldn't say ""the coin is biased"" until you hit the 5-sigma level, equivalent to a p=0.9999994  , so something like when you can say ""we measured the probability of heads to be 0.5040 with a 95% confidence interval of 0.0002"".



I think that's the major weakness I see in biomedical statistics.


",ron_leflore,2015-02-11 09:34:42
"I started to read Vaux's contribution to this volume, and was irritated by the condescending tone and not impressed by the clarity of his explanations.  If this stuff is really as ""basic"" as he keeps saying, he should be able to explain it more clearly.",AllenDowney,2015-02-11 08:49:39
"I agree with you one hundred percent. It is easy to lament the current state of things, much harder to actually do something to help correct it. Given an amazing platform like that, it seems that valuable page space could have been better spent on explaining common mistakes clearly, rather than simply brow beating the readership about their incompetency.",DrGar,2015-02-11 11:22:38
"This is a well characterized phenomenon in biology. The unfortunate reality is that biologists despite being scientists fell into the ""math is not for me"" group long before they became biologists. It doesn't mean they aren't intelligent, it just means they learned to think differently and less formally.",blank964,2015-02-10 21:14:16
"I did my undergraduate degree in Biology, and I'll add that the problem is systemic at this point. Introductory math courses are taught in a way that fundamentally doesn't make sense to math-shy people, and at best you come out of those courses having memorized algorithms for solving exam questions. The rest of your education typically entails using formulas as arcane incantations - magic tools whose formulation may or may not relate to the problem at hand. And since you rarely see how the formula was derived, you gradually learn to take a certain amount of quantitative analysis on faith alone. 
 
It's really a vexing problem, because you can't just solve it by teaching math in the applied context - this typically glosses over why the mathematical abstractions make sense in the first place. Personally I'd like to see what would happen if you taught math and computational thinking first, and then learnt your branch of science through that lens. A great example of how this is done is [Philip Klein's ""Coding the Matrix""](http://codingthematrix.com/), which teaches algebraic concepts from the ground up and lets you implement the ideas in software every step of the way. This anchors your understanding to something tangible, even though the subject is abstract. ",AmusementPork,2015-02-11 02:49:17
aerofoil,mszegedy,2014-05-03 17:50:49
If I had majored in something else :),bobbyfiend,2014-05-03 18:39:18
"Side note: it's in front of a mainly-student-occupied apartment building across from my university. In grandiose moments I imagine one of my students from years past absentmindedly sketching it with her or his toe in the still-wet cement, unaware of what it looked like, after a riveting class period spent learning about ANOVA. 

Sigh. This is the kind of pathetic fantasy we are reduced to. Kids, stay in school... but only long enough to get a non-academic job.",bobbyfiend,2014-05-03 07:10:16
"Do share (vent?) your resentment with academia. Though lurking, many are listening.
",GizmoC,2014-05-03 07:58:21
"Not resentment (well, there are *aspects* of current US academia that I hate, but those tend to be the non-academic aspects). More of a joke, very badly delivered. The literal interpretation might have been something like, ""Hey, kids, if you don't want to end up the kind of person who has these sad little fantasies about things that look sort of like distribution density function diagrams, and tie those sad little fantasies back to your sad little fantasies about your students thinking your teaching is far more important than it actually is... don't be a professor!""

But I actually quite love being a professor.",bobbyfiend,2014-05-03 10:24:30
I'm giving you an upvote just for the effort you put in to produce that decoration!,BigZen,2014-05-03 13:25:07
"Thanks, yo. It involved stealing some of my daughter's magnetic fridge letters and using local materials. If this were /r/art, I would brag that it took me ""4 minutes, at least."" ",bobbyfiend,2014-05-03 13:31:21
Now watermark it and sell it for $200 per file.,duhduhduhdiabeetus,2014-05-03 17:01:48
I can do that? My retirement plans have just become much less complicated.,bobbyfiend,2014-05-03 18:38:19
and the real question: how much of my existing code base is going to be broken by the update?,zdk,2013-04-03 09:11:25
"Approximately as much as with upgrades within the 2.x series. In my experience, about zero unless something was already deprecated. ",ThisIsDave,2013-04-03 10:15:58
"Yeah. 3.0.0 is an acknowledgement of how far R has come, not vast changes from 2.15.x. I doubt that [zdk](http://www.reddit.com/u/zdk)'s code would break.

Of course, you can always create a backup copy of your current R installment.",valen089,2013-04-03 12:46:02
"And Namespaces have been required for a while, now, so it should be mostly obscure stuff and implementation details.  It looks like very little pure-R code should be broken.",inspired2apathy,2013-04-03 20:03:28
[deleted],,2013-04-03 13:19:04
"&gt; hackish workarounds

Well there goes everything I've ever written in R",funkalunatic,2013-04-03 20:21:45
"Now I'm curious how versioning numbering works if there are no major changes. Many things in Python 3, for example, are not backwards compatible.",zdk,2013-04-03 13:28:06
[deleted],,2013-04-03 13:33:21
What packages are incompatible so far?,beaverteeth92,2013-04-03 14:13:52
"My understanding from reading about it and currently being in a course lead by one of the members of the R Core Team is a few minor changes (I recall, real() and as.real() being dropped if you use them). The biggest change will be that you have to reinstall your packages. Hardly a big deal. ",iacobus42,2013-04-03 12:57:35
You could read the release notes you know.,kirakun,2013-04-03 09:13:12
tl;dr :-P,zdk,2013-04-03 10:44:03
"I worked at a place where statisticians and data pullers were in separate groups within the same division. Data pullers were considered the lesser between the two and incapable of moving-up. I suspect it contributed to the high turnover among the data pullers. They took a lot of abuse and had no hope of getting more respect within the company. I am happy to see this is not the case at Google.  
&gt; Also, we want to guard against creating a class system among data analysts — every statistician, whether BS, MS or PhD level, is expected\to have competence in data pulling. ",sherap,2013-02-16 05:31:00
Another brilliant mind devoted to mouse clicks on ads... ,ICrepeATATs,2013-02-17 11:58:27
"I am a statistician working in the finance industry, specifically the online aspects of banking.
To give you a perspective from my world:
 Much of what Nick said in his interview ring true as well, with one critical difference: In finance, you are expected to interact with folks from the Business side almost constantly, and in some cases become more acquainted with the underlying problem than anyone else.
And the trade-off between accuracy and complexity, aka Occam's Razor, is perhaps even more prominent.",peppermint-Tea,2013-02-16 04:00:00
"Google Statistician:

&gt;we don’t want to miss out on a great candidate because he or she comes from a non-statistics background and **doesn’t search for the right keyword**

Pretty awesome stuff:

&gt;Our data sets contain billions of observations before any aggregation is done. Even after aggregating down to a more manageable size, they can easily consist of 10s of millions of rows, and on the order of 100s of columns. ",Qw3rtyP0iuy,2013-02-15 18:53:15
"Great article.  I am currently leading a statistics group within a telecommunications company, utilizing big data systems.  Google is most definitely an industry leader and Nick's commentary in the article above helps to prove why.  The ""new"" field of Data Science and big data analytics does not really fall under one field - it is rather the effective collaboration of multiple fields of Mathematics, Comp Sci, Business, etc.

Nick's comments on requiring visibility end to end, from where the data is at rest, through aggregation, to analysis and buisiness delivery (all while collaborating with engineers, business units, etc.) is very much where the value is.  There is no magic bullet, but companies that get the formula right (for their particular problems) can find themselves that coveted competitive advantage.",a2love,2013-02-17 11:41:05
"Looks awesome, thank you!",kagdollars,2014-06-24 05:12:51
"Good videos. I started watching. I found some of the stuff a bit confusing and counter-intuitive, though. R is probably a useful tool, but personally I think I'll stick with Python.

Actually, I just remembered that I've previously read about interoperation between R and Python. I googled and found two interesting projects:

* http://rpy.sourceforge.net/. ""rpy2 is a redesign and rewrite of rpy. It is providing a low-level interface to R from Python, a proposed high-level interface, including wrappers to graphical libraries, as well as R-like structures and functions.""
* http://rpython.r-forge.r-project.org/. ""This package allows the user to call Python from R. It is a natural extension of the rJython package by the same author.""

rpy2 seems interesting to me. Others may find that they prefer to do the interoperation from the side of R, so for you there's that second project.",doubleColJustified,2014-06-24 10:41:50
"I also think that there are some confusing things about R, but this is why I found the videos so useful. I too prefer Python, but there are some things for which you really need Python. For example, [bnlearn](http://www.rdocumentation.org/packages/bnlearn/) and [fGarch](http://www.rdocumentation.org/packages/fGarch) are fantastic R packages without good Python equivalents.

rpy2 is a useful tool which I have used before. However, it requires a little bit of knowledge of R. That's another good reason to know some basics about R even if you mainly rely on Python.
",carmichael561,2014-06-24 11:00:16
"&gt;rpy2 is a useful tool which I have used before. However, it requires a little bit of knowledge of R. That's another good reason to know some basics about R even if you mainly rely on Python.

Yup. My plan is to install rpy2 and then try and follow along the videos from there, making any adaptions I need to as I go.",doubleColJustified,2014-06-24 11:02:31
Let me know how that works out. Do you have any good tutorials on using rpy2? I ended up hacking something together last time I used it but never gained any proficiency.,carmichael561,2014-06-24 11:51:11
"&gt;Do you have any good tutorials on using rpy2?

No, as I said, I haven't actually even installed it yet. I would expect, however, that reading the official documentation should be enough to get started.",doubleColJustified,2014-06-24 11:56:36
I would much rather link r and python through a shell script(or python system call) than with rpy. It is too clunky. I would rather them communicate with csv files. ,,2014-06-24 18:44:00
"Note: The course and commands are for iOS.  If you use Windows, I highly recommend one of the EDx Courses, such as [The Analytics Edge](https://courses.edx.org/courses/MITx/15.071x/1T2014/info).",vencetti,2014-06-24 09:08:26
Are the R commands different?,faore,2014-06-24 10:23:43
"The R commands are the same. The OSX specific things in the videos are the Cmd+Return, etc. You can do these same things on any OS if you set up your tools appropriately. The editor you use is really a matter of preference.",carmichael561,2014-06-24 10:41:49
Some of the R commands are different based on the OS.  ,vencetti,2014-06-24 12:01:48
"I think you mean OSX. Also, there are only a couple of things specific to OSX (the Cmd+Return for instance). The overwhelming majority of the series is OS agnostic.",carmichael561,2014-06-24 10:40:02
There were several different &lt;cmd&gt;'s in the first video.  It was off putting to me for a video that's trying to teach me how to do something for the first time that I couldn't follow with.    That may just be me and I'm sure you could look up each of these as they come up and there may be very few.   ,vencetti,2014-06-24 12:02:09
"I posted this in /r/dataisbeautiful in response so some unfortunate posts showing seemingly related time series. It was deleted because it just looks at simulated (not real) data, so I thought I'd post it here instead.",glial,2014-05-24 05:46:09
"Good post, unfortunately many of the people in /r/dataisbeautiful are not known for their statistical expertise, especially since they became a default sub. ",redvelvetpoptart,2014-05-24 06:40:43
It's actually kind of frustrating to read the comments there now.  ,neurone214,2014-05-24 20:57:30
"There are several assertions that the author makes that are simply not true. I will focus on two of his major points.

(1) ""Correlation is only meaningful when the data is i.i.d.""

This is simply not true and ignores 60+ years of statistics research on multivariate time series.

(2) ""The main takeaway here is that the correlation coefficient r is NOT an estimator of the population correlation coefficient ρ when the time series are autocorrelated.""

This is also not true. The problem, which he alludes to, is that the correlation coefficient is a less reliable estimator when the time series are temporally correlated. 

In my opinion, the real takeaway from tyler vigen is that we have to be very careful about how important model selection is when the data are dependent. If we assume that the data are independent when they are not, then we will generally underestimate standard errors of parameter estimates, leading to overconfident conclusions. However, if we select an appropriate model accounting for the dependence, then we will get more realistic standard errors. Maybe this is what the author is trying to say, but the errors mentioned above make me doubt he has a strong grasp of the subject.

",galton,2014-05-24 18:42:27
"&gt;In my opinion, the real takeaway from tyler vigen is that we have to be very careful about how important model selection is when the data are dependent.

For those unfamiliar with the phenomenon the guy/blog referenced in both the original post and this comment is [Spurious Correlation by Tyler Vigen](http://www.tylervigen.com/).
",JoPlog,2014-05-25 04:32:04
"I think the first point is a little pedantic - correlation means *something*, but it doesn't strictly reflect the relationship between the two variables since it also includes the autocorrelation. But generally people don't interpret it like that, they just see a high coefficient and conclude that the two series have a strong relationship.

",glial,2014-05-24 20:13:57
"Very interesting read, thanks !",laMarm0tte,2014-05-24 12:26:53
Great link! I've not come very far in my stats education and this really cleared things up for me. ,efxhoy,2014-05-24 17:49:14
Glad to hear! That was the goal. It can be difficult for beginners to get an intuitive understanding of what's going on when there is so much unfamiliar lingo and seemingly arbitrary rules. I hope the visuals made it a bit easier.,glial,2014-05-24 18:09:46
"Definitely! If you have the time, perhaps you could post the source-code to generate the graphs? Am I right in thinking they look pythonish? ",efxhoy,2014-05-24 18:14:50
"Yeah, you can see the code [here](https://github.com/tom-christie/tom-christie.github.io/blob/master/assets/code/time_series.R). It's R code, using ggplot to make the plots. I think there's a ggplot for Python too based on matplotlib but I haven't used it.",glial,2014-05-24 20:00:44
[deleted],,2014-05-24 07:29:28
"This isn't strictly correct. At least coming from econometrics, the interpretation of coefficients for an OLS regression on cointegrated time series is different from the interpretation of coefficients for OLS on first differences. The cointegrated regression is interpreted as reflecting a long run equilibrium relationship, and doesn't tell you anything about short term correlation.

http://www.econ.ku.dk/metrics/econometrics2_05_ii/slides/10_cointegration_2pp.pdf",bholstege,2014-05-24 08:43:22
"Great link, thanks.",glial,2014-05-24 09:00:36
Yeah this is a common mistake.  Got to have stationary data.,FullSharkAlligator,2014-05-24 10:53:12
"This is a wonderful explanation of the issue.  Well done, Tom.  ",neurone214,2014-05-24 20:57:06
"Okay, go ahead Mr Christie -- what do *I* think they mean?

",efrique,2014-05-24 21:52:45
"Haha, I admit the title is ridiculous. I've seen so many people looking at correlations between time series and saying ""wow, these are huge!""  and either concluding that there's a relationship between the two variables (I'm thinking of neuroscience studies here) or there must be something weird going on but can't say what (Tyler Vigen's post on spurious correlation).  I have no idea what *you* think!",glial,2014-05-25 07:23:03
I think that's just called /r/politics,,2013-12-01 18:00:54
That and /r/finance ,ajmarks,2013-12-01 18:30:32
"snap  
  ",thinksthoughts,2013-12-01 18:20:19
"Sure. Lots of social pseudo-science, but probably a decent amount of hard pseudo-science as well. The discussions would be quite different though (one would hope). ",naught101,2013-12-01 19:40:13
"Personally, I'd be more interested in misuses or misinterpretations of stats than bad visualizations.",Palmsiepoo,2013-12-01 17:00:38
"Yeah, but they do often go hand in hand. I meant data visualisation that are bad for statistical reasons (e.g. comparing variables of different type, or what ever), rather than visualisations that are technically correct, but ugly.",naught101,2013-12-01 17:27:50
"I'd be down for that. The stats community is fairly small though, I wonder if it would get this idea more exposure if we kept it in /r/statistics but just made it like a weekly standing post rather than an entire subreddit. Just a thought.",Palmsiepoo,2013-12-01 17:34:30
"There's always multireddits, and side-bar links to keep it together. I think there'd be a fair chunk of people interested in /r/badstats, but not interested in stats enough to be regulars at /r/statistics.",naught101,2013-12-01 19:43:12
"I agree. For bad visualizations, see /r/dataisugly.",mwk11,2013-12-02 06:52:02
"I'd be happy to contribute; I have piles of examples. I also have a vested interest in seeing other people's examples, since I'm writing a guide to doing statistics wrong...

I ought to write a blog post about one I found recently: how Darrell Huff, author of *How to Lie with Statistics*, was paid by the tobacco industry to testify against the cigarettes-cancer link. He even wrote a manuscript for *How to Lie with Smoking Statistics*, which I managed to find about half of. It's hilarious.

Another example is [how right turns on red were legalized due to underpowered studies](http://www.refsmmat.com/statistics/power.html#the-wrong-turn-on-red).",capnrefsmmat,2013-12-01 17:59:44
"An appealing article. However, reporting of the effect size (eg sometimes as much as 100%) leaves me less than satisfied. ",neurone214,2013-12-01 21:44:16
"Yeah, sorry about that. My draft is more specific. If you want numbers:

&gt; From a review of the available literature it is estimated that at the approximately 80% of all signalized intersections where motorists are allowed to turn right on red all right-turning crashes increase by about 23%, pedestrian crashes by about 60%, and bicyclist crashes by about 100%.

Zador, P. L. (1984). Right-turn-on-red laws and motor vehicle crashes: A review of the literature. Accident Analysis &amp; Prevention, 16(4), 241–245. doi:10.1016/0001-4575(84)90019-8",capnrefsmmat,2013-12-02 05:37:30
"Go on then, fill 'er up. /r/badstats",naught101,2013-12-01 21:09:38
Definitely! This would be massively useful as a learning tool,musicaldope,2013-12-01 18:24:38
done: /r/badstats,naught101,2013-12-01 21:13:52
You might be interested in /r/dataisugly,SafeSituation,2013-12-01 22:36:51
"Thanks, I was looking for something like that. I've added it to the side-bar.",naught101,2013-12-02 03:29:37
"Hells yeah!  I love /r/badphilosophy, and a stats version would be great (whether more serious or not).",US_Hiker,2013-12-01 20:07:56
"Created /r/badstats and added  /r/badphilosophy as a side-bard link. If you know of any others, let me know.",naught101,2013-12-01 21:10:12
Subscribed!,US_Hiker,2013-12-01 21:11:19
I could help moderate.,unclemilty420,2013-12-01 21:15:07
"/r/badhistory has some good rules about how they run these.  Rule 5, for example. Worth a look.",ObeisanceProse,2013-12-02 06:40:46
"Fair call, I started a discussion: http://www.reddit.com/r/badstats/comments/1rymez/what_rules_do_we_need_here/",naught101,2013-12-02 18:28:36
I suggest the name shittystatistics so you can join the shitty network.,osqer,2013-12-02 11:53:36
"I'm down, but that place is gonna be overflowing with content, I don't think we can keep up!",mrpopenfresh,2013-12-02 16:06:54
There is a set of Google videos which covers the same videos although it's a bit slower: http://www.youtube.com/watch?v=zRsMEl6PHhM. Just stop on day 7 if you're not interested in data mining specifically. He also compares it to Excel quite a bit.,Qw3rtyP0iuy,2013-06-08 01:47:27
"Thanks!

this is really great and useful.  Cheers dude!

I... erm, I feel almost silly asking this, but i don't suppose I could ask for just one more favour from you?  Could I perhaps see the stuff?",cheeseybees,2013-01-09 09:07:49
"Woops, forgot the good part didn't I? 

http://www.computerworld.com/s/article/9214755/Chart_and_image_gallery_30_free_tools_for_data_visualization_and_analysis
",,2013-01-09 09:10:19
"Cheers dude! Please have a thousand and one thanks, as well as my daughter!

(ok, it's a sock puppet, but it'll treat you real good!)",cheeseybees,2013-01-09 09:17:00
This is what I have been looking for!,RoundboutWay,2013-01-09 21:56:29
"I'm so happy to help, gentle stranger! Have an upvote!",,2013-01-10 07:50:27
"Protovis and no d3 (made by the same team, more recent and more powerful)? ",bakonydraco,2013-01-09 18:36:38
"Nice, thank ya!",maybe_yeah,2013-01-09 18:45:39
"it's unfair, no D3",jdxyw,2013-01-10 00:09:15
"I was surprised as well. If d3 doesn't belong on the list, Google Charts certainly doesn't.",binaryechoes,2013-01-10 01:48:28
"&gt; Link didn't post quite like I planned...

... probably because you clicked the 'text' tab, which tells reddit 'forget the link, I want to make a text post'.",efrique,2013-01-10 02:32:38
"Well, lesson learned then eh?",,2013-01-10 07:39:34
Oh my god. This is perfect - the program we're forced to use in my stat class is horrible and I was looking for something better and now I have 30 options. You're gorgeous!,Ipsey,2013-01-10 03:59:48
"Seems heavy on the data visualization, very light on the statistical analysis.

PSPP is not listed, I have not used it but it seems like it should be on there.",kinross_19,2013-01-11 09:11:28
this is fucking rad,watersign,2013-06-08 22:09:05
"""All statisticians use prior information in their statistical analysis. Non-Bayesians express their prior information not through a probability distribution on parameters but rather through their choice of methods.""

This is a good distinction that I hadn't thought of.",,2012-11-10 14:04:36
"Also, a good frequentist uses multiple comparisons and at least some Bayesian thinking even if they aren't quantifying prior probability.

Even statistically-blunted biologists will repeat an experiment with wildly improbable/unexpected results, and if they get the same results again, they usually try to figure out what they screwed up in the protocol.

Being a good scientist, IMO, involves incorporating Bayesian thinking as the default even if the statistical tests remain frequentist.",BillyBuckets,2012-11-10 15:25:48
"If you were taught statistics, the fact that you hadn't thought of it reveals that there's a problem in how the frequentist approach was taught.",rottenborough,2012-11-10 17:11:37
"As a 1st semester graduate student, I have made this distinction. I also was pretty annoyed when I saw the comic.",,2012-11-10 22:10:54
"This comic irked me too. My stat friends were posting it on their FBs, and that bothered me more. Just because we finally get some attention, doesn't mean there's validity to it.",Msjuneday,2012-11-10 22:32:18
I am surprised that statisticians are so willing to bash each other in a manner that makes both parties look ignorant. ,,2012-11-11 00:55:47
"At first I thought it was unfair, but then I thought of it this way: it's making fun of how the frequentist approach is often being taught and abused, not how it is when done correctly.

Also I still can't get over the fact that we'd *all* bet *all* our money on the sun not having exploded. But that's a different issue.",rottenborough,2012-11-10 23:05:25
"""In teaching the correct interpretation of frequentist statistics, we are fighting a losing battle.""

I'm not a Bayesian, but I oh, so often agree with that point. I recently looked at a paper where the authors used a propensity adjusted logistic regression model and estimated an odds ratio between the main exposure and outcome of interest to be ""1.51, 95% CI... p = 0.063 which is significant at the 0.063 level."" I face palmed.",,2012-11-11 09:36:47
"""Non-Bayesians"", does he mean frequentists? He's right though. It is a deficiency of NHST to include superfluous joint probability distributions in a decision rule for a parameter. A frequentist test of your favorite hypothesis could be supplied by using a random number generator to generate uniform 0,1 RVs and reject the null hypothesis when it's found to be &lt; 0.05. Such a test is of the right size!

In general, that's why maximizing power (both through the analytic and methodology) is so important. Tests which are unbiased, maximum power in the frequentist world are usually very sane (unlike Mr. Munroe would have us believe). Coincidentally, these usually have analogous Bayesian approaches as well (tests which are asymptotically equivalent)!",,2012-11-11 09:34:13
"""As an aside, I also think the lower-right panel is misleading. A betting decision depends not just on probabilities but also on utilities. If the sun as gone nova, money is worthless. Hence anyone, Bayesian or not, should be willing to bet $50 that the sun has not exploded.""

Yes Andrew. That's the joke.",JMcCloud,2012-11-10 16:39:38
I think his mentioning of the lower-right panel was also a joke.,,2012-11-10 19:48:24
"This comic can explore and make assertions about love, politics, consumer technology, social conventions, and religion. But bring statistics into it, then controversy erupts. I love it.",deako,2012-11-11 09:36:09
"I'd appreciate feedback and ideas from anyone; I wrote this after taking my first statistics course (and doing a pile of research, as you can see), so there are likely details and issues that I've missed.

Researching this actually made me more interested in statistics as a graduate degree. (I'm currently a physics major.) I realize now how important statistics is to science and how miserably scientists have treated it, so I'm anxious to go out and learn some more.",capnrefsmmat,2012-07-30 17:44:59
"&gt;  I wrote this after taking my first statistics course

Wow.",harbo,2012-07-30 22:53:21
"&gt; ""There’s only a 1 in 10,000 chance this result arose as a statistical fluke,"" they say, because they got p=0.0001. No! This ignores the base rate, and is called the base rate fallacy.

True enough, but p=0.0001 is not a typical cut-off value (alpha level), so this example sort of suggests that the researcher got a p-value around 0.0001 and then interprets it as a probability (which is an ubiquitous fallacy). Even without a base rate problem, that would be wrong. You'd essentially be considering the event ""p &lt; [the p-value I got]"". If you consider both sides as random variables, then you have the event ""p &lt; p"", which is obviously impossible and thus did not occur. If you consider the right-hand side as a constant (you plug in the value you got), then you're pretending that you fixed it in advance, which is ridiculous, kind of like the ""Texas sharpshooter"" who fires a shot at a barn and then draws a circle around the shot, claiming he aimed at that. The results from such reasoning are about as misleading (this isn't just a theoretical problem).

&gt; But if we wait long enough and test after every data point, we will eventually cross any arbitrary line of statistical significance, even if there’s no real difference at all.

Also true, but missing an explanation. The reason is that no matter how much data you have, the probability (under null) of a significant result is the same. 

Note that the same kind of thing does not happen for averages, so this ""arbitrary line-crossing"" isn't a general property of stochastic processes (but the reader might be left with that impression). The strong law of large numbers says that the sample mean almost surely converges to the population mean. That means that almost surely, for every epsilon there is a delta [formal yadda yadda goes here ;)], i.e. if you draw a graph kind of like the one you did in that section for a sample mean with more and more samples thrown in, then a.s. you can draw an arbitrarily narrow ""tube"" around the mean and after some point the graph does not exit the tube. Incidentally, this is the difference between the strong law and the weak law - the weak law only says that the probability of a ""tube-exit"" goes to zero, it doesn't say that after some point it never occurs.",Coffee2theorems,2012-07-31 03:25:22
"&gt; ""There’s only a 1 in 10,000 chance this result arose as a statistical fluke,"" they say, because they got p=0.0001. No! This ignores the base rate, and is called the base rate fallacy.

To comment on this, since p-values are a frequentist method, the idea of a base rate is somewhat moot.  Either the null is true or not\*.

If you want to look at multiple testing, then one should use false discovery rates, i.e., q-values.

(\* or as I say to people, the third possibility is that it doesn't make sense at all.)",anonemouse2010,2012-07-31 08:48:04
"&gt; True enough, but p=0.0001 is not a typical cut-off value (alpha level), so this example sort of suggests that the researcher got a p-value around 0.0001 and then interprets it as a probability (which is an ubiquitous fallacy). Even without a base rate problem, that would be wrong. 

Yeah, that's what I was aiming at. I'm not sure I want to get into the Neyman-Pearson vs. Fisherian debate in this guide, though. I just want to stop news articles from saying ""Only 1 in 1.74 million chance that Higgs boson doesn't exist"".

(Fun fact: all the news articles quoted some probability that the Higgs discovery was a fluke, and almost all of them gave differing numbers.)

&gt; Also true, but missing an explanation. The reason is that no matter how much data you have, the probability (under null) of a significant result is the same. 

Thanks. I may work an explanation in when I get around to revising everything.",capnrefsmmat,2012-07-31 05:23:10
"&gt; the Neyman-Pearson vs. Fisherian debate

Wow. Either your first statistics course was a seriously exceptional outlier, or you weren't kidding about that ""pile of research"" :) Some *statisticians* have no idea what I'm talking about when I refer to that one.

At this level of sophistication you might be interested in [this article](http://www.stat.duke.edu/~berger/papers/99-13.html) about p-values, if you haven't seen it already. It is a serious attempt at exploring how you could interpret p-values as probabilities and explains problems with the naive interpretation (assuming no base rate problem). Essentially, the problem arises from observing ""p=0.0001"" and pretending that you observed only ""p ≤ 0.0001"" (= interpreting observed p-value as an alpha-level), causing severe bias against the null hypothesis as the latter observation is far more extreme. When I originally read that article, I knew that the direct interpretation of p-values as probabilities is wrong, but the magnitude of the error in doing so still surprised me, because the Fisherian approach does have intuitive appeal to it.",Coffee2theorems,2012-07-31 06:45:12
"It was a pretty damn good statistics class. We did cover the Neyman-Pearson vs. Fisherian question in class in some detail. Not surprising, either; you cite one of Berger's papers, and our professor got his PhD under Berger. I'm going to take another course from him next spring.

Thanks for the article. I'll read it once I get out of work. I may need to clarify some of my p-value explanations once I do.",capnrefsmmat,2012-07-31 07:11:36
"&gt; Thanks for the article. I'll read it once I get out of work.

Just noticed that I linked to an old version of it. [Here is the published version](http://www.stat.duke.edu/courses/Spring10/sta122/Labs/Lab6.pdf). Figure 1 at least in the old version is quite confusing, so better get the newer one.

&gt; Not surprising, either; you cite one of Berger's papers, and our professor got his PhD under Berger.

Nice. Much of Berger's work is rather too theoretical for me (I like very pragmatic subjective Bayesian statistics a la Gelman, and read the more theoretical stuff mostly out of sheer curiosity :), but it's good to see that someone is doing that kind of work. It certainly needs doing! I've gotten the impression that Berger's understanding of foundational issues in statistics is top-class.",Coffee2theorems,2012-07-31 07:43:42
"No offence, but how old are you? You say you are a physics major so presumably an undergraduate but your work on github alone is impressive let alone this article, doing a physics degree etc. etc.

I didn't realise Gauss frequented Reddit :P",alexgmcm,2012-07-31 06:35:18
"I'm 20. Going into my senior year as a physics major this fall. If I were Gauss I'd already be writing monographs on new fields of math and physics, but thanks. I'm just demonstrating how dangerous it is to let a physicist get bored.",capnrefsmmat,2012-07-31 08:27:28
"Haha - how did you get into your senior year at 20? I'm 21 going into my senior year, but it's also an integrated masters year as I'm in the UK so we have our weird British way of doing things of course...",alexgmcm,2012-07-31 08:58:47
"Long story involving moving, a crappy private school, and skipping 4th grade. Not sure it's made much of a difference in my education, apart from other students being shocked that I still can't legally drink.",capnrefsmmat,2012-07-31 09:27:30
"&gt; I still can't legally drink.

The secret of your productivity is out.",,2012-07-31 10:31:42
"But who has time for drink with all the physics anyway right? :P Although I can drink and rarely do just because of expense and I don't really like the taste of it.

Also it's weird to me that uni students can't drink as the age is 18 here.",alexgmcm,2012-07-31 13:37:23
"&gt;I wrote this after taking my first statistics course

Good on you. You must have had a hell of a class and a hell of a professor. Great work on the research, too- I'm familiar with everything in your article, but I also studied statistics at a grad level.

It's good to see science students taking serious interest in statistics.

",,2012-07-31 10:32:32
The illustrated examples were great. Add some examples on sales forecasting done wrong and this could easily apply to business people as well (and business people love easy to understand visuals). ,aaaxxxlll,2012-08-01 16:47:02
"I do statistics as part of science. When I published my first article it was really long in comparison to other similar works because I tried to explain why I used the particular technique vs the other common ones, and explained why I couldnt test a variety of things (to control for multiple testing), then I had a number of plots demonstrating that I didn't break the assumptions of the model.

My paper is 130-150% as long as similar works. I am guessing that makes it much less approachable to anyone else. 

I don't really have a point here, but I enjoyed your article. Maybe you could add in links to textbooks or articles that describe how to do each part of your suggestions correctly?",quatch,2012-07-30 20:33:12
"Do you think it's really necessary to point out you didn't break the assumptions of the model? I always figured that the assumptions are not broken unless specifically mentioned, in which case it might not even be a good idea to use the statistical method in question.",TempusFrangit,2012-07-31 09:37:03
"hah, assumptions are broken all of the time. That doesn't mean that the test is completely wrong, but it usually means that the confidence bars are too small or somesuch. In my opinion, if it isn't demonstrated, it probably is broken.

Also, I was applying a new model for this kind of research, I needed to explain that it was better precisely because it could avoid a lot of the problems the simple modellers ignore.",quatch,2012-07-31 18:41:16
You would really think so but people will bluster ahead without even being aware that they're breaking the assumptions of the model.,samclifford,2012-07-31 14:25:42
"I try to be careful about that when writing a paper, but do you think it's generally better to mention you're not breaking any assumptions? I figured that it would needlessly clutter up the paper with information readers generally don't care about, assuming that you're knowledgable enough about what you're doing.

I'm still just learning, and the only paper I've written was based on a student project. Any tips on writing good papers are more than welcome.",TempusFrangit,2012-07-31 15:05:37
"I think it can be pretty brief most of the time: applied such and such model, data was normally distributed, residuals were homoskedastic, some statement about multiple testing or sample size.",quatch,2012-07-31 18:43:29
"I think this is a good way to go about it. Probably also important to quantify autocorrelation in residuals when dealing with temporal data in order to explain how much temporal variation is left. I'd say that's more posterior checks than model assumptions.

Things like ""Levene's/Bartlett's test was used to test for equal variances. The variances were found to be unequal so a GLM was fitted of the form ..."" are good.",samclifford,2012-07-31 22:06:01
"&gt; Maybe you could add in links to textbooks or articles that describe how to do each part of your suggestions correctly?

I tried to include citations to papers on each error I discussed. Unfortunately I don't know much about statistics textbook; our professor used a book of his own devising, and recommended [OpenIntro Statistics](http://www.openintro.org/stat/textbook.php) for anything else.",capnrefsmmat,2012-07-31 19:35:05
"I have an MSc in computer science, and have only encountered in the curriculum a single one-semester course which had to cover both statistics and probability theory. Fortunately, through articles like yours, and similar ones I've seen in the past, I was already aware of my resulting statistical ignorance. 

The problem is how to fix it.   Advice to ""pick up a good book"" is not very helpful when there are so many bad textbooks out there. (At least there are in computer science, but I'm guessing many fields have such issues.) Like quatch, I'd be interested in more concrete recommendations.",Nolari,2012-07-31 00:03:48
Thanks for a lovely read. I spoke at an aerosol science conference recently about the need for better statistics in science. My focus was on moving away from just doing ANOVA and naive linear regression but you've done really good job elaborating on where we fall down with even more basic things like experimental design and interpretation of p values.,samclifford,2012-07-31 14:43:34
"What are the most unpleasant equations to understand?

I think the ones that relate to human violence. [Price's equation](http://en.wikipedia.org/wiki/Price_equation#Example:_Evolution_of_altruism) can have some pretty grim consequences. [Hamiltons kin](http://en.wikipedia.org/wiki/W.D._Hamilton) selection equations can have even grimmer consequences.

I remember hearing there was an equation for calculating a childs chance of dying in its first year. The presence of a non related adult male was one of the variables. Which is depressing.

Any other candidates for equations that give a dim view of human nature?",cavedave,2013-03-19 23:52:18
"Integral from t = 0 to T of 4 pi F(T-t) t^2 dt 

Relates to the probability that we'll survive to reach the stars, or be caught by the [Great Filter](http://hanson.gmu.edu/greatfilter.html).",khafra,2013-03-20 07:13:15
And domestic violence and conflict.,BullNiro,2013-03-20 16:51:41
"Interesting stuff, thanks for posting this. There's something intriguing about the mis-application of statistics in real world problems, other than your [humorously inane ones](http://imgur.com/zPymfXO)",,2013-03-20 02:21:35
God damn lemon thieves!,MagnusT,2013-03-20 10:41:03
"Short snipe from an evolutionary biologist:

""It is well established that there is evolutionary pressure towards greater variation within species--"" The author then goes on to offer an evolutionary just so story about variance in mammals. If male mammals are heterogametic because it is good for males to have greater genetic variance, why are female birds heterogametic? Why have genetic determination of sex at all?

I find it immensely irritating that such poor reasoning is included in this article, though I guess I shouldn't be surprised that the authors' brains seem to turn off once they are talking about evolution. It seems most reasonable to regard the choice of which sex will be heterogametic as a simple coin flip. If there is an evolutionary pressure to create a genetically determined sex (there is) and as a consequence of this one of the two sexes has reduced recombination (it will) then it follows immediately that whichever sex has reduced recombinations will become heterogametic. Initially there is no difference in the variance of the sexes, and there won't be for millions of years, long after the neutral process of drift has already determined which sex will be heterogametic. Like most layman, the authors assume that every trait must exist because of an adaptive advantage it confers. However, this is simply not the case.

[Basically, it is never the case that a species is both XY (where males are heterogametic) and WZ (where females are heterogametic). Since XY/WZ systems are not competing within a species, there is no opportunity for selection to favor XY systems over WZ systems (nor is there any evidence that it does favor XY systems given the opportunity), and as a result it simply cannot be the case that natural selection has chosen and XY system to provide males with greater genetic variation.] ",Larry_Boy,2013-03-20 09:11:24
"is it possible to say that once you are either XY or WZ then selective pressure will operate as he described? (eg, we might expect to see little differentiation as a result of WZ under his method)",quatch,2013-03-20 10:14:04
"Drift and mutation pressure ensure that the W (the non-recombining chromosome) in a ZW system decays in the same way as a Y in an XY system. Females should have more genetic variance for fitness in ZW systems, despite the fact that females gain no benefits from this higher variance.  ",Larry_Boy,2013-03-20 11:52:29
"If I recall correctly, there is also very little variability in the Y-chromosome itself, relative to the rest of the genome, no?",Neurokeen,2013-03-20 18:28:15
"That is correct. The Y-chromosome is non-recombining, present in only half the population and males have a higher variance in reproductive success than females. As a result variation in the Y chromosome is cleaned out more quickly than on any of the other chromosomes. The Y chromosome also generates variation more quickly than the other chromosomes because it is always found in males and males have a higher mutation rate than females (this results from the fact that more cell divisions are used to make sperm than are used to make eggs). But the effect postulated in the books actually comes from the X chromosome. Males inherit one X chromosome rather than two X chromosomes, and thus have a greater variance in fitness because all the recessive mutations on X chromosomes are exposed. This part of the authors explanation is correct. What the authors fails to appreciate is that there is also a change in the mean fitness because mutations are not unbiased in their effects. Mutations typically decrease fitness. The average mutation makes us less intelligent because there are more ways to make a poorly functioning brain than a well functioning brain. The expectation is then that the mean should decrease substantially more than the variance increases because mutations are typically recessive. Since male performance isn't substantially lower than female performance the X chromosome story doesn't hold water. Small disclaimer : these expectations are based on very specific assumptions about the frequency of alleles which may not be true. Unfortunately I don't have time to get into all the details. ",Larry_Boy,2013-03-21 09:54:29
You don't need a graduate degree to know this either. This is fairly basic evolutionary biology.,BullNiro,2013-03-20 19:03:15
"Thanks so much for posting this explanation. That was the one part of the chapter that I was uncomfortable with. I shared the chapter with students this week, and told them that I take the discussion of sex-based variance differences with a huge grain of salt because I don't know enough about evolutionary biology to assess it, but that it's an interesting conceptual example nonetheless.

I'll share your comment with them, and repeat my advice to take it with a grain of salt because I don't know enough to determine whether you're right (but it sounds like you know what you're talking about, and your reasoning makes sense to me). 

Care to recommend a good intro book to get me up to speed on evolutionary biology? ",nixxon,2013-03-20 17:34:23
"My top pick would probably be “The Origins of Genome Architecture"". Its title doesn't sound particularly broad, but it hits a number of really important concepts, including the evolution of sex chromosomes. If time is no object, you could try “The Structure of Evolutionary Theory.” However, despite the book's length a number of important concepts in modern evolutionary theory are ignored. Maynard Smith's “The Theory of Evolution” looks promising, but I haven't read it yet, so I can't really recommend it. Finally, it's always worth considering a book on population genetics (Nothing in evolution makes sense except in the light of population genetics) and for that I would recommend the industry standard : “Population Genetics: A Concise Guide.”",Larry_Boy,2013-03-21 09:25:18
"Thanks, these sound like some great starting points.",nixxon,2013-03-21 10:12:06
This comes from a terrific book.  Best pop statistics book I've ever seen.,berf,2013-03-20 07:31:13
"I'm a bit confused by this statement (bottom of page 11, top of page 12):

&gt; [;E(\text{achivement}|\text{small}) &gt; E(\text{achivement}|\text{big}) \nRightarrow P(\text{small}|\text{achievement}) &gt; P(\text{large}|\text{achievement}) ;]

Forgive me if this is super basic probability...  ",Pseudo_Scientist,2013-03-22 10:40:50
"&gt; E = MC^2

It's E = mc^2


It's especially important with the *c* not to capitalize it. That turns it from *speed of light in a vacuum* to what - *Celcius*, maybe?

Some interesting (if well understood here) stuff. It would be useful to people unfamiliar with those issues.
",efrique,2013-03-19 21:06:34
Thanks.  I was so confused before your comment.,,2013-03-20 03:49:56
"Nice article.  

I appreciate the author's choice; not understanding the distribution of the sample mean can lead to disasters for sure.  But if I had to pick a dangerous equation to be unaware of, I'd go for kinetic energy = half mass times velocity squared.  Source: car accidents.",jtr99,2013-03-20 02:01:26
"On the topic of car accidents, I would suggest

&gt; p = mv

as a better equation to use. The object with greater momentum always wins.",leonardicus,2013-03-20 04:56:41
I've always thought it was Euler's Formula..You can't mess with e^(i *pi)-1=0. ,drpepper322,2013-03-20 05:40:04
"well, you can. Heard of Tau?",quatch,2013-03-20 10:15:03
The updated version: http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf,srkiboy83,2013-03-17 02:18:57
Thank you!,beaverteeth92,2013-03-17 04:34:29
If you think that's amazing look through the rest of the documentation and intro to datamining by Google (youtube),Qw3rtyP0iuy,2013-03-16 23:53:45
"This is a shitty wordpress site but the links are related to ""figuring out R"" with free PDF links. I just bragged about not diving into documentation because of Google Answers but I found R afterwards and I'm happy I know how to navigate it well (although it took me forever to figure out how to install one of the two 'easy installers')

http://mazamascience.com/WorkingWithData/?p=619",Qw3rtyP0iuy,2013-03-17 04:06:53
Thanks so much for this! I'm participating in a data analysis competition next weekend and my team will be using R so this will be a great help,,2013-03-16 21:06:00
No problem!  Also just a disclaimer that I didn't create this.  And what kind of a data analysis competition?,beaverteeth92,2013-03-16 21:11:16
"As a relative R beginner, this is so awesome!  I will pass it around to my statistics class next term.  Thank you!",Revontulet,2013-03-16 22:17:40
[deleted],,2013-03-17 09:16:46
[deleted],,2013-03-17 13:09:43
[deleted],,2013-03-17 17:07:27
"&gt; I'm female, so I can bear my own children

that's probably for the best if 24601G is a man",CURIOUS_ABOUT_SEX,2013-03-17 19:45:28
[deleted],,2013-03-17 18:11:44
fgg,miniguy,2013-03-18 04:48:29
What is an anomaly?  I ask sincerely.,missinguser,2015-01-06 19:28:04
"In the context of this tool, I believe an anomaly is a time interval during which the process of interest generates a significantly different amount of activity (probably usually more activity) than one would expect based on the history of that process. One of the examples of an anomaly they give is the increase in number of tweets that occurs during the superbowl (relative to what is normal for a Sunday afternoon in Winter, presumably).",shaggorama,2015-01-06 22:10:42
This was also my interpretation. ,WhoWillSaveYourSwole,2015-01-06 22:30:46
This can help: https://en.wikipedia.org/wiki/Anomaly_detection,Barbas,2015-01-07 01:22:26
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Anomaly detection**](https://en.wikipedia.org/wiki/Anomaly%20detection): [](#sfw) 

---

&gt;

&gt;In [data mining](https://en.wikipedia.org/wiki/Data_mining), __anomaly detection__ (or __outlier detection__) is the identification of items, events or observations which do not conform to an expected pattern or other items in a [dataset](https://en.wikipedia.org/wiki/Dataset).  Typically the anomalous items will translate to some kind of problem such as [bank fraud](https://en.wikipedia.org/wiki/Bank_fraud), a structural defect, medical problems or finding errors in text. Anomalies are also referred to as [outliers](https://en.wikipedia.org/wiki/Outlier), novelties, noise, deviations and exceptions. 

&gt;In particular in the context of abuse and network intrusion detection, the interesting objects are often not *rare* objects, but unexpected *bursts* in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis) algorithm may be able to detect the micro clusters formed by these patterns. 

&gt;Three broad categories of anomaly detection techniques exist. __Unsupervised anomaly detection__ techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. __Supervised anomaly detection__ techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other [statistical classification](https://en.wikipedia.org/wiki/Statistical_classification) problems is the inherent unbalanced nature of outlier detection). __Semi-supervised anomaly detection__ techniques construct a model representing normal behavior from a given *normal* training data set, and then testing the likelihood of a test instance to be generated by the learnt model. [*[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)*]

&gt;

---

^Interesting: [^Anomaly ^Detection ^at ^Multiple ^Scales](https://en.wikipedia.org/wiki/Anomaly_Detection_at_Multiple_Scales) ^| [^Magnetic ^anomaly ^detector](https://en.wikipedia.org/wiki/Magnetic_anomaly_detector) ^| [^Network ^Behavior ^Anomaly ^Detection](https://en.wikipedia.org/wiki/Network_Behavior_Anomaly_Detection) ^| [^Proactive ^Discovery ^of ^Insider ^Threats ^Using ^Graph ^Analysis ^and ^Learning](https://en.wikipedia.org/wiki/Proactive_Discovery_of_Insider_Threats_Using_Graph_Analysis_and_Learning) 

^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cnh4gq5) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cnh4gq5)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2015-01-07 01:22:47
"[**@TwitterEng**](https://twitter.com/TwitterEng):
&gt;[2015-01-06 17:20:48 UTC](https://twitter.com/TwitterEng/status/552515138444869632)

&gt;Today, we're open sourcing our AnomalyDetection R Package. Learn more: [*blog.twitter.com*](https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series) [#TimeSeries](https://twitter.com/search?q=%23TimeSeries) [*pic.twitter.com*](http://pbs.twimg.com/media/B6rtlzHCMAAPu3q.jpg) [^[Imgur]](http://i.imgur.com/4gQrFn8.jpg)

----

[^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/2rj62r%0A%0APlease leave above link unaltered.)
[^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion)
[^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/)
[^[Code]](https://github.com/buttscicles/TweetPoster)
[^[Issues]](https://github.com/buttscicles/TweetPoster/issues)
",TweetPoster,2015-01-06 09:36:55
"Coool. Quick question before I dive into this: Does it need to be time series data? Can it be any successive measurement data, like distance?",really_so_sorry,2015-01-06 11:57:40
"I haven't looked at what kind of data structure it assumes, but the post does suggest that the method they're using is about the intersection between time series and big data, and in particular detecting anomalies amongst seasonal and trend patterns. So it should still work on something else like spatial data, but I suspect there are other algorithms better designed for it.",conmanau,2015-01-06 15:22:50
"I think the benefit of the algorithm they're implementing here is that it is able to account for seasonality effects. There's probably a better tool for your use case from the sound of it, but I'd need to know more about your problem to suggest anything.",shaggorama,2015-01-06 22:14:14
Thx for that post.,drsxr,2015-01-06 14:41:31
This will be fun,niaiserie,2015-01-06 16:41:57
[deleted],,2014-10-21 16:45:47
"Seriously.  It's so refreshing to hear someone essentially say ""Your question is bullshit."" in an interview.",beaverteeth92,2014-10-21 17:40:29
The initial response at /r/MachineLearning was funny.,maxToTheJ,2014-10-21 14:47:20
It's nice to see someone in machine learning emphasize the pitfalls of overly complicated models that are really difficult to interpret and who also understands the importance of good statistical reasoning.,beaverteeth92,2014-10-21 17:42:47
"That interview was so refreshing.  I'm constantly hearing people say ""with big data we are on the cusp of x,y,z, and we'll be able to predict a,b,c..""  

People have been saying the same stuff for 70 years, only the words change.  The tools are useful, and will become more useful.  There's a fascination with conquering uncertainty.  Or, probably more accurately, companies have a fascination with telling people they've conquered uncertainty.  I totally agree that a bubble is near.  All you have to do is look at the most popular data mining/science books on Amazon.  They all advertise that you can be clairvoyant with basic knowledge of Excel and a small toolbox of algorithms.  There's going to be a burst when people realize that this is a fantasy.",MrBrodoSwaggins,2014-10-21 20:00:42
"This is right.  No new mathematical methods have been really invented lately.  This is all old stuff.  Much of the ""new tools"" are inferior to more common methods taught in Statistics departments.  ",bwik,2014-10-22 01:35:35
"I'm a graduate student who takes some machine learning because of the hype and realizes that while useful, a lot of it is old fashioned statistics. I'm pretty convinced that soon more people will have the same realization. The question for me now is how to brace myself for that bust...",selectorate_theory,2014-10-22 07:08:11
Fantastic interview.  Thanks for sharing.,yakattackpronto,2014-10-21 13:03:09
"This interview made me go to his website to start downloading papers... but I was quickly overwhelming number of papers published, so goal shifted to going to some of his top recent publications on goole scholar ([link for those so inclined](http://scholar.google.com/citations?user=yxUduqMAAAAJ)).",DrGar,2014-10-21 19:26:23
"Wow, if you just found out about Michael Jordan's work, you are in for a treat.  Might I suggest this published in Statistical Science, a very good review journal in statistics:

http://projecteuclid.org/euclid.ss/1089808279",Floydthechimp,2014-10-22 11:41:26
"It does feel like a treat, and I really look forward to reading the paper you linked :-)

I actually recognized two of his papers from his website, but TBH I had not paid much attention to who the authors were when I read them the first time. It feels like I discovered a treasure trove here.",DrGar,2014-10-22 11:44:33
"On #7, it looks like people in warmer climates shower more. Seems reasonable.",MTGandP,2014-09-15 09:58:48
This post doesn't really feel to be in the spirit of this subreddit. ,makemeking706,2014-09-15 14:42:36
"agree..

it's more /r/mildlyinteresting 

not sure if /r/dataisbeautiful or /r/dataisugly 

",zzay,2014-09-16 03:17:15
or /r/mapporn,typesoshee,2014-09-16 17:47:32
Third most common language in California is Tagalog? Calling BS.,casualfactors,2014-09-15 11:16:14
"This is most likely based on US Census data. In the Language use section, [one question](https://www.census.gov/hhes/socdemo/language/about/index.html) asks about languages other than English. The self-reported answers are collapsed into a bunch (roughly 400) categories.

Relevant to the particular case of California is that there are separate categories for ""Chinese"", ""Mandarin"" and ""Cantonese"". So if you look at the [most recent data](https://www.census.gov/hhes/socdemo/language/index.html), you will see that roughly 927K people reported speaking some Chinese language, which would make that the most common second language in California except for Spanish and English. However, if you only look at numbers in the subcategories, that 927K consists of 505K people writing in ""Chinese"", 200+K reporting ""Mandarin"", 174K ""Cantonese"".

Tagalog on the other hand is not sub-divided, and because of that it does appear to come in second.",kohatsootsich,2014-09-15 15:26:35
The evidence is conclusive. Alcohol makes you smarter and bathing kills brain cells!,Coffee2theorems,2014-09-15 11:16:25
"So, uh, as a foreigner, what's the deal with North Dakota? ",Accountthree,2014-09-15 06:18:54
Assuming you're not questioning their love of creampies and are wondering about their economy: https://en.wikipedia.org/wiki/North_Dakota_oil_boom,Luonnon,2014-09-15 06:34:27
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**North Dakota oil boom**](https://en.wikipedia.org/wiki/North%20Dakota%20oil%20boom): [](#sfw) 

---

&gt;The __North Dakota oil boom__ is an ongoing period of rapidly expanding oil extraction from the [Bakken formation](https://en.wikipedia.org/wiki/Bakken_formation) in the state of [North Dakota](https://en.wikipedia.org/wiki/North_Dakota) that followed the discovery of [Parshall Oil Field](https://en.wikipedia.org/wiki/Parshall_Oil_Field) in 2006, and is continuing as of 2013.   Despite the [2008–2012 global financial crisis](https://en.wikipedia.org/wiki/2008%E2%80%932012_global_financial_crisis), the oil boom has resulted in enough jobs to give North Dakota the lowest [unemployment rate in the United States](https://en.wikipedia.org/wiki/List_of_U.S._states_by_unemployment_rate).    The boom has given the state of North Dakota, a state with a 2013 population of about 725,000, a billion-dollar budget surplus. North Dakota, which ranked 38th in per capita [gross domestic product](https://en.wikipedia.org/wiki/Gross_domestic_product) (GDP) in 2001, rose steadily with the Bakken boom, and now has per capita GDP 29% above the national average. 

&gt;

---

^Interesting: [^North ^Dakota](https://en.wikipedia.org/wiki/North_Dakota) ^| [^Bakken ^formation](https://en.wikipedia.org/wiki/Bakken_formation) ^| [^Williston, ^North ^Dakota](https://en.wikipedia.org/wiki/Williston,_North_Dakota) ^| [^Dickinson, ^North ^Dakota](https://en.wikipedia.org/wiki/Dickinson,_North_Dakota) 

^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+ckiuo91) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+ckiuo91)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2014-09-15 06:34:36
"Natural resource extraction pays baller in the US, and ND has an exceptionally low population, so per capita....",The_Youngblood,2014-09-15 14:30:37
I fucking refuse to believe that there are only 4 waffle houses in West Virginia.,The_Youngblood,2014-09-15 14:29:41
"Michigan, where the 'eh stands for.. meh.",spidyfan21,2014-09-15 15:45:49
"Even though the accuracy of any individual graphic could be called into question, I really enjoyed them as a whole.  Thanks for posting this.  Might do well to x-post to /r/mildlyinteresting or something similar.",bene34,2014-09-15 16:42:49
The IQ by state slide is interesting; it seems like northern states have higher IQs across the board.  Does anyone have more data on this ?  I wonder how some of the bigger states like California and Texas look county by county.,InfiniteState,2014-09-15 17:26:48
"My half baked theory is that warmer states attract idiots. As in, nobody grows up thinking they're going to elope to Montana. They do, however, dream of going to Miami or LA.",perspectiveiskey,2014-09-15 18:19:40
Go Montana! ,perspectiveiskey,2014-09-15 18:13:33
The TV show map looks like a joke,KingOfDaCastle,2014-09-15 18:55:08
"It's funny how many of these are basically ""find the cities"" or ""check out these states with alot of cities."" ",PhD_in_English,2014-09-15 11:14:09
Worst IQ is in Silicon Valley...?,anananananana,2014-09-15 06:27:14
There's a lot more to California than Silicon Valley,carmichael561,2014-09-15 07:13:59
"[Relevant](https://www.youtube.com/watch?v=N-BmxK-0Jts).
",perspectiveiskey,2014-09-15 18:20:24
http://blog.rstudio.org/2014/01/17/introducing-dplyr/,unfocusedthought,2014-04-07 08:43:35
"You're right to point this out, article skips it entirely. It's worth noting that performance is comparable between dplyr &amp; data.table. ",lenwood,2014-04-07 14:28:20
"I often use ddply to apply some function that might return a smaller data frame. I wasn't able to do this when I tried out dplyr since the summarise () function would only return single values. Just out of curiosity, anyone find a way around this? I haven't used it much since.",josephb,2014-04-08 19:17:47
Julia actually does have named arguments now: http://julia.readthedocs.org/en/latest/manual/functions/#keyword-arguments,G_Maximus,2014-04-07 10:39:19
"&gt;Never use the standard lib sapply, tapply etc functions. Back in the day I’d have said ‘use the hadleyverse’, but nowadays there are better, more performant alternatives.

What's the best alternative now?",riraito,2014-04-07 15:12:10
"Dplyr is a nice generalization for many operations one might do on a data.frame. It's part of the Hadleyverse, but it's a brand new part.",kiwipete,2014-04-07 15:44:50
Don't use S4? Can someone elaborate?,leftyteck,2014-04-08 07:59:42
"I actually find this insane. He even states that he put 6 students through an ordeal, probably threatening them with Fs or expulsion, when only 3 deserved it. I can't imagine being one of the students who someone else cheated off. Their grades and careers could be ruined by this overzealous professor. He should be focused on helping his good students and not be so unhealthily obsessed with the few bad apples.",mastahfool,2015-02-25 10:42:40
"It does seem a bit of an over reaction, but the mindset is ""crack down crazy hard now to prevent massive amounts of cheating in the future."" At least anecdotally, I've seen what happens when professors of larger classes don't do what they can to stamp out a students desire to cheat (once saw about 30% of a class get caught in a giant cheating ring).",Case_Control,2015-02-25 12:20:32
How does a cheating ring work? I've never known this to happen in any of my classes. I can be a little clueless though...,cacheego,2015-02-25 14:41:59
"Not sure to be honest, I just know there was a ton of students in one of the classes back in grad school  that all got busted together on an exam. Now that I'm on the other side of the fence I've had suspicions about large scale cheating on homework assignments, but even when you can prove it.... You can typically only prove it for one or two...

Why i have some sympathy for the professor involved.",Case_Control,2015-02-25 17:28:08
"A lot of professors re-use tests and assignments. So if you know an older student, you can get some of their old graded material to help you with your own. Then you know some other people doing the same, but they have different stuff than you, and suddenly it all becomes a commodity, as people trade flash drives back and forth of their collected libraries of old material. Eventually it just becomes a big ring as the stuff all gets shared everywhere to everyone ""on the inside"".

There is no reason to include people who don't have anything valuable to share, especially as poor grade performance of anyone outside the ring was just a potential benefit to the people inside. Brutal, a bit like an academic *Battle Royale*, but it can feel like the best way to play the university game.",luckywaldo7,2015-03-02 16:27:25
"Some professors would like to crack down on cheaters, but can't/don't want to because some students (foreign/out of state) are paying much more to attend, the administration just looks like other way. ",createanewaccountuse,2015-02-25 15:57:10
"&gt;only 3

at least 3",DigitalChocobo,2015-03-02 19:18:27
"Yeah this ""experiment"" was conducted without any ethical considerations at all.",nested_dreams,2015-02-25 14:55:40
It is also possible that groups of students performed similarly for other reasons - ie: they studied together.,msdrahcir,2015-02-25 10:21:54
But incredibly unlikely that they were randomly assigned to sit side-by-side. What I'd like to know is if the students were aware of their seating assignments before the test. ,Davleather,2015-02-25 10:42:20
"I may have missed it, but I didn't see anything about assigned seating. Just that they made maps of the seats after everyone had started. If there was no assigned-seating, wouldn't a study group be expected to sit together?

If there was assigned seating, and they didn't know about the arraignment before the test started, that makes it even less likely they cheated. What is the likelihood that four potential conspirators all happened to get assigned next to each other?

This is definitely not an open and shut case like the author portrays.",Ut_Prosim,2015-02-25 16:51:19
"Interesting analysis, but I think the guy is wrong. Let me explain:
 
This seems like textbook confirmation bias. He was so sure that he had been taken advantage of by the students that as soon as there was any statistical evidence to support that claim, he jumped to the conclusion.

&gt; The probability that this might have happened by chance is beyond ridiculous.

Agreed. However there are multiple explanations for highly similar results. It is disconcerting that the professor condemned them without even considering alternative hypotheses.

*Alternative:* What if these four are friends, perhaps even roommates. Say they had formed a study group for this class in particular. Say they worked their butts off studying for this exam, used the same notes, went through the same textbook examples together, used the same flash cards, quizzed each other regularly, worked the same sample problems, came up with common mnemonics to memorize test items, etc. Is it not reasonable to expect them to have almost identical strengths and weaknesses regarding the material? Similar knowledge base, similar preparation, wouldn't that generate similar results? After all, their tests were close but not identical. Furthermore, if they really were friends or roommates or a study group, would you expect their seating arrangement to be random? On the contrary, I'd expect them to sit near each other as friends do (blog said they made maps, but didn't mention assigned seating). Especially if they had studied for this exam together. This explains both similar the performances and the seating arraignment, and frankly I'd say it is just as likely as a conspiracy to cheat that was so effective that all the participants got above-average scores and still fooled the test proctors. 

There is a simple solution to this problem. Call all four students into your office, give them a similar reworded exam, and personally watch them take it. If all four get similar scores again, then clearly they were not cheating. If one or more bomb the test, then probably those individuals were probably copying off the high-scoring individuals. 
_____________

To be honest, the professor's attitude seems very toxic. His language made him seem extremely overzealous; he accused proctors of being incompetent without any reason to suspect as much, he mentioned that he gave the maximum allowable penalty but that he was not satisfied with that, and concluded with an us-versus-them statement implying the students are all conspiring to defraud the faculty (enemies that must be fought!). 

Perhaps this is not the case at McMaster, but at my alma mater the honor court is a drumhead trial. The only students who sign up to be jurors are the kids trying to brown-nose big name professors and/or pad their resumes. Whenever a well known professor accused a no-name student, they always sided with the professor regardless of the circumstances. It was near 100% conviction rate; once the accusation was laid, the kid was screwed. A bunch of statistical wizardry like this would easily fool them into ignoring alternatives and convicting. It is no surprise that the kids got punished, but I'm certainly not convinced that justice was served. 

This professor was so proud of his statistical accomplishment that he made a blog post about his heroism, but honestly I think he did wrong.
__________

**TL;DR:** The statistics work is very clever and a great start for someone concerned with cheating, but in his zeal to confirm his suspicions, he ignores all alternative explanations. Excellent example of confirmation bias, this should end up in a textbook under the heading ""*lying to yourself with statistics*"". ",Ut_Prosim,2015-02-25 16:50:33
"&gt;Is it not reasonable to expect them to have almost identical strengths and weaknesses regarding the material? Similar knowledge base, similar preparation, wouldn't that generate similar results? After all, their tests were close but not identical.

How would you explain them getting consistently the *same wrong answer*?

In college I had a study group with three other friends that was so close that we not only had identical class schedules for 3 years (same major), we even *lived* together in the same house. We spent all of our waking time together, including our study time.

But if it came to those handfuls of multiple-choice questions that we hadn't prepared for, we still often made different guesses to the correct answer.

Imagine there were 5 multiple choice questions with 5 possible answers each, the chances of us randomly chosing all the same answers independantly is (1/5)^5, or 0.032%. Of course, in practice the changes are higher, as some mutliple choice answers can be eliminated by being obviously wrong. But still, the chances of close collelation are low.

**Additionally**, it takes barely two brain cells to rub together to realize that having 100% identical test answers will make you a target. It's not hard to figure out that you should have some differences when cheating.

**Source:** Graduated college, may or may not have had experience with cheating.",luckywaldo7,2015-03-02 16:07:42
"As a counter-argument, virtually all students in my classes have had a study group or a study buddy. If we thought that studying groupings were leading to the similar outcomes, close groupings should be much more common throughout the entire set of pair-wise combinations. ",Iamthelolrus,2015-02-26 06:59:52
"It seems like this professor is trying to make a name for himself and get a reputation instead of actually maintaining some level of concern for the students. People blame CEOs, cops, politicians, but people who exploit others come in all shapes and sizes.",SmartSoda,2015-02-25 22:45:24
"If students were randomly assigned seats, it seems highly unlikely that all involved cheated. This would require both coordination and luck. More likely is that you have one of each pair cheating, in which case it is very important to figure out which it is. ",jlrc2,2015-02-25 13:37:31
"There's a similar method: The King Index for cheating.  It evaluates the probability of two persons having the same number of incorrect answers, using the binomial formula.  Given the cheater's number of wrong answers, what's the probability of randomly having the observed number of wrong choices between the suspected cheater and source?",Jofeshenry,2015-02-25 11:35:57
I'm just going to assume that there is a non zero probability that the knowledge required to answer two mc questions on the same test are dependent. Multiple students can have the same misunderstanding of a question or material.,msdrahcir,2015-02-25 14:09:43
"Again, though, what if students studied together?",Goat_Porker,2015-02-25 12:46:47
The consensus in cheating detection is that there must be some suspicion of cheating before analysis.   Most methods require designation of the copier and the source in order to run the analysis. ,Jofeshenry,2015-02-25 14:41:58
"What I'd like to know about those pair of students: were the exam answers identical or just the number of right / wrong answers?

Examining the answers to the questions could help identify if there was copying, collaboration or just pure luck.

What I do agree on is his methodology of identifying pairs students of interest and using the seating map to filter the pairs.",leeveson,2015-02-25 18:34:41
"&gt;just the number of right / wrong answers?

It's not the number of just right or wrong answers. It's the number of SHARED right or wrong answers",iamaquantumcomputer,2015-03-02 22:16:33
You are right.  This would explain why my comment was down voted.,leeveson,2015-03-03 15:32:25
"I'm an actuary, so I don't know if that really counts.

I learned some basic stats in school (I was a math major), but didn't really start getting into some heavy stats until I graduated.

I use what I learned somewhat. I would say I use stuff having to do with linear and exponential regression, curve-fitting, and simulation on a daily or at least weekly basis. And something we call ""credibility theory,"" which I don't know if they teach that in most normal stats classes?

I use generalized linear models, logit models, maybe a Markov model (and other more interesting stuff) maybe once every couple months. Hopefully more in the future!

The other part of my job is being able to put together coherent sentences in English and talking to humans on the phone. In my line of work, these skills are a bit less commonplace.

I like the work. It's a lot of problem solving. I'm a consultant rather than an insurance company actuary, so there's a wide variety of problems to work on. The main drawback to my job is that the most common actuarial methods (that EVERYONE in the industry uses) are based on bullshit and are totally backwards and stupid. Well, mostly stupid. Anyway, there's a big gap between theory and practice in actuarial science.

But I hope to be one of the people to change that.

Note: for those who don't know, actuaries basically are statisticians for insurance companies and governmental entities and similar stuff. Well, ""statisticians."" We aren't quite REAL statisticians. But when you guys are having statistician parties, some of us would be pressing our noses to the windows hoping for an invitation inside.

So, as an example of what I do, I might tell a city government how much money they will need to sock away so that they can pay out all the workers' compensation claims they're going to get next year based on information about their history of workers' compensation claims and the relative riskiness of the workers they employ. That way they have money to pay their injured workers.

I might also tell them how much money they'll have to pay out in the coming year for ongoing workers' compensation claims from past years. If I am doing work for an insurance company, I might tell them how much to charge for the insurance coverage they offer based on the relevant risks. And so on.",M_Bus,2014-11-02 21:05:59
Fellow actuary here working on my graduate degree in statistics. I couldn't agree more on your assessment of out of date actuarial standards that most blindly apply to any context. Let's move our profession to higher standards!,Roytee,2014-11-03 04:32:42
"I assume you work in P&amp;C? I work in Life, soon switching to Health, and the number of actuaries I encounter who lack fundamental statistical knowledge is staggering. I mean, I think in general, exam progress is overstressed and the importance of actually learning the material is deemphasized, but that's a story for another day. Could you expand on this whole gap between theory and practice/the reason why most actuarial methods are backwards, stupid, and based on bullshit? Not arguing, just curious as to your reasoning.  
  
Credibility theory is an actuary-specific topic (rooted in statistical theory); I never encountered anything similar in my stats undergrad or masters programs. ",wingsntexans,2014-11-03 05:07:35
"Sure!

I think that my complaints are about what you would expect, coming from the actuarial field yourself.

On the P&amp;C side, the exams and educational requirements include things like linear or log-linear regression, multivariable regression, time-series stuff, simulation, and a whole host of actual statistical concepts that go with those topics.

Then they teach the standard actuarial methods, which are the ""chain-ladder"" method and the ""Bornhuetter-Ferguson"" method. The chain-ladder is basically an impoverished version of linear regression, i.e., linear regression assuming that the intercept term is always 0. There is no theoretical basis for this decision. And, what's perhaps worse, is the actuarial reliance on ""professional judgment"" rather than actual analysis and comprehension. People think that they can see patterns and that they don't need any kind of math to tell them what's going on. As you might expect, answers tend to reflect the actuary's expectations and biases rather than the actual truth about what's happening.

The Bornhuetter-Ferguson method isn't much better than the chain-ladder method.

So the point is that the basic curriculum exposes actuaries to all of these complicated statistical ideas. In the past, the curriculum has *even included* papers that talk about why the traditional chain ladder method is stupid. And yet we persist.

The other part of this that bugs me is that in a statistical sense, it's unclear what the answer we're getting actually represents. As a matter of course, actuaries used to call the answer the ""expected value,"" but recently (last 10 years) that's been changed to the ""actuarial central estimate"" to emphasize the fact that it isn't a statistical measure. So once you get the answer, you don't really know what it means. Yes, we have to learn about variability around our estimates. We learn about percentiles of a distribution. We learn about standard deviations. And so on. But do we use those concepts? No.

Most of the time, when actuaries think of ""variability,"" they think ""I'm going to run a 'good', 'bad', and 'ugly' kind of scenario and that will be my high, medium, and low estimates of the answer."" There's no actual statistical thinking that goes on there.

What I want to do is take the statistics that are *required as a part of the basic syllabus* and actually apply them to the job. Actually, that's not accurate - I want to also use statistics that aren't required as part of the basic syllabus of education, but I'll take what I can get.

I think that the actuary shouldn't be blindly injecting judgment like that. I think that judgment should revolve around questions like: ""what model is most appropriate in this situation?"" or ""how should we adjust the data to eliminate outliers that are throwing off our answers?"" and things like that. It should emphatically not involve questions like ""what do I want the answer to be?"" I have heard this kind of thinking *more often than I think anyone would want to believe*.",M_Bus,2014-11-03 06:33:23
"Whoa. I did not actually realize you used THAT much stats in P&amp;C, clearly I chose the wrong industry. When I was doing experience studies at my first job, I got to play with some statistical methods, but that wasn't the key focus. I agree with this arbitrary ""actuarial judgment"" definitely being an issue in our profession, mostly due to those who made it through the exams but clearly don't have sound judgement/solid understanding of what they're doing and how it impacts their business. Also, communication skills shouldn't be as difficult to come by as they are.",wingsntexans,2014-11-03 07:28:37
"Well I mainly do reserving work in P&amp;C. I may be wrong, but I think that on the life side the reserving is actually pretty straightforward (because people are going to die eventually, and even then you have a mortality table). The problem has more to do with premiums and investments - duration matching, that kind of thing. Does that sound right?

On the P&amp;C side, reserves have a lot of variability (depending on the line) so there is pretty good opportunity to whip out some decent stats. For instance, I just worked on a project where I got to use a generalized linear model to predict losses based on geographic characteristics of some claim sites. It was cool! But that is only like the second time in my career I've had a chance to actually use GLMs.

I should say that one or two papers on GLMs are required on the CAS Exam 7 syllabus. Though I suppose they're touched on (lightly) in Exam 4.

Some lines, like automobile liability, get paid out very quickly, so there are basically no real reserves. I think I've heard that Health is kind of like that. Other lines, like Workers' Compensation or Medical Professional Liability, tend to drag on forever. Those are more challenging problems.

If I had the good fortune of doing more rate making work (for larger companies), they tend to use a lot more sophisticated models for that stuff. Although I haven't been able to design those models myself, I've been on the regulatory side reviewing those models from time to time. So I have to have a grasp of stats, even if I don't get to do the problem solving, myself.

A lot of P&amp;C work (both reserving and rate making) is still brain-dead chain-ladder and bornhuetter-ferguson methods with no statistical analysis or even basic comprehension. You can probably make a career out of it, in fact. I have just gravitated to more difficult problems and tried to find uses for statistical analysis where I can.

If you're looking for more stats, I would say to try and get over to rate making, whatever area you're in. I think you'll find more opportunities to flex your statistics muscles on that side. Also, anything having to do with ERM or predictive analytics will probably have *some* kind of opportunities for people interested in stats.",M_Bus,2014-11-03 08:17:31
"&gt; I mean, I think in general, exam progress is overstressed and the importance of actually learning the material is deemphasized

Based on some of the ""actuaries in training"" that I know, this is completely true.  A lot of them are the types of people who looked at US News and World Report and decided that since actuary was the highest-paying job listed, that's what they were going to do and programs don't weed them out.  I can't count the number of times I've heard one of them question why they're even learning something like in a high school math class and the number of times they complain about being challenged.  Most of the ones who actually enjoy doing math and statistics switched majors to one of those two fields.  I've heard a few prospective actuaries flat-out say ""I just want to get As and plug numbers in because that's easy.""  The personality type, combined with the overemphasis on exams instead of actually learning and applying material leads to a reduction in the quality of actuarial work.

I mean I know at this point I suck a lot at statistics, but at least I'm aware I suck and want to get better!",beaverteeth92,2014-11-03 13:41:04
"Honestly, the only reason I stay in this career is the management potential it unlocks. You don't have the same doors arbitrarily opened for you as you would in ActSci. Pure statistics is much more enjoyable than most aspects of Actuarial Science",wingsntexans,2014-11-03 14:38:10
"Hi!

so im a biostatistician for the pharma/CRO (contract research organization) industry. I have a masters in stats and i use what i learned all the time (obviously more some stuff than others, since my industry is specialized). The work is pretty great, although it's not obvious for students what it is that we do. The pharma industry is based on clinical trials (i.e. trials that run a drug through the different phases of drug development (there are 4, each with different goals to test/accomplish)).
These trials take a loong time to run (1-4+ years), but they need to follow a protocol that is written beforehand that includes a statistical section. figuring out which designs are best suited, calculating sample size, etc, are things directly related to what you learn in school. what you do that is perhaps NOT learned in school (depending what program and where you studied maybe) includes learning some medical jargon and a whole lot about how drugs are developed. While a trial is ongoing, statisticians are responsible for designing and overseeing the production of tables, listing and graphs (pretty SAS outputs basically, that are ultimately performed by designated SAS programmers (which is a great way to break into the industry by the way)). Discussing what should be in these outputs is time-consuming but i find it very interesting, especially in oncology. (Differences between intent-to-treat populations, response evaluable populations, safety populations, etc etc). It can be mundane, such as ""what should the title of this graph be? or should there be a footnote?"" but i think anyone who goes to grad school learns to deal with that stuff. There's a lot of teleconferencing with various medical/project management teams (seeing as how it's a collaborative job with medical writers, data management, clinical teams, etc), and some administrative burden in the corporate world (i.e. keeping track of project finances sometimes, or generally being aware of whether or not your department is profitable). I started out as a SAS programmer after my master's and switched into the statistician role after learning the ropes (~1year), but some people make very lucrative careers out of just programming, so it depends on your preferences. even among statisticians, there are people who prefer to think more about programming stuff, or theory stuff, or people who just like being more of a manager/less technical stuff. Hope some of this helps, feel free to ask if not",hypermonkey2,2014-11-02 20:20:00
"Just jumping in to add a bit of software discussion because quite a few people have questions about that here. 

/u/hypermonkey2 references SAS multiple times. In the pharma industry working in clinical trials, SAS is pretty much what you *have* to use for analysis of clinical trials for FDA approval. SAS is supported by the company, takes blame for issues in the software, provides customer support and is basically the industry standard... R being open source the way it is, doesn't seem to jive well with the FDA. 

I'm a biostatistics PhD student focused on clinical trials, and my department does a lot of trials work. Almost all the biostatisticians doing larger/later phase study (phase II-IV) use SAS, however in dose finding studies (phase I), which is the area I'm focused most heavily on, and in much of phase II, the preference is R. A lot of this stems from the type of designs used in dose-escalation, but it also stems from the fact that drugs are being FDA approved after phase I or II, but the results in phase III or IV (if done), are what will be looked at and thus need to be analyzed in SAS.",Distance_Runner,2014-11-03 05:00:25
I'm in a dose finding class right now and it is fascinating stuff! It's being taught by Ken Cheung; he does a lot of work on the CRM. ,Pseudo_Scientist,2014-11-04 07:26:52
How/where did you learn SAS well enough to get a job as a SAS programmer after master's? Was it just university experience?,Quintius,2014-11-04 05:26:36
"Hi! I did my grad school research in a hospital setting, where SAS was the norm for all projects. I knew nothing at first, but i read ""the little SAS book"" and got a fair amount of practice just by doing my work. just a solid base in SAS is enough to get into SAS programming for pharma. i learned most of it on the job, which isn't hard if you're forced to do it 8 hours a day, especially since you can follow previously written programs at your place of work, and you're usually assigned a mentor.",hypermonkey2,2014-11-04 06:41:17
[deleted],,2014-11-22 20:33:51
"there's a chance you may have to start as a SAS programmer for a year or so (as i did), but there's definitely a market for entry-level as well for biostatistician (especially if you are willing to re-locate. working remotely may not be possible at first, but with experience that's an option too for a lot of stats people). go for it. do you know which companies to target? ",hypermonkey2,2014-11-22 21:29:49
[deleted],,2014-11-23 03:55:54
"oh! my bad. depends on location. salaries vary across the country depending on living expenses, but starting is usually 55-65k, plus bonuses/benefits if applicable. the glassdoor ranges seem reasonable, except 74k in 2 years seems like a stretch, but i haven't worked at every CRO. good luck!",hypermonkey2,2014-11-23 08:45:21
I like how we had to wait until monday for this thread to take off.,mrpopenfresh,2014-11-03 10:46:56
What else am I gonna do while this MCMC runs?!,Sir_Cuitry,2014-11-03 13:58:37
Currently working as a statistician in a national cancer registry. So far I have mostly been programming epidemiological programs with R - but I have also helped out with a couple of researchers. I make use of the learning from uni maybe 10 % of the time because of the unusual focus on programming. I have been very happy to work there.,Humppis,2014-11-02 23:31:40
"Throwaway for obvious reasons.

I work in marketing consulting now. I get depressed cause I constantly abuse statistics and do stupid stuff  ( like finding ways to reject a null hypothesis no matter what e.g a very convincing yet stupid graph my qualifications are what makes these ridiculous abuse compelling) Apparently this is common place. People are obsessed with making uncertainty certain.

I should have done something that doesn't focus on uncertainty like programming.",statswannabe,2014-11-03 02:06:26
The problem is not statistics but that you work in marketing where ethics is not a priority.,reallyserious,2014-11-03 02:23:31
"Agreed but it was only place willing to hire me, actually only area hiring. I'm just trying to get experience and jump ship asap. Hopefully things improve in the next job but I have become very jaded of working with non staticians in industry.

",statswannabe,2014-11-03 02:31:00
"The same thing happened to me. I was working in a marketing heavy company that required ""statistics"" to back what it would say about its products. I jumped ship pretty quickly because it was depressing to abuse statistics like that. Since then, the only thing I really worry about in my interviews is if the company is for-profit and how much power the marketing team has.",Mockingbird42,2014-11-03 04:58:42
That's not necessarily true.,Sir_Cuitry,2014-11-03 08:57:21
"No, but with high probability. Only mathematicians deals in absolutes. ",reallyserious,2014-11-03 11:20:02
Do you have data to support that hypothesis?,Sir_Cuitry,2014-11-03 12:21:11
This is not the statistician you're looking for.,reallyserious,2014-11-03 12:39:29
And Sith.,towerofterror,2014-11-16 15:50:42
"Marketing statistics can be pretty exciting. I did a contract project where we clustered consumers based on purchase history, and then surveyed random samples of each of the clusters to get a profile of each. The result was a segmentation that highlighted the best return on investment, and potential sales strategies for engaging those market segments. I don't see anything unethical about this, especially if you believe in the product you are helping to sell.",Sir_Cuitry,2014-11-03 09:00:02
Any means to the ends that justify them?,glodime,2014-11-03 11:24:49
"I'm not sure I follow. 

You may have a product that will make everyone's lives better. If no one knows about it, it won't serve that goal. Marketing is what lets people know your wonder product exists.",Sir_Cuitry,2014-11-03 11:32:45
"Credit Risk analyst. I try and find ways to make us more money, and adhering to the new strict banking laws. Probably 95% of my time is spent doing the second part.",Iliketrainschoo_choo,2014-11-03 06:24:39
"I'm a biostatistician but not with clinical trials. I use what i learned in school everyday and get to expand on what i've learnt lots too. There's a lot less routine than i thought there would be. Work is relaxed and often complicated enough that it's interesting. I get to liase with scientists to discuss future experiments, look at new ways to improve analysis and I get to do more and more coding with R. SAS was what they all used here until I showed them R and now everyone loves R instead. Coding is more fun than the stats now.

Extremely relaxed atmosphere, almost too relaxed in a way but I get a lot of freedom with how I set about analyzing some work.

I would like to work with clinical trials at some point.

I have a BSc Maths and MSc Statistics.",kurokabau,2014-11-03 05:57:01
"I can almost repeat this verbatim for myself. I use SAS for data management and most analysis, S+ for graphics and certain complex programming, and Stata for anything it can do more simply than SAS (ROC analysis, certain applications of bootstrapping). ",metagloria,2014-11-03 06:22:29
"Out of curiosity, why S+ over R?  I'd think you're paying for a free product.",beaverteeth92,2014-11-03 11:43:05
"Because I'm not the one paying. : )

We've just had S+ here since long before I arrived, and I like the way it does graphics better. I could imagine sometime down the road they'll wise up and just tell everyone to use R instead, and migrating things is *mostly* simple, but there are little things we're just so accustomed to in S+ that we don't want to get rid of.",metagloria,2014-11-03 11:45:07
Ah okay.  Didn't know the graphics were different.,beaverteeth92,2014-11-03 11:49:55
"Subtle differences. I'm sure dozens of R experts could come in here and convince us that R's graphics are better, but it's all about doing what you know. I can make whatever I want in S+, whereas it would take me a long time to be able to do the same in R. ",metagloria,2014-11-03 11:56:24
Do you find that a regular MS in statistics often closes people out of Biostat jobs?,beaverteeth92,2014-11-03 11:45:53
"There's 4 in my department all did a straight MSc in statistics. I did my course alongside medical statistics people and to be honest, their course would add little advantage over mine for biostatistics. You may find it harder to go into more epidemiological jobs, but linear modelling, multivariate/PCA and ANOVA are the fundamentals of what I do. Most of the other stuff i learnt on the job.",kurokabau,2014-11-03 13:51:01
Ah okay.  I don't think I want to do biomedical stuff now but I'm keeping it open as an option in the future.  Thanks!,beaverteeth92,2014-11-03 13:53:14
"I learnt during my MSc that I wanted to, if i knew beforehand I probably would've done the medical statistics course.

For me, dealing with biological/medical data and the such is a lot more interesting than dealing with imaginary money.

Another thing to note though, you won't do any survival analysis on a straight stats course, which may be useful for clinical trials. I don't work with clinical trials so i don't know if it would be too much of an obstacle.",kurokabau,2014-11-03 13:56:18
"I can take a grad course in it next semester if I want.  Is it worth it even if you aren't doing Biostats?  I have a choice between that and Multivariate Analysis.  Most of my interests are in stochastic processes, machine learning, and Bayesian statistics.",beaverteeth92,2014-11-03 14:40:32
"Take a course in what? Survival analysis?

Multivariate Analysis is hard, but has more widespread uses. A lot in marketing for instance. It's also a lot harder to learn and understand, but that could be reason to do it at university since you'll have better resources than trying to learn about it on the job.",kurokabau,2014-11-03 15:38:26
"I work in a research centre at a university. I'm not a statistician (not yet, anyways), but many of my friends are. We work in social policy. They do a  lot of survey design and analysis, mostly around service use and/or delivery (e.g. outcome data). They also analyse large scale data sets such as the Census, workforce surveys, attitudinal surveys, etc. Looks like interesting work - hence why I am studying to join them!!!",bixgal,2014-11-03 02:36:53
"I'm an epidemiologist working on CDC's [NVDRS](http://www.cdc.gov/violenceprevention/nvdrs/) project. I started off with a role focused much more towards analysis, where I utilized a good amount what I learned in my stats and epi courses. Luckily our funding has been increased and we've been able to hire staff that does much of what I used to do, so my job is much more focused on data management and program development these days. We're also trying to incorporate GIS more into what we do, so I've been doing a lot more programming in Java and Python lately, rather than SAS and R. While the subject matter that we deal doesn't always make my staff and I warm and fuzzy feelings, it's rewarding. ",kidkerouac,2014-11-03 08:49:46
"The NVDRS project looks interesting! I am also an epi, but want to shift into more stats and data heavy work. What education and experience did you have before this job? ",Vulpes_lagopus,2015-01-13 18:29:47
Does anyone work with stats in a bureau? ,Dave_,2014-11-02 22:05:40
I'm currently interning at the Bureau of Labor Statistics. Started with more of an economics roll and have moved more into a statistical programming role...,harDCore182,2014-11-03 09:39:11
"Can you tell me about your education? What did you do with economics in the beginning and why did you move towards a stats role?

I am starting an econ quantitative analysis degree and I have been looking into future internships and job prospects. Im not really sure what im doing or where im going but I like the classes. My plan is to work for BLS or the BEA but I am looking for other fields that would interest me. I dont know if I belong in a private sector but I dont know much about it.",Dave_,2014-11-03 10:14:23
"Sure, current senior at George Mason with an econ major, BS.  Prior military, which I'm sure helped me with landing the internship (vets preference and all). I applied for their pathways program and was accepted.  Had a big summer project dealing with wages and unemployment and when that was over, I was able to extend my internship past the start of the fiscal year and I was given options of where I wanted to focus on next.  I picked more of an analytical position, and right now were are writing an in-house program in python dealing with all the info we have on wages/unemployment/employment for analysis.  Pretty neat stuff, but I'm also a fan of labor economics.

Out of this internship, I'm hoping to have more of a stats understanding.  I've taken a few courses and am alright with SAS.  The private side pays pretty well but the security of being a fed is appealing.",harDCore182,2014-11-03 12:42:08
"How do you like the BLS in terms of the work culture? I'm working on a stats degree right now and I've been curious for a while about it. 

",Suqraat,2014-11-03 18:38:00
"Hi there. Got my degree in applied mathematics (which focused more on asymptotics than statistics), then got taken by the graduate intake at our [national stats organisation](http://www.abs.gov.au). I use practically nothing I learned in university here (mainly because I didn't learn much stats, but also because the stats taught at my uni was more model-based rather than the design-based/model-assisted paradigm in use here), but it was still a good grounding of learning mathematical concepts and some basic programming ability.

It's a very interesting field to work in, and even though I'm in an area that's relatively behind-the-scenes I do still feel that I'm doing work that can have a significant impact on what will get published. And our organisation is in a position where it's relatively un-influenced by both political and commercial factors (because we're run by the government but through legislation that guarantees our independence).

We put a lot of effort into bringing our new hires up to a base level of proficiency, so I'd say that it's less important to know any details of statistical methods (although it certainly helps) and more important to have the ability to learn quickly.",conmanau,2014-11-03 18:41:17
"I am a statistician for agriculture. I do temporal and spatial imputation and model building mostly. I have an M.S. in statistics (with an emphasis in biostats), but I don't think that it prepared me *that* well for the work I do. I did take a few bayesian classes, and those have been very helpful in some of the models I work with, and data visualization is really important for the initial data exploration. All the work I do is in R, but some of my colleagues use python. 

I feel really good about the work I do, since it has a direct impact on reducing farmer fertilizer and pesticide use (which in turn lessens the impact of agriculture on the environment).",Sir_Cuitry,2014-11-03 09:04:04
Why don't you think it prepared you that well for the job?,Haustorium,2014-11-03 11:16:29
"Well, it's really just that my education didn't focus much on the specific techniques I often use here. Like, I had never heard of kriging before taking this job.",Sir_Cuitry,2014-11-03 11:31:36
Anyone doing stats work with just a BS in Statistics? I know that MS is usually the minimum to work on more interesting things,mangoat12,2014-11-04 05:34:07
"I'm sure someone more qualified will pop in soon enough, but my understanding is that a BS is the bottom of the food chain--you might definitely work on interesting projects, but don't expect to be leading anything. The general path seems to be doing a stint as an analyst of some sort for a couple years, then being shipped off to do a MS, quantitative MBA, or PhD, depending on your trajectory and goals. Others go straight to grad school, which may or may not be your style. I know it isn't mine.",ThePenguinPilot,2014-11-04 10:18:27
[deleted],,2014-11-02 21:26:01
"When you say ""database things"" what exactly are you referring to? Also where would you draw the line between a data scientist and data analyst given the nature of what you do? Reason I ask is that I'm focusing my major on quantitative research and have been delving into data mining recently as a result.",PDXBiker,2014-11-02 21:52:04
"I'm not the parent, but I can probably offer some perspective from someone who works in this industry and is 3~ years out of college.

One lesson I've learned over the past several years being a professional &lt;computer/numbers&gt; guy is that job titles are 98% meaningless. 

In this industry, in my experience, what you do depends on what you can do. You are not a slave to your job title. Data scientist vs. data analyst means that your job will involve data.. probably. Anything deeper than that is spurious.

My job title has undulated between about a billion things. 

Here's a list:
""Graphic Designer"", ""Application Developer"", ""Junior Application Developer"", ""Associate Application Developer"", ""Senior Application Developer"", ""Contractor Machinist"", ""Software Developer"", ""Senior Data Scientist"", ""Senior Developer"".

None of them have mattered one bit beyond what they did to initially attracting me to the job posting. 

After I get the job with the meaningless title, I bumble around for a few days or weeks -- doing whatever tiny tasks I'm given in the beginning, and feeling pretty useless. Eventually, someone will mention within earshot a problem that I have specific skills with which to solve. I volunteer to solve that problem. Work happens.

This process repeats, and in doing so I begin to put my thumb on the priorities and ""mission"" of whatever organization I am a member of. I then position myself for problems in the intersection between highish organizational priorities, and what I'm interested in doing.

Because of my skills, that means right now I'm writing mongo queries, javascript, doing some R, making a spreadsheet with some pretty graphs, a powerpoint slide here and there, creating backend tools for others use to manipulate and create data. In addition, I've created some predictive machine learning products that will hopefully make this company millions of dollars. *fingers crossed*.

The job title on my contract says ""Data Scientist"". My business cards say something else, and my resume says something all together different. None of it matters. I'm doing what i'm doing because I can do it, and the company finds it useful. 

My brain can't intuitively absorb the concept of creating neural networks. Until it can, if some weirdo phd rolled into my office who knew alot about neural networks, deep neural networks, reLU and sigmoid functions, etc etc -- he'd probably be given a job offer on the spot with my exact job title, even if he couldn't write a lick of code. 

Our specialties would be completely divergent at that point, our job titles would be the exact same, and no one would find that weird.

If you're trying to make yourself *marketable*, then just try and learn a lot of stuff that sounds like it could exist under the umbrella of some job title madlibs, and you're off to the races. Don't overspecialize if you don't want to. Learn anything and everything that's interesting to you. 

You'll be surprised how often opportunities arise to use your tertiary skillsets. I've had graphic design / 3d modelling as a hobby for years, and it's remarkable how in every job I've ever had (except kawasaki! didn't work there long enough ;D) I've made a 3d model or 200 for them, even though nothing in my job description says that'll be expected of me. It happens because I can do it, not everyone can, and it has value to organizations.

TL;DR: In the real world, nothing is as formal, objective, or standardized as you think it is, or think it should be.",MrLeap,2014-11-03 03:49:12
(econ undergad) It seems that most people here now work with some kind of code or completely deal with programming. Am I doomed to learn such if I want a shot at becoming some kind of data analyst? ,Dave_,2014-11-03 10:27:05
"You have to be able to retrieve your data before you analyze it. That's where database work comes in--being able to build your queries, do a bit of finessing, dropping, and joining data points, and dump it into, say, a csv for further analysis is a critical skill.

Once you have your data, you have to analyze it. Needless to say, you won't be doing that all by hand. You might want, say, averages, or to run a few dozen regressions based on different models and/or assumptions. You could technically do it by hand, but it's much, much faster to make a couple loops and have your computer chug through it for you (these examples are a bit trivial, but basically you want to automate repetitive tasks like data cleaning/processing as much as possible). Computers are for computation. Do the mental work, and then let the machine do what it does best.

I'm an econ undergrad and a research assistant in our business school. SQL and Python are my bread and butter.

I personally recommend looking into either R, Python, or both, and putting a database language or two on top of that. If you know one basic scripting language, and one basic query language, it makes it way easier to build up your expertise.

C++ might be nice, but we're getting into distributed computing where I am, so we're just mapreducing our problems rather than trying to optimize. Not sure if that's a good idea with respect to maintainability and cost-effectiveness, but there you go.",ThePenguinPilot,2014-11-03 11:26:04
"Learning to code will make you more competitive. I would say the closest thing to a ""required"" programming skill for a data analyst would be knowing how to use some kind of stat-focused language. R, SAS, matlab etc. Despite that, if you know enough things to tip the ""you are useful"" scale, you can become a data analyst without the programming ability.

If you're an econ undergrad, you're probably familiar with excel. For most applications, you can consider R/sas/etc to be excel with a significantly more powerful formula language. Rarely are they used to make full fledged applications. They're tools that make it easier to ask your data some questions. ""What's the sum of this parameter? what's the R^2 if I correlate these things?"" etc.

If you can interrogate your data using exclusively excel or whatever, then there's a place out there that will hire you. Once you really get into it, you'll probably find the stat tools to be helpful things, rather than drudgery that exists apart from the work you're doing.",MrLeap,2014-11-03 11:46:34
"Thanks, this is reassuring",Dave_,2014-11-03 14:33:40
"Unrelated: I do most of my stats in SPSS, but everyone seems to be moving towards R and SAS. What are the advantages and disadvantages of each when compared to SPSS and each other?",InOranAsElsewhere,2014-11-04 09:01:01
"I'm not an expert on their differences. I mentally group SPSS and SAS together as nearly-the-same-thing, which is probably not perfectly accurate. 

R is newish and free, which explains its popularity. Really, once you're comfortable with one it's really easy to transition to others. I think you're fine doing what you're doing.",MrLeap,2014-11-04 12:22:15
"Thank you. In my field, there's a move toward multilevel modeling and structural equation modeling, which I'm told R can handle, while SPSS can't.",InOranAsElsewhere,2014-11-04 17:08:54
"Wow thanks for the great response. Data science seems to be one of those buzz words that gets a bunch of hits to your linkedin profile, but from what you're saying is that its really a unique skill set that makes the difference.",PDXBiker,2014-11-03 20:30:54
"I do client support, R&amp;D and consulting in the conjoint analysis field. Conjoint is basically an experimentally designed survey methodology for measuring what drives people's decision making processes.

I use my degree every day, I almost always have work-relevant theory on my whiteboard, I talk to (mostly) smart people about their problems, and I get to explore new research methods.

It took 4 years and 4 jobs to find a good place for me. I hated working at big companies, and most jobs I saw that are open to newly minted statisticians didn't really require a high level of statistical rigor, lots of report making and shuffling data around. ",MipSuperK,2014-11-03 12:17:22
[deleted],,2014-11-03 12:18:33
"Data Scientist. Programming in R, Excel, Qlikview.  Do a lot of cost benefits analysis and staffing model though. ",trailblazery,2014-11-03 19:32:13
"Not a statistician per se, but close enough. I work as a data scientist in a large company in IT. 

I graduated with an MSc in Economics, and a passion for Econometrics. After a short period in a consultancy, I ended up moving in the IT industry because I wanted more data (foolish me). I now work mostly with Hive and Python, run A/B tests, analyse user behaviour, and I'm planning to get more in to Social Network Analysis. I have a lot of flexibility at work, so I get to learn new stuff that I find interesting.",MissLola_,2014-11-04 11:18:38
How did you convince people in the IT industry to hire you?  I'm interested in working with big data but feel like getting a specialized big data degree would be too limiting.,beaverteeth92,2014-11-04 14:06:19
"I was working in a consultancy, doing modelling, and some reporting. I started learning SQL on the side, and applied for an analyst position. Before the interview I read EVERYTHING in Google about the way you could analyse the type of data I would have to work with, and the industry lingo. I went in to the interview with a clear idea how to break apart their product, what, and why I would measure. They gave me the job! A year, and a bit later I had learnt a lot about the industry, and moved to a bigger company as a data scientist. 

The most important thing, besides good analytics/stats skills, is to really understand the product. Bonus points if you can also code a bit.",MissLola_,2014-11-04 14:34:52
"Work as a dba and MI developer for an  insurance company... My background/training is in IT/programming.. Been teaching myself stats and have recently started working towards a degree through the ou! 
",sam_cat,2014-11-03 09:38:04
"The senior quoted statistician thinks it's a bad idea:

&gt; Bradley Efron, a Stanford University statistician, said the complications make Bayes a bad fit for the Malaysia Airlines hunt. “Bayes’ Rule is good for refining reasonable (or at least not unreasonable) prior experience on the basis of new evidence,” Efron, who also expressed skepticism to Al Jazeera America, wrote in an email. “It is not good when new evidence changes the situation drastically.”

",deadsalle,2014-03-17 20:40:20
"""We've found the plane. It was way over here!""
""Yes that's all well and good but that location still isn't the posterior mode of the spatial field so we're going to have to keep looking"".

If you came across evidence that changed the situation drastically you'd adjust your model to deal with that, as your modelling assumptions are probably incorrect. This is true of both Bayesian and non-Bayesian modelling.",samclifford,2014-03-18 14:09:09
"&gt; By Carl Bialik

... So not actually Nate Silver. That article isn't exactly evidence of Nate being back.

Not saying it's not interesting, it's just not what the title claims.",efrique,2014-03-17 14:44:34
He's back with a lot of new writers. Go to [home page.] (http://fivethirtyeight.com/) Probabilities for NCAA tourney there as well.,deanzamo,2014-03-17 14:59:17
"Yes, I have already been reading some of the other articles, like this one:

http://fivethirtyeight.com/features/what-the-fox-knows/ (that actually is by him)

The point is ""Nate Silver's Five ThirtyEight is back."" would have been a better start to the title if you were just pointing to an article that isn't by him.",efrique,2014-03-17 15:08:26
"This was actually the use case which which Bayesian statistics was taught to me and, consequentially, the one that I use to explain it to others.  It helped me get it.

That or the much simpler ""the average power bill is $100 per month.  What will the Jones' next power bill be?  Okay, now you get the Jones' bill and it's $150 per month.  What would you guess their *next* power bill to be?"" etc. etc.  People get that, too.  Which is good, because if they press me beyond that, they'll see I'm not really that well versed in it, either.",DesolationRobot,2014-03-17 14:50:15
"In the book Blind Man's Bluff, about the US Cold War submarine program, it goes into depth (no pun intended) about using stats to find where the sub and nukes sink in one disaster in the 60s.",ImOnTheLoo,2014-03-17 15:42:25
"&gt;Arnold I. Barnett, a statistician at the MIT Sloan School of Management, worries that people who use the tools without fully understanding them may be led astray, “that the very act of quantifying a probability obscures the point that the numerical estimate is itself subject to uncertainty,” he said. “Thus the estimate might be taken more literally than is warranted.”

Shoot me if I'm wrong, but Bayesian agents are never uncertain about probabilities of observable events.",giziti,2014-03-17 16:40:48
"A Bayesian agent can think that the probability of a coin turning up heads is p, where p is equally likely to be 1/3 or 2/3.  So on one hand they may think that the probability of a coin turning up heads is definitely 1/2, or they may think that it's either 1/3 or 2/3 (ie, uncertain).  ",redditleopard,2014-03-17 21:05:47
"Yes, but the point here is that the Bayesian agent has taken your observable event (say, heads on the next flip) and has integrated out to a full marginal distribution for this event and has no uncertainty about this marginal distribution. ",giziti,2014-03-18 06:55:17
"There's a link in there to an article about [using proability to search for submarines in the second world war](http://www.defensenews.com/article/20130705/C4ISR02/307050013/Search-theory-big-data-Applying-math-sank-U-boats-today-s-intel-problems). It's not meant for a mathematical audience, apparently. Does anyone know where you can read about the actual calculations?",Bromskloss,2014-03-18 06:38:47
Have they found it yet?,pst2154,2014-04-01 07:46:14
"Welcome back. Been away too long.
",mstruck,2014-03-17 20:31:17
"What is the probability that statisticians didn't ""help"" in all the examples given and were simply present. You know, that whole correlation causation thing.",NetPotionNr9,2014-03-18 04:54:58
"I think this video is pretty unclear - while he does state that it's the sample *average* that's asymptotically normally distributed, if you don't already know this stuff, I think it would be very easy to come away with the idea that the sample *observations* will be asymptotically normal, which is wrong.",standard_error,2013-10-10 02:34:20
"Yes -- in particular, the fact that the bimodal distribution of dragon wingspans results in a normal distribution after sampling groups was just stated without any explanation, which misses the whole point.",jmmcd,2013-10-10 02:36:01
"He did show the bunnies being weighed in groups, which would imply that you'd take the group weight divided by the number of bunnies. Of course maybe I know too much going in and am assuming that's obvious to the casual observer.",radiantthought,2013-10-10 06:56:32
"Yes, he did, but I don't think it was done in a very pedagogical way - he then shows lots of bunnies following a normal curve, which is easy to interpret as the distribution itself going normal. In other words, I do think that you already know too much.",standard_error,2013-10-10 07:19:50
"If you don't already understand concepts like distribution and variance, you won't get anything out of this video. If you do understand those concepts, this video won't add much.",MattAsher,2013-10-09 11:45:50
Fun burglar. ,seemsez,2013-10-09 12:15:49
But...but...bunnies!,chrobbin,2013-10-09 16:51:03
"What's really scary about your name is that it's the combination of mine and my soon-to-be brother-in-law.  He has his M.S. in Statistics, and I'm working on mine.

[Couldn't help but think of this](http://www.youtube.com/watch?v=r4AsTD39wqA)

You make a good point too.",toss_it_leave_it,2013-10-10 07:38:23
"""Stated"" more than ""explained"", but very cute nonetheless.",Nocturnographer,2013-10-09 20:55:29
"Let me add to the other complaints about this video: the first half of the video (the rabbit example) is not really the CLT, but rather a simple property of the normal distribution.  If the actual distribution of rabbit weights is normal, then so is the distribution of the sample mean.  That's because the family of normal distributions is closed under convolution and scaling.

Only the dragon example is really about the CLT: even though the actual distribution of dragon wingspans is NOT normal, the distribution of the sample mean converges on normal as sample size increases, provided that the actual distribution has two finite moments.

But if, as in the example, the actual distribution is bimodal, estimating its mean is probably not such a great idea in the first place.",AllenDowney,2013-10-10 08:51:23
"I generally find these cartoon-y educational videos awful to watch, but that was really well done. Reminded me a little bit of Khan Academy for some reason...",Eist,2013-10-09 11:00:53
Where are the bunnies? ,manic_panic,2013-10-09 16:54:53
Animated and in the video... ?,ForScale,2013-10-09 18:55:50
I'd be down.  I think papers would be better than books because they're shorter.,beaverteeth92,2015-03-10 13:29:47
"I am down for this because while papers are shorter so we could do one weekly or so, and quite honestly, coming from applied stats, books I am good with, but some of the technical papers leave my head hurting when the proofs and equations hit.",engelthefallen,2015-03-10 22:25:40
"White papers are more incentivized to sound confusing (quote of quote smart), than books are incentivized to be understandable. I vote books all the way. Also, papers tend to be so specific.",hilldex,2015-03-11 01:23:45
I'm currently reading Naked Statistics. ,lattakia,2015-03-10 12:33:37
"I just finished reading that book, and to be honest I got nothing out of it. I'm also half way through the ""Open Intro to Statistics"" textbook, and I thought that Naked Statistics would be a good supplement in terms of conceptual clarification. But that book did not offer any further clarification that I did not already get from the textbook. I think this speaks to the weakness of Naked Statistics, but more to the immense strengths of the Open Intro textbook. ",acqua_panna,2015-03-11 05:00:10
Naked Statistics is a pop-science book.  It's not meant to be a rigorous introduction to statistics.  ,motley2,2015-03-11 06:00:54
I think different people learn in different ways. I like to survey the landscape &amp; see real-life applications right from the start. I used the openintro book for the coursera MOOC to have a more rigourous education.,lattakia,2015-03-11 19:56:26
"As for me, the openintro book taught me a lot more about real life applications than did Naked Statistics.",acqua_panna,2015-03-11 21:25:32
"It was good for an introduction book, but I really hope that most books would be of a more advanced nature.  I hope that after one stats class or book people know of concepts like the central limit theorem, sampling error, garbage in / garbage out and the difference between inference and description.

So yeah great book for people new to stats, maybe not so much for people who taken a stat class or work with statistics.",engelthefallen,2015-03-10 22:23:58
"Great list from a great blog, http://andrewgelman.com/2014/03/31/cited-statistics-papers-ever/",chasethewater,2015-03-10 11:00:09
 That blog should be on most people's reading lists itself.,engelthefallen,2015-03-10 22:18:53
"I (fourth?) the suggestion to read papers instead of books, but if I may be so bold as to suggest a book for relative newcomers to statistics, I highly recommend Alex Reinhart's [Statistics Done Wrong](http://www.nostarch.com/statsdonewrong). It's a quirky guide to the biggest pitfalls in statistics. Disclaimer: Alex is in my PhD cohort. ",kimolas,2015-03-11 00:47:36
Reinhart posts here right?  I think he suggested I read that himself.,engelthefallen,2015-03-13 05:50:31
"Yup, he does. ",kimolas,2015-03-13 07:32:34
Actual books or papers? Anyone have any suggestions? ,purin_purin,2015-03-10 10:51:40
"I would like to see papers more than books. Papers don't take commitment, and lets be honest, commitment and reddit don't mix.

Although I think someone made a post one time for a recommended book list for those interested.",Jimmy_Goose,2015-03-10 12:55:21
A paper club would also be very nice.,devil27,2015-03-10 15:39:55
I agree that a paper club would be better. Perhaps throw in some classic papers alongside contemporary stuff. ,mattdelhey,2015-03-10 13:56:06
I think that's what he means- the /r/math version only discusses papers.,redditthrowaway1995,2015-03-10 15:15:34
"Yes. I meant going very much on the /r/math model, of discussing papers.",devil27,2015-03-10 17:21:26
"I see, in that case I'm in. ",mattdelhey,2015-03-11 20:21:53
"Sounds great, whether book or paper club. Nice idea",xudevoli,2015-03-10 14:46:43
"I really like the idea. More so for textbooks, for me personally.



",cooked23,2015-03-10 16:43:36
I'd be down. I need some social pressure to read some papers. ,chelbylu,2015-03-10 18:31:38
I'd be very interested.,baayes,2015-03-10 18:34:13
"Sounds like lots of people are interested. But its going to require someone actually starting it. Would this be something the mods run, or would it be more average user driven?",kinako-,2015-03-10 19:00:44
"I would definitely like to see mods run it. That way, it can be stickied.

Maybe we might need new volunteer mods though, as I do not know if the mods would be willing to run it.",Jimmy_Goose,2015-03-11 07:35:04
"Before we can actually start talking about specific books/papers we'd all like to read, maybe it would be a good idea to start listing some general topics people would be interested in reading about? Either application areas or specific statistical technologies?",botBrain,2015-03-11 14:07:32
I would get on board with a paper or book club.,KustavGlimt,2015-03-10 13:00:27
Is anyone familiar with that Chicago undergraduate mathematics textbook list? Is there something similar for statistics?,corallok,2015-03-10 13:31:08
Do you have the list?,ordnance1987,2015-03-10 22:50:32
"https://www.ocf.berkeley.edu/~abhishek/chicmath.htm

I'd love to see a comprehensive list of well regarded books for statistics as well. I've looked but nothing quite like this.",corallok,2015-03-11 07:26:56
"Read this recently and enjoyed it: http://books.google.com/books/about/The_Empire_of_Chance.html?id=Bw2yKfpvts8C
It's readable for a non-stat MS/PhD but definitely not too pop-sci. Provides an interesting history of the development of probability theory and the ways it has shaped western society.",nraley,2015-03-10 21:37:15
Great book.  I noticed something too.  My favorite advisers all had it in their bookcases.,engelthefallen,2015-03-10 22:18:15
My adviser was the one who recommended it to me; he loaned me his personal copy! ,nraley,2015-03-12 19:52:46
I would love this if as long as the book is cheap enough.  For papers I have no academic access as I am on a break from school but I would gladly discuss what I can understand if the source article was posted.,engelthefallen,2015-03-10 22:17:47
"Great idea, we discuss papers in our lab too. A lot to learn:) ",remcotob,2015-03-10 23:47:14
[The Theory that Would Not Die](http://www.amazon.com/The-Theory-That-Would-Not/dp/0300188226),osazuwa,2015-03-11 05:45:11
I am just posting so I can be updated. I may be interested in following along.,SkornRising,2015-03-11 08:11:07
fun and informative book [The Tiger that isn't](https://www.goodreads.com/book/show/1823138.The_Tiger_That_Isn_t),darenasc,2015-03-11 08:35:18
"Just started reading ""How Not to be Wrong"" by Jordan Ellington, great so far!",cogogal,2015-03-11 08:39:51
So did nothing come of this thread?,cooked23,2015-03-12 18:01:15
"I do not think anything did. A lot of people are interested, but we have not seem to be able to come up with a system of how to do this.

I guess the first step would be to come up with a list of papers or topics people would like to read about.",devil27,2015-03-12 22:14:33
"As someone who uses R and SAS daily, I have to disagree with the infographic's contention that SAS is easier to learn than R. Both can be kind of a crapshoot, but SAS is horrible to learn. There is little uniformity to how procs are implemented. You can have three different procs that implement linear regression, and with them three different names for identical output tables, three different ways of defining reference levels for categorical variables and three different ways of X Y and Z. God forbid you ever have to change the default settings on a plot.

R is much easier for anyone who has any coding experience, though it can be a bastard too.

Totally disagree with the idea that one of SPSS advantages is that it is more like Excel. This seems like more of a liability than an advantage, especially if you want to replicate your work at a later date or with a different data set.",hotandtiredanddry,2014-06-16 13:49:08
"Seriously, I can get the same results for some ANOVA models using PROC ANOVA, PROC GLM, PROC GENMOD, PROC MIXED, and PROC GLIMMIX... The stupid thing about it is, I will use a number of those on a single data set because of what they output beyond the basic ANOVA table and how it's output.... So I can definitely see how SAS can be confusing, especially to new users...

On the other hand, each of those procedures give me a hell of a lot of information for 4-5 lines of short code, more than I'll get from R... sometimes too much info though, especially if you don't know what it all means (seriously, outside of actual statisticians, it's not like anyone knows the difference between -2LogL, AIC, BIC, CAIC, HQIC, Chi-Square etc., and which to look at depending on the goals of their model). 

In all honesty, they're all good in their own way. Sure, you can have a preference, but I think it's a bit closed-minded for someone to be ""only an R programmer"" or ""only a SAS programmer."".

Here's how I use them:

SAS: basic data exploration, analysis, and modelling. It's the first program I open up with a new data set.

R: Simulations, simulations, and more simulations. Plots and graphs, some modeling, and maybe some data exploration depending on the data set and where I want to go with it.

STATA: I literally only use this to transpose data sets between wide and long and for data sets that require a lot of ""fixing""... for renaming variables

Excel: I'll use to make pretty summary tables... that's it

SPSS/Minitab: Only open these when someone sends me a file in their native format. I'll open the program, output to a CSV so I can easily import it into SAS/R/STATA, and then I close the program. ",Distance_Runner,2014-06-16 18:58:06
"I'm pretty similar. (But you use Stata for wide to long??! I've never got the hang of that). I'll also use stata if survey weights are involved, I find that to be the easiest of the three.",jeremymiles,2014-06-18 16:48:51
"SPSS is more like Excel in terms of having WinTel-style menus that are easy to navigate, and a data view that is easy to manipulate and edit.

It is easy to replicate SPSS work, just ""paste"" your menu-generated commands to a syntax windows and save the batch job.",wil_dogg,2014-06-17 05:03:22
"Aha, I stand corrected.",hotandtiredanddry,2014-06-17 08:39:42
"This infographic is interesting, although it has very little to do with ""languages"". 

SAS and SPSS *languages* (not the entire package, which generally shields the user from having to deal with the language) are utterly wretched messes. R is somewhat better, although it leaves a lot to be desired as a *programming language*.",creeping_feature,2014-06-16 14:50:25
"I would never ever use R as a programming language. When I need some programming functionality, I run R from a shell or python script, get my data and then do other stuff. ",,2014-06-16 17:46:26
"I know you can't cover everything, and there are a million statistical packages out there, but no Stata?",jambarama,2014-06-16 13:41:18
"Does it have much usage outside of the social sciences? I learned the language for economics, and much prefer R, but I've never seen it used outside of the social sciences. As much as I dislike the language it does have excellent documentation.",ZSVG,2014-06-16 15:43:07
"I guess that's the only place I've used it - policy stuff.  I use SAS for data handling and processing, but for me stata is the most straightforward for doing the statistics I'm interested in and interpreting the results.",jambarama,2014-06-16 17:08:37
"Yes, its widely used in the Medical Statistics community.

Scan through the [Stata Journal](http://www.stata-journal.com/archives/) (and its predecessor the Stata Technical Bulletin) for some examples.",enilkcals,2014-06-17 04:37:40
"I'm in finance and I use Stata and SAS. Basically, if Stata can't do it, I go to SAS. So I don't use SAS that often anymore.",Jonathon662,2014-06-17 12:03:09
The lack of the one true faith (SYSTAT) worries me. Time to declare war on the unbelievers. ,offtoChile,2014-06-16 17:19:27
"Economist here -- Stata is my tool of choice for simple, out-of-the-box data analysis.  Maybe the included packages are tailored for economists.  

For anything deeper, though, the macro language is terrible.  One of my projects involved modifying thousands of lines of Stata macro code written by someone else -- it was hell!",veryshuai,2014-06-17 05:02:46
"This is pretty interesting. R is amazing given a bit of coding background, otherwise if might be too slow to start for lots of corporate employees who might prefer excel like user format. 

spelling error in (ease of learning, sas)

""shouldn't take you to long"", misuse of ""to"", replace ""to"" with ""too"" ",lego_jesus,2014-06-16 13:50:30
"I learned SPSS in university and used in the first couple of jobs that I had as a data analyst. Last year I decided to learn R using Stackoverflow and some MOOC courses and haven't looked back. Much better graphics, and knitr has been a godsend for creating reports. 

It's also nice that I can do just about everything in one program - no more cleaning in Excel/Access, analysing in SPSS and then going back to Excel to clean up the charts/tables. ",vidi_images,2014-06-16 18:07:57
"Knitr is amazing. I have no idea why every language doesn't have such a powerful tool.Seriously at my last job we were supposed to run SAS code, paste the output into excel to make graphs, then paste the graphs into PowerPoint and add flavor text. That entire process is done in one step in knitr, and it's way easier to make changes and understand what code is doing.",limes_limes_limes,2014-06-16 19:52:12
"I wonder how Python compares as a 'statistical language'.  It seems to me that that a lot of great features from R have been translated into Python (ggplot, pandas).",riraito,2014-06-16 15:53:46
"There are a variety of great packages for statistical learning and scientific computing with Python. I personally really like using numpy, scipy, scikit-learn, matplotlib, etc. Python is also really great for preprocessing data.

That being said, there are still some serious gaps. For example, there is no Python equivalent to fGarch even though GARCH models are a fundamental tool for time series analysis.",carmichael561,2014-06-16 16:58:59
"And for the things that are not yet in Python, you can use rpy2.",object_FUN_not_found,2014-06-17 03:39:56
"Interesting infographic.  The academic 'marketability' is a little misleading, as it will vary by discipline quite a bit.  SPSS is predominately used in the social sciences.

Most (but certainly not all) academic statisticians use R or some lower level language.  Many biostatisticians will know quite a bit about SAS as well.  In my opinion, few will choose SPSS as their first option.

In terms of which is easiest to learn.  I believe all these have some sort of GUI that will help you run analyses, but SPSS's GUI is used by its users by most often I believe.  It may be the easiest to use, but it's usually the easiest to use incorrectly for those who don't know what they're doing.",drwggm,2014-06-16 20:39:53
"R doesn't really have a GUI, which to me (and most R users) is a plus. There is however RStudio, which is a pretty great IDE. ",towerofterror,2014-06-16 22:18:27
"I guess it depends on what you mean by GUI.  Rcmdr (available on CRAN) has existed forever, and would help a lot of people get started with R who may be intimidated by the command line.  

I guess it's technically a package, but it has much of the same functionality as the GUIs in other software.   I've never used it much, except as a TA for a course that used it.",drwggm,2014-06-17 00:09:50
"R's shitty GUI has given us access to a whole suite of great tools like Sublime, Notepad++ and NppToR, Tinn-R and RStudio.

But yes its GUI is older than the students I teach it to.",samclifford,2014-06-16 23:53:05
"by ""doesn't really have a GUI"" I mean that it doesn't have a point-and-click GUI like SPSS. The default GUI isn't much different than a terminal window.",towerofterror,2014-06-16 23:54:54
"Ah.

Well in that case, R's shitty GUI has given us shitty GUIs like JGR and R Commander.

I definitely find the focus on scripting rather than menus very handy.",samclifford,2014-06-17 16:36:13
"R's killer feature is really that it's free. I only have a very little experience with SAS, but it seems that R can do everything the others can for free. Even if R sucks a bit more than the others, the cost will carry it. 

Although, as it's been pointed out, R (as a programming language) leaves *a lot* to be desired, it's still very easy to understand if you've got a little programming experience whereas SAS (and I think SPSS) seems more like an application for those who are not comfortable writing a bit of code. 

",object_FUN_not_found,2014-06-17 03:43:49
"SAS is free for academics and students, and basically free for non-profits.",jtth,2014-06-17 08:43:13
I know students who pirate SAS because it's a pain in the ass to get the free license.,towerofterror,2014-06-17 14:23:00
It takes five minutes. It's comically easy. ,jtth,2014-06-20 12:38:31
I agree with you. I think R will be the winner in the long run. Although many industries are so dependent on SAS the war may linger on.,tree_man,2014-06-17 03:52:57
"The last time this was posted (~2 weeks ago?) somebody pointed out that the true ratio of R:SAS stackoverflow posts is around 22, not 7.

To me that's a very strong leading indicator. I can only imagine that SAS's market share is falling fast.",towerofterror,2014-06-16 22:19:41
"I'm a little bit late but there is an obvious reason for that. SAS licenses include support, and I imagine a lot of users go to SAS for a lot of questions and don't need to rely on community feedback. I agree with you though that R has a better long term model, but using stackoverflow as an indicator might be misleading.",mhermher,2014-06-17 08:14:23
"SAS also has infinitely better documentation. R's documentation, especially from 3rd party packages, is basically non-existant. ",jtth,2014-06-17 08:43:52
"What does SAS's documentation look like?

While some of the 3rd-party (and sometimes even base) functions have shitty help files, I've always found good help on the first page of the Google search results. ggplot2, for example, has shitty in-R help but fantastic documentation online.",towerofterror,2014-06-17 13:12:29
"SAS documentation is all online to see, and is amazing. They tell you how every flag of every option of every proc works with notation and process info. You don't actually need anything else. ",jtth,2014-06-18 05:45:47
"I really don't see the benefit in using terms like ""war"" when comparing and contrasting software.  It creates the illusion of a fight, when its really a matter of user preference (which is unfortunately often biased by their history of learning).",enilkcals,2014-06-17 05:16:07
"One key thing missed about spss: it's a rabbit hole to learn programming. 

Its GUI lets you output it's code so you can learn how it works. Soon you're copy/pasting code. Then you're writing snippets. Then you're writing entire functions and integrating multiple code blocks and actually re-creating GUIs with your own code back ends to make them more efficient and oh wait now let's integrate other languages to fill in the gaps but now those languages are fair game too and oh fuck now you're a programmer. 

Source: I became a programmer thanks to spss. ",BillyBuckets,2014-06-16 21:46:53
Or [Psych](http://en.wikipedia.org/wiki/Availability_heuristic),,2013-05-17 10:15:46
Or economics,,2013-05-17 12:11:37
VSL (Value of Statistical life),biga415,2013-05-24 14:12:53
"Exactly, people are risk adverse!",maxtheman,2013-05-17 12:31:53
Or [both](http://en.wikipedia.org/wiki/Psychological_statistics),antisyzygy,2013-05-17 11:01:18
or [neither](http://sedatedtabloidreader.files.wordpress.com/2013/02/statistics.jpg?w=300&amp;h=225),alwaysonesmaller,2013-05-17 12:04:52
That prior probability is damn important!,ComicFoil,2013-05-17 13:44:59
[To gain super powers!](http://www.smbc-comics.com/?id=2944),Andrew-Leith,2013-05-17 16:04:07
Shark repellent really should come standard.,,2013-05-17 14:20:35
"I'm one of those combination CS and stats people, though I started out in stats.  I made the transition toward learning CS after realizing the problems I am interested in require knowledge and skills beyond just statistics (basically big data stuff, computational concerns, the usual).

I love this article, he really hits the nail on the head of a lot of things I complain about all the time.  

My number one pet peeve is reinventing the wheel.  Coming from a stats background I see it over and over again.  I'm taking courses on machine learning that I'm practically sleeping through because it's all the same stuff I've learned already.  I still have no idea why machine learning is a necessary term to have, it's all been done before.  ""Machine Learning"" tends to have a greater emphasis on computation and application, but there's not really anything new going on.  Maybe I just haven't gone deep enough, but I haven't seen anything fundamentally different from the same regression and classification stuff we've been doing since forever.

The hype around Neural Networks is another great example.  It's an old idea that has seen a new resurgence, but it is not at all fundamentally different from things we've been doing since forever.  It's a relatively small iteration on methods.

A lot of these ideas remind of [CGP Grey's recent video on automation](https://www.youtube.com/watch?v=7Pq-S557XQU).  It's a wonderful video, but at one point he starts talking about computers replacing ""thinking"" jobs.  He even puts up a graphic of a neural network (though he doesn't mention it by name).  People see terms like ""Machine Learning"" and ""Neural Network"" and think that we're relatively close to emulating a human brain in a computer.  In reality, we're insanely far away from any such thing.  Doing something like that will require a *fundamental* revolution in methods, something we haven't seen for a very long time (the recent big data and machine learning craze certainly haven't provided it).

Anyway, this article hit on a lot of points I agree with.  Thanks for posting it.",noelsusman,2014-08-26 15:21:11
"On the AP test:  Maybe the AP test ought to be rewritten by X-L!  Of course, he is too busy for that, but I wonder if anyone can salvage this.

In general, I think the problem is that machine learning carries both newness and prestige.  Statistics has been, over many years, beaten down to a series of rules.  One-tailed z-test here, two-tailed t-test there.  The fact is that most graduates should know how to do statistics in a general way.  In very few cases in practice does one have the simple setups that are shown in these classical examples, so why do we harp on these minute details?

Statistics must start opening the breadth and reducing the depth in undergraduate education.  Matloff  laments the fact that a teacher cannot explain why you divide by n-1, but I think the better question is why is that a core point of learning statistics?  It is such a minor detail that only comes into play for small data sets.  

If we acquiesce in our own rigidity, if we focus on rules versus concepts, statistics will forever be boring.  It can change, and it likely must do so soon.  But given the long history of statistics, maybe it cannot be done.   ",Floydthechimp,2014-08-26 10:09:49
"I completely agree that the emphasis on ""rules"" about z-tests and t-tests and whatnot over conceptual thinking and problem solving in the introductory statistics courses is bad for the field.

I think the problem extends beyond the beginning level, though, and even into the mindset of people who truly need to be using statistical reasoning. Look no further than the threads here in /r/statistics or in /r/AskStatistics with researchers struggling to figure out how to analyze data they've already collected. They want to know if they're ""allowed"" to do some test or not, lest the data police come after them. Ideally, researchers shouldn't be thinking about what they're ""allowed"" to do, but instead about if the model they are using and its set of assumptions makes sense for what they've done. The basic stats training they get in Stat 101 (or even Graduate Methods 501!) often doesn't equip them to think conceptually.

If it were up to me, I'd replace Stat 101 with a class emphasizing manipulating and analyzing data. The quantitative skills students actually find useful are mostly in making tables and charts, after all. Summarizing and presenting your data effectively isn't a natural skill for most people. I'd spend a lot of time talking about plotting in R (and realistically, Excel), a little bit on linear regression and comparing two samples or matched samples, but motivate everything through simulations. I think this is an area where MOOCs have great potential.",normee,2014-08-26 11:49:07
"Well put,  being ""allowed"" to do something is an arbitrary designation.  There are so many things that are not ""allowed"" just because they are suboptimal.  This is, in my opinion, completely and utterly pointless.  So you use a two sided test instead of a one sided test and you have slightly decreased power.  Who cares?  The conclusions drawn will be the same a huge amount of the time.

""...but motivate everything through simulations""  replace with real data and we have agreement

Also, delete ""(and realistically, Excel)""   and we have agreement.  There's no reason to not know how to use R, besides that people do not want to learn.",Floydthechimp,2014-08-26 12:06:36
Unfortunately the business world runs on Excel and people barely are competent in that.  Being a developer -- who can build beautiful and insightful analytic tools in HTML/Javascript -- I often run into business constituents who still want the data to dump into excel because that's what they know and like despite the fact that they can barely operate it. ,gradual_alzheimers,2014-08-26 15:48:25
"&gt; Unfortunately the business world runs on Excel and people barely are competent in that. 

Dear god this a thousand times. I assumed business school peers who ace finance and accounting courses would grasp Excel as a pretty amazing tool. They can follow specific guides to accomplish specific things but most struggle beyond that.

I start a new job as marketing specialist the 15th and the contract allows for a few hours a week of professional development. R is my first stop and a budget for a few used textbooks to keep refreshing myself. I refuse to be stuck in the realm where you're amazing as the short bus tools but haven't put your big kid pants on yet.",,2014-08-26 20:18:20
[deleted],,2014-08-26 22:34:59
This. Hopefully someone will implement an iPython extension for having pandas Pivot Tables work much like Excel's.,rjtavares,2014-08-27 00:46:07
"Well you can turn that discussion of n-1 into a discussion of unbiasedness and/or convergent properties. Ap stats at my school was the AP math class people who couldn't make it into calculus took. I hated statistics until much later, and eventually got a degree in it. ",getonmyhype,2014-08-28 10:16:22
"&gt; unbiasedness and/or convergent properties

Unbiasedness I feel is really unimportant to an average practitioner/student.  It depends on so many conditions.  Convergence is very important, and I would love for AP stats to cover exactly that.  But I doubt concentration of measure will be covered anytime soon.
",Floydthechimp,2014-08-28 11:37:54
"Matloff makes many great points and I encourage everyone to read the link in full. I'll just highlight a few of his statements for discussion:

* ""Though not a deliberate action by any means, CS is eclipsing Stat in many of Stat’s central areas. This is dramatically demonstrated by statements that are made like, “With machine learning methods, you don’t need statistics”–a punch in the gut for statisticians who realize that machine learning really IS statistics. ML goes into great detail in certain aspects, e.g. text mining, but in essence it consists of parametric and nonparametric curve estimation methods from Statistics, such as logistic regression, LASSO, nearest-neighbor classification, random forests, the EM algorithm and so on.""

* ""CS, having grown out of a research on fast-changing software and hardware systems, became accustomed to the “24-hour news cycle”–very rapid publication rates, with the venue of choice being (refereed) frequent conferences rather than slow journals. This leads to research work being less thoroughly conducted, and less thoroughly reviewed, resulting in poorer quality work. The fact that some prestigious conferences have acceptance rates in the teens or even lower doesn’t negate these realities.""

* ""There is rampant “reinventing the wheel.” The above-mentioned lack of “adult supervision” and lack of long-term commitment to research topics results in weak knowledge of the literature. This is especially true for knowledge of the Stat literature, which even the “adults” tend to have very little awareness of. For instance, consider a paper on the use of unlabeled training data in classification. (I’ll omit names.) One of the two authors is one of the most prominent names in the machine learning field, and the paper has been cited over 3,000 times, yet the paper cites nothing in the extensive Stat literature on this topic, consisting of a long stream of papers from 1981 to the present.""

* ""This “engineering-style” research model causes a cavalier attitude towards underlying models and assumptions. Most empirical work in CS doesn’t have any models to worry about. That’s entirely appropriate, but in my observation it creates a mentality that inappropriately carries over when CS researchers do Stat work. A few years ago, for instance, I attended a talk by a machine learning specialist who had just earned her PhD at one of the very top CS Departments. in the world. She had taken a Bayesian approach to the problem she worked on, and I asked her why she had chosen that specific prior distribution. She couldn’t answer – she had just blindly used what her thesis adviser had given her–and moreover, she was baffled as to why anyone would want to know why that prior was chosen.""

* ""Again due to the history of the field, CS people tend to have grand, starry-eyed ambitions–laudable, but a double-edged sword. On the one hand, this is a huge plus, leading to highly impressive feats such as recognizing faces in a crowd. But this mentality leads to an oversimplified view of things, with everything being viewed as a paradigm shift. Neural networks epitomize this problem. Enticing phrasing such as “Neural networks work like the human brain” blinds many researchers to the fact that neural nets are not fundamentally different from other parametric and nonparametric methods for regression and classification.""

* ""In my opinion, the above factors result in highly lamentable opportunity costs. Clearly, I’m not saying that people in CS should stay out of Stat research. But the sad truth is that the usurpation process is causing precious resources–research funding, faculty slots, the best potential grad students, attention from government policymakers, even attention from the press–to go quite disproportionately to CS, even though Statistics is arguably better equipped to make use of them.""",normee,2014-08-26 09:43:34
"In my field (criminal justice) we have a need for both, particularly as everything becomes ""data-driven"" and ""evidence-based."" The CS people build the data collection systems and the stats people figure out what data is needed and how to run the analyses. ",dweebcentric,2014-08-26 11:11:36
"&gt; The CS people build the data collection systems and the stats people figure out what data is needed and how to run the analyses.

I think the point Matloff is making is that the CS people are not only building the data collection systems, but they're increasingly also becoming the people who run the analyses and pushing statisticians aside (at least in terms of funding, publicity, collaboration opportunities, etc.)

What the CS people do is not so much, say, looking to see if there is effect of duration of prior incarcerations on time to re-offend holding all else constant, but instead trying to predict who will re-offend using a black-box model. Statisticians are very good at the former and pretty good at the latter. Machine learning folk coming from CS are not at all good at the former but very good at the latter, especially at scale.",normee,2014-08-26 12:03:22
"That last example is really a key point, I think.

When I was consulting, I found a lot of people mistakenly telling themselves they wanted a prediction model when they really wanted to answer a causal hypothesis.",Neurokeen,2014-08-26 15:23:34
"&gt; I think the point Matloff is making is that the CS people are not only building the data collection systems, but they're increasingly also becoming the people who run the analyses and pushing statisticians aside (at least in terms of funding, publicity, collaboration opportunities, etc.)

This mortifies the hell out of me.  They're hiring people that can program but have zero formal statistical training because the real statisticians don't know how to program well.",beaverteeth92,2014-08-26 18:20:35
"&gt; The CS people build the data collection systems and the stats people figure out what data is needed and how to run the analyses. 

That generally won't work. The best research is done by people who have their fingers in all the pies, people who can do maths *and* program a computer, and everything in between. In fact, I shudder to think what would happen if you put a typical statistician and typical computer scientist together and forced them to work your way.

It's possible, with CS knowledge but no stats, to process data and make some (naive) predictions and inferences. But a statistician who can't code is useless in this day and age, even if you tried to pair them with a computer scientist. The need to invest the time to become better programmers - they cannot seriously 'delegate' this to a computer scientist.

And anyway, 'computer science' isn't about 'data collection systems'. The data needs to be indexed and stored in suitable data structures, custom-designed for the algorithm the statistician wants to use. This means you need at least one person who is relatively competent at every part of the project.",SkepticalEmpiricist,2014-08-26 14:25:10
"I have an undergraduate degree in Math (with a few CS courses), and an MS in Information Systems Development. Now I'm in an MS Applied Statistics program.

I've been watching this data science thing unfold for about two years, and my overwhelming perception has been that the Stat people  have been blind (or in denial) to what's really going on.

Yeah, Statistics may be at the core of it, but there's more to it than that. 

In the same way that ```y = b0 + b1x``` is a Math equation, and ```y = b0 + b1x + e``` is a Statistics equation, data science has more to it than statistics. 

My Stat program makes this assumption that these clean, pristine data sets are going to be handed to us, and that we're going to have access to some computer program like SAS or SPSS or Minitab, and that we, the Statisticians, only need to figure out the appropriate model. Or maybe set up the experiment beforehand. Whatever. As if we're all going to go work for big pharma or cancer researchers or Chubb.

Young people are reading Reddit, Wired, Data Tau, Hacker News, and stuff like that, and seeing a job or career in data as containing the ability to scrape the web...analyze Twitter feeds...do NLP on the world's print media...and so on. And there is nothing -- and I mean nothing -- in my Stat program that even remotely acknowledges this or that there are additional skill sets that are necessary.

Those skill sets tend to be associated with computers. Not necessarily Computer Science as a CS professional might think of it, but computer skills nonetheless.

My advice to the Statistics community is to wake up and acknowledge that data science is more than just Statistics, despite what Nate Silver says, and start making it more relevant to what young people are looking for.
   

",vmsmith,2014-08-26 13:52:12
"I think you're absolutely right about coursework in statistics programs ignoring the realities of the computing grunt work, but I would say a couple things:

* It's not THAT hard to learn how to collect and clean data, especially since there are so many resources demoing things like web scraping out there. Doing text substitutions and such is pretty simple conceptually, it's something you might even learn in a beginning CS class. It's also something you get much better at with the kind of practice that's hard to make a major component of any course.

* Master's and PhD stat students working on a thesis involving an application to real data are going to get some of that kind of experience. All of my applied courses have involved data collection and cleaning (including web scraping) as part of final projects, too, though I'm sure this would vary from program to program.",normee,2014-08-26 14:40:15
"Regarding your two points:

* I don't know how old you are or what your experience level is, but I'm 62 and I've already listed my education. No, it is NOT that hard for me to learn how to collect and clean data. But I imagine that to an 18-year old going into college, it might seem pretty daunting. And that's what everyone seems to be doing, and the Stat department is not offering it. But the CS bubbas seem to be into it.

* The initial post was about undergraduates (I thought). People who are pursuing Masters and PhDs are looking at the world through an entirely different set of lenses.

Again, if Stat departments at the *undergraduate* level want to stay relevant and vibrant, they need to expand their view of the world of data.",vmsmith,2014-08-26 16:04:48
"&gt; The initial post was about undergraduates (I thought). People who are pursuing Masters and PhDs are looking at the world through an entirely different set of lenses.

It talks about all levels. The emphasis is both on attracting undergrads and research funding.",1337bruin,2014-08-26 16:48:36
OK. Maybe I didn't read it closely enough. My bad.,vmsmith,2014-08-26 18:05:23
"I'd have to disagree with him on that. It lists research funding as reasons to support his original statement of the intro paragraphs which were ""why are undergraduate statistics programs seeing a drop in people seeking the major"". Research funding was a tiny bullet and the article rounded out talking abour highschool AP classes. ",under_psychoanalyzer,2014-08-27 04:10:40
"There are two bullet points right at the top of the article

&gt; The field is to a large extent being usurped by other disciplines, notably Computer Science (CS).

This is an issue in terms of research in the field

&gt; Efforts to make the field attractive to students have largely been unsuccessful.

This is primarily addressed in terms of undergrads",1337bruin,2014-08-27 07:48:19
"&gt; Again, if Stat departments at the undergraduate level want to stay relevant and vibrant, they need to expand their view of the world of data.

The professor I had for both undergraduate stats courses had a long list of complaints from students about how ""he gives you bad data and expects you to clean it yourself."" That really was only the case in his 200 level course but he pretty much addressed that as ""You won't get summarized data to work off of in the real world, might as well get used to it now."" After a few months of web scraping competitor pricing data and doing my pitiful best to tidy it all up, he was right. Real world data makes me wish I could afford an intern.",,2014-08-26 20:24:18
"&gt; My Stat program makes this assumption that these clean, pristine data sets are going to be handed to us, and that we're going to have access to some computer program like SAS or SPSS or Minitab, and that we, the Statisticians, only need to figure out the appropriate model. Or maybe set up the experiment beforehand. Whatever. As if we're all going to go work for big pharma or cancer researchers or Chubb.

My undergrad department does this.  I'm in a class that teaches us statistical packages and the professor told us to treat computers as black boxes that always give the right result when you give them the right input.  Absolutely nothing on how computers can screw up if the programmers screw things up or on checking data sets to make sure they're fine.  Zero talk of debugging.  Plug and chug and trust that everything will work out okay.",beaverteeth92,2014-08-26 18:19:35
"That's just a bad stat department. If they tell you to plug in data without understanding the model, that's actually the opposite of stat, which is all about modeling.",selectorate_theory,2014-08-26 21:32:26
"Well we're expected to understand the model.  We just aren't expected to know how to actually program in R or SAS because ""the emphasis should be on the statistics"", so we use other peoples' code and plug our numbers into it.  And the black box quote indicated to me that statistics people tend to view computers as magic tools that they don't have to understand.",beaverteeth92,2014-08-27 03:18:39
"I cannot upvote this enough. 

So I learned to code at work to solve analytical problems and I decided to go back to school get a Masters in Statistics to get a handle on the underlying logic of the methods I was using. I was tired of having nothing but black boxes to work with.

So most of the people in my Masters program, especially the administrators, think I'm a lunatic and I don't really understand why. I want to know the details of the processes, the underlying algorithms used to get the results. I want to use a tool that forces me to build these things from scratch, even if only once, (read: not SAS) and really gives me a handle on what I'm doing, rather than just hoping and praying that some arbitrary level of assumptions are met in the statistical software I am using. However, I am told repeatedly, I should focus on the output of existing software and leave it at that. I fear my program is just a breeding ground for people who will know how to read an output, but little else. Those people will then have to spend another couple of years relearning to find any sort of meaningful work.",mrdevlar,2014-08-27 02:45:38
"Yup. Sounds familiar. I'm balancing out my MS program with healthy doses of Coursera's Data Science specialization classes. Whenever the MS assignments say to use Minitab, I use R and spend some serious time trying to understand what's really going on.",vmsmith,2014-08-27 02:57:41
"I am actually seriously considering taking an extra year to complete my studies and padding it with things like the Coursera program. I was not sure if it was worth it though or if it would just repeat the things I've learned so far in my program.

So I take it has your endorsement?

I am also seriously considering dropping R and taking the time to do everything in Python. Though my linear algebra and optimization skills are probably not sufficiently good to code everything by hand, but I'm willing to try.",mrdevlar,2014-08-27 04:35:47
"First, yes, the Johns Hopkins Data Science track has tons of good stuff I don't think you'd ever see in many college/university Stat programs. I find it quite complimentary to my Stat program.

Regarding Python, I code in Python and like it, but I do all my stat stuff in R. Even though Python is undoubtedly a better language from a CS point of view, the fact is that it's a general purpose language mainly developed by CS types of people. 

R has been developed by statisticians. As such, yes, it is lacking in certain areas, and a bit more difficult than Python in certain ways. But the support community for R is phenomenal, and guys like Hadley Wickham are continually developing new and better packages. Although these packages often get ported over to Python (e.g., ggplot2), the fact is that the original authors are over in the R community. And in my experience you'll find many more blogs about Statistics and R than you will find blogs about Statistics and Python.

Finally, the Coursera classes are in R. When you turn in an assignment, it's graded by peers in the class. They will be grading R code, not Python code.

So my advice would be to perhaps continue learning Python, but for the time being also continue using R in your Stat work.",vmsmith,2014-08-27 05:12:43
"Thanks for the advice. I'll keep that in mind.

Lots to learn still it seems. ",mrdevlar,2014-08-27 06:46:22
"I find it strange that this article depicts Statistics as a blameless victim.  I don't understand how CS, as a field, can ""usurp"" anything, or appropriate ""research funding, faculty slots, the best potential grad students, attention from government policymakers, even attention from the press,"" unless CS is offering something attractive to the people who control these resources; that is, funding agencies, college administrators and faculty, graduate students, policymakers and reporters.  

These markets are speaking.  We should listen to what they are saying.",AllenDowney,2014-08-26 13:20:40
"My naive not-particularly-well-informed opinion, but I think the close relationship of tech companies with the CS side has given them a huge advantage in the past decade or two which drives the hype. This connection exists for historical reasons: you need people who code and engineer data systems, you get them from CS/EE departments, so Silicon Valley has strong links with faculty in these areas. Also, must point out that PageRank came out of the CS side and became the foundation on which Google was built.

But what that means now is the tech companies primarily hire from and collaborate with ML/AI researchers in CS and EE departments, less so statistics departments. Google, Facebook, Yahoo, etc. will work with researchers in top CS/EE programs and grant them access to large juicy databases and computational resources that other researchers don't have the connections to pull off. The fast publishing schedule of the machine learning/AI conference proceedings means new interesting stuff is coming out all the time, even if it's not actually well-vetted (something Matloff mentions). The tech companies and conference organizers send out press releases about all the novel things coming out of their collaborations with academics, it makes the popular press, generally gets a lot of exposure. This leads to interest in these methods from other fields hoping to achieve the same successes, students gravitating towards machine learning (imagine how much more popular Andrew Ng's Coursera course is because it's called ""machine learning"" rather than ""statistical regression techniques and clustering methods""), more university resources funneled towards CS, etc.

Statistics doesn't have nearly that degree of profit motives of the tech industry to buoy it. Statisticians are traditionally allied with other scientific and social science fields -- after all, we figure out how researchers can analyze their experimental results and hope to learn something. But these fields, which are not interested in getting people to click on ads and buy more products from recommendation engines, are not well funded by private industry. As governmental funding opportunities diminish for these scientific fields, traditional statistics suffers too. We don't have as many big conferences, publishing is slower and in academic journals, which aren't putting out press releases about important new papers. Genomics is maybe the area that has done the best about making itself sound interesting to the general population and communicating results (analysis of massive gene dataset showed that the risk of cancer is associated with such and such genetic marker, cool!), so not surprising that it has done relatively well as a sub-area traditionally in statistics.",normee,2014-08-26 14:09:30
"&gt;Statisticians are traditionally allied with other scientific and social science fields -- after all, we figure out how researchers can analyze their experimental results

Machine learning will never replace this. This is a very important and useful area of statistics.",,2014-08-26 17:06:15
"To be fair though, also incredibly stagnant. ",uniform_convergence,2014-09-01 07:51:32
"Agreed.

I do agree with pretty much all the criticisms here, I've worked a little in both CS and stats and I recognize everything in his longer set of bullet points. *But* stats is facing a problem, and it needs the humility to accept that computers are important.

The only important skill that many statisticians miss is the ability to write good software. Of course, many statisticians are great developers, better often than their colleagues in the CS department. But some statisticians are woeful programmers. Undergraduate stats degrees needs to take programming seriously.

The sad thing is that it's well known that good mathematicians have an aptitude for learning programming. But they're not encouraged to do this. They all think ""Yes, I'd like to learn to be a good programmer, but it's more important to read this latest stats paper.""  This causes them to delay and delay, and they never learn how to code.

If you're going to spend decades in modern stats, I think you need to be prepared to invest thousands of hours, early in your career, on becoming a good developer. It worked for me, and that time has been paid back many times over. While my stats colleagues spend hours battling R and manually cranking the data through it, I can automate the process quickly and free up my time to think about the maths.

TLDR: statistics shouldn't worry about CS ""usurping"" their position. Stats just need to create stats undergrads that are as good as software development as the CS undergrads. If you see CS as the ""enemy"", then you will deliberately avoid learning how to operate a computer.",SkepticalEmpiricist,2014-08-26 14:10:47
"&gt; I do agree with pretty much all the criticisms here, I've worked a little in both CS and stats and I recognize everything in his longer set of bullet points. But stats is facing a problem, and it needs the humility to accept that computers are important.

100% true.  I'm in a more conservative department and we still make students do pencil and paper work and plug numbers into menu-driven statistics programs instead of teaching them how to code on their own.",beaverteeth92,2014-08-26 18:22:07
"I didn't get that vibe from the article.  He talks about that at the very beginning.

&gt;This is probably a wise move–most large institutions engage in extensive PR in one way or another–but it is a sad statement about how complacent the profession has become. Indeed, it can be argued that the action is long overdue; as a friend of mine put it, “They [the statistical profession] lost the PR war because they never fought it.”",noelsusman,2014-08-26 15:35:40
"All the state colleges in my area don't even have Stats available as a major. To be honest, I didn't even know it was a major until I started working on my graduate degree. I thought it was just a set of classes.",jerseyse410,2014-08-26 11:25:00
"Many colleges won't have an undergraduate statistics major because you are better served by getting a math degree and then doing graduate work in statistics. Stats depends so much on calculus and even analysis that you need the mathematical sophistication to really learn statistics well. My undergrad had an ""Applied Math"" degree where you could do a stats concentration, but no undergrad statistics major. ",TeslaIsAdorable,2014-08-26 13:49:36
"To be fair, CS is stealing a lot of maths, and the maths departments seem to be happy to let these areas go. Graph searches for example is almost exclusively CS nowadays as is any kind of algorithm development which is very mathematical. Linear Programming too is a another example.

Stats is the newest casualty and it's really sad. Having a team with both stats and CS guys is great, each has different strengths and the machine learning stuff requires a very good understanding of stats to fully understand.",ProfessorPhi,2014-08-26 16:46:32
"&gt; Linear Programming too is a another example.

As an academic in Operations Research, I can sympathize.  I have come to terms with the fact that our results will be appropriated by whatever other field overlaps, whether CS, applied math, stats, business, or economics.  It's not so bad, our best and brightest simply become known in the other fields, germinating areas with the OR way of thinking.  

If stats loses ground to Machine Learning and ilk (i.e. applying stats on databases), statisticians who work in the area will simply have to gear up their computer skills and compete as programmers.  Engineering and IT are rather meritocratic, so statisticians bringing rare skills will rise to the top.  In any case, statistics as a field will continue to be dominant in the foreseeable future in many areas.",helot,2014-08-27 14:12:10
"From the comments:


&gt;&gt; Switching to R would be doable–and should be done.

&gt;An article that complains about ""statistics losing image among students"" and then goes on and recommends a solution that will hurt just as much in the long term? One of the reason CS people at my institution are turned off by most statistics classes is R. Yes, it's still better and more engaging than say, STATA or SPSS. But as a programming language it is horrible and needs to die in a fire. This is a major turn-off point at least in my experience. If Statistics wants to stop losing ground on CS, I propose taking a page out of their book and switch to a nice, clean, all-purpose programming language. Python has grown to offer the same tools as R has (given pandas, statsmodels, matplotlib and sklearn), but does most of the work with much more class and is a same programming language.


For me as a Computer Scientist, this hits the nail on the head
",gangro,2014-08-26 23:47:15
"With regard to the AP program, I actually had a positive experience with Statistics.  I had a good teacher who made us repeatedly check all the assumptions before running tests and spent a ton of time going over different kinds of problems.  That class sparked my love of statistics and I never would have gone to school for it if I hadn't taken it.

Maybe it was just my high school, but I learned far more in AP Statistics than I did in my introductory class in college.  The AP class was more about interpretation, checking assumptions, and knowing what techniques to use when than it was about knowing the formulas.  Meanwhile, my intro class was taught by a fantastic professor, but we spent far more time learning formulas and plugging numbers into them.",beaverteeth92,2014-08-26 19:28:42
"As an academic statistician running the largest data science program in the world (10,000+ have completed a class) I think that CS/Data Science only pose a threat to statistics if we don't adapt. I wrote about it here: http://simplystatistics.org/2013/04/15/data-science-only-poses-a-threat-to-biostatistics-if-we-dont-adapt/",t_rex_tullis,2014-08-27 04:44:21
"My point exactly in my post elsewhere on this thread.

And as someone who has taken quite a few of your data science courses, I say bravo. They've been interesting, challenging, and very informative. Keep up the good work.",vmsmith,2014-08-27 05:16:28
"I view the ""data science threat"" as distinct from the ""machine learning/CS threat"".

I think the former is an issue about the relationship of statistics with industry: making sure that the statistical nature of what goes on in ""data science"" work is not neglected while stepping up efforts in classrooms and elsewhere (like your Coursera series) to give statisticians the computational skills to be relevant in these jobs.

I think the latter is more about competition for academic resources (which are increasingly entangled with industry resources) with CS departments. The threat from machine learning/CS is more of what I took away from Matloff's post, not so much from data science. I don't think it's just that the CS people have been more willing to work with messy data and do things at larger scale, but that the contributions of statistics to what makes machine learning successful are being diminished or ignored, as in the examples in his post. I also think that statistics, with its central emphasis on inference and interpretation and uncertainty, serves other scientific fields better than machine learning can. But I do worry that we're at risk of losing collaborators who have large observational datasets and go to CS for insights instead, and that is a acute existential threat to the field.",normee,2014-08-27 09:14:25
[deleted],,2014-04-30 11:19:13
saw the table of contents for the kindle version and bought it right away.  what have i done???,iconoclaus,2014-05-01 02:16:28
^THE best! ,JohnPaulStevens,2014-04-30 11:29:14
"That looks solid, thanks for the recommendation.
",shaggorama,2014-04-30 14:51:35
"This is an excellent introductory text on the topic, but frankly IMO the exposition is a little long-winded. There is one chapter worth of coverage of this in Gelman et. al.s Bayesian Data Analysis book, put in a bigger context that I found much more useful...",remington_steele,2014-04-30 20:19:46
"I keep all the other textbooks on the shelf for show, but I pull out my dog-eared and coffee-stained copy of Gelman and Hill out of the desk when nobody's looking when I actually have to get things done.",itsactuallynot,2014-05-01 02:56:05
"Elements of Statistical Learning by Hastie, Tibshirani &amp; Friedman

All of Statistics by Wasserman",stoplan,2014-04-30 13:01:30
ESL really is the go to machine learning text.,TheCaterpillar,2014-04-30 13:24:07
"ESL is quite good, but is more or less superseded by Kevin Murphy's book.",kjearns,2014-04-30 18:13:59
"Interesting, I haven't read Murphy's yet, but I still hear ESL recommend a lot more, at least among data mining types. Maybe the Machine Learners are switching to Murphy faster?",clm100,2014-04-30 18:49:35
"I take it this is the one: http://www.cs.ubc.ca/~murphyk/MLbook/

""Machine Learning: a Probabilistic Perspective"" by Murphy?",WearingAMonocle,2014-04-30 21:34:37
Yes.,kjearns,2014-05-01 01:21:03
How advanced is this for someone with an undergrad in engineering who has taken a couple of statistics courses?,elmiko6,2014-05-15 15:57:38
"Not necessarily related to your interests, but Mostly Harmless Econometrics is my goto book for research design and methods, and discusses the most prevalent econometric methods. I don't know why, but something about how they write the book really helps me understand common mistakes and things to avoid.",theycallhimhellcat,2014-04-30 11:26:24
"Lots and lots of example analyses using real world data. I love MHE, although I think they are a little dismissive of GLM. ",iacobus42,2014-04-30 21:04:18
"I'm in the middle of rereading it, so I'll have to pay attention to the discussion of GLM with your thoughts in mind.",theycallhimhellcat,2014-04-30 22:46:06
"Maybe not _advanced_ but I think the classic GLM book Generalized Linear Models by McCullagh and Nelder is quite good.  

For data mining I think Pattern Classification by Duda, Hart and Stork and Pattern Recognition by Bishop are great.  They do things from more of an engineering side though.",shazbotter,2014-04-30 13:38:59
"I've used Bishop, and although it's very common I couldn't really recommend it. Especially coming more from the stats side I found it very frustrating.",clm100,2014-04-30 18:26:14
How come?,,2014-05-01 03:56:46
"Also not directly related, but pattern recognition and machine learning by bishop is a standard in the field.",andrewff,2014-04-30 11:42:27
"*Sampling* by Thompson

*Bayesian Data Analysis* Gelman et al. 
",tekelili,2014-04-30 15:52:15
"Cohen and Cohen
",steffejr,2014-04-30 17:51:08
"I think that a rock solid foundation in mathematical statistics is really useful for reading about all other applied topics and the literature (to see the most advanced techniques you usually have to look beyond textbooks). 

So I vote for Bickel and Doksum, [Mathematical Statistics](http://www.amazon.com/Mathematical-Statistics-Basic-Selected-Topics/dp/0132306379).  Then for a good foundation for pattern recognition, I suggest DGL a [probabilistic theory of pattern recognition](http://www.amazon.com/Probabilistic-Recognition-Stochastic-Modelling-Probability/dp/0387946187).  

From those, you will have a great base to stand on and learn anything else.  ",DrGar,2014-04-30 14:54:45
"Two books on my shelf, for whatever it's worth:

*Hierarchical Linear Models*, by Raudenbush and Byrk.

*Doing Bayesian Statistics*, By Kruschke",PoofOfConcept,2014-04-30 15:06:40
"I'm not a fan of the Raudenbush and Byrk approach to HLMs.
",dasonk,2014-04-30 15:21:10
Why is that? What do you prefer?,PoofOfConcept,2014-04-30 15:34:45
I can't stand their notation.  I don't think it's a good way to think about those models.,dasonk,2014-04-30 18:28:43
Completely unrelated: Numerical Ecology by Legendre &amp; Legendre. ,fish_finder,2014-04-30 17:18:11
Thank you all so much for the great recommendations! ,AprimeAisI,2014-05-01 00:01:55
[deleted],,2014-04-30 11:44:04
Otherwise known as nightmare reading... Those were my grad school books. ,shaqed,2014-04-30 16:41:53
The first book was used for 3 of my undergrad courses :*),tree_man,2014-05-01 08:37:27
*Statistical Methods* by Snedecor and Cochran is a classic.,OhDannyBoy,2014-04-30 13:32:13
My go to reference for GLMs ins Fumio Hayashi's [Econometrics](http://www.amazon.com/Econometrics-Fumio-Hayashi/dp/0691010188/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1398907059&amp;sr=1-1&amp;keywords=fumio+hayashi). By far the best reference I have found for all the matrix algebra underlying linear models.,Taylor1137,2014-04-30 18:19:01
"Not exactly stats, but since you asked and no one else has answered 

[Fundamentals of Database Systems, 5th Edition, Ramez Elmasri and Shamkant B. Navathe](http://www.amazon.com/Fundamentals-Database-Systems-5th-Edition/dp/0321369572)

Fairly standard DB/SQL text, platform agnostic, well motivated, lots of examples.",wisps_of_ardisht,2014-05-01 11:06:13
"Thank you for this one, especially. I made the mistake of not taking a SQL/database class before exiting the Uni-system. ",AprimeAisI,2014-05-01 12:51:10
"A very good introduction to SQL is [Bowman](http://www.amazon.com/Practical-Sequel-CD-ROM-Judith-Bowman/dp/0201616386/ref=sr_1_7?s=books&amp;ie=UTF8&amp;qid=1399046584&amp;sr=1-7&amp;keywords=judith+bowman).

Also good:

[Data Analysis Using SQL and Excel](http://www.amazon.com/Data-Analysis-Using-SQL-Excel/dp/0470099518/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1399063669&amp;sr=1-1) by Gordon Linoff.",datadude,2014-05-02 09:04:12
"Well, I rarely use books once I've read them, but these are books I've had occasion to use in the last couple of years.

MASS (Venables and Ripley)

Casella&amp;Berger

Davison&amp;Hinkley

But they don't necessarily have good overlap with your list.

There are a number of areas I wish I had a book that I would recommend. 

On the other hand, I'd say - read *lots* of books and papers, and find what works for you.

",efrique,2014-04-30 18:56:50
"to study online, and get a good handle on basic concepts - use this - http://www.stat.berkeley.edu/~stark/SticiGui/Text/index.htm",witafox,2014-05-01 00:35:20
i love that not caring about spelling predicts religiosity,zmjones,2014-04-22 04:52:11
"I'm continually disappointed that OKTrends doesn't get reinvigorated. The author had verbalized intentions [to reboot the blog in March](http://contently.com/strategist/2013/08/19/whatever-happened-to-okcupids-oktrends-spoiler-alert-its-coming-back/), but it's almost May now and that clearly hasn't happened. I'm hoping he's just tied up with his book and we'll get OKTrends back later this year. Shit, I'd do it for free if they'd let me, but I sort of doubt their friendly with strangers in their data. 

Also, the company was bought by Match.com (a few years ago? Around the time OKTrends died?), so it's possible that the people in charge now don't want the blog to be a component of the company for one reason or another. Take note of the conspicuous disappearance of the blog post explaining [why you should never pay for online dating](http://www.columbia.edu/~jhb2147/why-you-should-never-pay-for-online-dating.html).",shaggorama,2014-04-22 08:27:56
"""Their friendly""

We've got a religious guy over here!",MagnusT,2014-04-22 14:06:06
"That data would be so cool, I try not to think about all the cool things I would try to do with it because I'll just get sad knowing I can't :(.",stupidreasons,2014-04-22 13:56:59
"Yep, I went on a date with a young woman who didn't like the taste of beer. No first date sex.",masskodos,2014-04-22 07:17:02
https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about,srkiboy83,2014-01-05 02:13:01
"Thanks, I'm signed up for a few Coursea but I really need to focus on R so this should be good.",mistified11,2014-01-04 12:39:53
Oh perfect! I signed up for the R course last semester but it interfered with the programming course. I'll be giving it a shot again.,GRANITO,2014-01-04 14:57:18
Thanks! I've been wanting to learn R for awhile now.,emsuperstar,2014-01-04 14:51:21
Thanks for the link - signed up for the R course.,truckbot101,2014-01-04 15:19:41
Thanks for this. I am learning R at the moment but a refresher course would be great. ,armchairdetective,2014-01-05 03:12:50
"The p-value question does seem a little unfair, I don't think it captures their true knowledge of the problem.

However, I work with doctors every day as a statistician, and of the 50 or so I work with I would trust around 5 of them to correctly interpret a PPV or a likelihood ratio.

The worst part is that most of them know just enough to be a danger, where they don't know what a P-value or PPV are but think they do",slammaster,2013-12-25 18:50:40
"Heyyo, I'm a pharmacy student. I think that (given that we know how well our medicines *don't* work) as healthcare practitioners we should really be critical of how intimately we know our statistics. The way the p-value question is framed sounds almost like a multiple choice question from an intro stats course, and I certainly don't think it's unfair. It should at least click that the p-value is a probability on the observed results, not on the hypothesis. Really, with the emergence of EBM, the definition of a P-value should be something a practitioner can recite off the top of their head, lest their job be simplified to reading medical algorithms.

I'm gonna send this to a professor of mine, he's gonna have a ball with this one",keel_bright,2013-12-26 00:11:12
"I don't think the p-value question is unfair. I'm a clinician (dietitian rather than MD), and every single statistics course, lecture, or continuing education course I've attended has addressed this question in nearly these exact words and most have made it clear that grasping the proper definition is the single most important take away from the course. 

I think the wording in this study is actually a bit of a softball, as you only need to recognize the definition as the false one all your professors warned about without needing to have retained the correct definition. 

(for the record: I got both of the survey's questions right without cheating, but the second took more deliberation than I'd like to admit)
",fat_genius,2013-12-26 07:20:55
"I've seen the mammogram question answered wrong so many times it's almost a cliche. I hope these sort of articles eventually make the rounds among doctors soon, but in the meantime, good luck keeping them in check!",mesolude,2013-12-25 19:57:35
"I have seen the mammogram question before, but the version in the survey is the easiest framing I have ever seen:

&gt;Ten out of every 1,000 women have breast
cancer. Of these 10 women with breast cancer, 9 test
positive. Of the 990 women without cancer, about 89
nevertheless test positive. A woman tests positive and wants
to know whether she has breast cancer for sure, or at least
what the chances are. What is the best answer?",veryshuai,2013-12-25 20:33:48
"I agree, the P-value question is a bit unfair:

|True or False: The P-value is the probability that the null hypothesis is correct.

",veryshuai,2013-12-25 20:30:20
"This is stupid.

One can be able to answer this correctly and still have no freaking idea what statistics is all about.

Conversely, one can be a pretty good user of statistics and not have any knowledge about this particular definitional issue.  A lot of people use math without really understanding math the way mathematicians do.

The notion that having an understanding of statistics (or anything in math) is being able to correctly barf back definitions is completely wrongheaded.",berf,2013-12-26 04:57:21
"I respectfully disagree. This isn't just semantics. The way we counsel our patients with positive test results can drastically impact their lives. 

Example from the clinic: a late 30s woman has been trying to conceive one last time before her biological window closes. She and her husband have had difficulty, but after about 14 months of trying she is finally pregnant. She's been having some strange parasthesias in her foot, so as part of a routine neurology work up she gets a HIV and rPR test. The latter comes back negative, but the former is positive. She works as a software engineer for a dialysis equipment manufacturer and has been married for 14 years. She has one healthy child. She and her husband are active in the community, participating in blod drives and leukemia walks annually. What do you make of the test?

That vignette tells you all you need to know that the test is almost certainly falsely positive, assuming you're statistically literate. She's well educated, has had many HIV tests before (blood drives), and has no other risk factors for HIV.  So a statistically experienced doctor would not be concerned with the result. Since HIV tests are quick and cheap, just retest and be sure not to alarm the patient. A less knowledgeable doctor may go in and simply state ""your HIV test is positive"", which does nothing more than panic the patient. ",BillyBuckets,2013-12-26 08:58:18
"I'm not arguing that getting the predictive value of a positive test result completely wrong is OK.  But I am arguing that fussing about some specific verbiage is not the issue.  You have to do the correct calculation and do it correctly, as everywhere else in math and science.",berf,2013-12-27 12:05:18
[deleted],,2013-12-25 22:29:26
"Residents are doctors... Agree about only one specialty being sampled though.
",austinap,2013-12-25 23:02:41
[deleted],,2013-12-25 23:32:15
"You're right that they're still in specialty training, but residents have passed all of their medical boards and can practice on their own if they so desired.  They are definitely full doctors, just not necessarily specialists.  Again, you're right that it's unlikely that they receive any additional statistical training during residency.
",austinap,2013-12-25 23:38:10
"Because the circular sets can overlap arbitrarily, the output of this app is often an example of an [Euler Diagram](http://en.wikipedia.org/wiki/Euler_diagram), not a Venn Diagram (which requires every combination of sets be represented).
",thatsnotavenndiagram,2012-05-23 09:34:15
I'm not usually one for novelty accounts... but I like this.,,2012-05-23 10:05:34
Thanks!,thatsnotavenndiagram,2012-05-24 20:37:08
"An [SE response](http://stats.stackexchange.com/questions/23313/compute-areas-of-venn-diagram-given-covariance-matrix) on why Venn Diagrams should be retired once you're capable of reading a table. Basically, they're conceptual and give less info than a simple table.",,2012-05-23 10:07:42
"Good read, thanks. ",,2012-05-23 10:51:48
"I concur. It's great to demonstrate how variance and covariance works. The third variable may not even be a perfect circle, but a weird oblong peanut to show how it can *theoretically* cover other factors. It would be confusing to display as a reflection of actual data.",neurorex,2012-05-23 19:10:59
"WTF? No, just write out the correlation matrix or the vcov mx.",,2012-05-23 19:51:34
"Oh goodie, the two worse ways of presenting data together at last! Can't wait to see those in biology studies.",canteloupy,2012-05-23 11:34:18
Hahahaha! So fucking true.,wankman,2012-05-23 12:02:43
"I mean....I don't like it, but if they're going to do this, shouldn't they at least provide you with some information about the area in the overlap? The percentages seem to only relate to the isolated pie charts. Where's the percentage of the intersection?",shaggorama,2012-05-23 11:02:41
"What does the intersection even imply? The regions intersecting sometimes intersect? There's no labels on any of the examples, so it's a mystery what you would even do with this. ",McDarling,2012-05-23 13:32:45
[Sales forecast](http://www.youtube.com/watch?v=vESquSEEeos),shaggorama,2012-05-23 13:47:58
"I made a quick visualization using [their methods](http://flowingdata.com/wp-content/uploads/2011/10/Venn-piagram-625x466.jpg)

not a bad outcome, if you ask me.",Llort2,2012-05-23 21:03:39
"Well, at least they got one thing right. This will turn the world of mathematics on its head after everyone in that world swan dives off a tall building on seeing this. 

I hope this is a troll. Who could ever want to combine pie charts (terrible) with Venn diagrams (at least I can see a use for those). Do the sections have to line up? Can you actually do this and still use circles and slices of circles for the pie chart? Or is it that 13% of A is also in B and A and B are both made of 33% x, 33% y and 34% z but the set A intersection B is just that 13% of A is in B or does the overlap of the x section of A in the intersection of A and B matter? How could this be useful at all? My head hurts just thinking about it. ",iacobus42,2012-05-23 15:12:47
"Gah. *Kill it with fire*.

This sort of crap is now going to infest presentations that I am going to have to endure. 
",efrique,2012-05-23 15:51:41
"Mother of god..... I honestly can't figure out if someone is deliberately trolling Tufte, or just insane ",petewilko,2012-05-23 13:08:29
"I laughed until I cried. This is some deep, deep trolling.",v33n,2012-05-23 20:55:51
"Great, the two worst ways to represent data have now been combined.  Another wonderful advancement brought to you by Mac.",KSUpsych,2012-05-23 10:41:35
What does Apple have to do with it?,bubbles212,2012-05-23 14:27:13
Method of distribution.,TheRealDJ,2012-05-23 15:04:28
That's like complaining about iTunes or Spotify because Nickelback sucks.,bubbles212,2012-05-23 17:00:13
Well you can blame drug dealers for selling black tar heroin.,TheRealDJ,2012-05-24 11:07:20
"Sadly, I think this would be pretty popular in /r/dataisbeautiful.",rz2000,2012-05-23 23:03:58
I hope that Tufte's work has enough influence that something as useless and terrible as these... things... would ever end up being called beautiful. ,iacobus42,2012-05-23 23:44:58
"Well, who do y'all think has the better model?  Sam Wang or Nate Silver?  

For me, I started off following Silver after 2008, but I found out about Wang's Princeton Election Consortium 2 years ago and liked what I saw.  I like that his model is open source, and I like that he explains the numbers to you concretely, while Silver's model is proprietary and we haven't heard much about how it calculates its figures until now.",Mister-Manager,2014-09-17 17:09:29
"Silver has too much secret sauce. The last 3 major elections in a row a simple polling average outperformed his predictions.

Not by a lot, mind you, but he basically takes the polls, adds some secret sauce which turns the conclusions a bit shit, and passes it off as genius. I'm a much bigger fan of the honesty of Sam Wang. ",andrewwm,2014-09-17 17:56:56
"I really don't like that he accused Sam Wang of adding ""secret sauce"" to his model.  It's the exact opposite! 

Wang also pointed out that Silver is putting a lot more weight on his predictions than he should.  He said something on Twitter about challenging Wang to a bet that the Republicans winning the White House when his model only had them as a 60/40 favorite.  
",Mister-Manager,2014-09-17 20:33:31
"I lean more towards Wang's style than Silver's.  From what I read here, it seems like Silver tries to adjust for everything he could possibly think of, which brings with it a laundry list of assumptions.  With that said, I think there's an inherent bias against Silver in the statistical community because he has become super famous without doing anything all that amazing.

I think both models could be significantly improved.  Has anybody ever tried modeling individual voting probabilities?  That's how the CDC comes up with obesity prevalence estimates at the census block level, and I think it could be interesting in this context.",kevjohnson,2014-09-17 21:35:56
Where can I find Sam Wang's model?,pandemik,2014-09-18 09:19:55
http://election.princeton.edu/code/,Mister-Manager,2014-09-18 09:42:55
"Didn't see this, just posted asking the same question to R/statistics!",_juicebox__,2014-09-18 10:31:35
"hahahahahahah ""bill,"" the social justice warrior of the statistics community.

i should check my privilege to the 95% CI.",Alors_cest_sklar,2014-05-10 14:31:37
"Type 1 error: ""I only needed *one* test and I found a significant result!"" &lt;-- idiot

Type 2 error: ""it looks like I'll need to perform a *second* test before I'll find a significant result..."" &lt;-- idiot",ATG77,2014-05-11 06:03:14
"I like using this table from UCLA
http://www.ats.ucla.edu/stat/mult_pkg/whatstat/",cherise605,2013-10-07 15:52:07
"I've been working on something like this, not really as an aid, but as... art? Decoration? A way to show something of the big picture to interested students? Anyway, it's not remotely original; it's an attempt to add some graphic organization elements to Tabachnick &amp; Fidell's chart from pp. 29-31 of *Using Multivariate Statistics* (6e). It's not very far along, yet; mostly I've just recreated their chart with minimal formatting changes. But I'm hopeful.",bobbyfiend,2013-10-07 17:22:00
"It would be a humongous project. Each branch of techniques fills up entire books. Any simple ""if A then B"" by one person or two is almost bound to be wrong.

Start /r/statsmethods, convince students to post their class notes, summarize, draw flow diagrams, and debate?

In all likelihood there are probably some ""statistics wiki"" around with varying level of quality and details, but they are more likely to start from the methods rather than the questions.",rottenborough,2013-10-07 15:51:27
"I like [this one](http://abacus.bates.edu/~ganderso/biology/resources/statistics.html)
(scroll all the way to the bottom)",Calamintha,2013-10-08 04:49:36
"Here is a quick and dirty guide that I use to select a regression model. It is not complete or comprehensive but might be useful:

**Regression Model -- Type of Dependent Variable**

Binary Logistic -- Dichotomous

Probit	 -- Dichotomous

Ordinal -- Polychotomous and Ordinal

Multinomial -- Polychotomous and Nominal

Poisson -- Count

Negative Binomial -- Count with Overdispersion

Zero-Inflated Poisson -- Count with Excessive Zeros

Zero-Inflated Neg. Bin. -- Count with Excessive Zeros and Overdispersion

Zero-Truncated Poisson -- Count with No Zeros

Zero-Truncated Neg. Bin. -- Count with No Zeros and Overdispersion

Tobit -- Interval-Ratio with Censoring

Treatment Effects -- Selection Bias on Independent Variable

Heckman -- Selection Bias on Dependent Variable",bubbleberry1,2013-10-08 08:38:25
There is a decent one near the beginning of Tabachnik &amp; Fidel's textbook.,HelloMcFly,2013-10-07 22:32:23
"And also at the beginning of Hair, Black, Babin &amp; Anderson...",srkiboy83,2013-10-07 23:55:51
"It's feasible but I don't know if I am a fan of an idea like this. It makes being a statistician more like a robot and less like a thinker. There's a lot of thought that has to happen before running regressions or doing ANOVA's. 

I think it's a totally reasonable thing to ask, I just think it sets a bad precedent for non-statisticians. 

More importantly, statisticians develop new methods all of the time. Most of the methods that are the simplest make stringent assumptions, but statisticians have come a long way in some cases and have found (platonist) much more flexible methods.

Sorry for the rant, I just think something like this could be both good and dangerous.


** INSERT SPIDERMAN QUOTE **",econometrician,2013-10-07 17:01:57
"I partially agree and partially disagree. I think that it can be easier for students to learn statistics if they have a better sense of what techniques are similar and different; I think that providing a little organization for them isn't a bad thing. However, I do agree that it would be impossible to exhaustively group methods appropriately on a chart and that there is a danger in creating a tool that takes the thinking/decision making out of the processes. My hope would be that such a chart would only aid students in better understanding when to use different methods.",valen089,2013-10-07 18:12:17
"I entirely agree that it could be very good; I would love that. But, I fear that graduate students in fields that lack a precise understanding of statistical models would write checks that table wouldn't be able to cash. A simple example of this is p-values and their egregious consequences.",econometrician,2013-10-07 19:12:40
"I think a lot of statisticians feel this way. But there is a niche to be had where quantitatively inclined people who do not have a PHDs want to do some sort of data analysis. I think it is the difference between the Research and Applied settings.



",Adamworks,2013-10-08 07:18:37
I think that's just an excuse to try to write statistical cashes said quantitatively inclined people can't cash. I've heard an absurd amount of erroneous claims made by people with a poor understanding of statistical methods. They tend to make promises the data cannot keep. ,econometrician,2013-10-09 10:45:01
"I looked high and low for one as well but to no avail. I finally just realized ,y work requires binomial tests, t tests, and Mann-Whitney U tests and stopped looking. :(",DarkXanthos,2013-10-07 15:34:10
"Yes, I would like something like this as well. Somebody's gotta have drawn one up at some point in time...",gAlienLifeform,2013-10-07 15:36:58
"I have one that is basic computations, but it worked well for the content
[Try this](https://app.box.com/s/yws26fgfym1b6ew1860h)",sjgw137,2013-10-08 05:58:34
"I have one uploaded here (PDF)
[Try this](https://app.box.com/s/yws26fgfym1b6ew1860h) 

I still think the UCLA is about the best you can get.",sjgw137,2013-10-08 05:59:11
Just Google statistics flowchart. We had to produce one as part of our graduate Capstone and found plenty to use as references.,sturg1dj,2013-10-08 14:18:57
This is amazing. Need to carve out time to put myself through this course. Thanks... uh... bullshitdicks...,amstarling,2015-03-16 06:31:31
I really should have put more time into selecting my username.,bullshitdicks,2015-03-16 06:39:01
It is perfectly fine.,wssbck,2015-03-16 08:10:20
"Nah brah, you're good",assballsclitdick,2015-03-16 14:18:13
"I'm taking this course right now. It's just wrapped up in fact. If you're interested in taking the course, I'd suggest bookmarking this page and checking up on it when they'll next offer it:
https://class.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about

Overall, I have no complaints in taking the course. I work in market research, and there are quite a few interesting things about it, although there are many others that I can't see ever using in my line of work (Support Vector Machines and non-linear regression techniques, most notably). I also found it quite difficult to follow at times, as I don't have a good background in regards to math notation (Non-linear regression was particularly difficult in that regard).

One complaint I have (and it's a small one, I understand it's a free course) is that the quiz can be very easy a lot of times, but in others they rely on ""gotcha"" quizzes (for example, I got 1 question wrong because of the way they phrased the question) - and it's made the other ""easy"" questions more difficult because I was wondering if it was ""too"" easy, as in I ask myself if they are not trying to trick me again.

However, there was one set of questions in the Support Vector Machines (the section using R) that I found extremely difficult. The problem is that many of the questions for the R labs have been ridiculously easy (for instance in the second half of the course, they seriously ask how to install a package), and then suddenly they basically ask you to write a complex code and function. I would prefer if the learning with R was incremental, where you build the skills as you go along and you're able to answer more complex questions towards the end of the course. In this case, the learning curve for the R labs questions went from extremely flat to extremely steep, and I don't find a particularly good teaching method.

Nonetheless, I highly recommend the course for anyone interested in the methods that they cover in their books (and their books are free too, which is great). Some topics not covered that I really wish was included is Bayesian techniques, such as stuff on Markov chains and stuff like Monte-Carlo simulations. I also wish they would cover a step on how to get the models out of R and into other softwares for external consumption (like Excel, for instance). None of my clients use R and would be interested in using it, so it's a step I need to learn more about.",AllezCannes,2015-03-16 10:49:19
"So, I'm confused. Is this a commercial product? If so, kind of kills the appeal. ",NOTWorthless,2015-03-05 20:08:55
"I Python is an open source programming environment for Python, much loved in the data analysis world. And now are starting to support R. ",Caos2,2015-03-06 04:39:00
"I know what IPython is, and I understand that *someone* has built an extension for it that supports R. The question is, (1) who made the extension and (2) is it free. All I see here is links to a bunch of commercial products and nowhere for me to download the thing. If I need to pay for Domino or Plotly Enterprise to use this notebook, then it is worthless to me. ",NOTWorthless,2015-03-06 07:32:07
"Thanks for checking it out. Both Plotly and Domino are web-based, so no need to download. Both are like GitHub: they have a free, web-based option for sharing and can be run on-premise for commercial users. The Notebook links the [Domino repo](https://plot.ly/ggplot2/) with the Notebook and [Plotly docs](https://plot.ly/ggplot2/).",Willi_Wilberforce,2015-03-06 10:55:42
"Thanks for the response, can I run notebooks locally? ",NOTWorthless,2015-03-06 11:04:51
"This is amazing. I love IPython, so already love R Notebook. Thank you for sharing!",eclore,2015-03-05 19:54:02
Nice one. I was just looking for something like this at work today. ,aksack,2014-12-17 23:03:24
Looks like this is from 1999? I'm curious if there have been any followups.,Sir_Cuitry,2014-09-18 08:00:23
"ITT: people who think they understand the gender wage gap issue because they know the 75% ratio is partially caused by different life choices generally made between males and females.

Read [this](http://www.pnas.org/content/109/41/16474.short) paper, an experimental study that shows that both male and female faculty propose lower salary offers for resumes with female names (that are otherwise identical to the resumes with male names)
",lboyles,2014-09-18 15:47:21
"I think it's obviously a heuristic decision based on the assumption of maternity leave. 

Academia is especially sensitive to maternity leave, because it keep faculty on the payroll who would otherwise may be denied tenure and leave. I know of one professor at my former undergraduate institution who had two babies in succession just after starting. This has pushed backed her tenure review substantially, even though her lab has been packed with students and functioning at the same rate as every other new faculty's lab. The extra time has allowed her to make up for the initial lack of results, which will give her a huge advantage in the review. 

Universities obviously would prefer to avoid this type of situation, as it makes many decisions much more difficult. It's hard to treat this type of problem properly. 

Even is the ad hoc maternity hypothesis isn't true, one study on hiring practices in academia says nothing about the gender wage gap across all occupations, so using to that point is pretty useless.",knockturnal,2014-09-19 05:54:32
"The decisions were for student lab managers, not faculty hires.  I think maternity leave is much less of a concern to those hiring students.

I agree that this is but one study.  Likewise, one study on the wage gap in engineering says nothing about the wage gap across occupations.  There is a competitive market for engineers, so I don't find the results in the posted paper too surprising.

I'm merely trying to point out that the wage gap is a complicated effect, and it is difficult to tease apart the contributions to it from either discrimination or choice.  For example, one variable frequently used to ""explain away"" the wage gap is experience.  However, experience can be affected by discriminatory practices, for example preferring to give promotions to males over females.  Of course, whether this is a problem or not for the study in question depends on how experience is quantified.  In the engineering study, experience is defined as the number of years since the first bachelor's degree earned, so it seems to me like it isn't an issue for this study.",lboyles,2014-09-19 12:09:17
"I would like to remind everyone that this is /r/statistics, not /r/SimpleQuantitativeAnalysis or /r/GenderIssues. The post doesn't belong here.",rottenborough,2014-09-18 13:50:23
"The main graph in this article, which is supposed to show that salaries are the same for similar years of experience, does not control for any other factors listed above it.

E.g., above the graph, they say that more women lived on the west coast than men - which, given generally higher cost of living and salaries on the west coast, could skew the results. There are also differences in employment sector and speciality (26% of female engineers are in software vs. 13% of male engineers). All of these could skew results in different directions.

The authors acknowledge this by a cryptic sentence ""When the other variables listed above are added to the regression, the estimated difference is lowered only another 1 percentage point.""

But they do not say what specific kind of regression they are using for multiple variables (ok, it's ""polynomial median regression"", with a smoothing function applied on top - but what are the parameters of the regression and of the smoothing?), and how they are summarizing the differences to a single number.

All in all, this study looks pretty useless to me :-/",jkff,2014-09-18 20:43:52
"Yes, the ""wage gap"" is virtually non-existent for people doing the same work. There may be some differences due to things like women apparently being less willing to negotiate than men are, but nobody is making 75¢ on the dollar for the same exact work.

From what I've read men and women simply choose different types of work, on the whole. One of the biggest differences is men generally being more willing to sacrifice their personal time to enlarge their paychecks than women are. So basically the ""wage gap"" only exists if you insist on stupidly comparing men *in aggregate* to women *in aggregate*.

It's not really clear why it should be considered desirable for women to pick exactly the same types of jobs as men do. And more to the point, if you could pay a women 75% as a man for the same exact work, *why would anyone ever hire men?*",Eurynom0s,2014-09-18 10:47:42
"Sociologists and economists have known the ""cause"" of the gender age gap for several decades now: men tend to be employed in occupations that entail ""specific human capital"" (such as applied math, programming, statistics, engineering) or dangerous working conditions (such as mining, construction, policing, security).

In short, men tend to be in occupations that are either hard and difficult, which tend to be compensated at higher levels monetarily.

The entire debate could be reversed: Why is it that men are pathologically taking the hardest and most difficult jobs? How can we encourage more men to take on careers that are more obviously creative, social, and humanistic? ",toofantastic,2014-09-18 12:07:32
"&gt;Why is it that men are pathologically taking the hardest and most difficult jobs? 

Because men are expected to be breadwinners. They go into fields that return high amounts of wages.

Women are not expected to be breadwinners. They go into fields that are more social and self-validating and have flexibility, and also pay less.

I once read a report that detailed that men's reported happiness goes up as they get older and eventually retire while women's goes down almost as an inverse. 

Men wont take on careers in the social services until those careers are not red flags to potential partners. ",profOhk,2014-09-18 12:29:47
"Exactly.  There is a cultural norm that men strive for high paying jobs, even if it takes struggle or danger.  End of thread.  It's amazing how 4,000 years of consistent human history can be so misunderstood.",bwik,2014-09-18 19:55:31
[deleted],,2014-09-18 20:19:25
"I don't think it's any gender's fault. If anything is to blame it's the massive advance of technology that has radically changed our societies needs in really a 200 year time after thousands of years of relative stagnation. 

I think it's when people enforce traditional gender roles that issue really becomes a problem. This is obviously an issue that goes beyond data because it requires so much self-reporting. ",profOhk,2014-09-18 21:14:46
[deleted],,2014-09-18 21:25:12
"&gt;Social problems are never the technologies's fault.

What? ",profOhk,2014-09-18 21:31:43
[deleted],,2014-09-18 21:38:42
"&gt; Tech can do all sorts of amplification, purification, and revealing; but it's all on us.

Like I said, it's no ones fault. But tech can amazingly make things worse. ",profOhk,2014-09-18 22:30:35
[deleted],,2014-09-19 08:32:51
I didn't downvote you. I don't really understand why anyone would need to justify going beyond data. It's obvious that data can't explain what's going on because it's a massively complex issue.,profOhk,2014-09-19 10:07:31
"It just blows my mind that people with PHD's go around spouting the 75 cent ""statistic"" knowing that it's just simple means. It's so unscientific, even for fields that are already really unscientific. ",profOhk,2014-09-18 12:23:06
"Ha!  My uncle is an MD PHD and has, for years, been vehemently misquoting and misrepresenting statistics to ""show"" that video games are addictive and create murderers.  He has had enough of my father's and I's constructive criticism that he actually took up a research position with the US Navy to ""prove it"" to us.

So far he has actually moderated his opinion a bit...but just because you're highly educated doesn't mean that you won't believe what you *want* to believe.",,2014-09-18 13:58:41
"I know it. I don't believe for a minute Obama doesn't have at least some understanding of statistics. When he repeats the $.75 factoid, it's because he is wanting to pander. ",profOhk,2014-09-18 14:08:16
What a coincidence! Why is this so?,Bromskloss,2014-09-18 12:11:32
"It wasn't exactly easy to find the courses from the original link, so here they are for the lazy:
[Statistics One](https://www.coursera.org/course/stats1)
[Computing for Data Analysis](https://www.coursera.org/course/compdata)",JustMeAndMyCats,2013-09-16 08:36:33
"Thanks, tbh I probably wouldn't have started, without those links!",JiggerD,2013-09-16 22:22:35
"Speaking of statistics on coursera, I just finished 'mathematical biostatistics bootcamp', part one from Brian Caffo at Hopkins. It's fully archived and a great course! Part 2 starts 9/30. ",manic_panic,2013-09-16 18:17:42
"I did that one last year, and loved it. Can't wait for part 2!",srkiboy83,2013-09-17 00:20:42
"I also did ""Case-based Introduction to Biostatistics"" but it was the most disappointing courses from Hopkins.",lustikus,2013-09-17 01:25:59
"Ahh, this is my time to learn R... 
Now to keep my eyes open for a class in analysis of high dimensional data",gubbin,2013-09-16 17:25:39
be aware that Computing for Data Analysis is quite challenging.,lustikus,2013-09-17 01:24:18
"Both courses are great! The upcoming ""Data Analysis"" also uses R a lot, and it's a great continuation of ""Computing for Data Analysis"".

""Social Network Analysis"" starts in October, I also recommend it wholeheartedly.",srkiboy83,2013-09-17 00:10:01
"Does anyone know how much difference/overlap there is between the two. It seems like stats 1 is covering all the basics while teaching R, but not sure what Computing for Data Analysis seems to be covering?

I'm trying to figure out if I should just take one or the other, or both at the same time. I already have some background with introductory stats from my finance/accounting degree but that was 12 years ago and am a bit rusty, so could use a refresher.

Would anyone be able to give insight on whether it's worth taking the Computing for Data Analysis at the same time as Stats 1?",NimbleBodhi,2013-09-23 09:44:56
"That's an excellent, educational column.  Thanks for posting it!",pandemik,2012-10-30 08:48:22
"On the topic of politicization of statistical illiteracy, has anybody reflected on [these analyses suggesting primary vote manipulation](http://www.themoneyparty.org/main/wp-content/uploads/2012/10/Republican-Primary-Election-Results-Amazing-Statistical-Anomalies_V2.0.pdf)?  Is media is deliberately ignoring this claim (as a conspiracy theorist may suggest), or is it rather a simple lacks of statistical literacy to understand their methods?",ATG77,2012-10-30 10:50:51
"It's been beaten to hell in this subreddit.

http://www.reddit.com/r/statistics/comments/tg0pv/i_know_little_about_statistics_and_my_friend/

http://www.reddit.com/r/statistics/comments/126y46/hey_rstatistics_step_up_your_fucking_game/

http://www.reddit.com/r/statistics/comments/11ydmt/20082012_election_anomalies_results_analysis_and/

http://www.reddit.com/r/statistics/comments/1279wd/xpost_askscience_need_a_scientific_analysis_of/",Neurokeen,2012-10-30 11:10:23
Much presh.,ATG77,2012-10-30 21:03:36
"It is neither ignorance or the media ignore the ""report."" The report is terrible, totally flawed and widely debunked. It keeps coming back because the people who believe in it want the reason their guy lost to be the result of cheating, not the fact that their guy simply lost. ",iacobus42,2012-10-30 12:45:08
[deleted],,2012-10-30 19:44:46
Neurokeen provided a link to at least 4 places on r/statistics alone where it has been beaten to death. It has been beaten to death in several other places on reddit as well (r/askscience and a few others besides r/statistics). I and all of the others who have commented on how it is total crap haven't just written it off. Well reasoned and detailed debunks have been posted in many places (including places directly linked from here). It does not make sense to repeated debunk what is so clearly crap when it is trivial to find the commentary. ,iacobus42,2012-10-31 09:15:54
[deleted],,2012-10-31 10:19:43
"My 'beaten to death' was supposed to reflect 'beating a dead horse' as a simple subreddit search would have turned up multiple results. I wouldn't say myself it's widely discredited as it's a fringe conspiracy theory, and it would require wide attention to be widely discredited. However, there are major shortcomings with the analysis that the authors fail to address which make it pretty invalid as a standalone assessment, such as the inability of the authors to address reasonable confounds as well as failure to address the fact that in most cases of suspected fraud results were in line with polling results.",Neurokeen,2012-10-31 17:22:12
"Stack-ranking. 

The proces of benchmarking employees, then firing the lowest 20% of them, and then repeat that process.

It results in employees being afraid of getting fired, rightly so, and working less efficient and becoming more stressed. Etc etc.

Stack ranking.",Icanflyplanes,2014-04-09 23:06:39
How is this misunderstanding statistics?  Isn't this misunderstanding human nature?,FullSharkAlligator,2014-04-10 14:44:53
"It is both Obviously, but cutting the worst 20% and expecting to increase output is misunderstanding to the extreme.",Icanflyplanes,2014-04-11 00:40:10
Ooh a mind map! Now I know everything!,,2013-05-31 09:51:13
[deleted],,2013-05-31 08:29:52
"Ah, works now, thanks. ",Ayakalam,2013-05-31 09:26:12
"FTFY: ""You Will Need"" -&gt; ""I have heard of"" / ""I like""

Cool tree graph. Is there a way to export mindmaps to RDF, maybe with [DOAP](https://en.wikipedia.org/wiki/DOAP) or just http://schema.org/SoftwareApplication ?",westurner,2013-05-31 10:52:52
"You should add Theano (symbolic math expression compiler) under GPU or efficiency...

http://deeplearning.net/software/theano/

",gaurdianofnations,2013-05-31 13:35:59
"""504 Gateway Time-out"". 

Doesnt seem to be a valid link...",Ayakalam,2013-05-31 08:14:24
works for me,somkoala,2013-05-31 08:33:07
Has anyone had much luck with foreach? I tried it for a bit and it didn't seem to speed things up at all.,,2013-06-01 17:40:29
Well it's about time.,RA_Fisher,2012-03-08 09:28:53
"Happy cake day, RA!",,2012-03-08 15:16:42
Thanks! I didn't even realize it was my cake day! I think in it's honor I'll invent Latin square design!,RA_Fisher,2012-03-08 19:30:47
"His writing is also still extremely relevant and *extremely* technical. He was amazing because he connected great insight to formalism. His paper on the permutation test should be required reading for anyone considering graduate school in statistics. 

Victorian era science (which I would say Fisher did much to end) relied on lengthy rhetoric and diatribe to establish scientific facts. Researchers like Charles Darwin and John Snow, collected much data but didn't know how to summarize it in a way that could convince. Furthermore, research then was based on this elite circle of gentlemanly honor. I understand Fisher eschewed that and did much to destroy Pearson's work who was on the inside of such circles. Pearson was an extremely bitter and jealous man and Fisher all but crushed his reputation. Take a look at ""The Lady Tasting Tea"" for some background on that.

Presently, I feel that we overrely on statistics in this era and, soon, there will be new modes of rhetoric of which statistics can support, but are less ""the meat and potatoes"" of the analysis.",,2012-03-08 09:50:30
What's the title of the paper on the permutation test?  Is it *Recent Progress in Experimental Design*?,OhDannyBoy,2012-03-08 11:31:58
"The Theory of Linkage in Polysomic Inheritance

Author(s): R. A. Fisher

Source: Philosophical Transactions of the Royal Society of London. 

Series B, Biological Sciences, Vol. 233, No. 594 (Jun. 19, 1947), pp. 55-87

Published by: The Royal Society

Stable URL: http://www.jstor.org/stable/92328

Note only 5 sources are cited, one of which is his own work.",,2012-03-08 11:50:25
Thank you!,OhDannyBoy,2012-03-08 11:55:22
"Not to take steam away from Fisher, but I think the way of determining his ""influence"" by simple using the number of results in google scholar is a little ridiculous

One could just as well search for Pythagorean theorem, Fourier analysis, or exp(x) and consider those as citations for Pythagoras, Fourier, or Euler.  ",hella_bro,2012-03-08 14:21:54
Descartes is obviously the most influential since everything we're talking about right now uses letters of the alphabet to represent constants and unknown variables.,rottenborough,2012-03-08 14:48:23
"I agree. I'd guess the author realises that though, I get the impression the post is more about highlighting how often Fisher's methods are used and how important it is to modern science.",the_birds_and_bees,2012-03-08 15:18:12
"It's clearly a boundary artefact. Fisher was the last mathematician who had a big cross disciplinary influence, and this happened sufficiently recently that his articles are on JSTOR.

If he'd been born 40 years earlier he'd be in the same boat as Karl Pearson who has only 1,000 citations for discovering PCA.",DoorsofPerceptron,2012-03-11 04:58:08
Every biology paper owes him as far as statistical analysis is concerned. Too bad he went off the deep end with his eugenics stuff.,,2012-03-08 10:47:31
Most influential methodologist...,picu,2012-03-08 14:20:40
"(sorry, this is only partially in response to OP's blog, and more of a general rant)

I find all this hate for frequentist statistics puzzling. For example, this statement by journal editors:

&gt;the problem is that, for example, a 95% confidence interval does not indicate that the parameter of interest has a 95% probability of being within the interval.

Exactly! But that's a feature, not a bug. Sure, Bayesian analysis will give an answer in terms of posterior probability - at the cost of requiring, in advance, a fully specified probabilistic belief over parameters. Frequentist methods don't require any such step, and in turn yield answers that cannot be interpreted as posteriors, but merely as estimators with good ""procedural"" (in the sense of long-run error rates) properties. Unsurprisingly, stronger assumptions lead to stronger conclusions - so what? One still must make the case that making the stronger assumption is actually worth it.

I, for one, am not convinced that the actual scientific process of incorporating prior information and updating beliefs with new evidence should be formally quantified in a Bayesian way (even though Bayesian updating can perhaps serve as a stylized model of such process). The reality of science is simply too complicated to be pigeonholed into the machinery of conjugate priors and Monte Carlo samplers. One could still make case for Bayesian analysis in particular applications on pragmatic grounds, and there's nothing wrong with that - but this kind of almost religious crusade against ""flawed"" practice of NHST, such as demonstrated by journal in question, is just stupid.

Because when we actually parse the usual complaints against p-values and hypothesis tests, the problem is not with underlying statistics, which work just as designed to. The problem is (or is claimed to be) that scientists interpret such statistical results in incorrect ways. In other words, the problem lies instead with prevailing standards of methodology and ""rhetorics"" (the way scientists persuade others about correctness of their results) in the field. But then so any solution, and arguments in its favor, must ultimately be related to the realm of philosopy, or perhaps sociology of science, not statistics. Banning p-values is just a sideshow distracting from real issues.",ivansml,2015-03-02 09:17:00
Picketty &amp; Taleb both comment on this in relation to economics. Your point is really well-made thank you. ,IndustrialstrengthX,2015-03-02 11:56:02
"To contribute to your point: I don't think social scientists would be comfortable with informative prior any time soon. All of our knowledge are built on very shaky ground, we disagree on the sign of effect, let alone the effect size. And that's another reason why Bayesian won't replace NHST any time soon.",selectorate_theory,2015-03-02 17:38:08
"Hm, I work with behavioural game theory, where experimental paradigms are re-used without much alteration and yield (relatively, of course there is variation) reliable results. Cognitive psychology is another field with similar features.",simoncolumbus,2015-03-02 21:58:26
"Lab experiment is an exception I concur. I was talking about studies of political institutions, various economic policies, etc. that differ widely across contexts.

EDIT: Now of course, even with lab experiment, there's a question of whether the reliable results is due to the fact that all experiments are run on the same subject pool? (US undergrad)",selectorate_theory,2015-03-02 22:09:21
"There has been significant research on cross-cultural variation in behavioural game theory, and the evidence is quite clear that culture matters in a huge way. Unfortunately, I do not know of any original studies that have randomly sampled cultures to truly estimate its effect, but there are [experiments in small-scale societies](http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=362321&amp;fileId=S0140525X05000142) that indicate the range of values, [in large cross-cultural samples](http://www.sciencemag.org/content/319/5868/1362.short) that show qualitative differences, and [meta-analyses with moderation by cultural factors](http://pps.sagepub.com/content/8/4/363.short). 

That's not to say that within Western societies, results don't generalise very well. But you are right, generalisation beyond that may be very difficult or impossible to reach; certainly for most findings for which large cross-cultural studies are not going to happen.",simoncolumbus,2015-03-02 22:43:31
"Naturally, everyone says this, so probably there is some kind of argument as to why the journal decided to do this instead of asserting stricter editorial control. (After all that'd be its primary responsibility.)

So, instead of having statisticians go over the articles to make sure their inference is sound, they just want to mechanically improve the quality of their publications. Which says that they have probably enough submissions to chose from and not enough money (or they like their profit margins high, thankyou).

Finally, instead of pointing out how statistics is okay, was okay and the problem is with how scientists will do everything to get published ... the first part should be just dropped and everyone should just point out that both journals and scientists have perverse incentives to publish as many articles as they possibly can without seriously hurting their credibility to maximize income. Against this is the nobleness of science. Which is fine and dandy, but that won't fix the problem, maybe on the very very long long long run.",Pas__,2015-03-02 17:56:05
"&gt;The reality of science is simply too complicated to be pigeonholed into the machinery of conjugate priors and Monte Carlo samplers.

The reality of science is simply too complicated to be pigeonholed into the machinery of null hypothesis testing and minimizing Type I error probabilities. This is not to rehash some ancient Bayesian vs. Frequentist cage match that is long since settled: I agree with you, pragmatically use the tool that is most appropriate. BUT, you need to recognize that the NHST paradigm has it's own strong (perhaps implicit, if not, at least widely taught) assumptions. The biggest of these implicit assumptions being that if p&gt; 0.05 then we cannot say the difference is statistically different, *and therefore it is best to assume the true difference is zero.* The second biggest, and intimately related, is the failure to recognize sampling variability and that if p&lt;0.05, but power is small ([""p = 0.06""](http://andrewgelman.com/2014/11/17/power-06-looks-like-get-used/)) your estimate is likely to be wildly inflated.

Here's my scientific philosophy, a surprising and counter-intuitive claim (e.g. telepathy, ""priming"" in social experiment, himmicanes vs hurricanes) should require stronger evidence than a mundane or intuitive claim. What better way to do this than to formalize the process with prior belief? Under the ""objective"" approach you are advocating, all hypothesis are treated equal, all experiments come into the world as newborn infant theories, a tabula rosa waiting for a p-value to be writ upon it. In other words, you are advocating for the statistical philosophy that brought us [right turns on red](http://andrewgelman.com/wp-content/uploads/2014/12/1154-The-Harm-done-by-tests-of-significance.pdf). ",tekelili,2015-03-02 19:03:19
"&gt; The biggest of these implicit assumptions being that if p&gt; 0.05 then we cannot say the difference is statistically different, and therefore it is best to assume the true difference is zero.

No, we should say that we don't have strong enough evidence against the true effect being zero. But if the uncertainty of the estimate is so high that we don't have even enough evidence to rule out zero effect, why should we take it seriously?

&gt; The second biggest, and intimately related, is the failure to recognize sampling variability and that if p&lt;0.05, but power is small (""p = 0.06""[1] ) your estimate is likely to be wildly inflated.

If there is lot of noise, it takes a large signal to reject null hypothesis, as it should. Gelman argues that if we assume true effect is small, then *within* the ""population"" of significant estimates, these will tend to be inflated. But most of the time the estimate will not be significant, precisely due to low power (in other words, this just a different manifestation of publication bias). And I find the idea that we can always formulate hypothesis on ""true"" effect size, without seeing the data, rather circular - I mean if I think true effect is small, then I will consider a large estimate overinflated, even without any arguments about statistical power. This sounds like a technique that can be easily used to dismiss any evidence questioning prevailing consensus.

&gt; Here's my scientific philosophy, a surprising and counter-intuitive claim (e.g. telepathy, ""priming"" in social experiment, himmicanes vs hurricanes) should require stronger evidence than a mundane or intuitive claim.

Sure. But that's what people already do! Especially in social sciences, you don't change your beliefs after reading one study. 

&gt; In other words, you are advocating for the statistical philosophy that brought us right turns on red[2] .

Now that's just a straw-man. There's nothing in frequentist statistics, or hypothesis testing specifically, that would prevent researchers from pooling data or conducting meta-analyses. On the other hand, I don't necessarily think Bayesian methods would help here - would there be some official time series of belief distributions for the effect of ""turn-after-red"" rule, updated each time a new study is published? Or would each researcher just recycle priors used in earlier studies, to follow ""conventions in the literature""? I think the latter is more likely.",ivansml,2015-03-03 07:14:28
"&gt;Now that's just a straw-man.

This is not a straw man. It is a not insignificant number of accidents, injuries, and deaths for which a widely held, if erroneous, assumption of NHST is partially responsible. It is a real life example of policy makers not taking seriously enough weak evidence against the true effect being zero. For another non-straw man demonstrating the perils of not recognizing that the effect size is likely smaller than you estimate, consider the potentially millions of people taking new drugs with potentially harmful side-effects on the basis of [optimistic effect size estimates](http://www.newyorker.com/magazine/2010/12/13/the-truth-wears-off).

&gt;Sure. But that's what people already do! Especially in social sciences, you don't change your beliefs after reading one study.

Really? Last I heard there was a [pretty serious crisis](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) in scientific inference which is particularly acute in the social sciences. You must also not be listening to NPR and the joys of Shankar Vedantam or Radiolab or, god help us, TED, all regurgitating the latest bullshit sporting a p&lt;0.05 as though it were the gospel truth.

&gt;On the other hand, I don't necessarily think Bayesian methods would help here - would there be some official time series of belief distributions for the effect of ""turn-after-red"" rule, updated each time a new study is published? Or would each researcher just recycle priors used in earlier studies, to follow ""conventions in the literature""? 

Now I think here you are getting to the heart of the matter on which I think we can both agree (even if I'm still surly, sorry, but you can take the person out Philadelphia, but you can't take the Philadelphia out of the person). The problem is statistical illiteracy. A research simply recycling priors used in earlier studies to follow the conventions in the literature is as bad as research simply assuming that lack of significance is the same thing as zero effect. It is a failure to think carefully, which can happen under any paradigm. For me, who claims to be at least moderately statistically literate (if we were English majors, I'd say I've read Melville, but not Proust), the prior is a rather convenient way of formally expressing my reluctance to accept counter-intuitive wisdom. I admit you could probably do a similar thing by simple adjusting your alpha to reflect the strength of evidence it would take to convince you. But the problem is that the vast majority of people do not know how to think about probability and to the extent that the Bayesian paradigm more closely aligns with people's intuitive understanding of probability (even if the mechanics are more difficult), I think it is a better way to go about things.",tekelili,2015-03-03 09:32:23
"&gt; I was defining CI by procedure; that is, a CI is what you get if you compute the sampling distribution and take the 5th and 95th percentiles.
My correspondent was defining CI by intent: A CI is an interval that contains the true value 90% of the time.

Your first definition most certainly isn't a realistic definition, because then in asymptotic intervals who don't have nominal level 1-alpha you will still claim these as 1-alpha CI's rather than approximate CI's.  

A confidence interval for theta should obviously have theta in the definition somewhere.  I mean the confidence interval is well defined, there is really nothing there to argue about.

&gt; If you use the second definition, the problem with CIs is that our standard way of computing them doesn't work, at least not in the real world.

I disagree with this conclusion.  You would observe less than 90% because of publication bias, not because of problems with CI's.  You can get publication bias regardless of what inferential procedures you're using.",anonemouse2010,2015-03-02 07:07:45
"Yes, publication bias is (another) reason published CIs are less likely to contain true values.",AllenDowney,2015-03-02 07:28:34
"I haven't read your whole thing just the admendment. Techinically CI are random intervals (as in no arguments about the definition exist), they have no real numbers to pin down per se. Something like this : (x bar + z_alpha...., ...) etc etc . 

**However** in practice people just call the computed numbers for a sample the CI. After they are computed they're not random anymore so it doesn't make sense to assign probabilities to them, the true value is either in the calculated interval or it is not.
",fanofDK,2015-03-02 12:15:38
"You got a typo in the first summary point:

&gt; 1. ""...the null hypothesis significance testing procedure (NHSTP) is invalid..."".   ""We believe that the **p&lt;0.5 bar** is too easy to pass and sometimes serves as an excuse for lower quality research.""

I don't think anyone will counter that p &lt;0.5 is indeed far too easy to pass :)",TomatoAintAFruit,2015-03-02 10:07:13
Thanks.  Will fix asap.,AllenDowney,2015-03-02 10:41:01
"Thanks for posting your response.

I don't agree with all of it, but I do like much of it, especially the Bayesian part. My major gripe is that p-values and CIs are defined based on infinite, exactly the same (except for so called 'random' differences), repetitions of an experiment. They do not mean the same thing that Bayesian credible intervals mean, but they are interpreted in the same way all over the place. 

Here is a quote from *Statistics Manual*, by Crow, Dravis, and Maxfield:

&gt; [W]e can make use of the sampling distribution of x_bar to construct an interval about x_bar and, with a specified confidence, state that mu lies in the interval. For a 95% confidence interval, the statement will be correct 95% of the time, in the sense that if we drew many samples and computer x_bar and the corresponding confidence interval for each of them, in about 95 cases out of 100, the confidence intervals would contain mu.

The probability of the parameter being in or out of the CI is either 1 or 0 (it either is or is not inside the CI).

I see the appeal in p-values and CIs, of course. They are fast to calculate, and they were easy to do before computers. The problem is that p-values and CIs are based around parameters, which have no physical or causal meaning. Too many papers and people think that the story is over when a small p-value is found. Why would the story be over? Just because you have confidence in a parameter doesn't mean anything for the real world.

The proper next step is to take significant parameters, map them to real-world (testable) things, and determine the causal mechanisms. This is my biggest gripe about p-values. They've started to stop the process too early. This doesn't happen everywhere, of course, but you do see it a lot.

The best fix, in my view, isn't necessarily to switch to all Bayesian. It's to always try to predict unseen data with the models and parameters that have small p-values. If the predictions do well, then we can have more confidence in the parameters.

Here's an example of what I am talking about: [Mobile phone use and glioma risk](http://www.bmj.com/content/344/bmj.e1147). That's an excellent paper that took many epidemiological studies on cells phones and brain cancer (all of which, no doubt, had small p-values and pleasing CIs) and forecast the risks forward to generate the expected data (based on non-physical/non-causal parameters!).

Consider just how powerful that paper is! No fancy or esoteric discussions of infinite retrials, CIs, correlation is not causation, etc. are needed. All that happens is that the same models that were created with past data are used to predict future data. If those models do not predict, then they are wrong. It's as simple as that!",PhaethonPrime,2015-03-02 06:57:12
"It is not correct that so-called frequentist inference is based on so-called frequentist foundations of probability.

So-called frequentist inference is based on the idea that sampling distributions of statistics and pivotal quantities are the basis of inference.

""Frequentists"" use sampling distributions, Bayesians don't.
Bayesians use priors, ""frequentists"" don't.
""Frequentists"" says theta hat is random and theta isn't.
Bayesians say theta is random, and theta hat isn't (after the data are seen).

Those are the real differences.  Infinite sequences have nothing to do with it.",berf,2015-03-02 14:40:23
"&gt; They do not mean the same thing that Bayesian credible intervals mean, but they are interpreted in the same way all over the place. 

""We can invent something else to say, but not something else to think"" -- E.T. Jaynes (approximate).",midianite_rambler,2015-03-02 09:02:43
What a great quote. I still need to read Jaynes' work as a whole...,PhaethonPrime,2015-03-02 09:04:25
Good points.  Thank you!,AllenDowney,2015-03-02 07:25:21
"&gt;The proper next step is to take significant parameters, map them to real-world (testable) things, and determine the causal mechanisms. 

Probably should be the other way around? Isn't this how half of the problems start?

Start with a real world question. Define it in terms of a model with some paramters, then do hypthesis testing on paramters",fanofDK,2015-03-02 12:08:02
"I think that depends on the problem or area of science. It's ok to do stats first to try to detect correlations or potential causal relationships. As long as parameters aren't considered a final product of a scientific effort, then I'm happier.",PhaethonPrime,2015-03-02 15:02:59
"There is something I really don't get. You seem to go on a tangent sometimes that p-values and conventional stats are bad (or worse than bayesian stats). For advanced machine learning and statistics, I would certainly agree, but for psychology and any other applied science, is that really a point we (the stats community) should be trying to make ?

As a guy working in neuroscience, let me tell you, most people publishing in those fields don't have a strong background in stats, and don't care about having the best stats. They just want A measure that tells them whether condition A is different from condition B, and, if so, where they are different. That seems like the exact kind of question that a p-value test (and other more complicated tests) is made to deal with. Certainly, you could do more, and in some specific situations, sophisticated stats paradigm are adopted to do some clever analysis, but for most analysis, basic p-value testing seems more than enough to me

no ?",Hairy_Hareng,2015-03-02 14:00:07
"I understand, and I am very sympathetic to people in neuroscience, or any other field of science, who are not interested in fancy new statistics; they want something simple and effective so they can focus on what they really care about.

That is exactly the motivation for the recommendation I made, which is

1) Focus on effect size, which is almost always what we really care about.  And frankly, if you want to stop there, that's fine with me.

2) Report CI or SE as a way to quantify the variability in the estimated effect size due to random sampling.

3) Report a p-value as a way to confirm that you are probably not getting fooled by randomness.

Hope that makes sense.",AllenDowney,2015-03-02 16:24:19
"Yeah that's completely reasonable. I doubt that journals would follow that, but we will see.

It sometimes feel like people in neuroscience, unless they are from a strong math background (and there are many) are just afraid of math. Oh well",Hairy_Hareng,2015-03-02 23:12:06
"I have always had a problem with p &lt; 0.05 being the threshold that everyone uses. The only thing I found that discusses it is the following: http://www.jerrydallal.com/lhsp/p05.htm

It seems it was originally introduced for computational simplicity, without much reasoning beyond that.",zphbtn,2015-03-02 14:11:38
"Granted that ""**the** probability"" that a confidence interval contains the parameter cannot be shown to be .95 (for example), I don't believe adopting 0.95 as your subjective probability would make you susceptible to a [Dutch Book](http://en.m.wikipedia.org/wiki/Dutch_book). 
BTW, Bayesians do not have a monopoly on subjective probabilities. ",dmlane,2015-03-02 12:31:26
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Dutch book**](https://en.wikipedia.org/wiki/Dutch%20book): [](#sfw) 

---

&gt;

&gt;In [gambling](https://en.wikipedia.org/wiki/Gambling) a __Dutch book__ or __lock__ is a set of [odds](https://en.wikipedia.org/wiki/Odds) and bets which guarantees a profit, regardless of the outcome of the gamble. It is associated with [probabilities](https://en.wikipedia.org/wiki/Probability) implied by the odds not being [coherent](https://en.wikipedia.org/wiki/Coherence_(philosophical_gambling_strategy\)).

&gt;In [economics](https://en.wikipedia.org/wiki/Economics) a __Dutch book__ usually refers to a sequence of [trades](https://en.wikipedia.org/wiki/Trade) that would leave one party strictly worse off and another strictly better off. Typical assumptions in [consumer choice theory](https://en.wikipedia.org/wiki/Consumer_choice_theory) rule out the possibility that anyone can be Dutch-booked.

&gt;

---

^Interesting: [^Coherence ^\(philosophical ^gambling ^strategy)](https://en.wikipedia.org/wiki/Coherence_\(philosophical_gambling_strategy\)) ^| [^The ^Autumn ^of ^the ^Middle ^Ages](https://en.wikipedia.org/wiki/The_Autumn_of_the_Middle_Ages) ^| [^Donald ^Duck ^\(Dutch ^comic ^book)](https://en.wikipedia.org/wiki/Donald_Duck_\(Dutch_comic_book\)) ^| [^Millennium ^\(Holland ^book)](https://en.wikipedia.org/wiki/Millennium_\(Holland_book\)) 

^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cp25zn8) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cp25zn8)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2015-03-02 12:32:27
"Non-mobile: [Dutch Book](http://en.wikipedia.org/wiki/Dutch_book)

^That's ^why ^I'm ^here, ^I ^don't ^judge ^you. ^PM ^/u/xl0 ^if ^I'm ^causing ^any ^trouble.",LittleHelperRobot,2015-03-02 12:32:06
"I believe p-values should be included in articles and more education should be provided to consumers in regards to the typical 0.05 cutoff. Provide the p-value and the reader can determine whether or not they find it significant at whatever alpha-level they care to.

P-values are evidence...they are not answers.",Dr_sh0ck,2015-03-03 10:51:52
"There is nothing wrong with frequentist statistics except that it is (still) the dominant paradigm.  Thus almost all of the cheaters are cheating on frequentist statistics.  If Bayes ever becomes the dominant paradigm, the cheaters will cheat on that.  To the extent that ""big data"" is an emerging paradigm that is neither (anything goes), the cheaters cheat on that too, although most ""big data"" is so unprincipled, that it is not clear what even would constitute cheating.",berf,2015-03-03 16:37:05
"I had a problem with p-values in the law.

Sometimes judges will disallow scientific evidence as 'not significant' if it has a p-value below .05.

Yet, civil cases are supposed to be decided based on ""the preponderance of the evidence"". That means ""more likely than not"".

That sounds to me like a p-value of 0.49
",hsfrey,2015-03-02 17:53:14
"lmao

",I-want-to-be-better,2015-03-04 18:12:30
"You're going to alienate some of your potential readership with the language you use to distinguish natural and social sciences (i.e., ""hard"" vs. ""soft"").",smfinator,2015-01-30 10:16:17
"I agree, especially since it seems like informal language for a published work. I'm surprised it got past editors in an abstract at a new journal who are likely trying to build a reputation of excellence. 

Even still, I'm sure most people across the research spectrum will agree at least to some extent with [this comic](http://xkcd.com/435/). Though I have always thought that the ""harder"" research problems lay to the left hand side of that scale, since the systems are inherently more complex. So I can certainly understand the appeal of sociological research, just based on the inherent difficulty of the problems faced. I can also understand the perspective that we may not yet have the tools available to address such problems in a truly satisfactory way...but when the problems are so important, it certainly behooves us to still attempt to address the issues to the best of our current abilities. I guess, given my above perspective, it makes sense that I have landed somewhere in the middle studying computational biology.",DrGar,2015-01-30 10:51:27
"XKCD?  Monday's release dove-tailed quite nicely with the PeerJ article:

http://www.xkcd.com/1478/",RandomFlotsam,2015-01-30 14:31:23
"[Image](http://imgs.xkcd.com/comics/p_values.png)

**Title:** P-Values

**Title-text:** If all else fails, use ""signifcant at a p&gt;0.05 level"" and hope no one notices.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1478#Explanation)

**Stats:** This comic has been referenced 7 times, representing 0.0140% of referenced xkcds.

---
^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_co676nm)",xkcd_transcriber,2015-01-30 14:31:43
They had me at questionable research practices.,DrHappyFunTime,2015-01-30 08:22:53
This is like Reading Rainbow of data mining. Good find.,,2014-01-13 04:33:21
i love it. thank you,heaven__,2014-01-13 08:52:18
this is pretty cool,letseatlunch,2014-01-13 11:06:32
wow this is fucking amazing. you rock. thank you,watersign,2014-01-12 19:32:32
"Yeah, the same guy is also [very active](http://stats.stackexchange.com/users?tab=reputation&amp;filter=all) (see the top row) on stats.stackexchange.com - in 5 months in the middle of last year he got 18K rep. Having actually had *one* month in the last three years where I earned reputation at that kind of pace, I don't know how he managed it for almost half a year.
",efrique,2013-05-10 20:49:04
"Chernick strikes me as a little off. See his responses to his book reviews on Amazon for an idea. His answers on stats.stackexchange typically (IMO) aren't that good he just puts in ridiculous volume (at least, he used to; I haven't seen him around much recently).",NOTWorthless,2013-05-11 22:58:54
"His volume is far lower over the last six months; what few answers I've seen recently were fine.

In any case I may not be in the best position to talk; my own reputation-per-answer isn't all that much higher; his is over 20, my own - last I checked - was only a little more than 26.
",efrique,2013-05-12 00:30:44
"Active yes, and the reviews are a good comparison to his view on statistics, but at least the first couple pages of reviews are him mostly just evaluating the different authors characterizations of bootstrapping, since he wrote his own book about it.",msdrahcir,2013-05-10 21:02:34
"wow there's an entire book on negative binomial regression? geez.

Also, it's hard to understand how people have this much time to create original content on the internet. I'm glad there are people who do though!",,2013-05-11 04:28:12
So epic.  I actually read like 5 pages or eviews... Don't know why but I enjoyed.,PissinChicken,2013-05-10 20:18:36
Every American family should own a copy of Laura Chihara's Mathematical Statistics with resampling. ,TheFrigginArchitect,2013-05-11 04:43:53
Thanks for sharing.,antisyzygy,2013-05-10 19:01:29
A lot of the people who write these really intense book reviews write reviews for publication and just post them on Amazon,MZITF,2013-05-11 10:37:29
Thanks. I'm a grad student who can really benefit from these. ,Spamicles,2012-07-18 19:27:11
"Heard an interview with this guy on npr. It's a shame, but not without ancient historical precedent. Greeks have always taken their numbers seriously. ",alien8r,2012-02-01 15:34:56
"I realize this is a bit late, but the story is very interesting.  I believe the interview you were referring to is here: http://www.thisamericanlife.org/radio-archives/episode/455/continental-breakup",darkforest,2012-02-07 14:55:00
Yes and thank you ,alien8r,2012-02-07 15:07:20
I'll get concerned when they give the confidence interval.,Bitruder,2011-11-23 06:48:18
I'm sure all members of this subreddit will agree that it is a great shame how few members of the public are familiar with the idea of a confidence interval. Their inclusion in news stories is largely nonexistent with the exception of the odd political poll.,capnza,2011-11-23 21:51:05
"&gt; exception of the odd political poll

Lucky USians, over here in Germany no exceptions are made.
",derwisch,2011-11-24 02:01:21
"What bothers me a lot on economy news: ""Institute X releases inflation estimates of 6.57% for this year.""

TWO DECIMAL PLACES IN AN INFLATION PREDICTION? WOW! I'd surely love to take a look at your data and models.",calsaverini,2011-11-23 06:50:08
"Again: no confidence intervals, no certainty.",,2011-11-23 09:29:20
"They are usually calculated, but not given in any news (as the ""public"" doesn't understand them). 

Therefore, the mean value is given and can have as many decimal places as is needed.",,2011-11-23 19:43:18
"Ah, now I know I am among my people.",tavernkeeper,2011-11-23 11:32:56
"To be fair, they normally report the number of confirmed deaths, which is normally a lower boundary but may well be an exact number.
",derwisch,2011-11-23 07:00:20
"Usually, but some times you have to deal with fractions.",im_only_a_dolphin,2011-11-23 09:05:10
"Well, okay, the situation is deliberately a bit silly but I have said vaguely similar things to my TV about other kinds of reported numbers; I had a nod of recognition at the kind of objection

The hidden extra panel (hover over the red button) made me laugh, though.
",efrique,2011-11-23 12:14:48
http://www.kaggle.com/c/march-machine-learning-mania,econometrician,2014-03-17 04:59:35
"Considering there's 67 rather than 63 games, the contest is impossible to win.
",jen1980,2014-03-17 11:02:31
"&gt; If you assume all brackets are equally likely, the odds are one in 9 quintillion. However, they are NOT equally likely -- for instance, a #16 team has never beaten a #1 team.

The probability depends on the available information. If you don't know that a #1 tends to beat a #16 team, then the probability of guessing correctly is 1 in 9 quintillion. If you do know such things, the probability gets large. If you have lots and lots of additional information, you might end up being certain or almost certain about which team will win.

I wouldn't say that the probability clearly and objectively _is_ one number or another, without specifying the information that is taken into account.",Bromskloss,2014-03-17 15:06:47
"Help me out here, please

I calculate 1/2^63 and get 0.000000000000000000108420217249,
that doesn't contain a 22.  What gives?

I used excel and the scientific calculator on my computer.

I tried .5^63, 1/(2^63) as well getting the same results.  ",LateOnsetRetard,2014-03-18 11:03:18
"I am doing a PhD in computer vision/machine learning and I'm so fucking sick of ""Big Data.""

Here is a recent example that stuck in my mind (skip to 5:10)  
http://www.bloomberg.com/video/a-mad-food-scientist-makes-our-waistlines-disappear-F2bPslRFQkOJyMDV7prf5w.html

Dude showing his spreadsheet for running a restaurant. Interviewer: ""So you're basically bringing sorta like Big Data into the kitchen?"" So much rage.

Top tier journalism from Bloomberg.",gmiwenht,2013-12-12 13:25:09
"I know what you mean. I almost never work with any data sets larger than 100 MB, but all I get is blank stares when I tell people about my analysis work.

But say the words ""big data,"" and suddenly everyone understands and wants to tell me about some news story they saw about analytics.",mapItOut,2013-12-12 15:13:33
"my father use to say: when people use a curse words, it means that they lack the vocabulary to express what they actually want to say.

""Big Data"", has this feeling for me... there are very few constructive uses for this phrase that don't say, ""I barley know what I'm talking about.""

It is bothersome, but the more I hear other people say it, the more I realize how grateful I am to have the vocabulary to NOT use that dreaded phrase.",datalies,2013-12-12 16:56:14
"The worst part is we have descriptors for large datasets.

Are there a metric crapton of variables? Then it's high-dimensional data. Are there lots and lots of subjects? It's a large-N dataset. These cases are handled differently and yet I've seen the word ""Big Data"" used for both. It's not a particularly useful category.",Neurokeen,2013-12-13 08:41:38
"I have you tagged as ""Future Me""  
  
Any advice on pursuing a PhD?",thinksthoughts,2013-12-12 17:45:11
"Ask me when I graduate. Halfway done, and my advice is *DON'T DO IT*, but that is the joy and the pain of doing a PhD. It sucks for a long time, but hopefully it will all be worth it.",gmiwenht,2013-12-12 18:11:25
"Do it for the right reasons; because you like science, you enjoy the topic, the supervisor does work you're interested, you're curious and want to learn.

Don't do it because you think people will respect you (they won't) or that you get something out of being called (doctor).

Decide, based on how you find your PhD if you want a career in research. Don't be stubborn and say ""this is what I'm doing"" you might not like it. I hate the facebook campaign ""i fucking love science"". Science is sitting in a lab for hours collecting data, rewriting the same paragraph over and over because because its a difficult thing to explain, being rejected by reviewers that don't understand your work and sitting in front of a computer for hours trying to figure out why your model won't fit the data even though it *should fit*. Nobody *loves* this.

Its hard work and no-one is going to do any of it for you. But, at the same time, getting to involve your creativity on problems and learn stuff you never thought you'd be able to grasp as well as work with some of the most interesting people you're ever likely to meet... There aren't many careers like this.",jazimov,2013-12-13 06:26:24
"Hats off to you sir. A fine response.Probably the toughest part of doing a PhD is the psychological turmoil of it. The thoughts of how useless your work seems to be in the grand scheme of things. This is definitely not made easier by all the glorification of science there is around the media these days. One feels a pressure to have spectacular results and, as a grad student, that's just so unlikely. Even as faculty, you are usually just a small piece (albeit considerably larger than the grad-student-sized piece) of a BIG puzzle.",iheartennui,2013-12-13 07:42:35
i can make spreadsheets. i must be da data science'r,watersign,2013-12-12 15:27:08
"I think this is my problem with the label as it is commonly used. It's a CV buzzword, or something used to make people sound smart and with-it in a world where the word ""data"" (often as ""big data"") is thrown around as though its a hot new religion, with ""Data Scientists"" as the prophets.

I think the biggest problem is that anyone can call himself a data scientist because no one really knows what that *is*. There's just a general idea they have a lot of data and do some regressions. I'd really like to see some more specific terms develop. There are some people who are good at data acquisition, some people who are good cleaning and organizing data, some people who are good at modeling, and some people who are good at all of those. It would be nice to see labels specific to those roles develop, so if someone was a ""Model Builder"" (it's a crummy name, but just for example) you would know they specialize in building models to interpret data. If someone was a ""Data Acquisition Specialist"" you could expect them to be talented in writing scripts or programs to scrape data, or pull data from various media.

It feels as though role specificity would get rid of some of the fuzzy nature of being a data scientist. Someone might be proficient in multiple roles, which is fine, but I think this is a case where listing two or three more clearly defined roles is better than listing just one without distinct meaning.",Dragonheart0,2013-12-12 19:22:49
"i agree, HR people are just fucking dumb. I use excel and R for data analysis purposes and im in the midst of teaching myself basic stats/probability along with more R/Python types of dealios but I don't feel comfortable calling myself a data scientist because i dont have a high level grasp of mathematics or stats nor programming. the term analyst is fitting for most of these roles, IMO. 
My definition of a data scientist is someone who is proficient at programming in C, Java, Python, R, Matlab, knows enough math/stats to get by in the real world. ",watersign,2013-12-12 21:41:35
"Yeah, I actually feel the same way. It seems like a ""data scientist"" should indicate a level of proficiency at all levels of the process, programming, math/statistics, analysis, etc. But that's clearly not the case for many people who use the label, so the term just kind of loses meaning.

I'm actually in a similar situation as you. I would love to be at a level where I feel qualified to term myself a data scientists, as we're defining it here, but I just don't have that whole package yet.

But I guess when I have the skills I hope to one day acquire my experience and skills will hopefully speak for themselves, whatever I term my positions. I don't know if that's true... I just hope it works that way.",Dragonheart0,2013-12-12 22:29:46
"I don't really agree with your definition of data scientist. The programming languages are just tools to implement the math behind manipulating the data; why are they important in being a data scientist? Data scientists should know the best ways to use math and statistics to solve problems with large sets of data. To me the math/statistics is WAY more important than knowing C, Java, Python, R or Matlab. ",neutralvoice,2013-12-12 23:40:01
"I'd go one further: the *Science* part is what is key here. The stats are just another tool - albeit very important ones - to test hypotheses. This is AKA ""science"".",SupaFurry,2013-12-13 07:50:31
[deleted],,2013-12-13 08:20:35
"I disagree. Source: Myself (PhD biologist).

- Why does it have to be *achival* datasets?

- More broadly, why would someone analyze a dataset? To answer a question; specifically, to test a hypothesis. Data analysis is a distillation of the scientific method (which is why I dig it so much).",SupaFurry,2013-12-13 08:24:59
[deleted],,2013-12-13 08:32:06
"One could argue that hypothesis testing / experiment design does not reveal insights into how the world works.

I can randomly assign patients to treatment and control groups, effect an intervention, measure group differences, apply simple statistics, and show that the treatment had an effect.

That doesn't mean I understand the mechanism by which the treatment had an effect.

I can go deeper, say it is a drug study and I'm gathering data at the molecular level.  So now I'm measuring both outcome (reduction in symptoms) and biological processes (changes in levels of say some neurotransmitter).  But that doesn't prove that the changes in the neurotransmitter level has a causal role.  Rather, I have a model that states the neurotransmitter has a causal role, and the data are consistent with that model.

Bottom line is that the definition of science and who a scientist is should not be constrained to be only people who design and evaluate experiments.  There's plenty of things that are not amenable to random assignment and experimental manipulation but are still studied using the scientific method and statistics.",wil_dogg,2013-12-13 11:30:01
[deleted],,2013-12-13 15:18:02
"Your definition of science is overly constrained.  

Check any general definition of ""science"" and you'll see that it does focus on the acquisition and organization of knowledge, and may emphasize the ""how and why"" (mechanisms of why the world works the way it does) more than the ""does it or does it not"" (simple prediction that creates value but doesn't depend on a causal mechanism discovery to declare success) but a basic definition of science does not set the bar that science must involve experimentation.

Stated differently, experimental design is a subset of the methods of science, but it is not a necessary method for one to be engaged in science.

Finally, when you say that you don't see data scientists engaged in science, you are basing that on a limited sampling of what data scientists do.  I can assure you that there are plenty of data scientists who are trained in the scientific method, who are seeking to understand the mechanisms, and when practical we are designing experiments and actually creating data, not just analyzing archives.

Source:  PhD, been doing science, data, and statistics in the domains of psychology, economics, and healthcare analytics for 30 years.  I'm that guy who everyone takes their research questions to so I can sort out their method requirements, help them to clarify their hypotheses, select appropriate measures and statistical models, do the analysis, bound their interpretations, and figure out what to do next.",wil_dogg,2013-12-14 07:59:30
[deleted],,2013-12-14 08:42:53
"Actually, we don't agree, your definition is constrained and does not conform to the commonly accepted definition of what science is.  

What are doing is attempting to graft rhetorical methods from other ways of acquiring and organizing knowledge to win a debate point.  But you lost the point when your initial definition was overly constrained.

Perhaps you should reflect on that causal mechanisms of what causes you to paint the world with such broad brushes and take a chauvinist approach to defining the domain you are showing interest in.",wil_dogg,2013-12-14 09:04:32
most data'r science jobs want programmnig skills though. but i agree,watersign,2013-12-13 14:26:40
"If you're teaching yourself ""basic stats"" you're probably not the guy who should be defining ""data science.""",spalunk,2013-12-12 22:15:51
"why not? if retards in HR can do it, why can't someone who has a broad understanding? ",watersign,2013-12-13 14:25:24
"It's another buzzword for ""statistics"", which is now sexier than ""informatics"" (bio-or otherwise).

That being said I do actually like ""Data Science"" since it accurately reflects what I do as a bioinformatics scientist more than the term ""bioinformatics"" does.",SupaFurry,2013-12-12 13:12:07
"If this was true you would know that computational science encompasses all the informatics, without the downside of being tied to profit/marketing.",homercles337,2013-12-12 14:11:27
[deleted],,2013-12-12 20:06:46
[deleted],,2013-12-13 06:35:35
"""Data Engineer"" seems to be used by employers to describe people that are building/maintaining large systems used for data analysis",1337bruin,2013-12-13 10:21:29
"I work for one of the hip ""big data"" companies and I really haven't run into any folks who argue about data size or whether or not they can ""code in hadoop."" That's something I would expect from mid level IT guys who really have no idea what they are talking about.

The data scientists I work with are math, stats, cs, etc PhDs who have extensive domain knowledge of the problems they set out to solve. Most of our customers are also relatively well informed about what data science actually is. ",spalunk,2013-12-12 22:25:44
"All science is data based, being a ""data scientist"" is a rather meaningless phrase as the implication would be that the need for the qualification is that there are scientists who don't spend their entire careers rigorously collecting and interpreting data. You don't get people calling themselves ""life biologist"" or a ""chemical chemist"".",jazimov,2013-12-13 05:57:10
Is there *really* a difference between as statistician and a data scientist? ,AlpLyr,2013-12-13 06:07:07
"I'm reminded of something I just read in Prof. Cox book on inference. 

In standard statistics, the natural flow is mostly question-data-analysis.
In data mining, on the other hand, the flow tends to be data-analysis-question.

 ""Big Data"" research tend to stem from machine learning and data mining to a large extent, so it does make sense that people often get stuck on the first two parts of the flow. It doesn't help that the data mining flow is actually much more challenging to set up a stringent framework for, due to all the flexibility and influence of prior information it allows. 

That's not to say that data mining is the wrong thing to do, just that it's very tricky to do right.  ",GrynetMolvin,2013-12-13 13:34:54
"Yes, the key word is ""science"", but also it's ""data"".",rz2000,2013-12-12 17:14:19
"No its not, its profit or marketing.  Please point me to a ""data science"" post that does not mention profit or marketing.",homercles337,2013-12-12 12:30:29
I've been trolled! I feel like a real member of Reddit now :-). ,t_rex_tullis,2013-12-12 12:37:55
"[here you go](http://storytime.booklamp.org/2012/03/01/thematic-currents-visualizing-the-thematic-flow-of-a-book/) - we don't use the ""data science"" buzzword, so I don't know if it counts in what you're looking for.",datalies,2013-12-12 13:26:06
Well then you should use the term Computational Scientist.,homercles337,2013-12-12 14:08:01
[deleted],,2013-12-12 15:02:47
We have a winner!,SupaFurry,2013-12-13 07:52:50
"Great find, thank you for posting. I'm following the accounting playlist now, always knew too little about that stuff :)",karmakit,2013-05-26 04:55:41
I like it. I didn't know where the chi-square distribution comes from and found a video that explained it very well and in context.,thefrontpageofme,2013-05-28 06:46:39
"Damn homie, ever hear of playlists?!",,2013-05-25 10:59:23
"Isn't ""Data science"" just ""Statistics""? Why do we use a different name to introduce confusion...",s2s,2013-05-12 18:03:45
"The words probably mean different things to different people.  As someone who will be soon taking a position as a data scientist (coming from a research background in natural sciences),  I view data science as the engineering version of statistics/machine learning.

I used to get very annoyed by the parallel jargon developed by machine learning and data science people, but I've decided to just live with it.  It's good for marketing and money.  Statisticians stand to gain a lot if they tap into it that marketing.",circumstantialeviden,2013-05-12 19:07:47
"Agreed. There are pretty substantial differences between statisticians and data scientists. My current position is a mixture of both and it's interesting to see the two different dynamics. I'd argue that a Data scientist is much more focused on the actual data (which sometimes doesn't involve very interesting statistical analysis), whereas a Statistician is, likely, much more interested in the model. 

In my, albeit short, experience I've noticed that the best in the fields are those that integrate both. A great data scientist (i.e., someone who is really great at programming and understanding software implementation) with mad statistical skills is currently a very hot commodity. Companies are willing to pay well for a great data scientist with pretty good statistical skills. I think companies value the data part more, but that's just my opinion.",econometrician,2013-05-13 05:59:16
I think that [Larry Wasserman's blog post](http://normaldeviate.wordpress.com/2013/04/13/data-science-the-end-of-statistics/) gives an interesting take on data scientist vs. statistician.,valen089,2013-05-12 20:22:46
"This is just marketing.  I have been a computational scientist for over 10 years (if i include my postdocs) and never heard this term until recently.  Its mostly economics/market/social media related.  From what i understand, it was envisioned as doing more than ""data analysts.""  Maybe more method development?  Big companies like Google, Yahoo, The Facebook, etc like to use this term, but you wont hear it in academic/non-profit circles.",homercles337,2013-05-12 20:17:23
"Marketing folks. Just like the new term ""Infographic"" is just another word for graph. 

Statistics has many related terms such as business intelligence, data mining, big data, data analytics, web analytics, data crunching, data slicing, modeling, etc.",,2013-05-12 21:25:32
"I'm not an expert in either, but I think of statistics as a set of mathematical tools whereas data science sounds like a process - as the description says, taking data through an investigative process and then reporting the results without losing precision and rigor.",agbortol,2013-05-12 18:23:49
The project part of this class sounds great.  It looks like they brought in some  academic and industrial partners and got the students to work on real problems with real data.  But it looks like the lectures blast through an insane amount of material; it's hard to imagine that they were anything but a waste of time.,AllenDowney,2013-05-15 15:45:56
The site seems to be down.,azth,2013-05-12 10:09:42
"post this to ~~[/r/pics](/r/pics) , [/r/woahdude](/r/woahdude) , /r/math, [/r/shutupandtakemymoney](/r/shutupandtakemymoney)~~ ... actually post it everywhere, this is just plain awesome...",gmiwenht,2012-03-25 16:48:51
Thank you! It took quite a bit of time.,maviegoes,2012-03-25 17:14:45
"Wow, that's pretty awesome.",prionattack,2012-03-25 15:28:59
You might be a Statistician if...,Distance_Runner,2012-03-26 17:52:50
I have an idea that this post is just preaching to the choir. It needs to go to the sinners.,Blacksburg,2011-10-20 15:35:27
"Some of these actually made me chortle, in my lonely apartment.",,2011-10-20 17:20:54
"I feel bad for the authors of #10. Sure the scale could have been done better, but it's not done hilariously bad.",ProbablyCanadian,2011-10-21 08:52:48
The lead author of the paper for graph #10 (Broman) is the guy made this list.  I think he was just pointing out something he wished he did differently.,mmmmatt,2011-10-22 12:11:33
"The diagonal lines are funny but these kinds of graphs for comparing distributions are not uncommon, and perfect straight lines tell the story more clearly than following the suggestion to include ""The results are indistinguishable"" in the text.

That being said, I wonder why we preach subject matter scientists to use Bland-Altman diagrams for comparison of measurements (in order not to waste the top left and bottom right of the scatterplot area) and then cling to the Q-Q-plot ourselves instead using, for example, [hanging rootograms](http://www.maartenbuis.nl/software/hangroot.html).
",derwisch,2011-10-21 03:42:52
"A good rule of thumb is to make sure your graphs have a high [data-ink ratio.](http://www.infovis-wiki.net/index.php/Data-Ink_Ratio)

An easier rule of thumb is to never ever use pie charts.

",bubbles212,2011-10-20 14:35:17
"I beg to disagree on that one. Maybe they have gone out of vogue, but if done right, they are powerful graphical representations. I offer for example the pie charts that Florence Nightingale made.
http://en.wikipedia.org/wiki/File:Nightingale-mortality.jpg",Blacksburg,2011-10-20 15:37:45
"In January of 1855, how much larger (as a percent) were the deaths from disease(blue/gray) versus wounds(red)?

I can't tell from the areas.  I get the story she is telling (i.e. disease kills more people than wounds), but I'd rather see it as stacked bargraphs.",blossom271828,2011-10-20 17:17:57
"That is not a pie chart! It's a polar area diagram, which is a very useful way to display periodical data.",Megasphaera,2011-10-21 04:07:08
that's more of an angle histogram or whatever right?,whao,2011-10-20 21:39:44
"The Nightingale charts are a lot more difficult to read than if you were to create a few bar graphs with the corresponding variables. Angles and areas are always going to be much more difficult to gauge through the human eye versus height or length. 
",elus,2011-10-20 21:53:28
I'm with Blacksburg. It depends on the audience and what you're trying to convey. People who can't grasp bar charts and scatter plots know what a pie is. Perhaps that's why they are overused and used incorrectly.,TerraByte,2011-10-20 17:48:35
Just get the [Rev. Bayes](http://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif) across your chest. Bitches love the Rev Bayes... ,percafluviatilis,2013-11-30 11:53:04
"There is a problem: this portrait is almost certainly of someone else named ""T. Bayes"", not of our reverend Thomas Bayes.

Could I suggest Pierre-Simon Laplace instead?",BT_Uytya,2013-11-30 15:02:11
"Balls. Better change all my lecture slides then. 
",percafluviatilis,2013-11-30 15:10:07
"Even if that is Bayes, that portrait is ugly. I think I could talk a tattoo artist into giving me a sexy [Ada Lovelace](http://en.wikipedia.org/wiki/Ada_lovelace) though. Not exactly prob/stats, but coding is a large component of my work.",shaggorama,2013-11-30 15:22:01
Hot diggity. ,TotallyNotTheCIA,2013-11-30 19:43:43
"I don't understand the downvotes I'm seeing here, it's a question I haven't seen posted here before. Some people will say this sub is for more academic posts, but where else would you have a better chance of finding  people with such tats than here?

To OP, tattoos aren't really my thing, but I like art/drawing, so I've doodled some things I think would make cool paintings. I'm a biostatistics grad student, and I spent some time trying to come up with a picture that encompasses the field.... My favorite picture I've come up with is a normal distribution curve, but formed by a strand of DNA with a pair of dice being rolled underneath it. 

",Speed_of_Light,2013-11-30 11:01:02
"Nice! I'm not a biostats guy myself, but I like what you've got going there.",shaggorama,2013-11-30 11:12:42
"And hey, ever been to the bars with ""academics"" during a conference? This topic would be too sophisticated under *those* circumstances. ",griffer00,2013-11-30 17:12:35
http://terrencechan.livejournal.com/347214.html,trip_2020,2013-11-30 11:55:37
"I dunno if it's just the unfortunate placement or what, but that kinda looks like a cross section of a nipple. ",naught101,2013-12-01 19:50:37
"I don't have any tattoos myself, but I've always wanted to get some mathematical statement in graffiti-style writing. Something like ""The Mixed Partials Are Equal"".",manygrams,2013-11-30 11:06:10
Have considered getting one of Bayes' theorem,JustMakesItAllUp,2013-11-30 11:14:35
"Yup, that's definitely on my list. Was also thinking of the formula for the beta pdf or kolmogorav's probability axioms. I think it's hard to make a straight up equation look good though. Maybe I should just get a drunken sailor?",shaggorama,2013-11-30 11:20:15
a drunken sailor and his random walk?,JustMakesItAllUp,2013-11-30 11:46:11
"Yup, that's the idea. Maybe throw this in there somewhere:

    x_t = N(x_{t-1}, s2)

Maybe he could be stumbling over the formula. Or maybe his uniform could say ""MARKOV"" on it or something.",shaggorama,2013-11-30 12:29:12
"I find your abuse of ""="" dismaying. ",randomsample,2013-11-30 12:56:43
:D,shaggorama,2013-11-30 13:37:23
"Don't be that guy.

http://plover.net/~bonds/cultofbayes.html",snapetom,2013-12-01 06:24:16
"Well shit, i've been a member of a socialist party for years and i've never been informed that Bayes theorem is only for right-wing economists and PUAs. ",importantgap,2013-12-01 11:35:30
Better update your priors. Maybe neg them a little.,shaggorama,2013-12-01 20:24:30
"I only skimmed it, but your article seems to be about geek culture and arm-chair statisticians, not about serious practitioners. The article rants against, lesswrong.com, the singularity institute, and PUAs, all topics that have absolutely nothing to do with Bayesian inference. 

Bayesian inference has gained a lot of traction in the last two decades as the necessary computations have become increasingly tractable with more powerful computers and new sampling and inference algorithms. If you want to present a serious criticism about Bayesian inference, this article isn't it. Frankly, all this article did was make me wonder what your credentials are to have taken it seriously.",shaggorama,2013-12-01 07:23:09
"Oh it's definitely not a serious criticism of Bayes.  It is, though, a serious mocking of any twat that would consider getting it tattooed. ",snapetom,2013-12-01 08:18:13
"This article is very poorly-researched (for example, the section dealing with *Proving History* looks like author haven't even read the book he is criticizing). 

Author uses name-calling and ridicule instead of arguments. He/she didn't address any specifics; instead he/she claims that everything is obviously wrong.

*Please* stop thinking that this article is a vaild critique of something.",BT_Uytya,2013-12-02 04:34:18
"Someone in my department has this equation tattooed on their arm: http://en.wikipedia.org/wiki/Lewin's_equation . Not really statistics, though. More of a conceptual equation.",GreenFrog76,2013-11-30 14:51:28
awesome.,dokodemodoor,2013-11-30 22:04:09
"I second the Bayes' Theorem, but here are some other good candidates:

1) Likelihood ratio. ""Something is worth your attention iff LR != 1"" was a great insight for me.

2) de Moivre's equation: accuracy of mean increases as a square root of number of observations (see: http://nsmn1.uh.edu/dgraur/niv/TheMostDangerousEquation.pdf )

3) Something related to the regression to the mean: an in-joke, linear regression with correlation coefficient (see chapter 17 of *Thinking: Fast&amp;Slow* for explanation). I have no idea about how to express this in a tattoo form, but if someone comes up with something, it would be great.

4) Something MCMC-related.",BT_Uytya,2013-11-30 15:15:05
"&gt;something MCMC related

Like Markov Chain Monte Carlo sampling, tattoos also require a burn-in period. ",bubbles212,2013-11-30 20:57:39
"I have a bunch of freckles on my shoulder, and have considered fitting a loess curve or a spline to them with some confidence bands",thestatsmancan26,2013-12-01 13:04:56
"The [Bertrand paradox](https://en.wikipedia.org/wiki/Bertrand_paradox_%28probability%29) (how do you define randomness?) would lend itself to a decent tattoo, with some careful design.

Edit: while on the topic of Bertrands, ""[Turtles all the way down](https://en.m.wikipedia.org/wiki/Turtles_all_the_way_down)"" could make a great tattoo. It's not directly related to stats, but it is to logic, upon which probability theory is based.",naught101,2013-12-01 20:07:49
p&lt;.05 ,ieatglass,2013-11-30 12:26:07
the tramp stamp of statistics!,farsass,2013-11-30 17:01:00
"&gt; p &lt; .10

What a whore...",shaggorama,2013-11-30 17:22:15
"No, we want everyone to forget about that particular ""rule"". ",randomsample,2013-11-30 12:57:11
If the P is low reject the Ho... ,tacojohn48,2013-11-30 13:42:41
THat is ***completely*** awesome! ,okcukv,2013-11-30 17:23:13
God damned frequentists...,beaverteeth92,2013-12-02 20:19:01
Throw this one at /u/manygrams,shaggorama,2013-11-30 14:09:37
No tatoos but I named my bong Markov.,,2013-11-30 15:26:13
Your future state depends only on your current state and the contents of the bong.,shaggorama,2013-11-30 15:46:37
"I don't have any tattoos, but I promised myself when I get buff I was going to get a tattoo of the proof of the CLT on my back.",,2013-11-30 17:06:37
One of my professors used to say he had [Slutsky's theorem](http://en.wikipedia.org/wiki/Slutsky's_theorem) as a tramp stamp. None of us ever checked though.,Bishops_Guest,2013-11-30 22:13:16
"I've got ∇ (called nabla, or the gradient operator) on my calf. Not specifically statistical, more from my atmospheric science days.",likelihoodtprior,2013-12-01 06:59:13
"Obligatory link to the [Science Tattoo Emporium](http://discovermagazine.com/tags?tag=Science+Tattoo+Emporium). Not sure if there are any stats ones in there, but there are a few maths ones.

I don't have any stats tattoos yet, but I do have a couple of sciency ones - one of a Fibonacci sequence and spiral, and one of Gosper's Glider, both hand drawn. For probability, a couple of dice with a binomial distribution in the background would be pretty cool.",naught101,2013-12-01 19:53:27
"That link is awesome. I like the glider: if you ever wanted more ink, you could just build out a more complex grid of automata :)",shaggorama,2013-12-01 20:22:27
"Off-topic because these are not statistics:

One lecturer told us that he would get Euler's identity, if he was to get one.

If I was getting one, it would be the triangle inequality. NO DIVERSIONS, NO SHORTCUTS",jmmcd,2013-11-30 16:37:36
"No tatoos, but I got pick up lines.",,2013-11-30 17:02:25
Get a Gaussian. ,doomsday_pancakes,2013-11-30 18:10:46
"[I've seen tattoos like this](http://terrencechan.livejournal.com/347214.html) and I think they look boring. Thanks for chiming in, in any event!",shaggorama,2013-12-01 07:25:29
"Yeah, that doesn't look so good.",doomsday_pancakes,2013-12-01 07:41:58
I think any visualization that needs axes probably won't work great as a tattoo.,shaggorama,2013-12-01 07:59:17
Stats is a big area...  What specifically do you specialize in?  It might help to have more specifics... ,sjgw137,2013-11-30 19:41:57
"classification, regression, computational stats, applied math... ""data mining/data science."" I'm just a masters student, I'm not super specialized like a PhD or anything like that. ",shaggorama,2013-11-30 23:38:29
"Dude, I'd say go for the classics. Go for the Normal Distribution and/or the formula. 1/2 π ∫e^-.5x^2 *dx*",mybalzich,2013-12-01 00:35:29
http://www.reddit.com/r/statistics/comments/1rs782/anyone_got_any_probabilitystatistics_tattoos/cdqxtub,shaggorama,2013-12-01 09:43:26
How about a forest plot from your favourite meta analysis? Like the cochrane collaboration logo. ,dinamo88,2013-12-01 01:31:23
I've got Boltzmann's entropy formula on my arm,bosoning,2013-12-01 03:55:53
http://felixsalmon.tumblr.com/post/5373539255/some-guy-has-got-the-gaussian-copula-function,naught101,2013-12-01 19:57:26
"I know this is an old post, but I've been thinking of getting one too. I was thinking of getting a probabilistic definition of a miracle (and how they aren't that special). Something like

[;\mathcal{M} \equiv A(\omega) | Pr(A) &lt; \epsilon;]

Which would read ""A miracle is equivalent to the realization of some event such that its probability of occurring is less than some value epsilon."" I'd also like to play around with the fact that just because something has a probability of zero does not mean it's impossible.",Pseudo_Scientist,2014-01-12 15:25:24
"It's evolutionary game theory, not statistics, but I always liked the [Hawk, Dove, Retaliator](http://xcelab.net/rmpubs/baryplot-hdr-demo.gif) de Finetti diagram.",languagejones,2014-05-03 18:49:18
"If I was going to get a tattoo, one I might go for is the [Cochrane Collaboration logo](http://i.imgur.com/WDe89aB.gif).

From wikipedia:
&gt;The logo of the Cochrane Collaboration illustrates a meta analysis of data from seven randomized controlled trials (RCTs), comparing one health care treatment with a placebo in a forest plot. The diagram shows the results of a systematic review and meta analysis on inexpensive course of corticosteroid given to women about to give birth too early – the evidence on effectiveness that would have been revealed had the available RCTs been reviewed systematically a decade earlier. This treatment reduces the odds of the babies of such women dying from the complications of immaturity by 30–50%. Because no systematic review of these trials had been published until 1989, most obstetricians had not realised that the treatment was so effective and therefore many premature babies have probably suffered or died unnecessarily.

Basically each line shows the range of effectiveness for the treatment from each individual study and the dot on the bottom left shows what the result of the meta analysis.

It's a pretty cool little logo that actually conveys some really important data and a good example of how powerful statistics can be.

If that particular graph wasn't your cup of tea, I think some kind of graph or visualization of data might look better than a formula or text. My second choice might be this visualisation of [Simpson's Paradox](http://i.imgur.com/XGoTRCU.png).
",,2013-12-01 02:19:19
"I like the ""paradox visualization"" idea. Maybe I could do anscombes quartet or the monte hall problem (three doors with one open and a donkey's head sticking out?)",shaggorama,2013-12-01 07:28:35
Yeahhhhhhh! New Zealand!,OpponentCorn,2013-12-01 19:52:36
Um. The Cochrane Collaboration is British in origin as was Edward Simpson. But NZ is pretty good. I've been there a couple of times.,,2013-12-01 19:56:06
"Statistics: a life of deviance

If you want something a little less permanent, and want to give me a couple bucks: http://www.zazzle.com/statisticswithwit",ICrepeATATs,2013-11-30 13:16:59
"let's face it, between pres elections political statistics gets dull as hell. don't blame him...",,2013-07-19 21:27:01
"An Olbermann/Silver talk show about sports, stats, and politics is definitely something I'd tune in to watch. Perhaps if they convince Silver to switch to decaf before shows he'll be less sweaty in front of the camera. Push that to decaf Irish coffee and this show is golden.",Troybatroy,2013-07-19 20:40:48
Didn't they ban olberman from talking politics?,universl,2013-07-19 23:39:54
"""They"" will let Olbermann do whatever brings in ratings and ad-dollars.  Bringing in Nate Silver and sitting him next to Keith and asking them to not talk about politics doesn't seem like a good way to accomplish this.",Troybatroy,2013-07-20 13:19:41
"I dunno, I kinda love his awkward, nerdy composure in interviews.",,2013-07-20 13:18:12
"I look forward to people ""unskewing"" Sergio Romo's RBI just to contradict him",Mister_DK,2013-07-19 21:27:16
Do you even baseball?  ERA are the three letters you were going for.  Unless I'm wooshing right now.,FuckingLoveArborDay,2013-07-19 23:15:19
It's advanced and stands for Real Baseball Index.  It tells you how you really are at baseball.,YaoPau,2013-07-20 06:15:21
Even ERA has [been supplanted](http://en.wikipedia.org/wiki/Defense_independent_pitching_statistics) among baseball-stats geeks.,kmjn,2013-07-20 09:53:26
"My point was that Sergio Romo is a closer.  In six years in the majors, [he's had four at-bats](http://www.baseball-reference.com/players/r/romose01.shtml) and zero RBI.",FuckingLoveArborDay,2013-07-20 12:04:19
"You are wooshing.  My joke was mocking the unskewers having no idea what they are doing, so they would try to do it.


Hey, it was funny when I was 4 beers in.",Mister_DK,2013-07-20 14:52:11
More interesting work outside of election season. I have no doubt he'll be doing politics in 4 years.,mrpopenfresh,2013-07-20 09:19:17
"I can't find where I read it, but apparently ABC will be using him for politics as well (Disney owns ABC and ESPN), so we won't be losing his ""witchcraft"" for the next election.",random012345,2013-07-20 09:42:06
"This makes me ridiculously happy. Definitely a show the I'm already planning to watch religiously; particulaly looking forward to a discussion about Nate Silver's ""is a-rod paid too much?"" article on the first episode ;)",Fogrocket,2013-07-19 21:49:55
"For those of you interested in the pdf, you can check it out:

[here](http://www-stat.wharton.upenn.edu/~hwainer/Readings/Most%20Dangerous%20eqn.pdf)

or 

[here](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCsQFjAA&amp;url=http%3A%2F%2Fpress.princeton.edu%2Fchapters%2Fs8863.pdf&amp;ei=i_1WU49bqKmxBJjsgKgL&amp;usg=AFQjCNGi9sKq92YJQTDVsiHJO7JT3SDhZA&amp;sig2=MsGub1toqKW3tNMC3Q0DFw&amp;bvm=bv.65177938,d.cWc)

",Iamnotanorange,2014-04-22 16:39:07
Just link to the .pdf.,JimH10,2014-04-22 16:28:05
"I didn't want to break the link, but I'll post a direct link in the comments. ",Iamnotanorange,2014-04-22 16:37:35
"From below:

For those of you interested in the pdf, you can check it out:

here

or 

here

",stevo494,2014-04-23 04:14:13
"Meh. Point taken and known. Though I would argue that Statistics is one of the places where this misunderstanding is the least prevalent.

But...

The trial of pyx example is off. If you assume that the people measuring the coins were merely concerned with the min and max allowable value, than they had the correct approach. If they are only concerned with whether the bulk of the weight of the coins is too large or too small (which seems to be exactly what they were concerned about), then their approach is correct. If every coin is at most 401/400* its nominal weight, then the most weight that would be allowed would be (# of coins) * 401/400 and the least allowed would be (# of coins) * 399/400. Plus, the basic math in this example is wrong. If a guinea weighs 128 grains, then 1/400 of a guinea is not 0.28 grains, but 0.32 grains. There's no explanation for why the people measuring the coins would have cared about DeMoive's equation; they weren't concerned with variability, just the maximum and minimum allowable weights.
",hotandtiredanddry,2014-04-22 19:19:59
"Yes, silly error on the 0.28 vs 0.32 grains. But the point was that by allowing 0.32 variability on the weight of the stack of 10 coins, they are not allowing 0.32 variability on individual coins, rather root(10)*0.32. So, I think the point was that you get a wider range of coin weights than expected, which could make it easier for coin-weighers to scam the system.

What struck me was when reading the next paragraph - about a low variability leading to underestimation of the mean, and high variability leading to overestimation of the mean. That just strikes me as wrong, but maybe I'm not following?",red_concrete,2014-04-23 07:07:20
"&gt;But the point was that by allowing 0.32 variability on the weight of the stack of 10 coins, they are not allowing 0.32 variability on individual coins, rather root(10)*0.32. So, I think the point was that you get a wider range of coin weights than expected, which could make it easier for coin-weighers to scam the system.

This is a good point if you assume that they were concerned with variability. But the author's description of their rationale for weighing the coins is that they didn't want too little or too much gold going out. To me, this means they were concerned with minimums and maximums and not means. DeMoivre's gives you the uncertainty of the mean. I don't think it has anything to do with minimums and maximums.

I agree with you about the next paragraph. Seems like some words were left out of it.",hotandtiredanddry,2014-04-23 07:30:53
"I agree that the example is off and that last paragraph is totally confusing. 

If I understand this correctly, the masters of coin are basically overestimating the variability of the 100 coins, based on the sample of 10 coins. Is that right? 

If so, the problem wouldn't be using too much or too little gold. It should be the fact that the coins were sent back to the coinsmith unnecessarily. 

But maybe I'm a little confused here. ",Iamnotanorange,2014-04-23 08:30:51
"&gt; Though I would argue that Statistics is one of the places where this misunderstanding is the least prevalent.

I understand the point you're making here, but most people using statistics are not statisticians and can make this mistake pretty damn often (e.g. department of education in new york). 

PS: I don't get why people are downvoting you. 

",Iamnotanorange,2014-04-23 07:55:48
I agree with you. I should have said that this problem was understood by statisticians. I can think of several PIs that I've done work for who needed to be reminded of the issues of small sample SEs regularly.,hotandtiredanddry,2014-04-23 08:09:41
Ofcourse iReddit would break the links through their share button,stevo494,2014-04-23 04:14:41
Very cool article. Thanks for sharing!,Quant_Liz_Lemon,2014-03-14 18:58:18
"Where you say ""If we look at the log-normal by itself, the fit looks pretty good!""

... the following plot says the curve is a Weibull.

I presume that's a mislabelling.
",efrique,2014-03-15 01:55:22
"If I'm looking in the right place, the label is correct now.  Maybe the author (who is not me this time) fixed it.",AllenDowney,2014-03-15 06:45:41
"Sorry Allen, I'd assumed it was yours. Carry on.",efrique,2014-03-15 13:16:00
It was a mislabeling!,RA_Fisher,2014-03-19 21:35:04
"Took it for a test drive, so to speak. First comment is that having to scroll to read the entire line is incredibly annoying (especially when a select box pops up and I cannot move the window without answering but also cannot answer without being able to read the whole thing).

(it may be a word wrap functionality in R itself... I typically use R in Linux and am testing it on Windows so don't know if that's the case.... if so the tutorial should mention to turn it on)",gicstc,2013-09-27 18:39:35
Thanks!,beaver11,2013-09-27 18:13:00
my sources of procrastination on the internet are merging ...,,2013-03-18 10:22:52
The significance test for Lasso is pretty cool ! This will leads to very interesting interpretations of the Lasso models.,leviathanxs,2013-03-18 10:33:09
"1. You have observed some data (from an experiment, retail data, stock market prices over time etc)

2. You also have a model for this data. That is, a little computer program that can generate fake data qualitatively similar (i.e. of the same type) to the observed data. 

3. Your model has unknown parameters. When you try to plug some number values for these parameters into the model and generate some fake data, it looks nothing like your observed data.

4. Bayesian inference ""inverts"" your model such that instead of generating fake data from fixed (and wrong!) parameters, you calculate the parameters from the observed data. That is, you plug in the real data and get parameters out. 

5. The parameters that come out of the Bayesian inference are not the single ""most probable"" set of parameters, but instead a probability distribution over the parameters. So you don't get one single value, you get a range of parameter values that is likely given the particular data you have observed. 

6. You can use this probability distribution over the parameters (called the ""posterior"") to define hypothesis tests. You can calculate the probability that a certain parameter is greater than 0, or that one parameter is greater than another etc. 

7. If you plug the posterior parameters back into the original model, you can generate fake data using the parameters estimated from the real data. If this fake data still doesn't look like the real data, you may have a bad model. This is called the posterior predictive test. 

",glutamate,2013-01-04 06:08:54
"Excellent explanation...I might add that often you can't analytically write down the probability distribution because summing/integrating the normalizing constant over all possibilities gets our of hand.

In these cases you resort to estimation with an iterative process that uses only the ratios of probabilities, dividing out the normalizing constant.* This means you aren't getting an explicit described probability distribution over the paramaters but a method for simulating possible sets of unknown paramaters from the distribution.

Bayesian Maximum Likelihood inference is the process of determining the most likely paramaters through lots of simulation...ie you simulate the paramaters a bunch of time and then average (or something...this part has some gotchas) to find the most likely value.  

This might be a bit much for a 5 year old but in practice it is a key distinction because estimating the likelihood of unlikely paramaters or dealing with bi modal distributions is hard and you don't get things like confidence intervals in the way many people expect.  

*see Markov Monte Carlo(MCMC)/Metropolis Hastings/Gibs Sampling",micro_cam,2013-01-04 08:41:46
"&gt; Bayesian inference ""inverts"" your model such that instead of generating fake data from fixed (and wrong!) parameters, you calculate the parameters from the observed data. That is, you plug in the real data and get parameters out.

This doesn't sound accurate.  You could probably ELI5 better with some discrete example in balls.",anonemouse2010,2013-01-04 06:48:39
"Do you mean ""discrete"" as in inferring discrete/binary parameters? I really don't like explanations of Bayesian **statistics** that use discrete parameters.",glutamate,2013-01-04 06:57:59
"I meant an urn problem.  But whether the parameter space is continuous or discrete is not really an issue that should bother you.

However, I still think this is a bad explanation.",anonemouse2010,2013-01-04 10:18:20
Wow thanks for that. This opens up so many possibilities for me.,DoorGuote,2013-01-04 06:28:18
"I forgot to mention the Prior, but you'll have to wait until you are six before I can explain that. 

Programs like BUGS and stan (and one I am working on called Baysig) are very flexible ways of defining Bayesian models that can be inverted in the way I described.",glutamate,2013-01-04 06:33:49
"I'm six. Can you explain, please?",misplaced_my_pants,2013-01-04 19:38:41
See samclifford’s explanation to OP.,glutamate,2013-01-06 06:18:42
"Does the complexity of the model developed in Step 2 make any difference? If it's a power function describing infiltration rate of different soil types, is that not complex enough for this type of rigorous analysis?",DoorGuote,2013-01-04 06:47:52
[deleted],,2013-01-04 07:00:56
"&gt; A model too complex will take a very long time to do calculations on, and the results may not be useful.

The ""not useful"" part is not true. The Bayesian approach deals well with complex models; they don't overfit in the way non-Bayesian approaches tend to do. The reason is that one does not choose one single best setting of parameters; instead all of them are considered possible, with various ""degrees of possibility"" measured by their posterior probabilities. Complex models are simply better at prediction than simple ones, but are more difficult to design and compute with.

Depending on the design, if you want to not only predict but also to assign meaning to the model's parameters (i.e. interpret them), a complex model may make it more difficult (kind of like a neural network is more difficult to interpret than a linear model), but it is also possible to design interpretable complex models.",Coffee2theorems,2013-01-04 12:22:30
"Often model complexity determines wether you can analytically apply bayes theorem to get the posterior or must resort to posterior estimation via MCMC methods. 

This sounds like it is a perfect use case in that the model and the prior distributions are probably well established from numerous studies. 
",micro_cam,2013-01-04 08:48:57
"That's a matter of some debate but Bayesian methods are thought to be immune or at least less sensitive to overfitting. 

[See this blog post](http://andrewgelman.com/2011/12/david-mackay-and-occams-razor/)",glutamate,2013-01-04 07:00:31
The Kardashian-Coco Test,cottagecheese420,2013-01-04 15:41:22
[deleted],,2013-01-04 06:56:28
"I seem to understand this when given examples about coins or medical tests, but what do you do when there aren't known probabilities?",shivasprogeny,2013-01-04 20:04:40
"glutamate's explanation is very good.  I am working on a free book (thinkbayes.com) that attempts to answer this question.  If you have done some programming, you might find it helpful.",AllenDowney,2013-01-04 06:41:29
"thank you for making a free book available
",,2013-01-04 06:58:17
"TIL Some things are not meant to be explained like you're five. I should have said: ""explain to me like I'm a 25 year old water resources engineer who first heard the word 'Bayesian statistics' on reddit this year""",DoorGuote,2013-01-04 19:18:46
"Like 5?  No, I don't think so.  Probability is too abstract.  A lot of college freshmen don't understand it.  This doesn't have anything to do with Bayes.

If you can grok probability distributions, then Bayes is easy.  Probability is the correct measure of uncertainty.  All uncertainty can be described by probability.  Anything you are uncertain about has lots of possible values, each value has a probability, and the collection of those values is the probability distribution.  If you were 5, I might talk here about the probability of what the weather will be tomorrow, because you might have absorbed the notion of probabilistic weather forecasting from TV weather reports (but a lot of adults don't really understand what weatherpersons mean by 50% chance of snow tomorrow).  I wouldn't even mention Bayes rule to a 5 year old.  I would just say that as new data arrive, your probabilities change, which is obvious.  Then I would say that there is a bunch of math that you can learn about when you're older that says exactly how to calculate that.",berf,2013-01-04 14:53:25
"&gt;50% chance of snow tomorrow

and what does it really mean?",y2kerick,2013-03-16 18:44:56
"Who knows what what meteorologists think about that?  IANAM (IAAS).  [Here's what the weather channnel says about that](http://weather.about.com/od/c/g/chance_of_rain.htm) and
[here's what Wikipedia says about that](http://en.wikipedia.org/wiki/Probability_of_precipitation).  But, putting on my Bayesian hat and speaking *ex Cathedra*, whatever ""snow tomorrow"" may mean, and you may have your own personal eccentric definition (that raises no issues), you are uncertain about that and probability is the correct measure of uncertainty (that's the axiom of subjective Bayes), so there is (implicitly) a probability distribution in your head that describes this.",berf,2013-03-17 05:18:19
"If I may piggyback:  

I'm an undergrad stats major. I really like the idea of Bayesian stats but my department doesn't offer any courses in it. I want to become educated in case I end up applying to grad programs in stats. How can I go about educating myself?",keepitrealcodes,2013-01-04 08:33:25
"If your department offers graduate courses, you'll likely find some introduction to Bayesian statistics there. ",kimolas,2013-01-04 18:42:31
"Does your university offer cross institutional study as an option? That is, can you take a class from another university and have it count as part of your degree? And is there anywhere near you that offers a Bayesian course?",samclifford,2013-01-04 23:20:58
"Bayesian statistics is (almost entirely) application of Bayes rule, which is just the definition of conditional probability rewritten.  Hence, if you know the math, you can read the statistics for yourself.  If you are really solid with calculus, you should be able to just read an applied Bayes book, like [Lewis and Carlin](http://www.amazon.com/Empirical-Methods-Analysis-Second-Edition/dp/1584881704/) or some such.  Hmmm, on second though, if you are really solid with calculus and probability theory, so you have to take the probability course first.  Does your department have a course taught out of [Ross](http://www.amazon.com/First-Course-Probability-8th/dp/013603313X/) or some such?  Take that, then read an applied Bayes book.",berf,2013-03-17 05:25:15
[Here](http://bayesianthink.blogspot.com/2012/08/understanding-bayesian-inference.html) is a good write up &amp; tutorial explaining Bayesian methods,broccolilettuce,2013-01-04 13:41:33
Have a look at this video: http://videolectures.net/epsrcws08_tipping_bmd/,AxiomL,2013-01-04 17:07:01
"Unfortunately I cannot, though I am very glad you asked this question. What I can do though, is very simply and briefly describe the feeling I had when a terrible lecturer (great researcher /= great lecturer) taught an intro class. 
     
Find a gun, load it with three shells, spin and pull the trigger while aiming at the wall. There was a bullet? Good job, you have observed that and can note there are two shells left over. Now, put the barrel to your temple - pull the trigger. What are the chances you lived given that there are six chambers, three bullets and you have just fired one bullet in one chamber? If you're lucky it doesn't matter because you're dead.
    
TL;DR: I had a terrible Bayesian teacher and want to relearn statistics in a friendlier way.",maybe_yeah,2013-01-04 18:42:47
"1. You (may) have some data.
2. You have a model which contains parameters.
3. You believe something about the parameters in your model and represent that belief with a probability distribution which says how likely you think the values of the parameters are.
4. You use your data to update your beliefs about your parameters.",samclifford,2013-01-04 23:24:30
"If I could join in, does anyone understand how this relates to neuroscience and how our brain updates its beliefs? I know it's a very prominent theory and would love to learn more about it.",emzywb,2013-01-05 06:04:29
"You lost your car keys. You think you left them in the kitchen. So instead of starting at your front door, you start in your kitchen. After looking for a minute, you haven't found them (although you haven't searched the entire kitchen). So instead of finishing your exhaustive search in the kitchen, you move somewhere else in the house.",DrLemniscate,2013-02-02 13:39:18
"There should be, alongside, an illustration of the statistical programmer at each stage.",picu,2012-07-30 07:10:47
Read that in William Shatner's voice for some reason.,CubicKinase,2012-07-30 08:15:13
"It was, all, the commas",,2012-07-30 15:24:12
I'm going to start doing more infographics in lego blocks.,shaggorama,2012-07-30 08:43:25
"Your graphic is very nice, however I have a few suggestions to make it better represent the typical infographic.

1. Get a presorted lego set from someone who supports your ideology. Acknowledge this source in a lengthy, 7 pt font url at the bottom of the graphic (or not.)

2. Exclude your least favorite type of lego brick from the final picture. Don't mention this, it's just data cleanup.

3. Instead of putting X lego blocks into an X high tower, for each color you should find X^2 (X-1) more of that color and make an X by X by X pile in the shape of a lego brick. That way, the differences between groups will really stand out.",myotheraccisatroll,2012-07-30 14:49:53
Why not just a graphic?,mrpopenfresh,2012-07-30 13:57:35
"While I understand that it is nearly impossible for a political representative to be an expert on everything they are expected to vote on, I do expect our lawmakers to have a basic underpinning of probability and statistical inference, which would serve as the basis for evidence-based legislating, rather than a lot of the ad hoc legislating we end up with.

Thanks, prionattack, for pointing this out.

edit: I just saw this gem of a quote toward the end of the article: “What really promotes business in this country is liberty,” he said, “not demand for information.”

Transparency, ye elusive muse.",toronto_gosh,2012-05-21 14:09:27
My gut feeling is that this representative's district commutes entirely by unicycle and therefore should not receive transportation dollars in excess of lifeline clown shoe purchases.,kiwipete,2012-05-21 17:07:30
"Well, that would at least be a more entertaining reason to point and laugh at them. Unfortunately, this kind of crap from Congress is getting to be an everyday occurrence. ",prionattack,2012-05-21 22:25:57
"I hope y'all found the 'not scientific - random' quote ironic, too. This guy is not just stat illiterate, he's completely illiterate. ",cottagecheese420,2012-05-21 19:08:39
My heart hurts.,r-cubed,2012-05-22 09:52:41
"So is this a potential reality (asks the Australian), if so are there any alternatives or suggestions for replacement surveys? How serious is the concern for privacy?",tiii,2012-05-23 05:29:20
"The responses in surveys like this are anonymous. The representative here is concerned about ""privacy"" and ""government intrusion"" because government should in his mind be small enough to not need data. Also, note that the Senate tends to be the more level-headed body, so it's not likely that it will be entirely eliminated.",prionattack,2012-05-25 12:33:54
"I think these representatives have no idea that the average person cannot handle the amount of data that they would have to sift through to maintain their same lifestyle.

The reality is, the majority of libertarians/market anarchists wouldn't survive without the systems we have in place.",AndrewKemendo,2012-05-22 03:47:27
Some of them would simply regard that as creative destruction.,humor_me,2012-05-22 06:46:23
Except it isn't. Unless there is something to replace that function its just destruction,AndrewKemendo,2012-05-22 14:09:09
That was about the worst journalism ever.,dontstalkmebro,2011-08-10 10:39:21
It is the Daily Mail which is a rag. But it is still a funny story.,cavedave,2011-08-11 01:54:34
"&gt; A professor at the **Institute for the Study of Gambling &amp; Commercial Gaming** at the University of Nevada, Reno, told Mr Rich: 'When something this unlikely happens in a casino, you arrest ‘em first and ask questions later.'

What the fuck is that?",tatamovich,2011-08-10 15:00:58
Looks like a faculty specializing in gambling related issues. Nevada would obviously be a perfect place to host such an institution.,elus,2011-08-10 16:08:16
"Looks like it's part of the business school. A bit boring (sorta wish it was a mathematics institute), but no worse than many other sector-specific business-school centers.",_delirium,2011-08-15 17:07:57
Looks like a system that allows people to arrest those smart enough to game their already questionable systems.,thavi,2011-10-03 20:34:44
[Ityosty link for those who don't want to give the Daily Mail any ad money](http://istyosty.com/b/?u=http://www.dailymail.co.uk/news/article-2023514/Joan-R-Ginther-won-lottery-4-times-Stanford-University-statistics-PhD.html#ixzz1UducAuc6),,2011-08-10 10:09:50
"Outed? That's like saying ""Pro-hockey player outed for practicing hockey when not playing games.""

Just because she figured out the system doesn't make her any less deserving of the prizes.",Bitruder,2011-08-10 13:07:09
It would seem that whoever is in charge of making scratch-offs in Texas needs to come up with a better algorithm.,tavernkeeper,2011-08-10 17:23:52
I really hope you don't read Daily Mail.,zach_will,2011-08-10 14:21:53
I do not,cavedave,2011-08-11 01:54:40
"i may be looking at this the wrong way, but does it really make any difference to the lottery commission who wins the lottery?? they still profit no matter who actually wins it. one person figuring out the system doesn't really make a difference, it only hurts other potential winners, right? ",notch22,2011-08-14 00:33:21
So... this means the lottery is non-random?  Whuuut?,godless_communism,2011-08-16 17:10:36
"""average girls"" ?",kabas,2014-05-09 01:26:03
'Mean girls',MIKE_FOLLOW,2014-05-09 02:25:52
Sample mean girls,SpindlySpiders,2014-05-09 04:38:53
"""x"" bar girls",shaggorama,2014-05-09 05:15:18
"Also to be read by some as ""ex-bar"" girls. Glad you all came to the math side! Good luck as your senior year comes to a close.",sjgw137,2014-05-09 04:21:20
That's what I read it as first haha,anu26,2014-05-09 10:08:45
"hahaha mean girls, that's cute. 

I think a cool shirt would say, ""𝛘-Latte"" and it would have a picture of the drink, and the foam on top would look like a 𝛘^2 distribution.

people who correctly pronounce the letter ""kai"" would never get it though. ",bacteriadude,2014-05-09 00:48:28
"Weird, your chi comes up as an alien head on my phone.",BullNiro,2014-05-09 00:59:42
"whoa, just checked on my phone and it's also an alien head, in a box. ",bacteriadude,2014-05-09 01:04:41
I just see a disembodied squared symbol.,CopOnTheRun,2014-05-09 01:20:58
its pronounced like kai like it rhymes with rye right?,Tankinater,2014-05-09 01:24:52
"I would have thought it would be more like ""key"". ",TimJBenham,2014-05-09 18:37:43
not in American English,1337bruin,2014-05-09 22:26:46
"Isn't classical iota ῖ more like the ""ee"" in meet than eye?",TimJBenham,2014-05-10 17:34:23
"Yep, but a lot of people (including myself) mispronounce it as chai (still rhymes with rye). ",bacteriadude,2014-05-09 01:47:39
[deleted],,2014-05-09 01:54:08
"Hm, weird. In slovene it is pronounced he. I most often incorrectly pronounce it qi :S.",ComplexColor,2014-05-09 12:35:51
I don't think it'd be unreasonable to make that shirt anyway. I think there'd be a market for that on threadless.,shaggorama,2014-05-09 07:56:59
"I'm excited you have a doctor working at high school. I wish we saw this more, though I do realize the huge pay cut he/she must be facing. 

Great shirt!!",LaylasLover,2014-05-09 12:43:15
"thank you!!
She was actually a psychologist before she became a teacher; I think she has enough to support herself. She also is retiring next year and her husband who is a doctor I believe.",chaiteaisyummy,2014-05-09 19:01:05
great shirt! but i'm surprised that she would insist on being called Dr with her students. is that common?,hypermonkey2,2014-05-09 22:03:44
It was standard when I was in high school (USA).,NOTWorthless,2014-05-10 09:14:40
Glad you didn't use 3.14.,skevimc,2014-05-09 04:44:50
[deleted],,2014-05-09 08:25:43
oh shut up.,OhanianIsACreep,2014-05-09 09:29:23
"Interesting.  Have you tried posting in places like kaggle, quora, datatau or hackernews? there are also /r/datascience /r/datamining and /r/bigdata subreddits",riraito,2014-04-24 11:50:13
"thx riraito!  great suggestion, will do.",smortaz,2014-04-24 15:29:34
"If you do try marketing it, I suggest going to icrunchdata.com

As someone that is about to graduate and looking to get a job, I am willing to learn new skills.",,2014-04-24 18:07:07
"nice, thx jeradds.  didnt know about it.  will look into it.  cheers.",smortaz,2014-04-24 18:15:21
"**Server-side** Excel library! We often have big, complicated workbooks from the business experts, and they want us to batch data from SQL thru the workbook, pulling result ranges back to SQL (without using the desktop Excel.exe, for a host of reasons). This is a major hassle, either requiring hacking a service queue around an instance of Excel.exe or using janky third-party libraries that replace Excel.exe on the server. Microsoft has the most amazing business rule modeling tool (Excel) with no means to deploy those rules on the server: that's where I'd start. ",mthoody,2014-04-24 17:28:35
Reading this comment made my hands numb,shaggorama,2014-04-24 20:40:23
i hope in a good way!,smortaz,2014-04-24 21:22:18
"Shirley, you must be joking.",shaggorama,2014-04-24 21:30:19
"there are companies doing exactly that.  eg:

http://www.spreadsheetgear.com/support/samples/calcengine.aspx

i know of two large banks that have inhouse developed excel compatible server side calc engines as well... they run them on linux farms.
",smortaz,2014-04-24 21:41:46
::shiver::,shaggorama,2014-04-24 21:45:07
Bridging the gap between MS Office (usually Excel) and a regular coding language &amp; database is always an un-fun experience.,godless_communism,2014-04-25 12:00:02
But more fun than translating an 80 sheet workbook of formulas into some other code platform (and then maintaining it).,mthoody,2014-04-25 12:46:04
"mthoody - that's a /great/ suggestion.  i believe there are some libraries that do that already, but imagine a world in which Excel /itself/ could spit out cross-plat logic in Python that was immediately runnable on a Linux server.  would you kindly add your comment / suggestion to the survey response?  it's a really desirable feature.  thx.",smortaz,2014-04-24 17:57:07
Way to reach out. Might I suggest posting this in /r/MachineLearning  ? /r/statistics  skews a bit more academic.,towerofterror,2014-04-24 11:18:24
thanks!  will post there as well - great suggestion.,smortaz,2014-04-24 15:30:26
VS = Visual Studio?,poiuy52,2014-04-24 14:09:14
"my bad - yes, Visual Studio.  Will stop assuming....",smortaz,2014-04-24 15:31:49
"Just a comment on the survey:

In context, what do you think a five point scale means to a yes or no question?",Icamehereto__,2014-04-24 15:43:39
"yeah, I hear you.  We preferred yes/no, but our 'survey' person said you want to know the intensity... eg: yes, I've read/watched a TON of Python books/videos/...  ",smortaz,2014-04-24 16:34:50
VS? ,efrique,2014-04-24 15:05:37
"I've never used Python, only R, SAS and Matlab. Is python like any of these softwares?",kurokabau,2014-04-24 13:19:36
Python has a rich data science toolkit and is as fast or faster than R and matlab on many benchmarks (per the [julia team](http://julialang.org/)).,shaggorama,2014-04-24 14:01:57
Do you have to do a lot of the basic coding yourself or are the pre-made statistical packages?,kurokabau,2014-04-24 14:09:03
"It depends. For the basic stuff there is enough, for most advanced things R beats python every time.",mguzmann,2014-04-24 14:22:47
"Python beats R for deep learning, computer vision, and natural language processing. Python is catching up to R on most machine learning and statistical applications (where python isn't already ahead or even), and where R does beat python, python can import the R libraries.",shaggorama,2014-04-24 14:30:16
"&gt;deep learning

Do you have a source for this one? That's not something I've done before.

&gt;natural language processing

But only barely since python is awful at serious nlp. Python is just too slow for NLP (so is R), and most cool libraries are written in Java and C++. Where python does win is in apis for some of those libraries.

&gt;and where R does beat python, python can import the R libraries.

Or R call python, so that's a pointless remark.",mguzmann,2014-04-24 14:39:05
Check out the theano library for deep learning.,towerofterror,2014-04-24 15:05:35
"Thanks for the tip. Im moving an entire project over to python from Matlab, and this will help immensely. ",StannisBaratheon_AMA,2014-04-24 20:47:15
Only if it runs on linux. MS is horrible for development.,GratefulTony,2014-04-24 17:27:14
"GT - as someone who grew up on Linux &amp; now uses both, they have their pluses &amp; minuses.  imho Windows could use a posix compatible Shell, a bunch of utils, and even include Python to start with...",smortaz,2014-04-24 17:58:39
Stackoverflow's existence proves otherwise ,alphabeat,2014-04-24 22:43:14
"Please dont.  Just, please do not do this.  I have been using VC/VS since probably 2000.  What do you plan to do?  Include profit/marketing tools?",homercles337,2014-04-24 17:53:49
"oh god no :).  1st of all it's an add-in like PTVS, you can ignore it.  

it'll be more like a cross between Matlab, Mathematica, with IPython repl integration + two way bridge to Excel.  why?  because a bunch of our customers are /already/ using these tools this way.  the target persona is the Data Scientist, quant, BioInformatics researcher, ... these are domain experts that have to code generally because they have to, not because they really really want to.  it'll be python centric primarily due to the decent lang/scripting features + awesome OSS libs.",smortaz,2014-04-24 18:12:35
Why not write a VS plugin which fully integrates with Mathematica? That way you'd have the best of both worlds.,5n0wm4n,2014-04-24 18:22:31
"as a big fan of Mathematica, i'd love for being able to do File/New/Proj and build some models in VS.  however, that's a /lot/ of work for a questionable return.  the investment in debugging, tooling, etc. would be substantial.  looser integration is possible, but our data so far shows a strong preference for open source tools.  hence we'll be focusing the V1.0 as least around Python &amp; its ecosystem.",smortaz,2014-04-24 18:26:47
There is a Mathematica plugin for Eclipse which is pretty nice. You may want to check this out: http://mathematica.stackexchange.com/questions/18183/how-to-install-the-wolfram-workbench-plugin-into-eclipse-kepler and this: http://www.wolfram.com/products/workbench/,5n0wm4n,2014-04-24 18:57:05
"aha!  i didnt even know this existed. thanks a bunch - looks pretty sweet.  it's similar to our PTVS plug-in for Visual Studio in spirit.  i mistakenly thought you meant reimplement most of Mathematica's IDE features from scratch in VS.   will look into this further &amp; thx for the heads up.

one issue that still remains is the movement toward OSS &amp; Python for many domains that we care about.  in our survey preference for an OSS language &amp; libs is quite pronounced.  there's also community center of gravity... see http://www.biopython.org for example.

cheers.",smortaz,2014-04-24 21:32:26
Yes please stick with python! ,taiidan,2014-04-25 10:11:51
"Ignoring ""frictional singleness,"" it's fun to take that data to mean that a 40 year old not in a relationship has absolutely no chance of ever being in one.",,2013-04-25 06:48:06
"I bet the folks in /r/dataisbeautiful would like this. I especially liked the ""moved from to"" visualization.",daytimejob,2013-04-25 11:10:12
"Indeed, this is what we refer to as alpha inflation, or that the more number of significance tests you conduct using an alpha level of .05 (let's say 20), you will make one Type I error (you find a significant relationship when in reality there isn't a significant relationship). Hence, scientists might recommend a conservative Bonferroni Correction to guard against this, which basically divides your alpha by the number of statistical tests you conducted. So an alpha of .05 becomes an alpha of .05/20=.0025 to establish more confidence in your findings, albeit while sacrificing power.",rubes6,2011-04-06 18:16:57
"Lots of good info [here](http://en.wikipedia.org/wiki/Multiple_comparisons)

When I covered multiple comparisons in my stats class we did Bonferroni correction and learned about the more advanced Benjamini-Hochberg correction, which is talked about briefly [here](http://en.wikipedia.org/wiki/False_discovery_rate)",thegabeman,2011-04-06 19:43:12
"False discovery rate is still a big, big area of research. Lots of stuff happening here, and the need for many bright statisticians to offset the bane of p-value driven science.",,2011-04-06 21:38:22
"Definitely. I'll just to put multiple testing in context of some work in bioinformatics using microarray data (e.g. looking at differences in gene expression between a healthy individual versus one that is sick):

Small n, but many comparisons: few biological replicates from patients, but looking at many genes in a human (comparing gene 1 in healthy versus sick): ~15000 genes by 50 samples. Using a Bonferroni correction will effectively reduce your power to nothing since 0.05/15000. FDR helps, but we still need better ways to reduce false positives while maintaining confidence.",showmethedata,2011-04-07 06:43:21
I really like the creativity of using Lego for this. I might take it for my own use. Thanks! ,leonardicus,2015-02-26 05:10:34
This is a really good way to illustrate this; and probability more generally. I've been teaching this stuff for years and I've never consider (or heard of) using legos. Great idea! ,KappaSquared,2015-02-26 04:03:26
"This is a nice idea, but it looks like there are some missing pieces in the article.",AllenDowney,2015-02-26 06:34:55
Such as?,AllezCannes,2015-02-26 10:23:44
It looks like the problem is that the equations are not getting rendered.  Might just be Chrome.,AllenDowney,2015-02-26 10:38:16
Refreshing the page should bring them back up. I had the same problem.,PhaethonPrime,2015-02-26 10:49:02
"I'm a big fan of the Bayesian, but this just isn't good advertising for it. The main problem is that there's no obvious way to generalize the Lego approach; people will get stuck in any problem that can't be reduced to a trivial counting problem -- that's most problems. (Same goes for illustrations based on Venn diagrams.)

Bayes' theorem is not the basis of Bayesian probability or its most important result or anything like that. The basis of  Bayesian probability is that it is a generalization of logic to degrees of belief between 0 and 1. Everything else follows from that. 

FTA:

&gt; The big takeaways from this experiment should be
&gt;
&gt;   Conceptually, Bayes' Theorem follows from intuition.
&gt;
&gt;   The formalization of Bayes' Theorem is not necessarily as obvious.
&gt;
&gt; The benefit of all our mathematical work is that now we have extracted reason out of intuition. This both confirms that our original, intuitive beliefs are consistent and provides with a powerful new tool to deal with problems in probability that are more complicated than Lego bricks.

Um, this is baloney.",midianite_rambler,2015-02-26 11:09:29
"Here is a one-off video that I think might fit the bill:

[John Rauser - Statistics Without the Agonizing Pain](https://www.youtube.com/watch?v=5Dnw46eC-0o)

It would be great to find more products like this.",vmsmith,2015-01-01 07:35:59
"Just finished watching. THAT WAS BEAUTIFUL! That was exactly the kind of stuff I wanted. As a  competent practitioner of computer programming, I followed every word of his talk. Thanks again!",jeffjose,2015-01-01 07:59:58
"Shit, this is EXACTLY what I did in my thesis! My stats advisor was up my ass about deriving properties mathematically but I realized no-one in soil science - my main field - would have any idea how to interpret what I'd done unless I did something more conceptually clearer and stumbled into the repreated randomization / bootstrap method. WHAT A GREAT TECHNIQUE!!!",Copse_Of_Trees,2015-01-01 23:05:18
"Actually, you make a good recommendation. I've noticed that cross-disciplines videos (statistics to a hadoop crowd) fit what I'm asking for. Thanks for this!",jeffjose,2015-01-01 07:46:43
"when he completes the 50,000 computations, does he get a confidence interval? or is it just that ""4.4 only occurred by chance x% of the time, which is rare""?",atomofconsumption,2015-01-01 19:55:11
That was the p-value.,AllezCannes,2015-01-01 21:03:05
"so because 14/50,000 is 0.028%, it is lower than 0.05% and thus null can be rejected?",atomofconsumption,2015-01-01 21:51:19
Yes.,AllezCannes,2015-01-01 21:53:50
"with a sample size of 50,000, .05 is way too high for an alpha level.",HodorNC,2015-01-03 19:57:16
I don't understand. ,atomofconsumption,2015-01-03 20:06:47
"Naked Statistics by Charles Wheelan. It's a read, but a quick read. Having taken two stats classes before and therefore not needing to read the material, I was still unable to put the book down. It does a great job explaining fundamental statistics.",JH_McK3,2015-01-01 09:04:26
"If you enjoyed Naked Statistics, check out Nate Silver's The Signal and the Noise.",reallyserious,2015-01-01 09:40:37
"Seconded. I wish I read this before taking statistics courses, where at times I would get bogged down in how to compute different statistics and forgot to see things for the big picture. Highly recommended. ",datakittenMEOW,2015-01-02 06:35:09
"Harvey Motulsky's ""Intuitive Biostatistics"" did wonders for me.",Cosi1125,2015-01-01 10:18:10
"Second this recommendation, its a very good book.",enilkcals,2015-01-02 00:03:43
"If you want it in a book format, go with The Cartoon Guide to Statistics by Larry Gonick",HodorNC,2015-01-03 19:55:36
I second this suggestion -- don't be fooled by the format.  It's legit.,AllenDowney,2015-01-04 16:48:54
"I am interested in this too, especially a good ELI5 tutorial for Bayesian stats",gigamosh57,2015-01-01 11:26:24
"How detailed an ELI5 tutorial do you want? If not very detailed, I could give one here. I also highly recommend ""Doing Bayesian Data Analysis"" by Kruschke.",glial,2015-01-01 12:06:33
I second Kruschke.  Easily the best book I've found for an introduction to Bayesian statistics.  Make sure you get the newest edition though.  It has Stan and JAGS code instead of BUGS.,beaverteeth92,2015-01-01 17:36:28
"This thread has been linked to from elsewhere on reddit.


 - [/r/mistyfront] [Resources that explain statistics intuitively? (/r/statistics)](http://np.reddit.com/r/mistyfront/comments/2ragy9/resources_that_explain_statistics_intuitively/)


*^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)*

",totes_meta_bot,2015-01-04 02:04:30
There's a correlation.,econometrician,2014-05-02 08:41:41
I know. I'm a TA and this was a test response. ,AL93RN0n,2014-05-02 08:46:13
"Is the correlation that the isn't one? Sorry if that's a stupid question, but as a TA you probably have the skills needed to deal with it.",IndustrialstrengthX,2014-05-02 08:55:49
"No. The answer is that there is a weak correlation between the two variables, but the students were supposed to elaborate on their answer. This student put because there are ""circles everywhere"". i just thought it was funny. Bad stat joke I suppose. ",AL93RN0n,2014-05-02 09:12:32
"I thought ""circles everywhere"" was funny, too.  I kinda want to create a blog now just so I can name it that.",Jofeshenry,2014-05-02 09:33:02
"I know one blog, that is ""circles everywhere"":

zombo.com",Seventh_Planet,2014-05-02 14:08:17
"If not-a-correlation was a correlation then every relationship would be a correlation, wouldn't it? 

In this case, these variable clearly aren't totally randomly associated. If they were, the dots would be even spread out within the range of their values. Instead, you can see that there's a sort of diagonal shape to the cluster, and it's kind of thicker along that diagonal. That's a (weak) correlation between the variables. ",WallyMetropolis,2014-05-02 10:39:17
"Teachers answer was nice, yours, sorta insulting.",IndustrialstrengthX,2014-05-02 21:39:01
Then you're being a tad sensitive. Because nothing I said was insulting. ,WallyMetropolis,2014-05-03 06:46:45
"Hmmm, I'd want to see more data over a larger range.",Sean1708,2014-05-02 09:07:05
"Yea, but had you been given that, you would still have said the same thing. You always want more data for your analysis, but it's not always available.",SpindlySpiders,2014-05-02 22:11:35
"Yeah true, I suppose.",Sean1708,2014-05-02 23:06:16
"One of my favorites from my time as a TA was, ""it takes the shape of a two-sided triangle.""

Absolute value.",Icamehereto__,2014-05-02 11:15:57
"Yes, I can picture it now.",SpindlySpiders,2014-05-02 22:12:34
I love that I can post this nerdy shit on here and it balloon in to this. Only one person in the office appreciated my find. You are my people. ,AL93RN0n,2014-05-02 13:49:12
http://i.imgur.com/UII6yiE.jpg,Philosopherknight,2014-05-02 22:40:13
Love a good stats joke! Can we place bets as to the degree of correlation? I am suggesting between 0.5 and 0.6!,armchairdetective,2014-05-02 10:28:34
"I bet one upvote for 0.4 to 0.5. If it is exactly 0.5, you win, but this will obviously occur with Propability 0.",booze_n_goose,2014-05-02 10:42:44
WINNER!,AL93RN0n,2014-05-02 13:07:41
Yay. Finally my statistics classes are paying off!,booze_n_goose,2014-05-02 13:16:10
"I'm throwing my money at your bin. Also, because of the limited numerical accuracy of the software, the probability is non-zero (the sample space of possible reportable values is finite). Since we're being pedantic :)

EDIT: Hurray, we win.",shaggorama,2014-05-02 11:19:18
You'd need to be more specific. I'm guessing you mean some linear correlation metric like PCC?,WallyMetropolis,2014-05-02 10:40:49
Pearson's r ,AL93RN0n,2014-05-02 10:52:52
PCC is Pearson's r. ,WallyMetropolis,2014-05-02 11:28:20
"I figured that it stood for the pearson's correlational coefficient, but there are so many measures that I'm not familiar with that I didn't want to assume. Every time I think I know something I get schooled.  ",AL93RN0n,2014-05-02 11:38:26
"To be fair, I would readily recognize that Pearson's correlation coefficient (written in full) as Pearson's r, but have never heard it abbreviated as PCC either.",Neurokeen,2014-05-02 19:14:56
"Oh, man, tell me about it. ",WallyMetropolis,2014-05-02 11:43:40
"Sorry, yes. That's exactly what I meant. Of course, that depends on how the variables are measured. Maybe we should ask OP to clarify the level of measurement. 

Inquiring minds want to know.",armchairdetective,2014-05-02 12:33:06
"Lets do Pearson's R. Ill have to get the data set, but I can run it pretty easy. from 0-1 with directionality. If you can guess significance level, it's extra points. ",AL93RN0n,2014-05-02 10:46:00
"well it's obviously positive, so direction isn't that important. ",AL93RN0n,2014-05-02 10:46:57
"Yes, you should get on that. We need answers!",armchairdetective,2014-05-02 12:33:57
"I'm trying to get the data set from my boss. I'm not really wanting to input all that data, but I will get to the bottom of this! 
",AL93RN0n,2014-05-02 12:43:24
What's the level of measurement? Is Pearson's r even appropriate?!,armchairdetective,2014-05-02 13:41:58
both are interval. it is acceleration from 0 to 60 and MPG. ,AL93RN0n,2014-05-02 13:43:08
Rock. I was getting worried we needed a spearman correlation coefficient. ,armchairdetective,2014-05-02 13:43:57
"That tops ""it's not a normal distribution because too many things are on the left"" and ""there is no interaction because the lines go in exactly the same direction.""",rottenborough,2014-05-02 10:52:01
[This](http://imgur.com/Y4bmk92) one is not too bad either. ,AL93RN0n,2014-05-02 11:00:14
"Hey man, that's not fair! The student was marked off for being right!",ZSVG,2014-05-02 13:56:18
I feel like I might have said the first one in my more naive years...,Feurbach_sock,2014-05-02 15:15:39
"The correct answer is .435, if I remember my transformations correctly. I can't get the data set, but the linear r-squared is 0.189. I found the fit line on another graph. 

edit:typo",AL93RN0n,2014-05-02 13:04:11
Good chance to post my favourite. Assignment for stats for engineers and one guy kept talking about his hystograms (which are some sort of fertility test) ,gicstc,2014-05-02 14:40:46
Stick a LOESS smoother through it and there is a nice relationship... ,offtoChile,2014-05-02 12:00:08
"It's a much nicer relationship than I first thought, but this isn't cross sectional data, so the relationship is still fairly weak imo.  ",AL93RN0n,2014-05-02 13:05:58
mtcars everywhere!,casualfactors,2014-05-02 13:16:25
Haha i miss this stats work mathematical statistics became so much harder so much faster.,chasethewater,2014-05-02 23:05:17
"That's an excellent article, I hadn't really considered the massive *potential for* sampling bias that exists in big data.

Edit: added ""potential for""",MipSuperK,2014-03-31 08:00:11
"The article is nicely balanced and a good read. However, there is really nothing new being stated: people working with data should know that even in completely random data, statistically significant correlations can be found. As such, stating that ""we are making a big mistake"" is somewhat far fetched, I would say.",zwartwater,2014-03-31 08:28:17
"Yeah, as the article says: 

&gt;Statisticians have spent the past 200 years figuring out what traps lie in wait when we try to understand the world through data. The data are bigger, faster and cheaper these days – but we must not pretend that the traps have all been made safe. They have not.

I think the issue is that computing power and an abundance of data has made it possible to explore many possible hypotheses very quickly, then pick and choose from the handful of positives which are littered with false positives. It makes it much easier to throw out the scientific process (i.e. testing a single hypothesis). 

",rm999,2014-03-31 09:24:40
I am finishing a PhD in genomics and even in science much of the scientific method is being thrown out in the age of big data.,canteloupy,2014-03-31 09:33:09
Can you give an example?,NotAHomeworkQuestion,2014-03-31 13:19:45
"Not OP but I can give it a shot.

There is a lot of research happening in fields related to genomics that is essentially a fishing expedition.

Step 1) Collect massive data.
Step 2) Look for any correlation.
Step 3) Explain away the lack of a clear hypothesis using tools like false discovery rate and generate your hypothesis post hoc.
Step 4) Publish.

Rinse and repeat.  I work in HIV research with behavioral survey data (not as big, but it suffers from similar problems).  One variable is correlated in in one study, not the other.  It all gets published and put on equal footing.  In the end it is very hard to determine what associations are meaningful and what is noise because it's all just being dumped into regression models.",Icamehereto__,2014-03-31 13:46:22
"That's exactly what I mean. As a grad student, you show up once the grant has been written for XX amount of sequencing and get a heap of data with the instruction to ""find something interesting"". I basically hated it.",canteloupy,2014-03-31 13:52:20
"You have more fortitude than me, I left after my master's.",Icamehereto__,2014-03-31 13:57:41
"My dissertation was on high dimension, low sample size (HDLSS) statistics, so naturally I looked towards genetic data. I sat in on a few lectures with the local biostats folks and ran as fast as I could back toward low dimension, high sample size (LDHSS) applications.

I may never discover a mutation responsible for a deadly disease, but every result that comes off my desk is biologically sensible science. Still, the number of people who plop databases in front of me and say: ""so, what looks good?"" is awe-inspiring.",,2014-03-31 16:06:36
"I am a psychometrician (delivering massive amounts of hypothetically unidimensional tests to large amounts of people), and even when we are trying to measure a single dimension with large samples there are a horde of issues.

",Mockingbird42,2014-03-31 16:25:35
Thanks!,NotAHomeworkQuestion,2014-03-31 13:48:44
"This may well happen (and quite widely) but its not always the case as many researchers are aware of the problems involved.

Often a different procedure for ""Step 3"" is taken which involves looking for corroborating eveidence from other avenues.  

For example rather than simply looking for the existence of a mutation that is associated with disease status the effect of that mutation on the encoded protein and how it affected its function/expression was also checked.  Whether the gene itself encoded a protein that had a plausable biological role in the disease aetiology was also very important.  In some cases mouse models with knockout genes are also derived to test if this results in a similar pheontype.  Also confirmatory studies are done with collaborators around the world to test and replicate the association seen before publishing.

Thats not to say that there aren't problems with the methodology that many who have jumped on the band-wagon have employed, but not all researchers clamour to publish every single ""significant"" association they discover.",enilkcals,2014-04-01 01:22:06
The philosophy of science is going to need to brush up its game and come to terms a little more with information theory. ,GratefulTony,2014-03-31 10:51:28
"You say that as if philosophy of science hasn't progressed beyond the hypothetico-deductive model. There have been many advances since, such as the scientific realist camp delving into just what status to give models of different types.",Neurokeen,2014-03-31 12:59:45
Can you give me some references? ,GratefulTony,2014-03-31 13:30:08
"For a broad view-from-above, the [Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/archives/sum2009/entries/models-science/) is usually a good place to start.

A lot of these types of problems with drawing inferences with little hypothesis input also relate tangentially to the proof of the four color theorem, which was a pretty hot topic a couple of decades ago. Even though it was in the context of a mathematical proof, it was pretty much done entirely by computer, and the controversy involved the reliance upon the computer for the veracity of the proof.

To add to how busy philosophy of science can be, even work like Judea Pearl's falls partly into philosophy of science because of his work on the manipulability explanation of causation.",Neurokeen,2014-03-31 14:00:05
"SEP, I should have figured. Thanks/",GratefulTony,2014-03-31 14:30:01
"How aware is the ""PhilSci establishment""--if there is such a thing--of more technical work like Judea Pearl's causal graphs, Ray Solomonoff's universal prior, and Marcus Hutter's compressive induction? ",khafra,2014-04-01 04:44:12
"I wouldn't be able to speak for the ""PhilSci establishment"" as a whole beyond knowing what I come across every now and then in PhilSci journals and textbooks in my own time. I'm pretty sure I've seen Pearl and Solomonoff mentioned in Phil textbooks before, though I can't recall seeing Hutter's work in textbooks, if that helps. Then again, I tend to gravitate toward philosophical work on induction, causation, and probability more than the more broad stuff like the demarcation problem.",Neurokeen,2014-04-01 08:25:31
"Serious question: is that bad?

If the answer is yes, then I look like a dumbass. If the answer is no, I look like I'm careful and precise.",seydar,2014-03-31 10:24:19
"It depends. It wouldn't be if all  projects had a mix. And most do. But some don't, and it's annoying. For stuff like genome-wide association studies it's understood that they find correlations and a good deal of scientists following up the work involves teasing out causal relationships, for example, but the problem is how it's reported and who follows up. It's usually over-hyped in the media and rarely the star lab who found the association who follows up. It creates some weird situations but science overall doesn't suffer.

But some functional genomics projects consist almost only of exploratory data analysis and validation experiments as an afterthought. There are papers claiming things they did not prove because of this.

On a more personal note, as a grad students starting out, it feels very unsettling to be thrown into the big data with the instruction to ""find something interesting"".",canteloupy,2014-03-31 13:45:38
"I'd say some large portion of businesses in the world do not have a staff mathematician, so I'd disagree its uncommon. I'd say it is most likely the norm.",IndustrialstrengthX,2014-03-31 11:14:01
"We have a large data science and data mining team, but no mathematicians or statisticians to be found. I even tried to refer one and they turned them down saying they did not know what statisticians did.",,2014-03-31 19:42:52
Lol'd hilariously true. I try to comment on the questionable effectiveness of forecasting when sales reps are involved and people look at me like I'm crazy.,IndustrialstrengthX,2014-03-31 20:27:56
"The issue that I see is companies like IBM selling BIGinsights like its going to do for advanced analytics what Excel did for simple analysis.  But a powerful computer and sophisticated algorithms don't make somebody a statistician any more than a scalpel makes somebody a neurosurgeon.
 ",MrBrodoSwaggins,2014-03-31 18:43:50
"I think the concern isn't necessary Type I error per se, but rather the long-term predictive validity of big data models. With no theory guiding WHY the model is specified the way it is, we have no underlying structure that holds it together. 

This is why, according to the author, the Google flu model failed eventually. It didn't specify WHY the model was created the way it was and therefore couldn't understand what was driving the effects. 

Just as ice cream and murder rates are correlated, a theoryless model would make the correlated assumption that as more ice cream is sold, more people will die when, in fact, the true effect is due to increased temperatures (deaths occur in hotter months, so do ice cream sales).

Theoryless models can't take these variables into account unless they're fed into the original models. If a key variable that drives the effect is omitted, the model (and researchers) will have no idea why it's predictions fail until it's too late. ",Palmsiepoo,2014-03-31 22:48:11
"People don't really believe that stuff about not having to do statistics because they ""have all the data"" do they?",NOTWorthless,2014-03-31 10:23:49
"I thought the same thing! I'm not even a statistician, my degree's in Economics (a lot of econometrics) and I thought immediately: the rules still apply. Sampling bias can and does exist. ",eskimojoe,2014-03-31 19:00:01
"Speaking as someone with a decent stats background in a field with lots of non-statisticians, yes.  It's not so much that they believe that they don't have to do statistics, it's that the see significant relationships, and then stop thinking statistically.",daledinkler,2014-03-31 19:47:36
"I had a colleague tell me this very thing, he's a math background and works with me as a data scientist. He's super smart but he was wrong on this topic.",econometrician,2014-04-01 06:06:47
"Some of them do, unfortunately. Argle bargle. ",giziti,2014-04-02 08:28:40
"This is such a refreshing article.  I work in the data analysis division at a consulting company and all I hear is praise of ""big data"" and that if you aren't using hadoop you might as well be throwing quarters into a wishing well.  All of the data quality issues that motivated classical statistical inference still exist.  Its like we are telling people its so much more efficient to drink from a fire hose than a glass.",MrBrodoSwaggins,2014-03-31 18:38:32
Anywhere I can get the article?,GCN2,2014-04-01 06:10:23
Bookmarked until after finals are over.  Thank you for sharing this with us!,ProveItInRn,2013-06-11 11:23:43
"You are welcome to my class.
Feel free to start browsing the material of this course at anytime.
Thank you so much! ",mkaabar,2013-06-11 11:29:27
"Awesome, I am looking forward to your class.

Thank you!",The_Tapatio_Man,2013-06-11 09:05:47
You are welcome to my class. Thank you so much!,mkaabar,2013-06-11 09:41:56
Cool. You should post this to r/math too,agent229,2013-06-11 14:01:51
"Yes; I posted it to r/math.
Thank you so much!!!
You are welcome to my class and I hope you enjoy it.",mkaabar,2013-06-11 14:14:29
Thanks for this.  I'm so excited for this class.  I was torn between whether or not to take Numerical Analysis next semester because it sounded fun.  Taking this intro would be a perfect way to decide!,smootie,2013-06-12 01:47:32
"You are welcome to my class. I hope you enjoy numerical analysis.
Thank you so much!",mkaabar,2013-06-12 03:51:45
Cool!  Thanks!!,ForScale,2013-06-11 08:15:45
You are welcome to my class. Thank you so much!,mkaabar,2013-06-11 09:31:37
"Not *really* ""analysis"" though, is it.",MeowMeowFuckingMeow,2013-06-11 10:07:28
"This course is an introduction to numerical analysis in order to discuss the main concepts of numerical analysis to introduce students to several methods of numerical approximation such as error analysis, root finding, interpolation, polynomial approximation and the direct methods for solving linear equations. In addition, in future, I will publish an advanced version of this course in order to discuss other advanced topics in numerical analysis . ",mkaabar,2013-06-11 10:31:56
What kind of math background should I have to get the most from this course?,MisterMeiji,2013-06-11 13:01:02
"If you have a little bit of background in Linear Algebra, you will do well in this course. If you do not have this background in Linear Algebra, please do not worry because I gave a review to Linear Algebra before going into the topics related to Linear Algebra that will be discussed in each section of this course. Thank you!! Good Luck!!",mkaabar,2013-06-11 13:38:46
"0) know your data. Look at it. Think about how to show the features of your data. Often almost every feature of the data worth communicating can be put into a few carefully considered displays. Analysis starts with displays and end with displays.

1) avoid the make the mistake of thinking you have the model that generated the data - your model is wrong

2) I've had to explain this one a lot of times, so I'll put it in: zero 'skewness' by some common measure (like third moment skewness, say) is not symmetry

3) checking your assumptions with hypothesis tests answers the wrong question (see point 1 - you already know the answer to that question anyway). The question is more about effect size (how badly wrong are my assumptions, what effect will that have) - you don't need to test it, you need to see how big it is. Such tests are sensitive to sample size (that is, you'll pick up small deviations with big samples, but miss big deviations with small samples) - but the *effect* of a given amount of deviation from assumptions doesn't normally depend on sample size (e.g. the effect on inference of some serial correlation in errors assumed to be independent depends on the size of the correlation, not on your ability to say that it is different from zero)

4) spend some effort figuring out which assumptions aren't so important and which ones are more critical; think about whether you can be more robust to the more critical ones

5) never be afraid to ask dumb questions. (this applies to life, really, but I regularly find out something important in an analysis by stopping to ask some dumb questions, especially if I am working with someone else in an area I don't know intimately. The proportion of times someone asked for an analysis that was actually what they needed to solve their problem was surprisingly low. Answering 'Can you show me how to do this particular analysis on my data' with 'sure' turns out surprisingly often to answer entirely the wrong question. And the analysis that answers the underlying question is surprisingly often not very complicated - sometimes simpler than what was originally asked; people often get tied up into ever more complicated epicycles of analyses that have become 'standard' in their application area without thinking about 'what is the actual question I am trying to answer?'. Being constructively dumb can pay rich dividends. On the other hand, if a question *can* be reasonably answered with an analysis that is standard in their application area, use it.)

6) while a handy tool for a number of problems, the bootstrap is very much *not* assumption-free. It's not a panacaea. And it definitely doesn't substitute for thinking. (Actually, those comments apply to pretty much any of the powerful tools that have come up in the last few decades - none of them are push-the-button black boxes; in every case you have to understand them and their properties and think about your problem, and often tailor a very specific solution)

7) whatever tools you learn (packages, languages, whatever), in five or ten years you'll be learning something different. Get used to it, it's just part of the job. I've learned dozens of languages and statistical packages over the years, I have no doubt in another decade I'll have learned a couple more that I haven't even heard of now.
",efrique,2012-02-04 13:12:13
"Could you talk more about item 6 here, please.  I am really curious.",DrNewton,2012-02-05 03:56:45
"Specifically in reference to the bootstrap? Or more widely? 

Is it that you are surprised that some people might think of it as assumption free or that it might contain assumptions?
 ",efrique,2012-02-05 13:57:37
"Bootstrap has the big assumption that the empirical distribution function represents the true population distribution function that we're interested in. Also, strapbooting has complications when data is coming from a uniform distribution. It's a useful tool, but, like everything in statistics, it comes with costs to bear in mind. 

Also, I'd like to add that efrique's statements were wonderful.",econometrician,2012-02-05 14:22:31
"&gt; the empirical distribution function represents the true population distribution function that we're interested in

Well, at least for the form of the bootstrap where you resample some form of residuals, you're also assuming they have a common distribution.

I've seen the bootstrap used for predictive inference in a situation where the Poisson (or a scaled-Poisson) is used as the model for data (used to estimate the mean parameters in a GLM for example) - but the shape of the Poisson changes with the mean; if you're interested in making inferences about quantiles (as I've seen in those same cases), this is a problem that simply can't be ignored - a low mean Poisson has an entirely different distribution than a high-mean one (more skewed, for starters, concentrated in only a few values).
 
It's even a problem just for estimation - consider a Poisson with mean 1, where the mean is fairly accurately estimated. In a GLM, its smallest Pearson residual should be -1 (a zero observation, less its mean (1), divided by the square root of the mean ). Yet if you allocate it a resampled Pearson residual from a Poisson with a large mean, you're using something close to a standard normal residual, which has about a 1/6 chance of being smaller than -1 (and hence producing a pseudo observation that's negative; impossible with a Poisson, and which can then screw up the estimation in the GLM).

That's not the only issue, but it's one I've seen come up more than once. It leads to problems that those same people then fail to attribute to attribute to the assumptions made by their chosen tool (because they think - and say - it doesn't make assumptions).
",efrique,2012-02-05 14:45:18
"Indeed, but the Poisson distribution usually isn't the most ideal to choose for Maximum Likelihood. The memoryless property of the Poisson is a huge problem; I'd probably recommend a negative binomial. :D",econometrician,2012-02-05 15:04:15
"I have no problem with the Poisson in and of itself. Often it works just fine.

The problem is in some circumstances, it marries *particularly* badly with the bootstrap, presenting both practical and theoretical issues. (The Negative binomial doesn't necessarily avoid those problems...)

The Poisson *can* in some situations be used with a form of the bootstrap, but you can't just black box it - you have to think about what the heck you're doing. As an example, in discussion of a relevant application in Davison and Hinkley (the one on AIDS reporting), they discuss stratification to approximately deal with the Poisson having 'different distribution shapes with differing mean' issue. That seems like a reasonable step that at least would avoid the worst of the problem - and is exactly the sort of thing I mean by adapting something to the specific circumstances rather than treating it like a black box.

That kind of thing crops up all over - not just with the bootstrap. You have to use your noggin; methodology doesn't substitute for the constant need for understanding both your tools and your problem.
",efrique,2012-02-05 15:51:45
"Agreed, but the same goes for the Poisson distribution in and of itself. One of the useful properties of the Negative Binomial is that, if the random variable, is, in fact, distributed Poisson, then it will collapse to the Negative Binomial and yield the same estimates. :)",econometrician,2012-02-05 16:01:10
"You mean that the Negative Binomial can collapse to the Poisson, not the other way around.

Yes, it's more general and so (unsurprisingly) more widely applicable than the Poisson.",efrique,2012-02-05 16:38:07
"Whoooops, super typo. Yes, what you said!",econometrician,2012-02-05 17:44:28
"Yea, the bootstrap.  The only assumption I can think of is ""this dataset is representative of what you expect to see going forward.""",DrNewton,2012-02-05 15:48:05
"Okay. Here's a few

1) you have to assume that any model for your data underlying your bootstrapping is correct; your inference is definitely not model free. 

2) The particular form of resampling has to fit with your assumptions about dependence (you can destroy dependence if you don't take it into account)

3) if you resample residuals they have to have a common distribution; I gave an example elsewhere in the comments on this post


",efrique,2012-02-05 15:57:50
"I don't understand, and I won't bore you with more requests for ""more!""  Could you provide a source so I can learn more?

Here's my take on #3--not sure if this is exactly what you meant or not.

The residual of any data point is, of course, going to depend on the X used to build the model:

r = y - f(y,X)

And it is true that r|X_1 has a specific distribution, and that r|X_2 has another distribution, etc... but it is also true that ""r"" itself has a distribution. (Its pdf is just Integral[ Pr(x) Pr(r | x), all x]).   A bootstrap on r will give you meaningful data about r, but you are correct that you are probably after meaningful data about r|x.   ",DrNewton,2012-02-05 17:41:10
"Well it relates to the importance (value?) of using pivotal or nearly pivotal quantities for resampling.


You seem to have the right sense of where the issue is.

In the situation where you're interested in inference about unconditional r at the particular mix of X's that are observed in the sample (the relative propotions), there may not be a problem - but that's far from the situation in some of the circumstances where I've seen it used.  

The sample data tends to be concentrated more in a different part of the X-space to where the predictive inference is taking place - there are many data points but few predictions at the x-values where the mean is large and few data points and many predictions at x-values where the mean is small. The distribution of the conditional predictions is totally wrong.

The thing is if the bootstrap is treated as a black box, situations where it will work okay if you just use it blindly are indistinguishable from cases where it won't - because treating it as a black box *ignores any features of the problem at all*.

I'd refer again to the AIDS reporting data in Davison &amp; Hinkley (I think they deal with it in Chapter 7, I don't have any of my boostrapping books with me now) - that's very close to the situation where I've seen it repeatedly misapplied.
",efrique,2012-02-05 19:31:38
"I didn't expect to see such a thoughtful list here. Good job. I would add to number 4) The independence assumption is often the most critical, so don't make the mistake of assuming it when it isn't true. We have statistical models that allow for dependent data. ",galton,2012-02-05 08:37:16
definitely #0.,ntlaxboy,2012-02-05 09:42:49
Being a statistician means about 95% of your time is cleaning and manipulating messy data and only about 5% is spent doing cool models.,,2012-02-04 13:05:23
"Arguably the 5% is what you get paid for the most, the liability associated with your forecasts/analysis",dassouki,2012-02-04 13:08:38
"Yeah, while I'm glad I get the same hourly rate for dicking around in SQL all day long, sometimes I wish I had interns to defer that crap onto so I could spend all day mucking and miring in Winbugs.",,2012-02-04 13:10:23
"Unless you're in academia, in which case you shove it off on your students.",Neurokeen,2012-02-04 16:23:15
"How to program a computer.

How to program a computer.

How to program a computer.

How to program a computer.

How to program a computer.
How to program a computer.
How to program a computer.
How to program a computer.

How to program a computer.
How to program a computer.
",DrNewton,2012-02-04 10:32:44
"I would add to this: learn MySQL. I've recently some across a number of people with _degrees_ in statistics and they've never used a database. How they store, manage and query large datasets I don't know (Actually I do: excel). Also: learn R. ",physicist100,2012-02-04 14:12:02
"Seconded on MySQL. I was surprised when I graduated that most of the jobs that I found required SQL, and I discovered that's because most of the data is stored in SQL databases. The first step to analyzing the data (for me) is getting it out of the database.

Providing clear and accurate numbers has usually been more important than the subsequent analysis.",kenlubin,2012-02-04 16:52:20
"If you need to use mysql because your peers do then learn mysql.

Otherwise learn postgres. Mysql's approach to data integrity is to say the least poor. Postgres also has a much richer featureset. I've used both extensively and if a client wants to use mysql I calmly explain to them why they probably shouldn't and they usually don't. Or they are no longer my client.",rgh,2012-02-04 23:28:32
"What if they use Oracle, DB2, or MS SQL Server (which has a pretty sweet OLAP analysis suite)? If you learn how an RDBMS works, and how to use SQL DDL and DML, you can use any of them and you aren't locked into any one tool/platform.",atlben76,2012-02-04 23:48:06
Would you recommend MySQL over something like Oracle? ,negative_epsilon,2012-02-05 06:26:32
"For a data user the differences are more trivial than the licensing differences. If you are using something that benefits from Oracle features later then you will use the same skills you'd develop using MySQL.

Other options include PostgreSQL, especially if you are using geographical data. SQLite actually has a sophisticated vocabulary and feature set, with one of the main constraints being that it cannot be used concurrently.",rz2000,2012-02-05 09:32:28
"I have to add SQL Server to that list as well - I think that PostGIS will still be the best way to manage spatial data going forward, but MSSQL was the first out with true geodetic support and 2012 ""Denali"" has some more complex geometry types. Pair that with Analysis Services and Solver Foundation (solvers can be used for [high performance logistic regression](http://fdatamining.blogspot.com/2011/02/logistic-regression-in-f-using.html) for instance), and the nice management tools, and I think it's the best overall RDBMS for statisticians.",zip117,2012-02-06 07:00:36
"I favor SQLite. I've found  it a fantastic engine for storing data. 
It's fast, easy ot use, and has excellent support in many languages (R and Perl among them).",wankman,2012-02-06 11:09:34
"&gt; How they store, manage and query large datasets I don't know (Actually I do: excel).

Agree 100%. I'm coming from web application/database development background and can't believe how many people in stats use Excel. For a thousand rows maybe Excel works fine. For hundreds of thousands of rows no way.

Learn basic SQL. Any SQL database. MySQL, PostgreSQL, SQLite, SQL Server, Oracle, DB2... 

",aaaxxxlll,2012-02-10 16:15:45
"There is an overwhelming number of resources for R out there, are there any particular tutorials or books you feel greatly improved your work in R? 

I'm using R in many of my classes but feel I'm barely scratching the surface, and simple tasks often seem to take much longer than they should. ",kerosion,2012-02-04 14:51:40
"R does have a steep learning curve. But once you get over that hump it'll become a very powerful and efficient tool. As for docs, probably the best place to start is here: http://www.r-project.org/. ",physicist100,2012-02-04 15:08:52
"my problem with r, and maybe you could help me with this, is being overwhelmed by the amount of resources. every book i've looked at seems to start from wildly different places, some with more programming, some pointing to certain analyses, etc. i'd like an r resource that maybe just goes from the beginning of stats, but assuming you already know stats, i.e. learning r from the beginning. does this post make any sense? i am a total r beginner.",bennymaths,2012-02-08 00:10:45
Scatterplot your data to visualize correlation before you waste time modelling anything.,bayleo,2012-02-04 11:54:25
With jitter?  I think jitter is one of the most unappreciated little commands.  It makes scatterplots so much easier to read!,evaluatrix,2012-02-04 12:18:01
"How to visually analyze and communicate data. 

(disclaimer: visualization is my research area)",visual_life,2012-02-04 11:59:49
"In terms of visualization, what are some of the upcoming innovations in that field? I do visualizations as well and I follow David McCandless, flowingdata,  and Aaron Koblin quiet a lot. Also read some of the blogs; however, I haven't been in the loop over the last year.",dassouki,2012-02-04 12:04:10
"&gt; what are some of the upcoming innovations in that field?

The visualization community tends to push in many directions at once. Here are a few:

* Data-driven Documents ([d3](http://mbostock.github.com/d3/)) is a fantastic small Javascript library for making interactive visualizations.

* [Tableau](http://www.tableausoftware.com/) already has a widely-used tool for doing visualizations, and they're pursuing ""big data"" analysis via Hadoop. 

* Network visualization has seen a recent push in the form of new visual metaphors for networks (see [this](http://eagereyes.org/techniques/graphs-hairball) post on eagereyes.org)

* Finally, there has been interesting work combining visualization and psychology/cognition. For instance, your locus of control (internal/external) can affect your performance with certain types of visual tools.


",visual_life,2012-02-04 12:20:52
Flowingdata is great. I think david mccandless is extremely overrated and should be treated more as a graphic designer than a statistician.,shaggorama,2012-02-04 16:33:03
visualize.  You'd be surprised how many people don't do this as first instinct even though any stats/math program worth its salt will drill into you that the first principle when encountering new data is to visualize it.,daidoji70,2012-02-04 12:12:06
"Just because you can, doesn't mean you should. 

Said otherwise, you have to know what the fuck you are actually doing when you run an analysis. Just because you can CREATE a p value, F score, what have you, doesn't actually mean that it has real-world interpretable meaning. ",thedazzler,2012-02-04 12:22:42
"* Don't overthink it - it's better to do something simple well than doing something difficult poorly. If you can do an analysis with a simple OLS, don't go rambo with something much more intricate.

* Realize the difference between correlation and causalisation; the former is often presented as the latter - and being able to see when this is not the case will definitely help you. A good dose of humility will help when you're presenting as well, so that you won't oversell your results.

* Be structured - when programming, it's important to keep your code clean and explain what you do and why (at least when you're starting out). This is especially when you're sharing code with others!

* As several people have already said; look at your data. Use summarizing statistics, visualizing, whatever you need to properly understand what you have in your hands.",pkg_inc,2012-02-06 06:27:45
"* What DrNewton said.

* Truly understanding complex experimental designs

* What Independent Variable _really_ means (e.g., genes are not IVs in people; they are measured).

* The three Rs: R, R, R.

* Also MatLab and SAS.

* The SVD

That's all I have for now.",dearsomething,2012-02-04 10:49:54
"What do you mean when you say genes are ""measured"" and not IV?",hella_bro,2012-02-04 12:03:56
"In humans, you cannot manipulate genes. You can only measure someone's genotype or expression level or whatever. But these are often treated as IVs in a way that imply the same level of causality as IVs that are manipulated.",dearsomething,2012-02-06 07:53:18
"Ah ok, I think I see what you mean now, thanks.  In mice experiments, for example, you can consider the genes as IVs because you can manipulate them and make a reasonable attempt at inferring causality as long as the experiment design is good.  So you are saying that in humans, because the genes can't be ethically manipulated, you can't rule out confounding variables thus making causality assignment difficult?",hella_bro,2012-02-06 10:31:17
"&gt;Ah ok, I think I see what you mean now, thanks. In mice experiments, for example, you can consider the genes as IVs because you can manipulate them and make a reasonable attempt at inferring causality as long as the experiment design is good.

To a degree. A lot of the ""identical"" mouse and rat and other critter experiments are still not accomodating epigenetics and environmental differences. They try, but it's not guaranteed how that turns out either. But yes, in these cases, it's much more an IV than a DV.


&gt;So you are saying that in humans, because the genes can't be ethically manipulated, you can't rule out confounding variables thus making causality assignment difficult?

Exactly. In my opinion, it's all strictly correlational with lots and lots of support to start (and continue) theory building of the role of certain genes. But we don't have a way to make them IVs in humans, we just observe the gene and the outcome (be it disease, behavior, etc...). ",dearsomething,2012-02-06 10:41:11
Psshhhh on SAS.,daidoji70,2012-02-04 12:10:18
"SAS on Psshhhh.
""Psshh"" might actually be a good name for a language",y2kerick,2012-02-05 13:45:24
"There is an overwhelming number of resources for R out there, are there any particular tutorials or books you feel greatly improved your work in R? ",kerosion,2012-02-04 14:52:00
[Stat Methods]( http://www.statmethods.net/)!,dearsomething,2012-02-04 16:14:59
yup.,itsaroboticbear,2012-02-04 11:43:28
"Experimental design, law of large numbers, Central Limit theorem, visualize your data.  (you'd be surprised how many ""experts"" I run into on a daily basis in the business world who don't understand these concepts very well).

Also, shamelessly stolen from other posters, experimental design, R, and what an Independent Random Variable really is.",daidoji70,2012-02-04 12:14:24
"Simulation: a lot of things that are hard with analytic techniques are very easy with simulation, provided that you take DrNewton's advice and learn to program a computer, program a computer, program a computer.

Also, Bayesian methods.",AllenDowney,2012-02-05 12:30:18
" - Bootstrapping
 - Never discretize continuous valued variables
 - Know linear algebra inside and out
 - Penalized regression (ridge, lasso)
 - know your distributions, use glms instead of trying to force normal distributions
 - RKHS's

",nblarson,2012-02-04 12:49:39
"YES. So many people want to just run OLS on the whole entire world, even when it's not appropriate. That's the most depressing thing to me. I hate having conversations with people trying to tell them not to use OLS, too, because they refuse to listen. I don't want to sound pretentious, but it'll give you the wrong marginal effects!",econometrician,2012-02-05 14:26:27
"I agree with most of the article, but I think it's a mistake to point to the small sample size as a problem. The real problem is that the sample is not random. You can always draw valid inference from a random sample, no matter the sample size. If it's too small to learn anything from, the confidence intervals will tell you.",standard_error,2012-12-01 00:40:20
"Thank you so much for sharing this! I am a stats person, and my research field is special education. I have posted here multiple times about the value of having people who know content and the value of people who know research methodology on a research team. When you don't have both, you end up with this level of work.  ",in_hell_want_water,2012-11-30 15:08:51
"This is a common problem across the board. I'm in I/O, so any news article that wants to talk about unemployment, hiring practices, workforce effectiveness, etc. will always procure a healthy dose of facepalm. The writers usually take the first paragraph of the Discussion section and run with it, and almost never deal with the limitations that comes after.",neurorex,2012-12-01 04:45:54
"Not that it would change your conclusions in this case, but Bonferroni is too conservative so you've just swapped a liberally-biased procedure with a conservative one. Try the Benjamini-Hochberg False Discovery Rate. It seems to have nice properties in between the two extremes. ",misanthrope237,2012-12-01 06:35:55
"I know that Bonferroni is usually conservative, but it's interesting that we're using this in the case there we have so few tests (9). Does the conservatism in Bonferroni kick in immediately? Does it get considerably worse the more number of tests we have?

For example, in this case, the corrected p value threshold should be .05/9 = .0056 (ish), which seems like a severe drop. But on the other hand, re running the code in the article with a FDR correction yields

    &gt; adjustedPvaluesPain = p.adjust(pvaluesPain,method=""fdr"")
    &gt; adjustedPvaluesPain
    [1] 0.1600000 0.1866667 0.7500000 0.4342857 0.4000000 0.1866667 0.4000000
    [8] 0.4000000

which means nothing would be significant (unless alpha = .2, which I'm sure isn't good enough).

In general, I guess it's good practice to use multiple testing corrections more often than not, but I wouldn't wouldn't call 9 tests a troubling situation. Am I wrong?",greensmurf30,2012-12-05 10:42:54
Look at Bayes theorem. Any disorder that is rare will have many false positives.,,2012-12-01 10:02:28
They didn't include cited bible verses in their bibliography. Lazy reviewers.,samclifford,2012-03-12 17:16:14
"""We declare that we have no conflicts of interest.""",Yeahbutnah,2012-03-13 03:14:22
"&gt;The risk can be postponed, but it cannot be reduced, as Wen and colleagues claim, and nor can it be eliminated.

We'll see about that, come meet us over at [/r/singularity](/r/singularity) ",AndrewKemendo,2012-03-13 02:55:42
"The authors of the original study politely reply:

""We agree with Ashley Croft and Joanne Palmer that the risk of mortality is an absolute that can be postponed but not eliminated. We emphasised the potential of exercise in reducing the mortality rate in a given year, not per se. Although the probability of death is 100% in the long run, we can reduce the speed of approaching death by walking briskly 15 min every day and thus extend our lives. It comes with a better quality of life, and that applies to us as well as to the prophets.""¹

¹Chi Pang Wen, Min Kuang Tsai, et al. The Lancet, Volume 379, Issue 9818, 3–9 March 2012, Pages 800-801.
(http://www.sciencedirect.com/science/article/pii/S0140673612603420)",jt512,2012-03-13 13:57:25
The first thing I checked was that it wasn't published on April 1st. Hmm.,Neurokeen,2012-03-12 16:55:40
"Joke day for British journals is January 15th, I believe.",cookie_partie,2012-03-13 04:02:17
Never heard that before... ,TheLeaderIsGood,2012-03-13 08:23:33
"Sorry...it appears that it is in December.

[Santa Claus](http://www.onlineopinion.com.au/view.asp?article=9881), [No control group for parachutes](http://www.bmj.com/content/327/7429/1459.long)",cookie_partie,2012-03-13 15:19:36
"Oh do you mean medical journals specifically?

I love the footnote on that parachutes one :)",TheLeaderIsGood,2012-03-13 15:39:16
"Those are the only journals I am familiar with since I am a pharmacist.  It might even only be BMJ, actually.",cookie_partie,2012-03-13 15:44:19
I've certainly never noticed it in any general newspaper or anything like that. It's usually April Fool's Day and that's it.,TheLeaderIsGood,2012-03-13 16:31:55
Hilarious,xxrealmsxx,2012-03-12 16:49:19
In Python? Very nice!,,2011-07-20 23:54:06
"I read through the first couple of chapters, and I think this is a keeper.  Nice conversation style with plenty of good classroom type examples. ",manova,2011-07-21 06:10:40
"I took a 5 minute look and as a programmer who knows statistics, I think this book looks nice.  The contents are a good introduction to a statistics novice, and the explanations seem clear and accurate.",0111001101110000,2011-07-21 11:33:53
/r/badstats/ already exists for this purpose,normee,2015-02-11 09:13:50
Ah look at that.  Thanks for pointing it out!,Jrizzler,2015-02-11 10:31:00
but it's not very active. i'd love to see more action there!,hypermonkey2,2015-02-11 17:30:00
Post some stuff then.,naught101,2015-02-11 22:05:13
done. just posted a paper i found this week.,hypermonkey2,2015-02-12 07:58:47
"If you like this sort of thing, you should listen to [More or Less](http://www.bbc.co.uk/podcasts/series/moreorless) - it's a podcast from the BBC devoted to finding and calling out bad statistics in the media.",usrname42,2015-02-11 12:50:07
"Subscribed, looks sweet!  Thanks!",Jrizzler,2015-02-11 21:06:32
"Posting of well constructed arguments pointing out flaws in statistical approaches or interpretations are encouraged for submission here. 

It would be more beneficial to also have constructive comments on how to approach the problem and/or interpretation (instead of just pointing it out and only criticizing).",dearsomething,2015-02-11 17:29:59
"/r/politics

/r/lawyers?",mo_ntresor,2015-02-11 12:54:22
r/damnliesplus,syntaxvorlon,2015-02-11 08:00:27
r/showmethepower,DrHappyFunTime,2015-02-11 08:11:20
"Apart from the ones already mentioned, I'd look at /r/dataisugly -- not purely statistics but definitely still enjoyable to browse through!",random_sampler,2015-02-11 20:07:00
/r/badstats exists and I took over modship of /r/badstatistics.  If you guys submit to it I'll moderate and stuff.,beaverteeth92,2015-02-11 20:16:34
This reminds me of one of the supplemental texts we read in my Stats class in college: [How to Lie with Statistics](http://www.amazon.com/How-Lie-Statistics-Darrell-Huff/dp/0393310728/),RalphMacchio,2015-02-11 20:46:47
r/statlies,Er4zor,2015-02-11 08:25:40
/r/QuitYourStatisticalBullshit,chrico031,2015-02-11 09:04:46
Kind of weird thing in there - correctly analyzing the Monty Hall problem has nothing to do with Bayesian vs null-hypothesis testing statistics. It's just basic probability.,waterless,2014-09-30 09:42:08
I think there's a common myth that Bayes Rule = Bayesian statistics. ,1337bruin,2014-09-30 10:23:12
"&gt;~~myth~~

misconception",jayjacks,2014-09-30 10:27:59
mythconception,somkoala,2014-09-30 11:39:46
Mike Tyson- statistician.,,2014-09-30 23:16:20
Mike Tython - thtatithtithian.,remington_steele,2014-10-01 06:44:49
"MHP is Bayesian because the probabilities in question are epistemic. Even the initial uniform distribution (the prize is equally likely to be behind each door) is unjustifiable to a frequentist, because the actual distribution of prizes is unknown. Short-term, the actual distribution is one of (1, 0, 0), (0, 1, 0), (0, 0, 1), not (1/3, 1/3, 1/3), and long-term it is (p, q, 1-p-q) for unknown p, q decided by the show staff. Or it may not even be a random process at all. From a Bayesian perspective the uniform distribution arises from the principle of indifference, it describes the fact that the doors are all the same to us, not any fact of how the prize is actually distributed in short or long term.",Coffee2theorems,2014-09-30 11:40:04
"Monty Hall isn't a statistical problem. It's pure conditional probability, with the assumption that the contestant is guessing uniformly and the host isn't guessing at all.

The Bayesian vs. Frequentist question is totally irrelevant.",1337bruin,2014-09-30 12:55:03
"That's an interesting argument. I'm not sure I agree that assigning a probability without reference to frequencies makes something Bayesian per se, or at least not in the sense of Bayesian vs frequentist statistics. I mean, the MHP isn't even a matter of *statistics* at all. We're not making inferences about populations from a sample, we're solving a closed probability problem. A statistican using a frequentist approach to inference or hypothesis testing isn't somehow forbidden from using common sense or ideas like symmetry in order to set up a chance model.

But you mean the more fundamental conceptualization of probability I think. I think that there's an underappreciated trick that expands how far you can get with frequentist models, which was introduced in my undergrad statistics classes as kind of a given: you can consider imaginary replications to develop a distribution*. Something like, you can't toss a coin more than once, but you can imagine doing so infinitely many times in the same given situation, which provides your distribution. Even if you have a unique situation, you could imagine how it could realistically vary, and how often within that variation an event would occur.

* I want to coin the term ""counterfactual frequentism"" for this.
",waterless,2014-09-30 12:02:45
"Also, if *I* choose a door uniformly at random, the price is behind that door with probability 1/3 in any case, regardless of the rule with which the price was assigned to a door.",shele,2014-10-01 01:28:59
"There is a distinction between Bayesian methods and ""the Bayesian interpretation of probability."" It is possible to take the Bayesian interpretation of probability and still use frequentist techniques (i.e. draw conclusions based on the sampling distribution rather than the posterior; this isn't ""coherent"" from the logical standpoint, but neither is the use of frequentist methods within the frequentist probability interpretation). 

Regardless, the MH problem can be phrased in terms of Bayesian and Frequentist probability without issue. The frequentist simply embeds the particular MH game in a countable sequence of ""equivalent"" games, and *assumes a priori* that the long-run frequency of the car appearing behind any given door is 1/3rd. Then one works through the calculations as usual and reaches the same conclusion as the Bayesian. ",NOTWorthless,2014-09-30 13:04:05
"&gt; There is a distinction between Bayesian methods and ""the Bayesian interpretation of probability.""

This is like saying there is a  distinction between ""applying Bayesian reasoning to problem A"" and ""applying Bayesian reasoning to problem B"". It is true, but it is hardly an objection to what /u/Coffee2theorems said. The idea, which seems to be entertained by some here, that the ""basic probability"" you apply to puzzles like Monty Hall is somehow a distinct concept from the probabilities in statistics is confused. 

You can set this up as just comparing the expected values of two random variables, talk about infinitely many games or whatever, but this really is a decision theory problem, and a perfect illustration of how natural the Bayesian approach is. 

You *can* call yourself Bayesian and use frequentist methods, and vice versa. You can do whatever you want, but the interpretation and methodology debates can not be wholly separated. [Some of the most important problems in statistics](http://en.wikipedia.org/wiki/Behrens%E2%80%93Fisher_problem) are really about interpretation. The ""pragmatic approach"" consisting of jamming all your data through whatever procedure, Bayesian or frequentist, and choosing whatever gives you the best results is indefensible, even if it ends up being mostly harmless in many cases. Results have no meaning if you lose track of what ""probability"" means in your setting.",kohatsootsich,2014-09-30 14:25:05
"I'm not really sure how this justifies what /u/Coffee2theorems said about the MH paradox being specifically Bayesian; I explicitly gave a framework for grounding the problem within the frequentist interpretation of probability. Whether you choose to interpret probabilities as long-run frequencies or as degrees-of-reasoned-belief has absolutely no bearing on the calculations underlying the Monte Hall problem. 

WRT interpreting probabilities in a ""Bayesian"" manner but still using ""frequentist"" methodologies: I can refuse to put a prior on the parameter space and merely specify what my degree-of-belief of each data realization would be for each value of the parameter (or more generally for each measureable subset of the sample space). Then to conduct inference about the parameter, I could use the sampling distribution - again, interpreting the sampling distribution as the a priori degree-of-belief I *would have* about the data under each value of the parameter, rather than as a long-run frequency. So, I have interpreted probabilities logically as degrees-of-belief and *not* as long-run frequencies, but I am still being essentially using frequentist technology because my inferences are based on a sampling, rather than a posterior, distribution. So we see that there is a difference between being Bayesian about probabilities and choosing to conduct *Bayesian inference* which requires the additional step of agreeing to place a prior on the parameter space. ",NOTWorthless,2014-09-30 16:50:05
"&gt;I'm not really sure how this justifies what /u/Coffee2theorems said about the MH paradox being specifically Bayesian

No problem is ever ""specifically Bayesian"" (although there are definitely setups that are easier to interpret in one framework or the other). I would venture to guess that no one, when presented with the Monty Hall problem, thinks about what would happen if they played an infinite number of games. The Bayesian solution is simple and corresponds closely to intuition. This is the only meaningful interpretation I can imagine for /u/Coffee2theorems statement that ""MHP is Bayesian"".",kohatsootsich,2014-09-30 17:16:45
"&gt; I would venture to guess that no one, when presented with the Monty Hall problem, thinks about what would happen if they played an infinite number of games. 

Laypeople don't think about playing an infinite number of games; they also don't think about rationally updating their prior beliefs. They are laypeople who haven't thought about what probability means. A probabilist presented with the MHP might think about an infinite number of games. I've witnessed a probabilist immediately do exactly that when confronted with the Sleeping Beauty paradox to help reason out what he thought the answer should be. For what it's worth, though, demonstrating to a skeptical layman that the probability of winning is not 1/2 by setting up an actual repeated experiment is very effective... 

&gt; The Bayesian solution is simple and corresponds closely to intuition.

There is no ""Bayesian"" solution to the MHP, there is just the solution. The  answer is that you have a 2/3rds chance of winning (up-to ambiguities in the problem statement), regardless of how you have conceptualized probability. ",NOTWorthless,2014-09-30 19:06:55
"&gt; Laypeople don't think about playing an infinite number of games ...  They are laypeople who haven't thought about what probability means

 Laypeople absolutely do have a concept of what probability means. Our strong preconceptions are perhaps the one reason the interpretation debate won't die. To the extent that they think about probabilities, people think in terms of relative, i.e. conditional probabilities. Bayes' rule is just the appropriate calculus.

&gt;I've witnessed a probabilist immediately do exactly that
 
I've also witnessed a fluid dynamicist immediately suggest using an iterative fixed point method when faced with an equation presented to him. I don't think he had any particularly deep insight about the behaviour of the fluid it was supposed to be describing.
 
&gt;There is no ""Bayesian"" solution to the MHP, there is just the solution.

Answer and solution are different things. Drawing a tree, enumerating every event in the sample space, using Bayesian updating, etc. are different solutions. In this example, difference may be minute because the problem is so simple, but in terms of how easy they are to comprehend, generalize, and implement, the many possible explanations/solutions are not the same.",kohatsootsich,2014-09-30 19:33:47
"&gt; Laypeople absolutely do have a concept of what probability means. Our strong preconceptions are perhaps the one reason the interpretation debate won't die.

They have intuition; the intuition of the layperson is also notoriously bad when it comes to thinking probabilistically, so I'm not sure how much that is worth. My point was they don't ground their beliefs in any kind of well thought out formalism, they just go with what feels right without thinking too hard about it.

&gt; To the extent that they think about probabilities, people think in terms of relative, i.e. conditional probabilities. Bayes' rule is just the appropriate calculus.

Thinking in ""relative terms, i.e. conditional probabilities"" can be done in both frequentist and Bayesian interpretations of probability. Bayes rule is a theorem of probability; it has nothing to do with the Bayesian or Frequentist interpretations of probability, aside from the fact that Bayesians happen to make use of this hammer more often. Frequentists use Bayes rule all the time, they just don't use it to update their priors. 

&gt; I've also witnessed a fluid dynamicist...

To be clear,my anecdote in response to your statement that ""no one"" would think of an infinite number of games. Clearly your guess is off, since anyone who sincerely adhered to the frequentist conception of probability would obviously think about an infinite sequence of games, considering that is what probabilities *literally represent* to a frequentist.",NOTWorthless,2014-09-30 19:57:09
"&gt; They have intuition; the intuition of the layperson is also notoriously bad when it comes to thinking probabilistically

I absolutely agree, which is why mathematics exist: to make complicated things clearer, not to needlessly introduce constructions that obfuscate things. The procedure of Bayesian updating closely follows our intuitive understanding of probability, and is thus easy to accept and remember.

The ""man on the street"" may not be able to adjust priors correctly (even at a rough, qualitative level), but it will be easy to convince him that the Bayesian inference procedure is sound, precisely because it corresponds closely to he believes he is constantly doing with new information. 

&gt; To be clear,my anecdote in response to your statement that ""no one"" would think of an infinite number of games.

""No one"" is an exaggeration. The fluid dynamics example was just an illustration of the fact that people (including experts) often just ""reach into the toolbox"" without further thought, so your anecdote about the probabilist did not seem convincing to me. My impression is that many people who profess to be ""frequentists"" are really pragmatists observing the law of large numbers at work, and deciding there is no reason to toss out the trusty old methods for what they perceive to be uninteresting philosophical issues. 

I'm afraid we've gone astray a little here. Let me just make a final comment: when thinking about the Monty Hall problem, the reason everyone (yes *everyone*) accepts the premise that, without further information, all the doors have the same probability, is really belief, based on prior experience, not some thought experiment involving infinitely many doors. You can invoke symmetry if you want, but that only displaces the problem. Your point about epistemic probabilities not necessitating priors is well taken, but the Bayesian procedure is only a small step away from deciding to make your parameter a random variable. You could, as you say, make inference about the conditional distribution given the of the parameter without specifying a prior, but now what if you encounter new data?",kohatsootsich,2014-09-30 20:27:36
"&gt; The Bayesian solution is simple and corresponds closely to intuition. This is the only meaningful interpretation I can imagine for /u/Coffee2theorems statement that ""MHP is Bayesian"".

Sort of. My point was more about the *philosophy* of probability than *methods* of probability. You can use whatever methods you want, but unless there is some underlying interpretation to them, it's just pure numerology. The fact is, if you put epistemic probabilities in, you get epistemic probabilities out; objective probabilities in, objective probabilities out. You can use whatever Rube Goldberg device you want to get a number out, but in the end I ask you, *what does that number mean*? If it means nothing, congratulations, you just wasted some time! If it means something, which type of probability is it? You can't really avoid this issue, unless it's an exercise in a book and you get a peanut if you write the right number in the right box and you don't really care if it means anything at all.

It's pretty contrived to assume that the host opens a random door when the contestant picks the prize door initially, because people just suck at producing randomness without the aid of machines, and because you just don't know he does that. It's hard to come up with a credible argument that the uniform distributions required to solve the problem are objective probabilities, but very easy to argue that they are epistemic probabilities. This makes the problem natural in a Bayesian setting and contrived in a frequentist setting. An integral part of frequentism is to say that some probabilities simply do not exist, it's Bayesians who willingly assign probabilities to everything. Now people try to throw that out simply because MHP is *popular* and Bayesians shouldn't have it all to themselves? No. Just no.

I realize that MHP is often used as an exercise in introductory probability theory courses, where the probability distributions are given as part of the problem. In that setting, it is a problem in probability, one of those ""word problems"" that are very thin (and usually meaningless) veneers on the mathematics being demonstrated. When it's presented as a real-life problem, i.e. you see the show and then wonder what the answer is out of your own curiosity, then it becomes a problem in statistics, because you have to model the problem yourself instead of your professor doing it for you. It is this real world problem that is Bayesian, and it is Bayesian because the probability you want to know is epistemic. You have no idea what the staff on the show does, just the information you see on the screen, and that's not enough to figure out any objective probabilities. Taking epistemic probabilities and claiming they are objective is really untenable and far worse than what frequentists accuse Bayesians of. The Bayesians at least don't claim that their priors are The Truth somehow. Claiming that e.g. the prize is objectively distributed uniformly is simply saying that the prior for its location is uniform and then trying to claim that you're not being a nasty Bayesian using nasty *subjective* things like priors because *your* prior is The Truth, you see, it's really out there in the objective world and what do you mean how do I know the show staff places the prize uniformly, well, call it a hunch, a really objective hunch! ... *right*. Because that *so* stands up to scrutiny.",Coffee2theorems,2014-10-03 01:04:32
"&gt; There is a distinction between Bayesian methods and ""the Bayesian interpretation of probability.""

Agreed. I'd go further and say there are no such thing as frequentist *methods*. I emphasize *methods* because that's an important word - perhaps I should say *estimators* instead.

TLDR: frequentists are not necessarily better than Bayesians at creating estimators that satisfy the properties that frequentists desire.

Given a complicated model, a person might say ""I want an unbiased estimator for Parameter X"". However, there is no recipe that is guaranteed to construct an estimator with the desired properties. You may have to just try a bunch of different estimators arbitrarily and hope you find something with low bias. A frequentist does *not* care what form the estimator takes, nor what 'mindset' was used to create the estimator. The frequentist just performs an objective test (""Is this an unbiased estimator for Parameter X?""), but doesn't really care about the estimator otherwise. Perhaps the best estimator ('best' according to the frequentist) was an estimator created by a Bayesian.

So, if the model is complicated, where does the frequentist start looking for estimators that are well behaved? (consistent, lowish bias). A good place to start is to write down a prior and calculate a posterior. i.e. A frequentist is perfectly entitled to treat priors-and-posteriors as simply a form of ""estimator factory"". In many cases, the Bayesian estimator will not have the properties the frequentist wants, but sometimes it will.

TLDR2: Some people that are called ""Bayesian"" are actually ""frequentists that use Bayesian ideas as a source of candidate estimators"", and sometimes they don't even realise this themselves.",SkepticalEmpiricist,2014-09-30 15:13:48
"If the player originally chose a door with a goat, then the policy of 'always-change' will lead to them getting the car. If they originally chose the door with the car, that policy will result in them getting a goat.

The question ""should I change door?"" then becomes equivalent to ""did I originally pick a door with a goat?""

Then two questions are relevant. Was the car originally distributed uniformly at random among the three doors? Was the player's original choice distributed uniformly at random among the three doors?

I think the first question doesn't matter if the answer to the second question is 'yes'. If the player originally chose a door uniformly at random, then the chance of them having a goat after the first choice is 0.66 and the chance of having a car after changing is 0.66.

Some have suggested that the placement of the car isn't random, and therefore an arbitrary prior for that is needed. But it is sufficient to have a uniform initial choice by the player.

If the player's original choice wasn't uniformly distibuted, then we all have to know why before we can offer any advice.",SkepticalEmpiricist,2014-10-03 00:08:33
"Andrew Gelman's response: [No I didn't say that](http://andrewgelman.com/2014/09/30/didnt-say/)

",AstroZombieDC,2014-09-30 08:26:47
The irony of a media source publishing spurious conclusions when trying to discuss frequentist statistics is not lost on me.,ATG77,2014-09-30 09:43:09
"Since they mentioned the Monty Hall problem, I wanted to provide the expanded version of the problem in case anyone is interested or confused by the reasoning. A ""mental mindset"" if you will.

Imagine instead of 3 doors, there are 10 doors.

1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10

You choose door number 3, and he reveals a goat in 8 of 10 remaining doors, leaving only 2 doors remaining.

_ | _ | 3 | _ | _ | _ | _ | 8 | _ | _ 

Would you switch doors then? Of course you would! You had a 1 in 10 chance of picking the car on your first choice, and now you're looking at way better odds! Well, the same conclusion applies in the three door version of the problem, though with a considerably smaller increase in the odds from pre-reveal to post-reveal. Hope that helps!",ATG77,2014-09-30 09:50:31
Thanks! That was actually really helpful. ,antaries,2014-09-30 16:12:50
"What I don't quite understand is this: I start with 1:10 chance and randomly pick three. Then I can choose again with 1:2 chance - but both options are equally likely, right? So why not stick with 3 (or, phrased differently, ""pick 3 again"")?",flrrrn,2014-10-01 06:43:48
"Sometimes this happens. Try imagining it with 100 doors or 1000 doors. 

If that doesn't work, try thinking about it this way; Monty is never going to remove the door with the prize, so when you pick door 3, one of two things can happen to the other 9 doors: either Monty will remove the 8 other doors that were incorrect (if you were wrong when you chose door '3'), or Monty will choose a random door to not remove and will remove the other 8 (if you were right to choose door '3' initially). The former happens 90% of the time, and the latter happens 10% of the time. You should rely on your expectation that the former will happen more often, and thus you should switch on your second choice.

The same applies in the original Three-Door Monty Hall problem, but instead of 90/10 the probabilities are 66/33. So you see, it's still preferable to switch, but less so.",ATG77,2014-10-01 07:44:43
"Alright, that makes more sense now. Thanks!",flrrrn,2014-10-01 08:57:28
"Good read on the topic and history of the rise of Bayesian Statistics here:

http://www.amazon.com/The-Theory-That-Would-Not/dp/0300188226

",AndersonCoopersDick,2014-09-30 18:21:59
Unless they tacitly acknowledge that both depend fundamentally on the Central Limit Theorem to make any sense and that the differences on paper are *actually quite subtle* I don't want to read it.,casualfactors,2014-09-30 23:18:42
What's the difference btwn bayesian &amp; frequentist? I'm a n00b in statistics!,-Ulkurz-,2014-10-01 08:29:03
"Two different, but similar sounding, Coursera courses started today.

One is *Statistical Inference*, which is a one-month segment of the nine-segment Johns Hopkins Data Science specialization track. It's actually the 6th in the series.

The other is *Data Analysis and Statistical Inference*, which is a 10-week course being taught out of Duke University.

The Johns Hopkins course assumes you've taken some of the previous Data Science track courses. I would not recommend starting this one cold.

The Duke course, from what I can tell, is much more basic and doesn't seem to assume much prior knowledge. For example, in the first week they have an introduction to R, RStudio, and R markdown.

So if you're new to all this, I would suggest Duke over Johns Hopkins.",vmsmith,2014-09-01 17:35:26
I took this in Feb but found the weekly workload too heavy. I decided to work my way through the textbook first and then take the course next year.,HungryAuryn,2014-09-01 07:35:14
"Thanks for pointing this out. I just started another course anyway, but will take this one at some time in the future too. Just downloaded the pdf and will prepare a bit and get some good idea what it's all about.",PilotPirx,2014-09-01 08:12:13
"I signed up for Statistical Inference and they send me few messages today. Excerpt from one of them:

&gt; Stretch the course out: Since the course is offered each month, you should feel free to stretch it out if you need more time to master the concepts. If you decide to do this, just be aware that you can’t carry your quizzes over from one session to the next.

So, if you find workload too heavy, you don't have to wait entire year - you may just sign up for one course again and, effectively, give yourself two months to complete it.

But, to be fair, I don't know if that applies also to other classes in Data Analysis course.",mzalewski,2014-09-01 09:33:14
"It's a different course, not the Johns Hopkins data science track.",thefrontpageofme,2014-09-02 03:45:28
"Right, you are correct. I thought that since they have very similar names and are both on coursera, they are the same thing.",mzalewski,2014-09-02 08:34:31
"It seems like almost all the classes repeat monthly, and that spreading the class out over more than a month is possible for them all.

My own experience was with ""Exploratory Data Analysis."" I signed up late last month since I finished early with the work for ""Getting and Cleaning Data."" I've already been able to turn in a couple week's work that I did last month. Hopefully I'll still have enough time to do well in Statistical Inference!",ossicones,2014-09-01 11:41:32
[deleted],,2014-09-01 11:36:40
"i dont know what exactly happened but if youre saying that since the mean and median are within 3% of each other and the standard deviation is 13%, they should by (like under a hypothesis test) ""equal"". was the problem that they are not necessarily ""equal""?",BahBahTheSheep,2014-09-01 12:13:10
[deleted],,2014-09-01 13:30:56
"yea but the standard deviation is small enough that an apparent ""closeness"" is actually not that close",BahBahTheSheep,2014-09-01 14:09:13
Is it better than John Hopkins course?,javasharp,2014-09-01 20:30:28
They're both different. The Duke one is basic.,HungryAuryn,2014-09-02 05:52:14
"I'm on the Johns Hopkins DS track right now and found their Statistical Inference Course extremely lacking.  I managed to pull it off with distinction but I really don't understand the material!  I really don't know who that course is tailored to, If you had the math background to understand the material, chances are you don't need the course.  They assume some level of familiarity with Linear Algebra and Basic Stats.  

My last stat course was over a decade ago so I kind of need to start from the basics.  I'm currently taking the Duke course as a kind of supplement before I jump into the Regression Analysis and ML courses.

As for the other courses I can say overall I'm pretty happy with the content so far.

BTW, If you're really looking for a balls to the walls Intro to Data Analysis, I'd highly recommend MITx Analytics Edge.  That course is tough but incredibly well done!  The exercises cover everything from Sabermetrics to Regression trees in Health Care Data to Text Analytics and more.  Sadly, life got in the way around the week 3 point and I never got to finish the course.",quik69,2014-09-02 15:09:50
Thanks.Will they re-conduct the course?,javasharp,2014-09-02 18:24:54
"""Please use a modern browser with JavaScript enabled to use Coursera.""

NoScript and Ghostery.  I see why they block this rubbish, i allowed coursera.com and their cloud storage and my memory and CPU skyrocketed.  No thank you.",homercles337,2014-09-01 13:14:07
https://github.com/coursera-dl/coursera,farsass,2014-09-01 13:34:06
"http://swirlstats.com/

this. so much. It will teach you R within R. 

just type:
install.packages('swirl')
library(swirl)
swirl()",persedes,2014-08-06 10:55:08
Also a Hopkins tools and used in the Data Science series. ,t_rex_tullis,2014-08-06 12:12:46
"&gt; install.packages('swirl') library(swirl) swirl()

If you're going to put a sequence of commands all on one line, put `;` between them (so copypasting the code will work). Better still to put them on separate lines in code format:

    install.packages('swirl') 
    library(swirl) 
    swirl()

But useful information",efrique,2014-08-06 17:45:37
"Hey this is pretty awesome, I just completed the first session.  I work with large survey data with hundreds of variables.  Is there anyway to do data mining with this style of data.  The problem is that I get lost with the amount of variables, do you have a program that helps/practice for that type of data?  Thanks.
",_Widows_Peak,2014-08-06 18:37:02
"You probably want to look into Factor Analysis/PCA, that'll help you narrow down the variables into something manageable.",Adamworks,2014-08-07 09:08:04
There are tools like Weka that are meant for data mining and will use R as a backend. Your general bet is to use something like a scatterplot matrix (or scagnostics) to do your variable screening. ,TeslaIsAdorable,2014-08-07 10:15:18
"This was a timely post for me. I had just installed R+RStudio last night in the hopes finding some learning materials today.

I just did the first 'course' in swirl. It was maybe 10 minutes, but it was engaging and it a lot of fun. I'm going to go do a bunch more of them.

Thank you!",SnOrfys,2014-08-06 18:52:31
"There's a coursera course on R programming (which 'runs' every month I guess) it seems decent, although it could be quite the leap if you never programmed anything before (like in never wrote some loops etc). After that course you could go on to the other courses in the series, which focus on cleaning data in R, and more advanced programming (designing packages etc.). There's also a course 'the data scientist's toobox' which gives an intro to things like github (which is a powerfull system to ""save"" you're code online using version control (git), thus you can go back to previous versions of code without saving hundreds of files).

https://www.coursera.org/course/datascitoolbox",Noshgul,2014-08-06 10:39:50
"I highly recommend the entire [Johns Hopkins Data Science](https://www.coursera.org/specialization/jhudatascience/1) series.

It is probably a highly effective way to save time, since it starts at the beginning, but also pushes and challenges you to explore the software in depth enough that you can later do stuff on your own.",rz2000,2014-08-06 11:44:56
"I done a lot of courses in statistics of John Hopkins in Coursera, and i'm doing the data science series, and it is really great, good choice of topics, and as professors offered some courses before the specializations one, i think the got the ideia of what is important in a mooc style course.",squiercg,2014-08-06 12:28:32
I'm doing it right now.  For the amount of time required (small) it's really impressive in how good it is.,FullSharkAlligator,2014-08-06 13:37:33
"Currently doing the John Hopkins series right now. I actually go to bed thinking about the problems I find in the problem assignments so I can fix them the next day.  Very exciting course, the assignments and discussion forums are very useful. I have no prior programming experience in industry. 

Edit: I thought I should mention Udacitys foundations of programming in python was very insightful since it taught me how to search for solutions to problems one stumbles upon. Codecademy taught me a lot  of syntax, but didn't quite get across the value of searching for information. ",Petrichor94,2014-08-07 17:51:21
"How is this one different than the data science toolbox?

Which one would be better for learning R?",Sonofparttimetrnsfer,2014-08-07 01:24:53
"The data science toolbox is a course in the Data science specialization, this specialization includes multiple courses. If you only want to learn R the relevant courses would be:    
* The data scientist's toobox (not strictly necessary tough, more of a practical how to save/share/control programming code)    
* Getting and Cleaning Data (getting data from the web/xml files/API's ...)    
* R programming    
* Reproducible Research (how to create reports that goes along you're R code, how to document etc. Again not strictly R programming but very nice to know since statistics is useless if no one else gets what you've done)    
* Developing Data Products (creating R packages etc.)    

The other courses are more statistics oriented, but of course you'll only learn R if you use it a lot so those might be interesting too.",Noshgul,2014-08-07 01:44:52
"Ive been taking it. From my perspective, the first course was a waste of time just by itself. It really should have been combined with another class. The R Programming class was totally fine.

I'm in the 3rd and 4th class now. I am more familiar with Python at the moment for data analysis purposes, but I've heard good things about R. I figured I'd give it a go.",antisyzygy,2014-08-06 15:46:57
"The first course could be completed over a Saturday, but it is probably a good introduction if you are completely new to R.

I thought the projects were just the right size. They were open-ended enough that you couldn't trick yourself into thinking you understood everything, like you can with the quizzes, but they weren't huge amount of repetitive busy work either.

As I've described the JHU courses elsewhere, I think they are a lot like learning something from a smart peer in the workplace. They are direct and don't waste your time, but that means they don't try to sell the subject matter to you either.

A couple months ago I took the first four at the same time because I thought it would mostly be review. Though I could go through some things really quickly, I think they filled in holes. The projects took longer than I thought they would, but I'd be able to do them really quickly now.

If you're already comfortable with the concepts from using python for analysis, you might want to take many more of them at the same time and ignore the prerequisites.",rz2000,2014-08-06 17:37:30
"I've thought about taking three at a time, however Im taking them with someone at work. We meet up once a week to discuss so I don't really want to blast ahead of him. I was just saying so far I think everything I've seen but the first course is fine.

Installing R-Studio and using Github (for what they had us using it for) are not difficult enough to warrant an entire class. The other material was useful to know as like an overview of ""Data Science"", but it could have been incorporated into something that perhaps covered a little bit of programming or stats. Hell, Swirl could have been an actual assignment in that class instead of extra credit in R Programming.

I guess I didn't mention it but I already work in ""data science"". I've got a grad degree in mathematics and did some ML research mostly in computer vision. My mathematician professor that supported me as his RA was involved in a biomedical lab under the control of a ""distinguished"" member of the CS department. I basically was the only math guy working with all CS guys and gals.

Anyway, I figured I am weak in R and I could use a review so I may as well take the courses. I've been reading up on stats on my own for the last few years. I will totally admit one good thing that came out of that ""Data Scientists Toolbox"" course was that they compartmentalized the difference between descriptive, inferential, casual, etc. statistics. Understand that in CS/Machine-Learning we call them something different, and that was what I was exposed to first.",antisyzygy,2014-08-06 20:35:46
Responding to save on mobile.,Fallline048,2014-08-06 17:11:59
"I'm a bit confused, is this course used solely for R programming? I'm on the site right now and haven't heard of git or github before so I'm wondering how it fits. 

I started using the Swirl package that someone else in this thread mentioned, would the coursera course be a good supplement to the course?",Sonofparttimetrnsfer,2014-08-07 01:20:15
"Well git or github aren't R specific, they are just awesome tools for programming.   
Git is a system of saving multiple versions of you're code (in the git jargon you'll commit a piece of code). It basically allows you to save a lot of versions of you're code (in chronological order) without having to create hundreds of files.    

This is great because lets say you want to try adding some new functionality to your code but you totally ruin some other functions/the flow of your code/..., if you've commited the older version you just need to click a few things or pull you're code form an online repository and voila you've got your old working code back.      Github is a way of 'saving' your work/git repository online, this also allows you to share your code with other people. You can also ""fork"" existing programmings avaible on github and make changes. Fix bugs in other peoples code and push you're code to their github 'site' (they then review the changes and might allow them to become final in their own code ...).     


edit: since you mention swirl: the code for swirl is also avaible on github https://github.com/swirldev/swirl_courses#swirl-courses thus you could create extra content and easily sent it to them :). And I only used swirl when it had barely any content available, but I guess the coursera courses would be a nice supplement (or at least the slides are).

     
TL;DR: github/git isn't necessary but will possibly save you a lot of time once you get into some serious programming
",Noshgul,2014-08-07 01:58:20
"Thanks a lot for both responses! Right now I'm gonna get into Swirl and then the Data Scientist's Toolbox, and try to use Git in the process. I will try to cover the other courses as well but the $49 price tag is a little steep so I may only do a couple of them. 

Either way, this is all useful information, and so far Swirl has been awesome and I can't wait to start the Data Science Toolbox course next month.",Sonofparttimetrnsfer,2014-08-07 02:24:31
"You only need to pay to get the 'certification', the rest is free! Thus you can watch the lectures, do the quizzes, do the assignments without paying. The only 'negative' is that you don't get a certification for every course ... ",Noshgul,2014-08-07 03:12:51
That's awesome! And do I have to do them at a certain time frame? Other people said they did one of the courses in one day when on the site it says it runs for a month..,Sonofparttimetrnsfer,2014-08-07 15:46:17
"The Toolbox course only involves installing RStudio and setting up a Github account (which you need for course projects in the subsequent courses).

You can probably jump right into the R Programming course and do the Toolbox stuff during the first week.

 ",urubu,2014-08-07 03:01:20
"I see, I'm already a bit experienced in R + Rstudio, so maybe I will do the Toolbox course fairly quickly then do the other ones.",Sonofparttimetrnsfer,2014-08-07 15:47:54
"Google has some lecture videos for learning R

http://www.r-bloggers.com/google-developers-r-programming-video-lectures/",carmichael561,2014-08-06 09:52:04
"TryR.codeschool.com

Interactive tutorials",Corruptionss,2014-08-06 12:59:32
"http://r-dir.com/learn/tutorials.html
There are a few e-books listed on another page. Full disclosure, this is my blog.",lenwood,2014-08-06 10:25:36
https://www.datacamp.com/,skripp,2014-08-06 13:37:05
"https://courses.edx.org/courses/MITx/15.071x/1T2014/info I found this one useful, but now it is in archive mode, not sure if you can get access.

This one looks useful, http://blog.revolutionanalytics.com/2013/04/coursera-data-analysis-course-videos.html, but I still didn't get to it.

Also, this one https://www.youtube.com/playlist?list=PLHPcpp4e3JVr6ssgmSamERz_39TuxEris

It is always good to combine multiple sources for better learning.",anirdnas,2014-08-06 13:57:44
"I am self-teaching R right now. I am using the following books:

http://health.adelaide.edu.au/psychology/ccs/teaching/lsr/
http://www.routledge.com/articles/latent_variable_modeling_using_r_a_step-by-step_guide/
http://www.amazon.com/Handbook-Statistical-Analyses-Second-Edition/dp/1420079336
http://www.amazon.com/Data-Analysis-Graphics-Using-Example-Based/dp/0521762936
http://cran.r-project.org/doc/contrib/Lam-IntroductionToR_LHL.pdf

All can be found for free online either legally or pirate.",Deleetdk,2014-08-06 16:15:25
[deleted],,2014-08-06 17:24:34
"I appreciate your broad insight into how to use R. It's really helpful to have a big picture (even if it's blurry, without details) when learning a new subject. 

Thanks!",afatsumcha,2014-08-06 18:17:08
"There are really great resources in this thread, you should absolutely use them to get started. I would also recommend just picking a data set that you are interested in - there are tons [at UC Irvines repository](http://archive.ics.uci.edu/ml/) if you need inspiration. I personally try to do things with sports data or something fun, but googling around how you do different things is always a great way to expand your knowledge beyond the intro chapters.",trousertitan,2014-08-06 15:27:28
"Something important has not been mentioned yet that everyone learning R should know. Once you get the hang of basic programming aspects, this will be your best tool:

?

Typing ""?"" and any function name into the console will launch a browser window with the help file for that function. R has an amazing built-in documentation system, better than any other language I know of, chock full of cross-references to similar functions and examples clarifying their use.",saruwatari_takumi,2014-08-07 00:47:54
"This actually helped me understand some of R's... ""unique design choices"" shall we say. [The R inferno]( http://www.burns-stat.com/documents/books/the-r-inferno/).

Abandon all hope yee who analyze data here...",Dr_Roboto,2014-08-07 06:04:54
"&gt;the most R experience I have is where the professors basically give you the code to input so you can interpret and analyze the data.
   
Sounds like my university. Funny thing, people will then put ""knowledge of R"" in their resumes, unbeknownst to them that when we need someone who actually knows R, they will trigger a hit and then... you can imagine.",Hrim,2014-08-06 21:09:15
"Commenting to find
",BrettMorningwood,2014-03-29 01:50:13
"Hey! You said to report back to you on if I still like mowing my yard after a year, and Im just bored enough to do it! SO:

Its been crazy year for me: I split up with my wife, my grandpa died, and then I found someone super amazing. And through it all, my yard has been there.

I still enjoying mowing the yard after all this time. I am looking forward to mowing it tomorrow! Anyway, hope you had a fun year also! Happy Mowing!",no1flyhalf,2014-04-20 21:17:25
"Haha! Cheers for writing, I remember asking you!

Good to hear your garden stuck with you through rough times, I guess gardens just may be man's 2nd best friend. :)",BrettMorningwood,2014-04-21 02:03:57
Looking forward to reading/viewing this,,2014-03-29 02:46:02
"The sexy thing everyone's talking about now is ""real-time analytics"" just one step beyond what you're saying here. Despite the fact that your proverbial wall is also built to fragment any enterprise reporting team (i.e. old analytic software does not mesh nicely with web-apps, or typesetting software, etc.), I wonder how and whether we can do really simple CGI based stuff to produce continuous reports.

The enterprise usually looks like this (ordered by tiers, bottom first)

1. Data Warehouse Layer (SQL, optimized storage, really tough to configure)

2. Expensive proprietary ETL software that both developed the DWL and *must* be used to extract the data lest you spend millenia trying to reverse engineer their algorithms. These generate analysis data that lives on production severs and gets blasted away pretty often as updated runs of data summaries happen.

3. Reporting tools: which are bulky javascript programs built by enterprise developers who need a web reporting framework that suits every possible population (and yet none). Which render graphics, tables, and other aggregate measures from production data.

As an analyst, you rarely have a hand in much except an intermediate step between 2 and 3. This is a sad state of affairs since leadership is wont to tasking things to large distributed teams instead of letting one renegade analyst run wild and try to do 1-3. It's the bane of us who are moderately technically savvy.",,2012-10-08 11:12:01
"As a Data Scientist, I could not agree more.  ",ICrepeATATs,2012-10-08 17:40:07
"I have a different, more historical perspective. I graduated before R, or for that matter, R's predecessor S was released. Fortran was the language most people used, outside of the COBOL-using business majors. SAS, SPSS, BMDP and a host of other statistical packages were really *programming languages* much like R is today.


A key constraint back then that I don't hear much about now is code validation. If you wrote a program in a programming language, it would probably not be acceptable in many venues, such as high-risk businesses like pharmaceutics, the scientific community, and Courts, without convincing validation and documentation. That's part of why *canned* procedures became so popular.


I think the key is *flexibility.* Learn how to learn new things. Your next job my not want you to use R or Python. Some businesses don't want an employee to use a tool that only he or she can use. They want assurance that another employee can step in if you get sick or leave the company. But that's just my opinion.
",bucketlist60,2012-10-09 07:28:12
"Thanks for this link. I am a math major in school and am looking to get into statistics. I already know some Python, so this definitely a morale booster for me!",,2012-10-08 07:52:37
I tried to make the case to my employer that my r and python skills save them the cost of an spss license. They looked at me like I was speaking a foreign language. It was in the context of requesting administration privileges on my 500.00 laptop. Such foolishness. Still running everything off a thumb drive. ,alien8r,2012-10-11 06:48:21
"And even in cases of quite mild overlap, things get very, very rough.

And then you have the occasional crippling, crippling slowness at times.

And then it's still only a local optimum!

And how are you supposed to choose *k*? Or initial centers? 

This is all rough even when all the assumptions are met and you're generating from Gaussian distributions with an identity covariance matrix. Life is awful and everything hurts.",giziti,2015-01-19 13:45:18
"So this is a new clustering algorithm which, while not exactly super principled, works really well in practice.
http://www.sciencemag.org/content/344/6191/1492",TheCaterpillar,2015-01-19 14:57:50
"Huh, interesting. Documentation for the R implementation can be found [here](http://cran.r-project.org/web/packages/densityClust/densityClust.pdf) (PDF), in case anyone's interested.",tf113,2015-01-19 15:34:49
"Looked for a free copy of the paper but couldn't find one unfortunately.  However, there is some info [here](http://people.sissa.it/~laio/Research/Res_clustering.php), and a Python implementation [here](https://github.com/jasonwbw/DensityPeakCluster)  Another [implementation](http://eric-yuan.me/clustering-fast-search-find-density-peaks/) in C++, with some info on how it works.  And [Dcluster](https://pypi.python.org/pypi/Dcluster/0.5.0) another Python implementation.",radarsat1,2015-01-20 00:41:44
"That looks pretty neat, thanks. I unfortunately don't have access to Science papers right now. How do they define the density of a particular data point?


Edit: According to a python implementation it seems like it's either a simple convolution with a Gaussian kernel centered at the datapoint or a count of the number of data points within a certain distance.",breadwithlice,2015-01-20 04:11:25
Thanks for the paper recommendation!,PhaethonPrime,2015-01-20 05:27:10
"Nice. This helps to explain why Klustakwik keeps letting me down when trying to sort spikes from bursting neurons (amplitude decreases with # of spikes in a burst, leading to oval/elongated clusters).",griffer00,2015-01-19 17:16:24
"Has anyone ever done a wide study on the most commonly occurring real-world statistical models?  That is to say: as this article states, you can always come up with a dataset that will ""break"" under a given set of assumptions.  But my question is, how often does ""real"" data (that is, all ""data"" across many different disciplines) line up with these artificial cases?  Even if there is ""no free lunch,"" is there any lunch with has a strong tendency to be ""free-er"" than others?

Globally what I'm wondering is, can you state that, for any problem, given *enough data*, then something as general and simple as K-means will *tend* to handle the problem?  Are real-world datasets asymptotic towards any particular model, as the number of data points goes to infinity?",radarsat1,2015-01-20 00:34:27
Wouldn't that just be non parametric methods? Usually they tend towards a precise solution given enough data. Correct me if I'm wrong.,thai_tong,2015-01-20 03:34:57
Nonparametrics are nice but don't forget that they can be very prone to overfitting.,beaverteeth92,2015-01-20 07:38:45
isnt this what ISODATA was for?,quatch,2015-01-19 19:35:02
"That one dataset in particular, sure, but it's not a free lunch.",giziti,2015-01-19 19:38:03
"few things are, but at least for any set where kmeans is applicable, and the problem is a question of the number of k or initial centers. I'm willing to trade in the longer run time.

(this sounds grumpier than I mean, sorry. I'm just a user, not a particularly proficient statistician)",quatch,2015-01-19 19:44:13
"Nice article for starting a discussion on the subject, but has some unfortunate holes. I think the arguments here overlook the difference between exploratory/hypothesis generating research and confirmatory hypothesis testing. There's a similar lack of nuance regarding decisions to control experiment-wise error rates vs family-wise error vs. error per comparison (vs false discovery rate of that matter). 

In both cases, it's important to respect the pragmatic requirements of research, where it is often going to be more efficient/effect to collect a lot of measures on subjects rather than only the minimum measures for a pre-registered study. There needs to be a balance between maintaining the strictest statistical controls (which is really an issue of proper caution in interpreting results) and allowing researchers some flexibility to pursue analysis of unexpected trends or mediating factors observed in a study beyond the limits of pre-registered primary analyses. I suspect self-replication is a much more useful initiative than the stronger emphasis on pre-registration in this article.",dinkum_thinkum,2014-10-16 18:56:15
"None of this is new. People have been complaining about this for a while and with the recent emphasis on replication, it's getting better. However, despite the incredibly lengthy discussion of the problem, the authors provide almost no real solution. 

As a researcher, we all know what the problem is. The real problem is that  I haven't heard a good solution. ",Palmsiepoo,2014-10-16 20:29:55
"&gt;  I suspect self-replication is a much more useful initiative than the stronger emphasis on pre-registration in this article.

My read on the article is that they agree with you and the discussion on pre-registration is to mainly point out its limitations and impracticality in most instances. ",itsactuallynot,2014-10-16 21:20:54
"&gt; I suspect self-replication is a much more useful initiative than the stronger emphasis on pre-registration in this article.

I agree with this part whole-heartedly, but why would report *p*-values if you are just doing exploratory analysis? Why report a precise number if you are not willing to give it precise meaning?",kohatsootsich,2014-10-16 20:58:44
"I'm totally in favor of reducing the influence/reliance on p values, but in their defense they still give you info on the per comparison error rate, and work in cases where effect size/confidence intervals may be unavailable or difficult to interpret (e.g. Omnibus tests, model comparison, any case you resort to permutation p values). For publication purposes, reviewers/readers probably have a more intuitive (if flawed) grasp on interpreting p values than raw test statistics.",dinkum_thinkum,2014-10-16 23:27:13
"&gt; between exploratory/hypothesis generating research and confirmatory hypothesis testing

'When is something exploratory?' 'When you get a p-value bigger than 0.05.'",gwern,2014-10-17 20:04:28
"Overall, in my opinion the problem can also be tackled by good and honest interpretation of your data and the stats you have *chosen* to do. The chosen statistics are at the discretion of the researcher, an important thing to remember. However, publication bias towards only publishing significant effects has been a problem in research for a long time, and since I am currently writing up a study just now, I know the issue all too well. 

So you get an effect - the graphs show it, your participants discuss it during debrief, a decent amount of literature supports the effect... but you have some comparisons that turn out to have p&gt;0.05. For example, I have two conditions which are hugely significantly different from eachother, but they are not significantly different from zero in of themselves. What does this mean for my overall explanation of the data? Does that one null effect, which I believe can be explained by the subtle differences between my study and previous work, mean my chances of publication are lowered? Probably. Is that an ok situation? Well, no - I put time, effort and care into my work as much as anyone else, plus the rest of the study demonstrates a very neat and interesting effect. But those null results still affect the chances of being published, at least in a high impact journal.

There are always the solutions of changing to Bayesian statistics, which offer a move away from standard Null Hypothesis Significance Testing (there's a good discussion of this idea [here!](http://www.researchgate.net/post/Bayesian_vs_frequentist_statistics2)) and reporting [effect sizes!](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444174/). The latter is the simplest solution, since most statistical packages allow you to easily calculate effect sizes as part of your analysis. Effect sizes are useful because they allow you to envision how much of the overall effect was explained by your manipulation, so you can see how 'important' the effect was. **Large** effect sizes mean your manipulation can explain a lot of variance in the data, **small** effect sizes mean your manipulation is not explaining much variance in the data. So if you have a situation where you have a **large** effect size but no statistical significance, you have a way of demonstrating that what you are reporting is still important and interesting - it is explaining a good deal of change in the data, but the effect failed to reach statistical significance perhaps because you didn't test enough participants, or the effect is just difficult to detect due to its nature. See [here!](http://psychsciencenotes.blogspot.co.uk/2012/08/the-small-effect-size-effect-why-do-we.html) for a good description of an area in perceptual psychology where this is especially true. 

It's a thorny issue indeed.",Diasalae,2014-10-17 00:39:27
journals have been requiring effect sizes for many years now.  that's nothing new and doesn't solve the problem,,2014-10-17 06:02:35
"Not all journals. Also, in some social sciences people haven't even heard the term.",mguzmann,2014-10-17 07:34:31
relevant: http://andrewgelman.com/2014/10/14/didnt-say-part-2/,doryfore,2014-10-16 23:22:30
"Thanks for posting this. I'm vaguely interested in MCMC but I haven't got very far yet.

Something I don't understand on the very first line of the Wiki entry: ""In statistics, Markov chain Monte Carlo (MCMC) methods (which include random walk Monte Carlo methods) are a class of algorithms for sampling from probability distributions based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. The state of the chain after a large number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.""

Why would you want to learn a chain that has the desired distribution as its steady-state? Why not just learn a histogram of the desired distribution? Sorry, I know this is a stupid question.

The linked pdf gives examples which make a lot more sense to me, eg learning the transition matrix for the prison text makes sense because language is sequential, so you need to model transitions from one letter to another. You're not interested in the steady-state. How can I understand the Wikipedia comment?",jmmcd,2013-09-25 08:42:08
"[Warning: fuzzy answer to a fuzzy question...]

If you are thinking of a univariate distribution, then it would be feasible to represent the distribution with a histogram (aka PMF) and sample from that.  But as the number of dimensions increases, the number of bins in the histogram grows exponentially, so the amount of data per bin gets small fast.
",AllenDowney,2013-09-25 11:21:49
"Okay, so I started this, and it got really long. I had gloss over a lot of details and am technically incorrect in some parts, though pedagogically I feel my explanation makes more sense. I could write about this all day, but I do have a day job. Just reply with followup questions if this doesn't make sense.

I'm going to go over Gibbs sampling which is a kind of MCMC method that is much easier to explain. In general, MCMC is used for sampling from multi-variate distributions. The difficulty using other methods to sample from such distributions is how you simultaneously sample multiple variables that are  correlated. For example, imagine you have two binary variables X,Y with the following probability table:

|X | Y | P(X , Y)|
| :---: | :---: | :---: |
| 0 | 0 | 0.5 |
| 0 | 1 | 0 |
| 1 | 0 | 0.25 |
| 1 | 1 | 0.25 |

Clearly this table shows that the possibility of (X,Y) = (0,1) is impossible. If you tried (a very naive way) to sample each variable independently you could end up with that possibility (i.e. sample X, then sample Y). 

We can solve this we *rejection sampling* where you simply throw away inconsistent samples, but this means every time you sample an inconsistent point, you are wasting time. This can be a problem in a distribution with a lot of zero-probability events.

Another approach approach to fix this is *prior sampling* where we sample from P(X) then P(Y | X). This way we can never sample zero-probability events. The issue here is that our sampling is dictated by the initial distribution P(X). Let's construct a (pathological) distribution which might screw this up:

|X | Y | P(X , Y)|
| :---: | :---: | :---: |
| 0 | 0 | 0.4 |
| 0 | 1 | 0.4 |
| 1 | 0 | 0 |
| 1 | 1 | 0.2 |

Here, P(X=0) = 0.8 so we are going to get a ton of samples with X=0, and we will need a LOT of samples to get a representative amount of cases where X=1. Note that Y is uniform when X=0, but it has a clear skew towards 1 if X = 1. We will have to sample a LOT of cases to represent this distribution.


One way around the above problem is to do *likelihood weighting* where we don't count our samples as a single case, but rather weight them by how likely they are (I'll leave the technical details to the reader since this is getting rather long). This weighted sampling gives us a representative distribution via samples much more quickly. 

Keep in mind, these are all pathological examples, and there are a lot more reasons to use (or not use) the above methods. I work a lot with graphical models where there are even more kinks in sampling methods (like conditioning on evidence nodes). That being said, Gibbs gives us a way to do sampling that gets around the above problems. For X,Y we simply do:

1. Set t=0

2. Sample P(X^0,Y^0) from random initiation (e.g. uniform dist.)

3. Sample X^(t+1) from P(X|Y^t)

4. Sample Y^(t+1) from P(Y|X^(t+1))

5. t++

6. Goto 2

Under certain conditions, this will give you a representative sample of your distribution by converging to a steady state. It's relatively quick (assuming it mixes well) and easy to use.

For general MCMC methods, I highly recommend the ipython notebook [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers). 


 







But now we have a new issue.",fjeg,2013-09-25 12:10:09
"Wow... that is really excellent. Thanks to you and the other answers also. The fact that variables could be correlated is one thing I was missing. The fact that MCMC scales better than a histogram, as mentioned by the other posters, is surprising -- but that is the other thing I was missing.

I'm reading *Markov Chain Monte Carlo in Practice* by Gilks, Richardson and Spiegelhalter, and the early chapters are good, and readable, but like all textbooks it fails to anticipate all the ways the reader might be confused. Having a forum like this to ask questions, where the answers are great, solves this problem.

EDIT: Errr... I'd better check I understand it right. I know you're talking about the Gibbs sampler, which is not the same -- a special case? -- of MCMC. But maybe the question still applies. In your setup, there are two binary variables. If I wanted to make a Markov chain for MCMC, would I then have 4 states 00, 01, 10, and 11? Or would I somehow make one Markov chain per variable?",jmmcd,2013-09-25 13:13:49
A classic use of MCMC would be to use your MCMC samples in a [Monte Carlo estimate](http://en.wikipedia.org/wiki/Monte_Carlo_method) of some function of interest -- it could be as simple as the value of a parameter and its variance under the posterior.  Learning a histogram would suffer from the [curse of dimensionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality): if your distribution had 20 dimensions you would need on the order of 10^20 cells in your histogram.  MCMC methods scale fairly well with dimension.,lboyles,2013-09-25 11:20:41
nice find! I'd really like to know more about the history of statistics,zdk,2013-04-09 20:16:32
Saved! I will read this when I am not scrambling for a jyerrb :&lt;,somehacker,2013-04-09 22:50:40
"This is how maths papers and journal articles in general should be written.

If you're bright you should at least be able to make your work interesting and fun. Bravo sir",,2013-04-10 04:30:17
"Instead of developing anything new they should all be histories? 

I'm a fan of Stigler's work chronicling the history of statistics, but I don't think it would work if everyone did what he does.",efrique,2013-04-10 06:30:34
"Fascinating read. I'd love to read the author's biography of Fisher, who is further solidified in my mind as a true genius of his time as a result of having read this paper.",kusetsu,2013-04-10 05:34:18
This is one of my favorite historical accounts of MLE! Stigler does some awesome work. [Here's another one of my favorite papers by him.](http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.aos/1176345451),econometrician,2013-04-10 07:48:10
Very dense read.,featherfooted,2013-04-09 20:56:17
"Took this course.  It was...alright, at least the first iteration of it.  Prof. Peng provides a great introduction to the language, but does not really delve into statistics at all.  The bulk of the course is spent learning how to import and manipulate data into useable forms, which can be tricky in R.

I would recommend having at least some programming experience prior.",itsaroboticbear,2012-10-29 07:43:04
"This is Jeff here, from Simply Statistics. Roger's course was designed to teach the mechanics of R. I know he made a pretty strong effort to help folks who didn't have much background, but obviously there is variation in backgrounds. He would definitely love feedback on the course. 

If you want to learn the statistical component, my course in Data Analysis: https://www.coursera.org/course/dataanalysis is the natural continuation of Roger's course. Hope to see you in that one!
",t_rex_tullis,2012-10-29 08:23:34
"good to see a course on actual stats. I signed up for the course mentioned. And while moving data around R can be a bit tricky, its certainly nothing that should take weeks and weeks to learn.",zdk,2012-10-29 09:24:59
"Hi Jeff!  Fittingly, I already have signed up for this course when I saw the email from Roger yesterday.  See you in January.",itsaroboticbear,2012-10-29 09:44:12
"thanks for posting. do you plan on taking down the course, or will it be there for a while? EDIT: never mind, I just realized it starts again on the 22nd and I signed up. Looking forward to it.  ",oreo_fanboy,2012-10-29 10:28:11
"Could you explain what you mean by ""the natural continuation""? I ask as I found the programming assignments in Roger's course quite demanding despite my having a pretty strong background in statistics (probably because of my lack in programming skills). I guess my question is, if you expect that the participants in your course are able to program in R at the level from Computing for Data Analysis?  ",electron_thief,2012-10-29 13:43:04
"From my experience, the easiest way to manipulate data in R is to use STATA or SPSS to do it, and then transport it back into R.",makemeking706,2012-10-29 16:12:48
"Ha.  I never even thought of that.  I'm well versed in STATA, and you're right about it being easier to manipulate.  I like R for analysis a bit more, so I'll be using this technique in the future.

Thanks for the pro-tip!",itsaroboticbear,2012-10-29 16:29:20
"Just in case you weren't aware, the foreign package allows you to read/write data in various formats. ",makemeking706,2012-10-29 17:34:08
"I've just started with R.  I don't know STATA of SPSS but I am somewhat familiar with SAS.  Manipulating data in R seems easy, so far.  What are some things that are hard and that would be easier in other programs?",,2012-10-29 19:19:52
"I would like to take this, but it sounds like he is going to take it down",oreo_fanboy,2012-10-29 10:28:30
"It's not clear when we will be able to run it again through Coursera, but Roger is putting together another version that he'll be able to run shortly and all the material will be freely available online. ",t_rex_tullis,2012-10-29 10:38:45
Any idea how long the course material for computing for data analysis will stay up after the course ends?,randombabble,2012-10-29 18:06:55
"Did anybody actually thinking that taking an online ML course made them a ""data scientist?"" Attention all straw men, you are being knocked down.",zmjones,2012-10-16 07:23:11
"Agreed, the article confuses mastery with taking a course.  The article does point out what might constitute mastery.",whoMEvernot,2012-10-16 13:15:48
"I think a lot of people actually do, yes. In the same way that a lot of people are excited about becoming ""developers"" after taking a 12 week Ruby on Rails course. Basic ML is becoming commoditized... and probably deserves to be.

Then again, the challenge of democratized education is that you lose the gatekeeping.",tel,2012-10-16 13:51:25
"I don't know. Even basic ML requires far more math than most people taking the Ng course undoubtedly have. I think what will happen is what has happened in many of the applied social sciences, where most of the researchers have little math/cs background, and regularly make use of statistics for relatively complex problems; shitty inference. ",zmjones,2012-10-16 15:04:22
"""Big Data"" and ""Predictive Analytics"" being what they are right now, the demand will definitely drive people to fill that void. Then, after people start to realize it's a lot harder than it looks and most of the surface-promising techniques fail more often than not there will be a rush for people who understand more of the math, I feel.",tel,2012-10-16 15:20:38
"Yea you may be right about that, but then math isn't something that is easy to jump into since it builds from the very lowest levels. On a longer time scale for sure.",zmjones,2012-10-16 17:31:51
"I think a big part of the reason for bad statistics in social science is that it is rewarded -- you are more likely to be successful with dodgy statistics and `interesting' findings that are not real than you are with flawless statistics and a null finding.

However the same situation does not apply to `predictive analytics' -- if my predictions are wrong or at least worse than everyone else's I will fail.",deadsalle,2012-10-17 04:06:50
"That is certainly part of it. Still, it isn't like people do the analysis well and then poorly and then pick the ""interesting"" finding that is likely bs. For some of the more complicated social science problems it would be difficult to find ""interesting"" results with a too-restrictive model. ",zmjones,2012-10-17 05:10:06
"I am not sure your application of a Straw Man fallacy applies to your first statement, perhaps to the article yes.",antisyzygy,2012-10-16 16:23:50
"Well if ""people"" don't generally think that all that is required to become a data scientist is taking an online ML course or two, then slapping that argument down is slapping down a straw man.",zmjones,2012-10-16 16:29:07
"I understand. The article is definitely building up some ""Straw Man"" over one Coursera class, which most people taking it who aren't arrogant dilettantes probably realize isn't the only requirement for being a data scientist.",antisyzygy,2012-10-16 16:45:32
One does not sit in a box and just sit down and analyze data. For my data analysis I am a part of a team of biologists. So I need to know what questions are interesting to them and if my data can answer them. I also need to be able to communicate the importance of my findings in a clear way to them and provide suggestions for how to use it. This requires quite a bit of intuition and experience. ,,2012-10-16 01:18:41
"I agree with this respectable opinion.

I think operating costs of bio are a little bigger, though, so you think like an experimenter. Data scientists aren't really scientists. They don't collect data from a population, they have *all the data*. So when someone asks me a question, I produce numeric summaries with no confidence intervals. They say, ""How certain are we of this?"" I say, ""I'm 100% certain because it uses 100% of the data.""",,2012-10-16 09:26:22
"Agree, the best analysts are scientists and relate the data to the real world and understand what implications you can draw robustly ",pipie314,2012-10-16 06:18:08
"yet ANOTHER discussion of the ""new"" *data scientist* without a single mention and no obvious understanding of **informatics**. We've been doing *data science* for 30+ years in medicine and biotech, as real science.

",jmdugan,2012-10-16 15:34:00
"Agreed. Data-science and all the lingo associated with the ""skill-set"" they need is just some bullshit reformulation of language for skills people already have with the only goal of sounding impressive. I suspect a business-person is responsible for that.",antisyzygy,2012-10-16 16:42:25
"What is informatics? Or at least, what do you mean by it?  [Wikipedia](http://en.wikipedia.org/wiki/Informatics) ambiguates ",ICrepeATATs,2012-10-16 17:06:23
"off the top of my head, see

http://bmir.stanford.edu/

http://www.dbmi.columbia.edu/

http://www.cbi.cmu.edu

http://newsinfo.iu.edu/news/page/normal/8698.html

http://www.uthouston.edu/sbmi/

http://www.regenstrief.org/medinformatics

https://cbmi.med.harvard.edu/

http://www.bhi.washington.edu/history

http://www.cbcb.umd.edu

http://www.broadinstitute.org/

http://www.ncbi.nlm.nih.gov/

plus about 50-100 more academic training programs at top universities that are well known nationally.  

The early/older programs started in the late 70s and early 80s.  Basically all these programs teach computer science and data analysis using the tools of programming, databases, machine learning, structured vocabularies, ontologies, time series analysis, statistics, and **all the other stuff people are talking about when they say** ""*data scientists*"".  

All modern biology labs have lab partnerships with or staff on hand doing informatics because all modern biology research requires huge amounts of data and data analysis.  Those of us trained in PhD programs at centers like these listed above are actual data scientists, it's not a new field, we've been doing it, and training scientists in the field for over 30 years.


",jmdugan,2012-10-16 21:15:19
"http://en.wikipedia.org/wiki/Biomedical_informatics

and

http://en.wikipedia.org/wiki/Bioinformatics",jmdugan,2012-10-16 21:26:02
"why wasn't ""Data Scientist"" listed on my possible vocations when I was trying to figure out what the hell to do with my life???  This is so me.

Alas, I'm too old to change fields.",BellicoseBaby,2012-10-16 09:24:11
"Because it's just a buzzword for a trained statistician who knows something about programming. Good gracious, what a novelty!",dwf,2012-10-16 20:05:29
you are never too old.  ,avonhun,2012-10-16 15:06:03
"Because it's very new. Education isn't about training people to fit a specific role, its about educating them in how to think, apply knowledge and learn new things. There exists a significant disconnect between employers and their relationship with education.

http://spectrum.ieee.org/podcast/at-work/tech-careers/why-bad-jobsor-no-jobshappen-to-good-workers",antisyzygy,2012-10-16 16:38:54
"actually, the field is not really new at all, only the phrase ""data scientist"" is new.  Please refer to my other post in this thread.  

This meme that using and handling data with science is ""new"" is completely misinformed.",jmdugan,2012-10-16 21:18:15
"FYI I actually responded to your post I believe saying the same thing. My point here is that it's a new ""field"" as far as employers are concerned, and they don't bother working with Universities to tell them their needs for these people nor to find out that people like this already exist. There is a disconnect so Universities aren't always aware what is going on out in the business world, and business doesn't try to work with Universities to try and influence how people are trained for the roles they need.

It really just ends up screwing people sometimes since HR and Recruiters are sometimes dullards and can't be bothered to find out that people who took some statistics and informatics courses in their STEM field probably are actually a fit for the position they are recruiting for.

The business world is full of stupid buzz words.",antisyzygy,2012-10-17 06:25:42
"Having read that article late last night, I am so glad this rebuttal was posted. I am working towards becoming a data scientist (getting my masters in stats) and after reading the original article thought, ""Well damn, what am I wasting all my time for then?""",s87jackson,2012-10-16 10:35:04
"In other news, if you think anything worth being skilled at is ""easier than you think"", I have some magic beans you might be interested in purchasing.",dwf,2012-10-16 10:47:56
I'd only take issue with the Big Data Software-Hadoop point.  There is tons of Data Science that exists below the distributed-processing threshold. ,ICrepeATATs,2012-10-16 17:05:04
"The first article saw 3 people do it and thought everyone can do it? Man, maybe that first guy should take a stats class.",trousertitan,2012-10-16 09:24:30
"This comment sums up how ""data science"" actually works in the real world:

Marat responded:

Big percentage of ML-class students are already excellent programmers. So most of listed issues are known by most of students. For example, we have just created a team to participate kaggle contests, there are 1 statistician, 1 data analyst, 5 programmers, 1 student is getting phD on ML field. Everyone knows at least 2 programming languages, everyone knows how to use git.
All we need is just a gentle introduction to ML",rootmarshfield,2012-10-16 11:17:52
"Yes, no legend and I truncated the percentages instead of rounding them. I'm sorry. I promise I won't do it again.",LastVagrant,2012-08-03 08:16:10
Why not use fractions? There'd be no loss of precision.,VSMNeophyte,2012-08-03 10:21:03
"I started writing the 16.66% and then realised it would all be crowded. So I stopped at the 16. So once I started truncating, I had to continue.",LastVagrant,2012-08-03 10:28:53
"After writing ""16"" you could have extended it into ""16 2/3%"" is what VSMNeophyte is saying.",Nolari,2012-08-03 20:37:40
Or even better just write 1/6 ?,,2012-08-06 04:00:17
Gotta love Settlers of Catan :D,Cirri,2012-08-03 11:07:09
We could take a page from [Data Without Borders](http://datakind.org/our-mission/) and try to work through datasets for organizations that need the help but can't afford the service.,shaggorama,2012-07-16 06:21:57
I like this idea and I'd like to take part in the project regardless.,not_hitler,2012-07-16 08:13:33
"That is a brilliant idea. Doing an analysis for its own sake has pedagogical value because we can pick and choose the type of data and limit our scope to the methods of interest. However, something like what you suggest is both a good learning experience and also has the potential to impact the world for the better. I think this potentially rewarding avenue will definitely increase enthusiasm down the road.

Could you possibly start browsing for some data sets that we could use? And perhaps accompany them with some questions that organizations may wish to see investigated?",CommentSense,2012-07-16 08:33:12
I like the idea. It might be neat to set up a separate sub for this.,ViewofDelft,2012-07-15 18:55:18
"That would be great if it picks up steam and more people participate. Although, my personal preference would be to put it somewhere with maximal exposure. Maybe cross-posting?",CommentSense,2012-07-15 19:16:14
"Cross-posting *once in a while* in relevant subreddits would allow for the exposure you want, and keeping it in a separate sub would be a good way to let several discussions happen at once (without overloading folks that don't care for this sort of exercise). Just my 2 cents.",ViewofDelft,2012-07-15 19:25:15
"Your point about having multiple discussions is reasonable and it isn't something that came to mind earlier.

So let's see how this pans out and if it's successful then I would love to get your input on how to go about doing it. I am a mod in another sub but I know very little about CSS and all that good stuff.",CommentSense,2012-07-15 19:36:39
"I would absolutely want to be involved in this. I see you're planning to start will a small analysis, that's a pretty good icebreaker. This is a great idea.",,2012-07-15 19:38:18
Thank you. The idea is to get everyone comfortable with the process. How small or big the analysis will eventually end up is up to our collective imagination.,CommentSense,2012-07-15 21:30:48
"Absolutely. I don't think anyone would want to do a large-scale analysis right off the bat, but maybe after the group has some momentum?",,2012-07-16 05:17:12
"That'd be fascinating. I'm not all that advanced with stats yet (I'm in my junior year of my stats undergrad), but I think it'd be a great project. :)",alhanna92,2012-07-15 18:55:34
I think with any level of statistical competency you will be able to contribute in a meaningful way. Once the analysis is underway you can add to it what you know and build on the work of others.,CommentSense,2012-07-15 19:14:11
I also have access to SQL Server Studio and SSRS if we wanted to visualize the data as well. I am pretty good at writing queries and developing reports for analysis. ,marcopastor,2012-07-15 21:06:20
That's great. The idea is for everyone to chip in with whatever their strengths may be so that we can all learn a little something.,CommentSense,2012-07-15 21:25:48
I would totally love to be a part of this! ,anthonyhong,2012-07-15 18:59:20
Great! Stay tuned for an update this week.,CommentSense,2012-07-15 19:11:39
"If this turns into an actual thing, I might want in.",franzbiberkopf,2012-07-15 19:08:56
Glad to hear. Look for an update in the next couple days. I figure that to get the ball rolling I will start with a mini-analysis and have others chime in.,CommentSense,2012-07-15 19:11:06
"I would like to be part of this, I'm very interested in statistics and practice in group would come handy for me ",pdibella,2012-07-15 20:32:10
I look forward to your contributions. Updates coming real soon.,CommentSense,2012-07-15 21:27:45
"Very cool idea - What would be the most convenient way to make the process (code snippets, data preparation steps, and so on) available for community members to view? Would participants be willing to share their bags of tricks so openly? This could be a fantastic learning opportunity.",Phaedrus85,2012-07-15 20:38:19
"I think posting snippets of code (or pseudo-code even) will be the way to go. If people share their thought process as well as code, output and conclusions/remarks then you end up with a cookbook-type guide to analyzing that kind of data set.

Of course it all depends on how much people are willing to contribute.",CommentSense,2012-07-15 21:24:35
Totally into this. Make sure to keeps us updated.,myfourthHIGHaccount,2012-07-15 20:56:56
Glad to hear you're on board. Look out for a follow up post in the coming days. ,CommentSense,2012-07-15 21:27:01
I'm in as well. Keep me posted!,marcopastor,2012-07-15 21:03:58
Great! Check back in a couple of days for updates.,CommentSense,2012-07-15 21:29:21
I am interested. I am taking stats classes but I want to apply this material to real world situations. ,jergenjuice,2012-07-15 21:32:47
That's the best way to learn IMO. Let me know if there are particular methods/topics you would like to see addressed.,CommentSense,2012-07-15 21:45:38
I'd be down to get involved. So far only intermediate level undergrad knowledge of stats. If we're stuck for projects there's always [kaggle](http://www.kaggle.com).,ilki,2012-07-15 21:48:48
Kaggle is a fun site but the projects can get rather complicated. Let me know if you see something doable there. ,CommentSense,2012-07-15 22:03:33
"Very true haha, but will do.",ilki,2012-07-15 22:05:24
"You might be interested in checking out [kaggle.com](http://www.kaggle.com/) if you form a full-fledged data analysis team. Kaggle's a small site, but it's basically dedicated to data science via competitions with prizes. The best analysts can receive anything from honor and glory to upwards of $1,000,000! (Of course, that prize may be out of reach for whoever you gather here on Reddit. Nevertheless, it'd be working together with large-scale data sets to solve relevant problems. You may even be able to make resume points with this.)

I've personally barely used the site at all, largely because I don't have any team whatosever for this sort of site. I could use the practice, as I've graduated two months ago and have yet to find a job. I might not be able to get started right away, since in less than two weeks I'm going to the Joint Statistical Meetings conference in San Diego.",Recamen,2012-07-15 21:49:07
"I've been on kaggle quite a bit and even played around with their challenges. It's a nice place to apply your stats knowledge to some real-world problems (and maybe get rich along the way). But if you look at the write-ups on the winning submissions, you'll find many use machine learning and other non-standard methods -- some even ad-hoc -- that get very complicated very fast.

I am all for the idea of maybe having a /r/statistics kaggle team someday but for what I was suggesting maybe it's best to work on simpler problems. Or we can use kaggle data and then limit the scope of analysis.

Great suggestion though and I encourage you to give it a try even if you don't have a team. 

",CommentSense,2012-07-15 21:58:54
"I had a feeling those who were winning these contests would be above and beyond my level. Ah, well. In any case, I'm up for getting involved in this, as I think the group projects can help keep my skills sharp. I graduated with a Master's in Applied Math, and I have some old R codes and R itself to remind me of the programming, but I could use this stuff to keep me busy and up-to-date.

EDIT: Btw, if you haven't seen this post yet, I wanted to ask: How would groups keep in touch with one another? How would we coordinate who wants to do what? Or would it be just, everybody does his/her own thang and we all compare answers later?",Recamen,2012-07-15 22:07:22
"Don't feel bad, I read some of the reports and think not in a million years will I have thought of that. But then again, it's hard to judge what we are capable of until we really immerse ourselves in a given problem. I think as a mathematician you can appreciate that!",CommentSense,2012-07-15 22:17:19
"Haha, true that. I look forward to seeing some data sets, and if I think of any particular hypotheses, I'll let you know.",Recamen,2012-07-15 22:28:28
"I'm no expert in either, so I'm curious to know, what's your interpretation of the difference between stats and machine learning? It seems like there is a lot of overlap, and the communities seem to be working together more and more.

I guess what I'm getting at is that statisticians may be painting themselves into a corner by declaring ""classification is ML, and we don't do ML, we do stats"". Are they really all that different?",awap,2012-07-16 06:43:36
"From experience, and this is in no way comprehensive, I find that when we label something as 'statistics', it usually means that it is governed by a certain set of definitions, axioms and theorems that conform to mathematical rigor. ML approaches may use statistical methods or may turn to ad hoc procedures that don't have a theoretical basis.

Also, ML is traditionally used in vastly complex situations where the widely used models fall short. As a result, heuristic functions replace exhaustive ones so that computation may be completed in a reasonable time. We do use approximations in statistics, but they are based on some mathematical result (read asymptotic theory).

This question does appear in discussion circles from time to time. A simple google search will take you to articles on this by people far more knowledgeable than I am.

https://www.google.com/search?aq=2&amp;oq=difference+between+statistics+and+m&amp;sugexp=chrome,mod=17&amp;sourceid=chrome&amp;ie=UTF-8&amp;q=difference+between+statistics+and+machine+learning",CommentSense,2012-07-16 08:43:49
"This sounds cool! I'm an ecologist, not a statistician, but I would be interested if you chose something somewhat related!",Eist,2012-07-15 21:52:12
PM me a data set you might like to see analyzed in the future. Perhaps you could suggest some interesting facet that ecologists would like to see investigated.,CommentSense,2012-07-15 22:01:38
"I'm definitely interested in this, as a big reason I subscribe to this subreddit is to improve my painfully bad statistics. Personally, I'd love to see some statistics done on some data sets which are bio medically relevant - there's a lot of data coming out of the 1000 genomes project, for example, although that might be a bit too massive amount of data, unless it was able to be put up on an S3 server somewhere...",neurobry,2012-07-16 03:14:59
This could be very interesting!  I'm in. ,plf515,2012-07-16 04:25:24
This sounds great I am also very interested in this. Shaggorama suggested data without borders and I'd like to second that idea.,giror,2012-07-16 07:48:48
Thirded!,CommentSense,2012-07-16 08:33:42
I would definitely love to contribute my expertise in clinical design / biostatistics.  Can't wait for an update.,topheroly,2012-07-16 14:30:58
Any updates for us?,Phaedrus85,2012-07-25 14:46:30
"It is so ridiculous that the Census's budget is being cut when the Depression was the greatest increase in data collection, specifically for the reason of knowing what's going on in order to inform public policy. There's a ridiculous number of people, even here, who are convinced they know everything about complex developments in recent history, all while they have fundamentally misinformed basic facts.

Take the mention of the percentage of children immunized against MMR. Plenty of people will just make up whatever trend supports their beliefs if there were a sudden outbreak.

The mistreatment of the economics census by the untrained is of course even worse.",rz2000,2011-08-22 18:04:38
This is huge. It's hard to solve problems if you don't know what they are or how they work.,phosglue,2011-08-23 12:47:03
"Any researcher worth their title will know how to obtain the data they need in an electronic format online. The loss is to older researchers who are not comfortable with computers, and news reporters that are 'path dependent' on using the Statistical Abstract.

The data will still exist, and be available. All Statistical Agencies have important goals of data dissemination and do so already. If the Census has to shrink, should they axe a program that produces information or a program that publishes already published information?

If the Abstract is so important to Americans, then a private company can go gather the data, publish it, and sell it.",SmoothB1983,2011-08-26 18:47:12
It looks like it can do what I do now with eclipse+StatET+sweave but cleaner.  I'm excited to try it out,wtf_ftw,2011-02-28 09:32:32
"Dude, I'm as giddy as a school girl, this looks awesome. Thanks, broheim.",hadhubhi,2011-02-28 18:05:42
"Happy to see it runs on Linux. I'll have to check it out when I get home. 

Kind of curious what IDE/GUI other people use. At work (Windows machine), I use the default GUI (RGui), which I actually like for its minimalism (laugh all you've want, but I've been using R for 10+ years, and am perfectly happy with RGui). At home (Linux machine), I mostly use RGedit, which, like RGui, I like for its minimalism. Over the years, I've also tried ESS, RKWard, JGR, R Commander, and probably some others I'm forgetting.",,2011-02-28 16:33:51
"I use emacs+ess, because I use Sweave so much, and it's nice to write Latex, R and Sweave in the same editor.

It's a bitch to learn though",slammaster,2011-03-01 05:39:50
"I'm pretty new to R, but so far I've used RGui (I really like it for it's minimalism).  I had a brief stint with Rcmdr, because not having a drop down menus is scary.  I quickly realized that it was keeping me from actually learning the language and went back to RGui.  I have been using eclipse+StatET for a little while now and have been liking that.  After spending an hour or so in RStudio just now I think I may be converting (though the fact that it is in beta makes me hesitant to switch fully). ",wtf_ftw,2011-02-28 17:18:00
"YES!! As much as I love R, the default IDE is just terrible. Even from the screenshots I see a few features that I like (colour coding, tabbed editing, etc.) I don't know why it never occurred to me to look for another IDE...",dontstalkmebro,2011-02-28 15:04:36
"Not bad.  But they don't have the short function definition that shows up after I write the function name.  On the Mac gui, just typing 'rnorm(' brings up a little line in the bottom of the window that gives all the arguments.  I have to admit that I like the tabbed editor, but I wouldn't give up the syntax hints for anything.

The first thing I've noticed is that I can't change the working directory using the gui.  That is pretty annoying, but I'm sure that is just a beta release thing.",blossom271828,2011-02-28 21:20:01
"Try hitting tab after typing the function name, the arguments should pop up in a small floating box.",stetson9,2011-02-28 21:49:27
"Ooohhh... shiny...

That actually is pretty nice.  That just changed it from something I wouldn't bother with again to something I will.  Thanks!

Oh  I just figured out how to change the working directory...  In the files pane under the more tab... there it is.",blossom271828,2011-02-28 22:17:33
"I am often surprised that [this Excel add-in](http://rcom.univie.ac.at/download.html) isn't discussed more often. Even though it isn't ideal for large datasets, I think it is convenient if you use Excel on a regular basis for other tasks. The license is a little restrictive though.",rz2000,2011-02-28 15:11:02
protip: stop doing statistics in excel. stop using excel in general is optimum,boxfire,2011-02-28 15:55:33
"I'm not the biggest fan of Excel, but I also don't think it's as awful as people make it out to be. Sure, you can do stupid things with Excel. Guess what? You can do stupid things with most software. (See also: why I'm not a fan of Edward Tufte's anti-PowerPoint pamphlet; and for the record: I'm not a fan of PowerPoint.)

I've been in this field for a while, and I'm a real stickler and a real pain in the ass about a lot of things, but I don't believe in being an elitist about Excel.",,2011-02-28 16:47:15
"There have been a lot of problems with using Excel for statistics. [This article](http://portal.acm.org/citation.cfm?id=1377402) discusses some of the recent problems, while [this article](http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/ExcelProblems) mentions some earlier issues. Of course there was the infamous [850*77.1 = 100000](http://groups.google.com/group/microsoft.public.excel/browse_thread/thread/2bcad1a1a4861879/2f8806d5400dfe22?hl=en#2f8806d5400dfe22) bug from a few years back. My favorite was [this gem](http://i.imgur.com/twwIw.jpg) from Excel 2002. I don't think that Excel should be recommended for statistical work; I would advise people to not use it.",TheAntiRudin,2011-03-01 18:31:50
"Wow. I wasn't actually aware of any of this. I guess I should just stick to being an elitist!!

I do stand by my assertion that -- in general -- people should use what works for them... if of course, ""what works"" really works!

Thanks a bunch for the links; they really made my day.",,2011-03-01 23:35:38
"Thank you for posting, it looks like a great front-end and I have been looking for one for awhile since I have been having problems with JGR.",BirthDeath,2011-02-28 14:00:36
"I tried this out on my macbook. It works really well. I'm a fan of having everything contained in the one window. Plus, the UI is really intuitive.",NinjasInTheWind,2011-02-28 14:34:41
The thing I love about the mac R gui is the command hint they give in the bottom bar that gives the syntax... is that completely overwhelmed by the usefulness of the manual page?,blossom271828,2011-02-28 21:01:23
"This is fantastic, I've been playing around with it for awhile and it does everything I'm used to and more.

EDIT: It displays the help page within the IDE!  Awesome.",topheroly,2011-02-28 18:09:50
Baller!,,2011-02-28 19:38:37
"Please, please, please, please someone get this on Archlinux!

It's been done, it's in the AUR under the name: rstudio-desktop. Be warned though; it needs a huge dependency, qt-sdk, 500mb.",cbrunos,2011-03-02 08:27:49
"use the Mann-Whitney U test instead of the t-test.
Under ideal conditions for the t-test, the U-test is about 95% as efficient. In non-ideal conditions it is just better",I4gotmyothername,2015-02-11 22:14:01
G test instead of chi-square as well. Chi square is just an approximation of the G!,backgammon_no,2015-02-12 03:31:36
"In my opinion, many statistics programs do not put enough emphasis on software engineering skills, by which I mean not just programming, but also version control, automated testing, and agile development.

Here's an article I wrote about it: http://allendowney.blogspot.com/2013/05/software-engineering-practices-for.html

(And yes, distance correlation is a good one, too!)",AllenDowney,2015-02-12 05:20:20
"Unfortunatley R doesn't handle any development very well. Does stats very nicely, but doesn't do actual programming very well, including things like exception handling and such.",Europa_Explorer,2015-02-12 14:01:12
R is what you get when your language is designed by statisticians instead of computer scientists.,CrazyStatistician,2015-02-13 07:58:34
"Still better than SAS, though.",psychometry,2015-02-13 17:13:38
"I think this is true of any program that isn't comp sci/software engineering. I had to learn a bunch of this (and still am learning) for engineering work, and I see these issues all around.",PhaethonPrime,2015-02-12 05:49:14
In my experience those types of topics are not really taught in a computer science curriculum (at least undergraduate) either. My exposure to version control came during an internship. Software engineering at a graduate level may be different.,forgothowtoadd,2015-02-13 04:36:52
knowing a lot about computers and programming in general is super helpful for anybody studying math.,ryanmcstylin,2015-02-13 11:16:00
For my final project in my R programming class (the ONLY statistical programming class I had to take for my degree...) I scraped some data off of job posting websites to demonstrate how many statistical jobs require the knowledge of programs we never even covered in any class. So... yeah. ,lasercows,2015-02-12 07:23:08
how many?,DemonKingWart,2015-02-12 17:47:03
"Are you asking how many jobs require knowledge of programs that weren't taught in my program? I don't remember the exact amount, but it was a fairly big chunk. ",lasercows,2015-02-12 18:58:35
"Just a rough estimate of the proportion rounded to the nearest 10 percent
",DemonKingWart,2015-02-12 22:21:11
It's quite a new concept. It's usually more important to cover the foundational concepts than tackle something that's only been around for 8 years.,Palmsiepoo,2015-02-11 21:20:03
"Definitely true, it just seems like such a breakthrough that I thought someone may have told me haha. But I'm happy to know about it, and wondering if there are other things I'm missing!",bromeliadi,2015-02-11 21:22:46
Could you explain why it's such a breakthrough? I've never heard of it before,zphbtn,2015-02-12 05:10:30
"I don't really see what's so awesome about it. Correlation has a clear interpretation (strength of linear relationship), distance correlation doesn't seem to AFAICT. The example on the wikipedia page of points in a nearly perfect circle, where there's extremely strong dependence but the distance correlation is only 0.2, doesn't really excite me all that much.",CrazyStatistician,2015-02-12 10:19:27
"If you liked distance correlation, you should probably also look into the Renyi correlation, that has been around for a longer time, and shares many of the nice properties (while perhaps having some other nicer properties). See [here](http://projecteuclid.org/euclid.aoas/1267453934). 


**Edit**: Also don't know what degree you got, but the point of most college educations is to provide you with the means to go out and *continue learning* about the more advanced materials. To be ""in the know"" about the most recent statistical techniques, you simply have to read the literature. No one can teach it all to you, there is too much. So when you have a problem, you should go investigate what is the current state of the art in that area. ",DrGar,2015-02-12 14:44:20
"Haha, I got an undergrad, and I do know that! My classes have been really repetitive and I've learned a few things multiple times. So I guess I felt like there could've been more material and finding things like this makes me wonder why they didn't just mention it once, you know?",bromeliadi,2015-02-12 18:00:45
Also this paper is sweet,bromeliadi,2015-02-12 18:41:40
"Everything ever written by [Bickel](http://www.stat.berkeley.edu/~bickel/) is worth reading. For what it is worth, he wrote my [favorite text on mathematical statistics](http://www.amazon.com/Mathematical-Statistics-Selected-Topics-Edition/dp/0132306379). It is a good book for someone with a BS in statistics to start with if they want to go into graduate level depth of understanding of statistics. ",DrGar,2015-02-12 19:00:11
"This looks nice! I actually know most of the topics in this book, but I'm totally unfamiliar with asymptotics so I'll read that!",bromeliadi,2015-02-12 21:53:36
"&gt;I actually know most of the topics in this book, but I'm totally unfamiliar with asymptotics so I'll read that!

Be careful young Jedi. Mathematical statistics is a different beast. You might ""know"" maximum likelihood estimation, but do you understand under which conditions it works in exponential families (well enough to prove Theorem 2.3.1 in B&amp;D)? What problems might occur if you look in a curved exponential family (via Theorem 2.3.3)? etc.

My graduate stats prof says he tries to re-read Bickel and Doksum once a year. Being that I am not a statistics prof, I only set aside time to do it about once every three years. I do this because the material is very dense, and I do not claim to understand everything in that book. Things I once understood might also slip from memory, or things that didn't ""click"" before will suddenly click thanks to other things I have encountered and read since my last time going through the book. 

So no offense, but I doubt that you already know most of the topics in this book. Unless someone is a professor of statistics, I pretty much will assume that they do not fully understand this book's contents.",DrGar,2015-02-13 08:01:16
"Haha, very good point. You're absolutely right, thanks for the humbling lesson :). And thanks for the book recommendation, will read it soon!",bromeliadi,2015-02-13 09:24:36
"Keep your great attitude and it will take you far :-)

",DrGar,2015-02-13 09:30:31
"&gt; so much better than correlation in every way

In *every* way? So, for example it has better *power* to detect linear association than say the Pearson correlation? 
",efrique,2015-02-12 05:51:21
"It's actually surprisingly close for detecting linear associations:

http://statweb.stanford.edu/~tibs/reshef/comment.pdf
",nrs02004,2015-02-13 01:24:45
"Thanks for the link; that's useful. I'd point to the warning at the end of their first paragraph, which points out why distance correlation (any more than the MIC) *can't* be best at everything.
",efrique,2015-02-13 14:56:58
"I definietly agree, but it is surprisingly effective at least for the types of dependence we tend to imagine (though there's no reason those must be the most important).",nrs02004,2015-02-13 17:38:47
"I wish Marchov chains were talked about more than just a quick ""This is a Marchov Chain, next subject...""",Iliketrainschoo_choo,2015-02-12 10:43:42
"Under-emphasis of SAS/R/etc.

Yes I know it's elegant to prove the beta is the prior for the binomial or that something us the UMVUE for something else (it's been way too long to even attempt making an example) but I have literally never used any of that in the real world. My most valuable class was non-parametric methods and experiment design since they used SAS.

I learned more useful (meaning, take it right into the office and start doing cool stuff) stats stuff from MIT's AI course and Stanford's ML course on youtube.

Nobody should graduate from school with a BS and not be proficient, if not fluent, in a stats package. In my opinion. Pen and paper math is not what business pays for (usually).

Disclosure - I'm highly biased towards industry employability as opposed to academic work.

EDIT: I also think, and this may just be my curriculum, classification problems are under-emphasized in our education system. ",dza76wutang,2015-02-13 10:03:52
"I agree with classification problems, any idea for stuff to look up? But my degree actually used R in most classes and so I'm fluent in it now, which is already helping in my current job =) and yeah, my question was geared towards industry too!",bromeliadi,2015-02-13 13:05:17
"Empirical Process theory is really cool (and useful)


Stein's method is also really cool (actually people said this during my degree, but I didn't really listen)


Semi-parametric efficiency theory is also also really cool.


Between those 3 things, you can prove like 80% of statistics :)


But I agree dcor is quite cool! I think one reason it hasn't caught on is that the original paper about it is a bit mathematical: rather than defining it as the correlation of the differences, in the original paper it is defined in terms of a decomposition of the characteristic function, weighted by a particular kernel. 

Out of curiosity, what clued you in to dcor?
",nrs02004,2015-02-11 22:51:55
"Awesome, I'm excited to look them up. I just got my first stats job a month ago in a lab (part-time, haven't graduated yet) and my boss makes us do presentations every Thursday on interesting things we've learned recently. So I was just browsing around stats research when I ran into it - I think it was mentioned in a paper I was reading from arxiv?",bromeliadi,2015-02-12 17:58:43
"ahh; so this stuff is pretty theoretical. Extremely neat, but not necessarily super immediately useful for applications.

In terms of applied stuff that is really cool to learn, I suggest L1 trend filtering!",nrs02004,2015-02-12 21:20:49
What do you like about the distance correlation more? Do you have example scenarios where you've found it more informative/superior/helpful? Thanks!,PhaethonPrime,2015-02-12 05:50:00
"I haven't got a chance to use it actually cause I just found out about it, but there are tons of papers about it and what I've seen from it looks very useful",bromeliadi,2015-02-12 18:38:33
Why are you disappointed in your professors?  ,masterrod,2015-02-12 09:03:36
They could have formatted the statistics curriculum SO much better if they just got together once in a while to talk about their separate classes. There are some concepts that I have learned three times in different classes now (for example accept-reject algorithms) and some things that were kind of beaten to death (for example simple properties of moment generating functions). It would have been nice to have less repetition and overlap and instead more material,bromeliadi,2015-02-12 18:33:22
"Were they not competent, no  outside experience, no PHDs? ",masterrod,2015-02-12 19:36:39
"They all had PhDs, didn't even know you could be a professor without one. They were competent at teaching what they did teach, but as I said, I felt like there was no communication between them and the degree was very unorganized.",bromeliadi,2015-02-12 21:51:49
"My thing is they had PHDs, I would trust what they told you.  But now you graduated you should ask them why they didn't Tracy you what you thought they should have ",masterrod,2015-02-13 04:04:17
"It's important to learn classical statistical techniques, but you should learn machine learning and artificial intelligence techniques to gain significant productivity and credibility.",Wafflehousefan,2015-02-12 05:35:10
Would you be so kind to post some of your results?,y2kerick,2015-02-12 09:05:34
"Very nice article! However, I am surprised the author didn't mention Abraham Wald even though much of the article was dedicated to sequential tests. 

Interestingly, Wald couldn't publish some of his research, which concerned sequentially testing the quality of munitions, because of its importance to the war effort and thus kept secret. His paper on sequential probability ratio tests (70+ pages) is quite wonderful and wasn't published until 1945.

Another interesting tidbit, Wald worked with Prof. Wolfowitz, the father of the former US deputy secretary of defense. Ah, the cycle of war.

Anyway, thanks for posting, OP.",CommentSense,2014-12-27 16:51:58
"I'll just leave this here:

http://www.intelltheory.com/rthorndike.shtml

Trained in Psychology, but a major focus on applied statistics / psychometrics.

Applied his skills to improve personnel selection for the 8th Army Air Force during WWII, which then bombed the tar out of the Huns.",wil_dogg,2014-12-27 15:55:45
"If you, or someone else who knows about it, could tell us a bit about his techniques, it would be nice to hear.",Bromskloss,2014-12-28 03:11:54
"Bob Thorndike was my mentor's major adviser / lab director at Teacher's College, Columbia University.  My mentor didn't talk a whole lot about the specific research he did in Thorndike's lab, but I just googled and found his PhD thesis.  It was written in 1957 under a contract with the Air Force, and focused on personnel selection for ground crews -- mechanics, laborers, munitions loaders, drivers, those kind of roles.  My mentor then focused on educational research / program evaluation and statistical analysis for the rest of his career.  I was his last teaching assistant and inherited his teaching library, which included Bob Thorndike's ""Personnel Selection"" which is the classic text on the scientific basis of personnel selection.

Lee Cronbach's summary / review of Personnel Selection states it better than I could:

http://psycnet.apa.org/psycinfo/2005-10850-001

Basically, Thorndike was a classically trained psychological researcher, his father was a big name in research on basic learning theory and who also had some good contributions to psychometrics.  Bob mustered into the Army in 1942 and went to England where he worked on the scientific basis of placing the right people in the right roles on bombers -- who should be the tail gunner, who should be the radio operator, etc.  Pilot training happened in the USA, and had it's own selection methods, so Bob focused on mass testing and blending aptitude testing with physical attributes and abilities (after all, a ball turret gunner has to be a small and agile fellow, whereas a bombadier needs to be good with math and have excellent eyesight and nerves of steel).  So what happened is that after WWII Bob mustered out of the service and ""wrote the book"" on personnel selection.   Apparently he was still doing research in that domain on behalf of the military industrial complex in the late 1950's.  He also continued work on psychometic theory and methods, and applied work on developing norms and calibration of IQ tests such as the Stanford Binet.

In sum, just a hard core psychological scientist with great math / stats skills and a knack for figuring out how to solve problems and then teach others to apply those solutions to new domains.",wil_dogg,2014-12-28 11:22:45
"Interesting. Thanks!

If you know anything more about how he'd select people for the various bomber positions, or for other jobs, I'm all ears.",Bromskloss,2014-12-28 12:34:39
I have a copy of Personnel Selection and I can recommend it.  Vintage copies are on Amazon.  Also if you Google the armed forces vocational aptitude battery test you'll get a lot of info on personnel selection but I don't know if Thorndike was involved in AFVABT.  All I k ow is I took the test in 1981 and the recruiters didn't stop calling until I was about 30.,wil_dogg,2014-12-28 15:33:55
This reminds me of a story from my undergrad class. Does anybody rember er the sort of statisticians using serial numbers on German parts to estimate the number of the German tanks?  I'd like to read about it again now that I've had a few more classes under my belt. ,bickbastardly,2014-12-28 07:38:58
"https://www.google.com/?gws_rd=ssl#q=estimating+german+tank+production+from+serial+numbers

A bunch of links will come up that you'll enjoy reading.  I recall hearing about this years ago but had not dug into the details.",wil_dogg,2014-12-28 11:40:54
Thank you for the leg work. I don't know it had a colloquial name to search for. ,bickbastardly,2014-12-28 18:04:45
"I didn't know either, I just tend to google keywords as a phrase and it usually works.  Makes sense that it is now called ""the tank problem""",wil_dogg,2014-12-29 06:23:20
"http://en.wikipedia.org/wiki/Sequential_analysis

I'll let the autowikibot, which I usually despise, put the relevant blurb here.",,2014-12-27 16:36:15
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Sequential analysis**](https://en.wikipedia.org/wiki/Sequential%20analysis): [](#sfw) 

---

&gt;In [statistics](https://en.wikipedia.org/wiki/Statistics), __sequential analysis__ or __sequential hypothesis testing__ is [statistical analysis](https://en.wikipedia.org/wiki/Statistical_analysis) where the [sample size](https://en.wikipedia.org/wiki/Sample_size) is not fixed in advance. Instead data is evaluated as it is collected, and further sampling is stopped in accordance with a pre-defined [stopping rule](https://en.wikipedia.org/wiki/Stopping_rule) as soon as significant results are observed. Thus a conclusion may sometimes be reached at a much earlier stage than would be possible with more classical [hypothesis testing](https://en.wikipedia.org/wiki/Hypothesis_testing) or [estimation](https://en.wikipedia.org/wiki/Estimation), at consequently lower financial and/or human cost.

&gt;

---

^Interesting: [^Comprehensive ^metabolic ^panel](https://en.wikipedia.org/wiki/Comprehensive_metabolic_panel) ^| [^Sequential ^probability ^ratio ^test](https://en.wikipedia.org/wiki/Sequential_probability_ratio_test) ^| [^Sequential ^estimation](https://en.wikipedia.org/wiki/Sequential_estimation) ^| [^David ^Siegmund](https://en.wikipedia.org/wiki/David_Siegmund) 

^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn6yowv) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn6yowv)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2014-12-27 16:36:25
"It is a lovely visualisation, but shouldn't we call it a visualisation of a Gaussian mixture model rather than the E-M itself? I would love to see a visualisation which really showed the E-M steps separately in a generic way.",jmmcd,2014-12-01 03:42:12
"You are right, it should maybe be called visualization of parameter optimization via EM for the GMM. 

I'll see if I can make the lower bound estimate plot for you, I'll be back in a moment :)",Gumeo,2014-12-01 03:53:08
The only nice way I see to visualize this is when you keep all the parameters at the optimum except one or two and then the optimization only uses 1-2 iterations. I decided not to make a plot but rather refer you to [the plot in this post](http://stats.stackexchange.com/questions/65876/confusion-related-to-em-algorithm) which clearly demonstrates the idea.,Gumeo,2014-12-01 04:22:38
Thanks indeed.,jmmcd,2014-12-01 05:38:02
"EM and other probability-based model fitting methods have always interested me, but I'm always worried that they'll run too slowly for my purposes. How long did this take to converge?",holdie,2014-12-01 07:45:15
This is 100 iterations on a standard desktop machine and without rendering the plots it takes around 5-10 seconds. This is of course a naive implementation in matlab. There are papers on faster versions of the algorithm out there.,Gumeo,2014-12-01 12:37:34
"Just wanted to acomment, as I am sure you know, that people often use gradient descent (or variants) of the negative log likelihood to fit models. Some very fast implementations exist for these algorithms. ",mdooder,2014-12-01 15:20:28
What data is being displayed in the top-left image?,fuzonc,2014-12-01 03:21:40
"It is the image histogram or rather image pdf, i.e. the histogram of the intensities from the pixels in the image. The intensities with value zero (black pixels) have been removed because there are so many of them and I am not modeling them. The range of this intensities is this wide because of the image modality, i.e. an image from an MR device in this case.

The moving curves are the fitted model, three Gaussians that represent the intensities of the pixels corresponding to the three classes.",Gumeo,2014-12-01 03:28:43
"and are you planning to share this? I mean the code, the data?
It would be great if you do so :)",shaker82,2014-12-01 09:09:27
"I'll maybe write a short blog post about it later, then I'll put the code with it and the data of course! I'll try to remember to comment here. ",Gumeo,2014-12-01 12:40:53
"This is very nice, but...can you fix the axis for the left plot, and turn off (or hold on) the gridlines? Sorry, OCD kicking in. It is really cool though!

Edit: It looks like the axis is actually fixed. My bad.",erasers047,2014-12-01 10:23:04
I'll fix the gridlines if I'll do a short blog post! Thanks for the comment :),Gumeo,2014-12-01 12:43:29
What do you do with WX and Bayes? If you don't mind me asking.,brews,2014-12-01 16:26:55
Are you asking in general or in this specific case? Do you mean WX as the weights calculated for the iterations in EM or?,Gumeo,2014-12-02 00:14:24
"""wx"" == weather.

You mentioned that you normally work with meteorological data.",brews,2014-12-02 05:12:22
"Ahh sorry! Yes, Bayesian inference on meteorological data is a huge field. I did some hierarchical modeling of annual precipitation using a Latent Gaussian model in my MSc program, I have also some experience working with Gaussian Markov Random Fields on weather data. Basically Bayesian statistics gives a very nice way to structure and create models and the process all boils down to using the Bayes rule. If you want to read further on this I suggest looking at Hierarchical Bayesian modeling, there are plenty of examples on simple weather data.",Gumeo,2014-12-02 05:54:36
"This is a very nice brain segmentation - I have many questions. First, is the technique published and if not is there somewhere I can read about it? Second, what was your input - T1 MPRAGE? Third, how does this technique deal with magnetic field inhomogeneities (or is it corrected with a fieldmap)? Are the segmentations this nice across all slices? 

Very nice work.",quaternion,2014-12-01 04:30:35
"Yes, the technique is published, it is a Gaussian Mixture Model. It has been used widely for segmentation in images but more complex models like the Markov Random Field are more appropriate for object segmentation, i.e. tumor or a localized clustered object.

This is a corrected map.

I only did this on one slice. I assume it would look nice on all the slices but better methods exist.

To get started learning about this I suggest reading/googling about Gaussian Mixture Models and statistical image analysis.",Gumeo,2014-12-01 04:37:39
"How are the cutoffs determined for the thresholds? Is is the cross points of the mixtures? 

Or can each pixel be a mix of the three Gaussians?",forever_erratic,2014-12-01 11:43:30
"Do find the corresponding classification for a given pixel you choose the class with the highest probability which is equivalent to split at the cross points for the mixtures. 

Then again the results from these models are often used as input for other segmentation/classification/Machine Learning algorithms, so in that case it is just preprocessed data.",Gumeo,2014-12-01 12:34:35
"Great article, but there's an even better response to Randall's misleading cartoon. The frequentist would really say, ""let's press the button 100 more times and record the data"". Well, both would. But the point of the article holds true: the caricatures of statistical philosophies are uninformative straw men. ",bacteriadude,2014-10-13 13:31:32
I always disagreed with it for the reason that the p-value the frequentist uses is stupid. If the bayesian had a similarly incorrect prior the situation would be flipped. However if the comic is taken as a jab at how common it is to use a p value of .05 in scientific papers I'm more inclined to agree.,bluecheese33,2014-10-13 21:39:34
"I have to nitpick - we're using an *alpha* value of 0.05. And I don't really have a problem with that. While an alpha of 0.05 is very common, the actual P-values in question are very often much lower. This may differ by field, though. So it's not like 5% of science is wrong. 
",bacteriadude,2014-10-14 01:47:26
The percentage of science that is wrong for any given threshold is probably higher in some (many?) fields due to publication bias.,sowenga,2014-10-14 05:20:18
"Except people rarely, if ever, repeat experiments.",giziti,2014-10-13 17:21:36
"My point was that in an experiment, n &gt; 1",bacteriadude,2014-10-13 17:33:32
Interesting article. Although I still struggle with understanding the differences between Bayesian and Frequentist statistics. ,redvelvetpoptart,2014-10-13 10:32:37
"&gt; I still struggle with understanding the differences between Bayesian and Frequentist statistics. 

Try this series of blog posts out (the rest are linked from the top of this first article)...

[Frequentism and Bayesianism - A Practical Introduction](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)

Its really a philosophical difference a lot of the time because with enough data both methods are highly likely to converge on the same answer.",enilkcals,2014-10-13 11:58:21
"Many of the differences between these two approaches are structural, but since they're predominately discussed based on their computational differences here on/r/statistics, it can be confusing.

If you use your lit review to construct a simple null hypothesis of what you expect should happen based on past experiments, you'll end up using frequentist methods to test that null hypothesis. If you're able to develop a computational model of why things happen based on how they happened in the past, you'll end up using Bayesian methods to evaluate that model. Everything else that follows are partisan arguments. 

Truthfully, some problems, particularly new problems, are not well suited for a Bayesian approach yet. Likewise, it is irresponsible and somewhat negligent to ignore Bayesian methods because they are computational more complex. Traditional science have always had trouble with new ideas, particularly new ones. We forget how long Fisher was alienated by supporters of Pearson.

I work in simulation, imagine how I feel when these debates come up. 

""B-But if you know that much about a system, why not...."" 

""shut up, ATG77, we're using t tests""

""Sigh, okay""",ATG77,2014-10-13 20:39:13
"Intuitively, Bayesians adjust probabilities of observed data meaning something by a prior belief over the events in question, whereas frequentists only measure probabilities that are seen.

A good example (loosely recalled from Hinton's neural networks lectures on Coursera) is that of figuring out whether or not a coin is fair. You could, say, flip the coin 100 times and see how many times it comes up heads vs. tails. 

If, say, it came up 53 heads and 47 tails, a frequentist would observe that the coin is biased with a 53% chance of coming up heads, instead of 50%. A Bayesian however, would use the prior belief that the coin is fair to get a probability distribution over the possible biases of the coin (including a high probability that the coin is completely fair).",zmjjmz,2014-10-13 11:29:18
"That would certainly be a frequentist point estimate, but if the frequentist is interested in making any sort of conclusion (ie ""The coin is biased!""), then they would either do a hypothesis test or make a confidence interval, since the result is consistent with an unbiased coin. For the Bayesian, the probability that the coin is fair is 0 since the probability distribution of the parameter is continuous.",giziti,2014-10-13 11:39:49
"It wouldn't be 0 if the Bayesian used a discrete or partially discrete prior.   For example, a reasonable prior would be that there is a 99% chance that p=.5, 1% chance that p follows some beta distribution.  ",DemonKingWart,2014-10-13 12:00:37
Sure. I'm not too familiar with notions of Bayesian hypothesis testing - is this sort of idea a common one?,giziti,2014-10-13 12:46:44
It comes up in methodology for gene studies (e.g. differential expression analysis) somewhat frequently. For example - an a priori belief that 80 percent of the genes in a given experiment satisfy the null hypothesis of no difference between two groups might be encoded with a mixture prior of 80 percent point mass of no difference and 20 percent continuous distribution.,statisticalhornist,2014-10-13 18:51:36
I've seen it before in the context of zero-inflated models. That's a really cool example though.,Fireflite,2014-10-13 13:05:14
[deleted],,2014-10-13 14:37:13
"Certainly - just as no frequentist would say the coin is biased. Having most reasonable credible intervals containing p = 0.5 is, as we would both agree, quite different from saying that there's a high probability that the coin is unbiased.",giziti,2014-10-13 14:41:51
"According to this [article](http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf), biased coins are a myth.

On the other hand, as nothing in this world is perfect, there is no unbiased die; no matter how many times you've seen a die rolled, you should be absolutely certain that there is some bias, even if it is too slight for you to detect from the finite sample you've seen so far.",ben3141,2014-10-13 19:14:54
"&gt; How should I design a roulette so my casino makes $? Does this fertilizer increase crop yield? Does streptomycin cure pulmonary tuberculosis? Does smoking cause cancer? What movie would would this user enjoy? Which baseball player should the Red Sox give a contract to? Should this patient receive chemotherapy?

This shows very clearly a Frequentist wrote the article. It is frequentist thinking to ask questions of the nature: ""Is there a difference between x and y?"", while Bayesian thinking asks questions of the nature ""By how much do x and y differ?"".",log_2,2014-10-14 04:19:22
"So a Bayesian isn't interested in whether smoking causes cancer, but rather how much cancer does it cause?! The different questions of effect size vs existence of effect is not the same as the difference between Bayesians and Frequentists, both sides ask both questions.",giziti,2014-10-14 05:23:30
"Precisely. A frequentist would ask: ""does the amount of cancer due to smoking go above x?"" and a Bayesian would ask ""how much cancer does smoking contribute to?"". All frequentist test that (I've seen) are decision questions, Bayesian tests give you an effect size without needing to specify the effect size you're looking for before conducting the test. Bayesian stats can of course ask decision questions too.",log_2,2014-10-14 15:47:20
[deleted],,2014-08-20 20:11:22
"Agreed. My basic stats course was taught by the math department. The result was that I had no clue what to do when I had actual data from surveys and behavioural experiments in front of me. 

Learning stats from a social scientist makes a big difference if you want to actually apply stats. That said, the math-y side of stats eventually came in handy as I progressed in my training.",schotastic,2014-08-20 20:57:22
"Jacob Cohen wrote a book, _Statistical Power Analysis for the Social Sciences_, back in the day, showing how poorly social scientists (psychologists and sociologists) understood statistics, specifically power.

He wrote his first edition 20 years before the second edition, and things were almost exactly the same: most social sciences research was woefully under-powered.

The same thing holds true today, I suspect, and not just in social sciences, but in GMO testing, and probably just about anything outside of a physics-derived engineering discipline.

Nobody ""gets"" power, or at least, durned few do.",saijanai,2014-08-21 00:13:42
"You'd be surprised.

I know issues of power are very salient to my supervisors. I've been trained to consider power in my data gathering and analyses, though I admittedly still have a lot to learn.

Maybe our team is outside the norm, but as far as I can tell people in my discipline are definitely not oblivious of statistical power issues.",schotastic,2014-08-21 02:14:22
"Professor Cohen would be pleased, then.

I spoke with him briefly about 30 years ago, when I was doing a research paper for my English class. I'd found a review paper comparing various forms of meditation, concluding that they all had the same effect.

The experimental group size of the studies that were reviewed ranged in size from 7 to 30 with an average of about 11.

The conclusion was that all meditation practices had the same effect. 

This review was cited in every psychology text I could find in their chapter on stress and meditation.

Students and professors alike insisted to me that the findings were perfectly valid. The meme that emerged from all the freshmen psych students going on to become college professors persists 34 years later:

all meditation practices have the same general effect: [Holmes 1980] proved it.",saijanai,2014-08-21 02:33:11
"Hmmm that's a shame. Now it's SOP to run experiments with at least 30 per cell (i.e. group). I know that influenced my proposed sample size when I wrote my thesis proposal.

Speaking of Cohen, lately there's been backlash against Cohen's prescriptions for effect size (d = 0.1 small, d = 0.3 medium, d = 0.5 large). Too simplistic apparently. I'm inclined to agree.",schotastic,2014-08-21 02:58:03
"&gt; Speaking of Cohen, lately there's been backlash against Cohen's prescriptions for effect size (d = 0.1 small, d = 0.3 medium, d = 0.5 large). Too simplistic apparently. I'm inclined to agree.

Well, perhaps, but you have the numbers wrong anyway, and he made it clear in the book that those were figures picked out of the air just to give a framework for discussion.

d = 0.2 as ""small"", 0.5 as medium and 0.8 as large.

And, just as a p of 0.05 or an alpha of 0.05 was a convenient thing to use because there were tables available that used those values, Cohen had to use limited tables in his own book as well. Even the second edition was basically pre-computer -at least for use by a 65-year-old writing a book  published 1988.

So, the simplicity was out of necessity and he chose values that he admitted were arbitrary because he had to chose SOME some specific values, so why not those? He gave real-world examples of situations having each specific effect size, showing that they weren't completely random. 

He basically says that quite explicitly in his book. I'm guessing that the people who object to his specific values never actually read his book, but only a second-hand account using the d = 0.2, etc


",saijanai,2014-08-21 06:47:42
[deleted],,2014-08-21 08:11:14
"To be honest, I've never had a class in statistics, but I read a lot.

The biggest problem with undergraduates has always seemed to me that they don't read except for class. That could be said of many people in most professions, of course.",saijanai,2014-08-21 09:04:05
$0.0002 is not often the way I hear that phrase used...,bored_me,2014-08-20 23:38:00
"My intro stats course was intro to biostats, taught by the head of the physics department. I'd say it gave me a pretty good grasp of what statistical concepts actually mean, sufficient even to hack my way through some more advanced stats topics on a semi-self-taught basis.

And in general, I'm a fan of learning any kind of math by figuring out a physical example you can connect it to.",Eurynom0s,2014-08-21 13:42:10
"I definitely think most people anymore tend to memorize operations rather than understand meaning, or understand the abstraction. This is actually a larger problem than one might think. I've seen it occur in high school kids and new undergraduates.

In grad school I sat in as a helper of sorts for my professor's statistics 101 course. I was his RA, but in academia that usually means you help out wherever you can. In this case he wanted me to take part in the class so I could learn more through explaining things. Anyway, I digress.

The kids in this stat 101 class were young enough to be affected by ""No Child Left Behind"" as well as the general tightening of public education budgets prior. They were absolutely pitiful, and I mean that because I don't think it was their fault. I remember hearing the professors discussing how that year they had the worst-prepared group of kids they ever remember teaching.

The curriculum and grading policy for the stat 101 course was actually pretty standard. I wouldn't say it was easy, but I would say that it was a typical college level statistics course. In spite of that over half the class failed, another quarter had a C or D, and only 3 people had A's out of 51. The year prior to that, there was a ~20% failure rate with a wider spread of As, Bs and Cs for *the same curriculum*.

I think the problem is that there isn't any discussion of ""why"" things work in modern pre-college curriculum. It's mostly all about ""how"" to do X. Kids grow up meeting that expectation. Then they go to college and try to apply the same strategy they learned to the coursework, but you can only go so far without reasoning and abstract thought in a college level course.

I believe that this focus on ""how"" rather than ""why"" is related to how we educate them, particularly in the earlier years. Everything is a formula or a ""program"". Do these steps, and you get the answer. Take this multiple choice test, nothing is open to interpretation. Solve this equation for X, don't formulate and then solve an actual real-world problem. Learn your multiplication tables, not what multiplication really is or how to think of it algebraically.

I also believe teacher quality leaves a lot to be desired. It's either that they are paid terribly (and thus isn't a sought after, competitive position) so you get the graduates that couldn't cut it elsewhere, or you get people that are talented and care but are stretched too thin and/or burned out. If you do have a teacher that is smart, talented and whom cares, well, they are overworked, underpaid, not given enough resources and not allowed to deviate from an extremely strict curriculum meant to help you pass a test rather than learn.

Anyway, I realize I don't have a link to a formal study nor do I have data to show you, but your question seemed open-ended enough that I thought I'd give a more informal/opinionated answer.

EDIT : Moved a paragraph to a place it made more sense since apparently I write essays rather than responses",antisyzygy,2014-08-20 21:32:57
[deleted],,2014-08-20 22:09:36
"Well, I don't mean to say our education system was always bad. I think education was great here for quite some time, but political ideologies and disagreements that developed over the years ruined it. 

Pull yourself up by your bootstraps! If you can't achieve success it's your fault! Never mind that my parents sent me to a prestigious private school and I had a bank roll.",antisyzygy,2014-08-20 23:13:53
[deleted],,2014-08-21 06:32:24
"I was the class of 03 and also grew up in a majority white / middle class school. We had two black kids, and maybe several hispanic kids in our class of ~1000. My experience was quite different.

There were great teachers particularly in STEM and social studies. They had their share of easy courses but if you wanted to you could take harder courses like calculus or an honors track. They also partnered with our community college so you could spend your junior and senior years taking college courses for credit in both high school and college. 

In many non-dual-credit courses you had homework due daily and from what I remember the teachers were passionate about teaching. The town also allowed for a middle class lifestyle on a teacher's salary though, so maybe they were happy and did the job because they loved it rather than did it because they needed a job.",antisyzygy,2014-08-21 07:08:36
"Probably, but that's likely because there are a LOT of students who are required to take an intro to stats class; for many folks it's ""just a silly math requirement"" they need for a degree.


To be fair, not many professors/TAs do a great job of explaining (or don't have the patience to explain) what a confidence interval or a p-value really means.",KCB24,2014-08-21 03:19:37
"&gt; To be fair, not many professors/TAs do a great job of explaining (or don't have the patience to explain) what a confidence interval or a p-value really means.

This. Exactly this.
",andrewjsledge,2014-08-21 05:22:56
"My statistics professor said it something like this (and he was quoting someone else he had heard it from):

""Nobody really 'learns statistics'; what people need to know is A) which of the underlying principles of statistics are required for a given problem and b) how to learn/teach themselves the statistical techniques they need to use.""

If statistical problems were all the same, then there wouldn't be any point in learning statistics at all; it could be fully automated. When you face a problem that requires statistics, you need to know which tool to use, and if you aren't familiar with that particular tool, you need to know how to teach yourself how to use that tool.",ATG77,2014-08-21 06:23:55
"&gt;""Nobody really 'learns statistics'; what people need to know is A) which of the underlying principles of statistics are required for a given problem and b) how to learn/teach themselves the statistical techniques they need to use.""

That doesn't make much sense. They teach themselves the ""technique"" as if it's wrote memorization? Who then discovers new theorems? No one, because ""nobody really 'learns statistics'""?

Both A and B sound like ""learning statistics"" to me.",antisyzygy,2014-08-21 07:26:36
"He meant it in regards to lower level statistics classes and statistics students. Those students aren't going to be able to learn how to test the boundaries of statistical thought after a 12 week course; they will, however, hopefully understand the fundamentals enough so that there are fewer unknown unknowns. ",ATG77,2014-08-21 07:31:39
"I took two intro stats courses at the University of Michigan--one in the Sociology Department and the other in the Statistics Department. The soc stats was okay, but it was waaay more about the math than interpretation. But the stats stats payed close attention to making sure we were able to correctly interpret our findings. They were sticklers and predictable on this matter, so a lot of students actually did learn to interpret a confidence interval, level, type I/II errors, beta, alpha, etc. I learned much more from the stats stats than from the soc stats, even though I took soc stats first.  ",nkobel,2014-08-20 22:56:33
I wish I wasn't one of them as a kid. Now that I realize how powerful all this stuff is I've got to go back and relearn it all.,Cheekio,2014-08-21 07:58:30
"Just going to insert [this](http://calnewport.com/blog/2008/11/14/how-to-ace-calculus-the-art-of-doing-well-in-technical-courses/) link to a blog post on how to do well in calculus.  [Here](http://wtfprofessor.com/how-not-to-blank-on-exam-problems-the-practice-of-active-recall/) is another post on active learning that talks specifically about physics.  I think the ideas found in both articles can also be applied to a statistics course, but many students don't know these things.  

We as statistics teachers need to teach students how to study for our courses!  If they've never had a college level math or science before, they haven't had the opportunity to learn how to study for one.  It's okay to teach students how to study.",kestrel2,2014-08-21 09:40:44
"I think it depends on the level/age of the student.  If you're a high school student or undergrad, your objective is getting the highest possible marks.  If you're a grad student or professional, your objective is understanding the material.  ",McScumbag,2014-08-21 13:13:24
"I think it is important to teach the ""why"", when you are doing the ""how"".

It's a balance for sure.

Focusing too much on the ""why"" and students will roll their eyes, focus too much on the ""how"" and they won't understand the underlying concepts.

",matrix2002,2014-08-21 13:37:51
"I worked as a tutor at my local community college's Academic Success Center for three years. Being as I was the only tutor who actually enjoyed statistics, I became the designated ""stats guy.""  In my experience, the vast majority of students that took STA 2023 - Elementary Stats did so because they had to for a degree requirement, either at the community college or for their eventual bachelor's degree.

I'd be willing to bet that very few, if any, students would be able to explain to you what a p-value or confidence interval *actually* means. It was pure memorization for them. ""How do I do this?"" was the most common question, as opposed to ""What does this mean?"" 

Some people might point out that since this was at a community college that the students' skill level may be less than that of their university counterparts, but I had enough students come in from our two local 4-year schools that also struggled mightily.

In short, based on my few years of experience, students just wanted to get through it easily and without real understanding. Many students also took the course because they believed it would be easier than another math course such as trigonometry, college algebra, pre-calc, or calculus.

To be fair to them, I also took my first stats class at that community college, and while I found that it was very easy, I also did not have that understanding. I didn't gain a true understanding until I took Probability Theory and Mathematical Stats (amongst others) at the graduate level. ",emery3,2014-08-22 07:09:05
"&gt; I'd be willing to bet that very few, if any, students would be able to explain to you what a p-value or confidence interval actually means. It was pure memorization for them. ""How do I do this?"" was the most common question, as opposed to ""What does this mean?""

I tutored someone in Statistics one summer and the same thing happened.  I had to teach her how to properly interpret hypothesis tests and that a p-value is NOT the probability that the null hypothesis is true.

If I'm ever in a position to teach an introductory statistics class, once we reach the hypothesis testing unit, I'll make them say ""The p-value is not the probability that the null hypothesis is true."" at the beginning of every lecture.  I'll tell them in advance that a question on their midterm will be ""True or false: A p-value is the probability that the null hypothesis is true.""  Anyone who gets it wrong *loses* five points on their midterm, in addition to getting it wrong.  It's a misconception that exists pretty much everywhere.",beaverteeth92,2014-08-22 14:02:19
"Yes. Buy mostly for anecdotal reasons. I'm not even entirely sure there's a good way around it. When I fully realized what I pvalue was, not by its definition, it was because I needed to understand it. The problem begged the answer, so it just made sense at the time. I'm not sure how to really get around it in a way that can be taught.",red_magikarp,2014-08-20 19:28:56
Do you think that one problem is that the professor doesn't have enough class time to explain the concepts behind the problems in way that can make students fully understand them?,GSav88,2014-08-20 19:33:51
"Hopefully its because the professor is taking an approach that will convey the ideas correctly to the largest number students. If they're good, they will use quizzes and homework to get an idea that their students are comprehending things. So I don't think it comes down to just lecture time, I think that most classes have a reasonable amount of lecture time and then added office hours and labs for more technical subjects.",red_magikarp,2014-08-20 19:49:57
I don't think lack of class time is the problem.  It doesn't matter how much time professors spend explaining things because people don't learn by having things explained to them.  They learn by working on things they care about.,AllenDowney,2014-08-22 16:41:55
"thats what I did, for the easier classes its easy to understand the theories behind it. but as theories stacked up, my understanding faltered and i really regret doing what I did.

confidence interval and p values are intro classes and are fairly easy to understand, but i got lazy somewhere when studying cubic splines and smoothing. subsequent classes became a gibberish mess of desperation trying to pass tests. 

",lego_jesus,2014-08-20 19:34:15
"Can you explain what a confidence interval and p value actually mean, please?",blemming,2014-08-21 03:15:13
"Thanks! This is so awesome. I'm definitely using a few of these in future presentations. 

I read xkcd but didn't realize how much statistics is on there. ",pulsetsar,2014-08-20 18:11:28
"I completed 5 out of the 9 classes already, currently taking the 6th one. They have been good, but inconsistent, I think the statistics classes are really good (at least I learned a lot in those classes), while others like the first two are pretty weak, but probably because I have a computer science background.

Overall I am happy with the specialization, I have even started using R and some basic statistical analysis at my job and it has really helped.",jalagl,2014-08-19 13:38:43
"Really enjoying this course - I'm on the final 3 courses.  This looks quite interesting but I wonder whether they'll be offering any other captstone projects, perhaps in health statistics or environmental statistics. ",AnathemaFan,2014-08-19 16:26:00
"I looked at this specialization a few months ago and was frustrated at the scheduling.  Coursera had these classes up, but no future dates.  They all happened months, if not years ago.  Looking at other classes, this seems like a common problem with Coursera.

I'm really happy to see classes available, but all nine classes are only available September, October, and November.  Am I supposed to take three at a time concurrently if I want to finish the certification?",snapetom,2014-08-20 01:01:02
"The courses take one month each and are repeated every month. From the blog post linked here:

&gt; The 9 courses have only been running for 4 months but already 200+ people have finished all 9!

So it's about 7/8 months to complete 9 courses, each taking a month (some take less than that, particularly the introduction).

EDIT: I know Jeff Leek posts here on /r/statistics, maybe he can confirm that the Specialization will run again next year. I'm pretty sure it has been successful so far, so I would bet on it.",rjtavares,2014-08-20 02:31:54
"I can definitely confirm that we are running all the courses all the time (including Oct/Nov!). We will take a break during December, but then are planning on coming back strong in 2015. ",t_rex_tullis,2014-08-20 07:37:25
"Awesome, you're doing great work. I only wished you used Python instead of R (it's still great to follow most courses, though), but I understand why R is more appropriate.",rjtavares,2014-08-20 09:56:02
Stay tuned. We have some things brewing :-). ,t_rex_tullis,2014-08-20 10:01:18
"Awesome. I'm just following most courses (watching the videos) for now, but hope to enroll in a certified course/specialization sometime in the future, and I feel that you guys really understand the medium.",rjtavares,2014-08-20 10:19:16
A fairly terrible headline but a good read.,,2014-04-20 14:41:06
"Great read, terrible title (not OPs fault), hope this paper gets a retraction",LeptonBundle,2014-04-20 16:20:03
"I'm not sure what problem other commenters have with the title, I thought it was quite amusing. 

Without having read the paper in question, It would appear that the reviewer's case is a strong one. 

The section on study design cuts right to the heart of the matter. In making what many would consider strong claims about recreation marijuana users, the Gilman paper draws upon a sample which does not seem to fit the common understanding of 'recreational'. Even their own claims about the paper refer to people who 'only used marijuana to get high once or twice', and yet the reviewer points out that the majority of the sample smoke more than 10 joints a week. You can't study people who aren't there.

The section on multiple testing rightly raises questions about the significance values used in multiple testing, but here the reviewer is a little unclear. He seems to say that the p-value *should* be 0.0125 for the volume analysis, but later refers to this as the authors' stated required threshold (with an instruction to see a caption which appears not to be on the figure referred to, but a later figure). This is a minor quibble, as it seems that in any case the analysis failed the test, but the authors called the low p-value indicative of a trend towards significance.

Even worse is the evidence later in this section that the authors did indeed not correct for multiple testing, and repeatedly published their data in the most suspicious manner possible regarding the effects they were claiming were significant.

Even disregarding all these critical implementation problems (not testing the right people and fiddling with the tests) and the presentation problem (misrepresenting the significance values), the reviewer is absolutely right that the claims being made about the study just can't be supported by its methodology. No causal link *could* be found from this study.
",baetawolf,2014-04-20 19:45:28
"""Trend towards significance"" does not mean anything... It's designed to fool people in thinking they have something when they don't.",canteloupy,2014-04-21 00:03:57
"Well it introduces more subjectively. If the experimenter chooses a priori a type 1 error rate, they shouldn't be able to go back and change it after they see their results.

However the actual choosing of the type 1 error rate a priori is subjective",LifeFlatCircle,2014-04-21 01:57:41
"Basic scatterplot and a linear model.  You can get a lot of miles off of those two things.
",partspin,2012-05-07 13:43:01
"I'll second this. In particular, the practice of plotting your data before trying anything more complicated.",Gorrable,2012-05-07 13:57:25
"This.  You need to be able to describe to the average person the""story"" of the data (trend, seasonality, cyclicality, i am an econ major...i am sure in physics or lab experiments it could be different changes in the parameters in the environment that can explain the change in data) in order to **justify** what you actually do(OLS, Data descriptions, ARIMA, GARCH, etc.).  ",CivAndTrees,2012-05-07 15:42:58
"This.  Plus knowing whatever it is your modeling/describing.  I did biofuels for my business forecasting class and trying to explain an 10 year linear trend gone exponential while explaining seasonality in the data would have been impossible with a good referenced timeseries (monthly data from 83 to recent) and a good understanding of the data (subsidies for corn -&gt; Ethanol).  Also, depending on data, a good histogram can go a long way, especially to a client to give an idea about quantity wise for each bin parameter.",CivAndTrees,2012-05-07 15:36:19
Robust/non-parametric versions of tests. wilcoxon rank sum comes to mind.,vx14,2012-05-07 15:11:14
this was actually hammered into me in my 2nd half of my 2nd year econometrics (non parametric tests that is),chewxy,2012-05-07 20:54:40
"I don't how much these get used by others, but for me the most important tool are frequency tables. Nothing gives me a better idea of the data i'm working with than those.",Tripplethink,2012-05-07 14:45:30
A ton of mistakes can be avoided in categorical data analysis by just running a frequency table or crosstab. I always make sure to run one just to catch mistakes when recoding variables if nothing else. ,Case_Control,2012-05-07 16:40:48
In a not-necessarily-statistics vein: Python+shell scripting.  These two tools are great for parsing data and preparing it for modeling.,whoflungpoo,2012-05-07 15:23:13
Can you elaborate more on this? How would you parse data efficiently through shell scripting?,aguyfromucdavis,2012-05-07 15:46:32
"imagine if instead of one data file, you need to extract your data out of 10k similarly formatted log files. within these files, you're only interested in records meeting specific criteria. you could hire an army of temps to go through your data, cutting and pasting or manually entering data as they go (inevitably introducing error), or you could write a script that intelligently aggregates the data by looping through all the files and only keeping records matching your needs.",shaggorama,2012-05-07 16:51:05
"An alternative method is to use wildcard characters when reading the files into SAS, provided they're all the same type of file.  Most statisticians will probably be more familiar with SAS than python, methinks.  ",HughManatee,2012-05-07 21:53:43
it was one example. It's easy to spend more time preprocessing data than actually performing analyses. Knowing scripting languages and being able to automate tasks and scrape data is extremely useful. ,shaggorama,2012-05-07 22:51:19
"Yeah, I just use JMP. it has a concatenate function for data sets. I recently just joined 400 data sets into one without spending more than 2 minutes. 

Hmm... is JMP made by SAS?",,2012-05-08 08:46:18
"Not sure.  I just know there was a time I had to append 1700 data sets together from separate files once and that's how I did it.  There's usually more than one tool for the job though, which is nice.",HughManatee,2012-05-08 10:13:11
"I believe JMP is basically the GUI version of SAS, made by the same people, but I could be wrong!",last_oryx,2012-05-08 19:29:26
"Here's another example:  Imagine that you would like to run an R script (called foo.R), in parallel, on all the files in your directory, using 50 cores.

ls * | xargs -P50 -I% foo.R %

It's just that easy.  The point is, there are tons of quick ways to process data in unix environments that don't require big heavy stats packages like SAS, they just require a little knowledge.",whoflungpoo,2012-05-08 18:47:17
"[QQ plots!](http://en.wikipedia.org/wiki/Q-Q_plot)  They are a great way to test for normality (or another distribution) and give a lot of really useful information in one plot.  

",mamluk,2012-05-07 17:29:55
"Power analysis, probability sampling and design, and missing data/multiple imputation.  Edit: and multilevel modeling--though this is not necessarily underutilized in general so much as not as prevalent in a particular field as I might like ",r-cubed,2012-05-07 13:31:18
"This this, and this this this this. ",BillyBuckets,2012-05-08 06:00:11
"Nonparametric tests (Wilcoxon rank-sum, Kolmogorov-Smirnov, etc) and plotting data (all relevant and not-so-relevant) variables. Principal components analysis. Basic Bayesian models are good too - much more easily interpreted, and the pictures seem to be impressive to non-statisticians - all those posterior distributions. ",prionattack,2012-05-07 15:13:43
I'm having a hard time finding a good book that has a collection of nonparametric tests.  Is there a book that you like or use?,Jofeshenry,2012-05-07 18:02:55
I use Wikipedia :),prionattack,2012-05-07 18:18:10
Nonparametric Statistical Methods by Hollander and Wolfe is what I use.,,2012-05-07 21:54:19
"I had a hard time using nonparametric tests for my research during my last semester of college. This is something people need to be careful when using data that is not normal.

I agree that this is one of those techniques that is often overlooked.",djent_illini,2012-05-09 08:27:55
boxplots with the data points jittered on top. and never bar charts. especially not with those little antennas,ntlaxboy,2012-05-07 19:06:38
Check out violin plots if you haven't allready. ,micro_cam,2012-05-08 08:02:08
damn that is cool. Any big software packages able to plot them (outside of R)?,,2012-05-08 08:48:01
"Time series: trend plots.

Cross-sectional data: Kernel densities (sometimes multiple, conditional ones), scatter plots, and then, personally, plots of the predicted values against the actual. It tells you what the R-Squared does, but it does it in a much more visually striking way.",econometrician,2012-05-07 20:52:25
"Categorical linear regression instead of ANOVA, one-tailed tests **when appropriate and designated a priori** to increase power when experimental replicates are prohibitive...

Oh and using excel to generate code for real statistic programs. Contatenating strings with auto filled numbers is a great time saver, even if it isn't elegant :)",BillyBuckets,2012-05-08 06:03:44
Isn't categorical linear regression the same as logistic regression?,djent_illini,2012-05-09 08:29:07
"Sorry, I meant using categorical coding of independent variables with a continuous dependent. Basically, it's a better way to compare multiple treatment groups to a single control than ANOVA w/ post-hoc or serial t-testing, both of which require adjustment for multiple comparisons. ",BillyBuckets,2012-05-09 10:15:35
"Graphing empirical CDFs.  A lot of people like PMFs because they are easier to interpret (for beginners).  But once you get used to looking at CDFs, there are several advantages:

1) You don't have to smooth the data,

2) You can plot several CDFs on the same axes for comparison,

3) With simple transformations of the axes you can test for uniform, log-uniform, exponential, Pareto and Weibull distributions.

Here's an example:

http://greenteapress.com/thinkstats/html/thinkstats004.html#toc24",AllenDowney,2012-05-08 08:16:12
"Cross validation. I never trust any model until I cross validate it
",BlueBayou,2012-05-10 11:20:17
"I feel like the problems he is running into are due to the way the data are being smoothed, and the way the density heat map is being colored, than any actual issue with the data. None of the density plots actually give a value for the density contours, so there is no way of telling whether what he is saying is reasonable or not. It looks like he is forcing the colors to go from strong to weak over the range of the data, rather than allow the entire thing to stay in the middle (but I could be wrong).",NOTWorthless,2014-12-27 08:42:55
Nice! Thanks for sharing - I'm going to try to get through this next week!,discountepiphany,2014-12-09 05:52:34
8/10 would have been more helpful if writer used a data reduction technique,Iamnotanorange,2014-09-26 06:22:10
"this is very nice, thank you!",mguzmann,2014-09-26 13:03:28
"Super interesting stuff, I feel like I need a more basic introduction to get some of the things referenced.",paulgp,2014-04-09 08:05:41
Best post here for a long while!,Gumeo,2014-04-09 15:45:35
"I took an introductory knot theory course in undergrad. This post blew my mind at least three times. That was an amazing read. 

This is why I visit reddit. Thank you for sharing it.",TheDrownedKraken,2014-04-09 18:24:38
Fascinating stuff. It's always nice to have visualizations of what the neural networks are actually doing.,StoicGentleman,2014-04-09 16:53:39
"&gt;...the R language will graduate to Version 3 around April 1. This is only the third time that R has incremented its primary version number.

This is helpful advice for the many R users that don't know what the number 3 means.",,2013-01-02 21:19:59
"This seems like a big change.  Enhances support for larger data frames and vectors could see a marked increase in industrial use.  Hopefully, RAM and speed will likewise improve in the future.",anonemouse2010,2013-01-02 18:14:14
and upgrading ease of use in cluster environments (?),zdk,2013-01-03 09:59:26
Hooray!,samclifford,2013-01-02 17:34:23
[deleted],,2013-01-03 01:38:23
"I think you're comparing apples with oranges. I'm sure that Sofa is quite handy for busting out some straight forward stats, but R is meant to be a complex, flexible tool. It's quite intuitive, but it ain't hand holdy. I'd really recommend the website Quick-R http://www.statmethods.net for introducing the basics if you want to get a kick start in learning R.",thelatemail,2013-01-03 02:22:21
He means: http://www.statmethods.net/,GilTheARM,2013-01-03 06:48:24
True - edited my post.,thelatemail,2013-01-03 12:24:01
"R literally took me hundreds (if not thousands) of painful hours to learn properly and I'm almost certain that it is the only software that allows me to get close to what I want to achieve. 

Quite simply, it's much more flexible than anything else and I recommend anyone that has to do more than the most rudimentary statistics learns it. It will make you a better statistician.",Eist,2013-01-03 07:23:59
[Direct Link](https://www.quora.com/Google/What-do-statisticians-do-at-Google),1337bruin,2012-12-26 23:17:07
The data repositories for Google are gargantuan. I would imagine most of their team and management would have to have some familiarity with statistics to be able to manage that scale of data.,,2012-12-26 18:34:25
Good read.  Love seeing statistics applied to things that interest students and make them see that statistics are a part of their everyday lives.,engelthefallen,2015-02-10 08:09:43
"Yes, definitely.  The students in that class came up with their own project ideas (nobody wants to do the ones I suggest), and they did a lot of interesting work!",AllenDowney,2015-02-10 10:49:27
"Interesting stuff, have you seen this: 

http://crockpotveggies.com/2015/02/09/automating-tinder-with-eigenfaces.html",daf1411,2015-02-10 16:19:01
That's excellent.  Thanks!,AllenDowney,2015-02-11 04:07:28
"No problem, love your books!

",daf1411,2015-02-11 09:10:52
"I'm over plyr and am all about data.table now. Way faster! Worth the small investment of time required to learn it, I think. (Only works well on 64 bit os, though).",arvi1000,2013-12-31 12:24:40
I would add RJDBC to the database one.,gicstc,2013-12-30 19:11:43
"Python and Matlab can do all those too, and they are readable and portable.",homercles337,2013-12-30 17:19:33
MATLAB is not free. Not even in the same conversion as R or python.,shaqed,2013-12-30 18:00:03
"Matlab is ""free"" to millions of academic users, why is it not in the same conversion as R or python?  I write matlab all the time and convert to python or C++.  Seems like a perfectly valid conversion to me.",homercles337,2013-12-30 18:49:04
"Maybe free, as in beer, for some, but not free as in freedom. No source code available to browse, right.",shaqed,2013-12-30 18:53:05
"&gt; No source code available to browse, right.

Nope.  Clearly you have never used matlab and like to disparage for no reason.  I dont understand this blind hatred for matlab.  Its as powerful as C++ and as flexible as python, but people hate it and spread lies about it.",homercles337,2013-12-30 19:01:01
"Where's the entire source code for matlab? You just accused /u/shaqed of spreading lies about matlab, after he(?) pointed out that it doesn't have source code available.

So if that's lies, where's the source code?


",efrique,2013-12-30 21:15:10
"No, i accused him of not using matlab.  Nearly all source is viewable/usable/readable if you use it.  The are parts that arent though, but i already said that.",homercles337,2013-12-30 23:14:36
"I'm pretty sure you are infringing copyright if you use their code, whereas you are allowed to use R's code as you will under the GPL terms",farsass,2014-01-01 11:08:08
the statistical support for python is so minimal that I don't see it fully replacing R anytime in the near future,iamthecheezit,2013-11-18 16:21:07
"Sincere question: can anyone give examples of important &amp; widely used stats functions that have not yet been implemented in any Python libraries (scipy.stats, sklearn, statsmodels, etc)?",galton1,2013-11-19 06:25:44
"lme4 is one, anything regarding survival analysis",iamthecheezit,2013-11-19 08:09:46
"They have there own little data niche, making a fine ecosystem when put together.",brews,2013-11-18 19:40:35
"But then, we have rpy2 for that. ",calsaverini,2013-11-19 01:33:28
"I wouldn't use rpy2 for anything but moving data between systems. There are big downsides to having two different, mutually dependent, interpreted languages in the same source.",NeuroG,2013-11-19 07:18:35
/r/pystats,Iskandar11,2013-11-18 17:23:20
The original article is [here](http://www.talyarkoni.org/blog/2013/11/18/the-homogenization-of-scientific-computing-or-why-python-is-steadily-eating-other-languages-lunch/).,lenwood,2013-11-18 19:17:54
"&gt;Python (MatPlotLib) for plotting and visualization

Matplotlib is awful compared to ggplot2. 

&gt;Python (NumPy/SciPy/pandas/statsmodels) for statistical analysis;

It's impossible to match CRAN's 5k packages. 


Imho, tying yourself down to one language is a terrible idea. It is very possible that in three years we'll be talking about how amazing Julia or Clojure is. ",,2013-11-18 22:23:04
python has a port for ggplot: http://blog.yhathq.com/posts/ggplot-for-python.html,dandrufforsnow,2013-11-18 23:35:42
"&gt; Imho, tying yourself down to one language is a terrible idea. It is very possible that in three years we'll be talking about how amazing Julia or Clojure is. 

I agree. That's why people had so much trouble getting rid of Fortran in the 90s, and why we are still saddled with so much Matlab now. It's because of people who, long ago, decided to marry one language.  At the same time, there are big cognitive costs to switching back and forth between languages unnecessarily. 

I think the solution may be in finding better ways to integrate and interface systems. Rpy is a decent first step, but I think with languages like R and Python, it should be possible to interface them such that it doesn't matter if I prefer to write Python or R syntax, I can make use of packages from either. I know it's a tough technical nut to crack, but with opensource interpreted languages that are so similar in design, and with so much overlap in users and use-cases, it should be possible.",NeuroG,2013-11-19 07:26:01
"Speaking of Clojure, I know Incanter is slowly getting really good.  But why hasn't anyone made a good statistics environment for Haskell?  It's a very popular language among mathematicians and seems like it would be be easy for them to pick up for statistical work.",beaverteeth92,2013-11-19 08:51:15
Ill have to look into Clojure. I know renjin has some stuff on the jvm for R.,,2013-11-19 13:06:47
"&gt;It's impossible to match CRAN's 5k packages.

Python's packages are growing rapidly, but you're right: R has a huge head start and that's very, very unlikely to overcome any time soon.  I can picture a time when R will be that thing you have to use once every three years for some very specific task, but I sadly doubt it's going away any time soon.  I say sadly because it's just such a crappy language otherwise.",ajmarks,2013-11-19 03:14:51
"You know, that's almost word for word what you could've said of Perl and its extensive CPAN a long time ago in a galaxy far, far away..",Coffee2theorems,2013-11-19 12:21:09
"True, but look at how long that took!",ajmarks,2013-11-19 14:14:21
"&gt; Matplotlib is awful compared to ggplot2. 

How so? I'm not saying it isn't, just asking what makes it so in your opinion?

&gt; Imho, tying yourself down to one language is a terrible idea.

Maybe, but from a programmer's perspective, does the alternative really have to be **R**?! Yuck! It's a language only Perl aficionados could love, with the exact same benefits (""Why Perl? One word, CPAN!"" &lt;=&gt; ""Why R? One word, CRAN!""). When DWIMminess is taken too far, you get [stuff like this](https://github.com/tdsmith/aRrgh):

&gt; if you do something like i in 1:foo, the wrong thing will happen if foo ever holds the value 0. 1:0 is the sequence (1, 0), since the : operator can and will count backwards. Always check whether foo is zero before you run your loop if you use :.

More gold from the same source:

&gt; Do not ever use T and F for TRUE and FALSE. You will see people doing it but they're assholes and not your friend; T and F are just variables with default values. Set T &lt;- F and source their code and laugh as it burns.

Right. Do you want to trust any computations done in that language? Maybe not too much.

&gt; It is very possible that in three years we'll be talking about how amazing Julia or Clojure is.

Julia is very nice. It doesn't do too much DWIM; if anything, it goes a bit too far in the other direction (""if 0"" and ""if []"" fail, as it requires booleans..). Anyhow, aside from the math stuff and good performance, it has programming niceties like [multiple dispatch](http://docs.julialang.org/en/release-0.2/manual/methods/), [metaprogramming capabilities](http://docs.julialang.org/en/release-0.2/manual/metaprogramming/) and [tasks](http://docs.julialang.org/en/release-0.2/manual/control-flow/#man-tasks) (green threads / coroutines). It's a language that's actually been *designed* by people who know what they're doing instead of just being cobbled together like R.",Coffee2theorems,2013-11-19 12:11:24
"&gt;How so? I'm not saying it isn't, just asking what makes it so in your opinion?

We wont use them at our company for presentations. Too ugly. Also tweaking it to get the data you need is a nightmare. 

R isnt that great for programming, but tbh a lot of people question Python's speed in a production environment. ",,2013-11-19 13:11:46
"&gt;&gt; but tbh a lot of people question Python's speed in a production environment

Even with Cython?",tangerinemike,2013-11-19 15:20:58
"As someone who almost never uses R for anything except data analysis, this post is a little threatening to me. It seems to imply that the people who do a huge variety of things (coding, writing apps, etc.) might shove R out of the limelight--and thereby reduce its support base and development--because they want a single language to do everything *they* do. Me, I want R to replace SPSS. I don't want it to replace C, Java, or anything else I've never used.

Sigh.",bobbyfiend,2013-11-19 04:38:10
"&gt; Me, I want R to replace SPSS.

Me too. Well, the second part. Python may prove to be a better tool-set for pulling people away from SPSS. Here in my department, people are interested in learning Python to program their experiments. When people find out they can also do their analysis and make excellent plots with this newly learned skill, they can be quite receptive. Any way to get rid of SPSS (and Excel), and I'm on board.",NeuroG,2013-11-19 07:37:04
"Oh god SPSS.  I'm using it in a class now and it's terrible.  I thought menus were supposed to make things easier.  That isn't the case when you have like 15 menus with options within options, labels are useless, and finding help is more impossible than it is for a tool with no commercial support like R.",beaverteeth92,2013-11-19 08:52:48
"Just wait till someone sends you a SPSS data file that is half-processed, but they haven't saved the source, and they want you to finish or verify the analysis. ",NeuroG,2013-11-19 10:53:13
Isn't SPSS replacing SYNTAX with Python?,ajmarks,2013-11-19 12:24:48
Don't know. Haven't worked with it in a couple years. It won't help if people still don't save their syntax though.,NeuroG,2013-11-19 12:26:50
"&gt; Me, I want R to replace SPSS. I don't want it to replace C, Java, or anything else I've never used.

Funny, I want some nice programming language (like Python) to replace R. I don't want it to replace SPSS, Excel, or anything else I've never used and have no fear of ever having to use even the tiniest bit (I worry about having to use R sometime due to CRAN). OK, from what I've heard it sounds like replacing SPSS would be doing the world a huge favor, so on a second thought, I'm all for it :) 

The first thing I heard of SPSS (a long time ago) was that it's a point-and-drool statistics system for social ""scientists"" and other computationally challenged folk who want to sound smart by spouting statistics they haven't the faintest clue as to the meaning of, so my view of such packages might be slightly.. uh.. *colored*, and probably not entirely fairly so. ""Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?""",Coffee2theorems,2013-11-19 12:44:16
"Your points are taken... except the territorial pissing match with (apparently) the entirety of the social sciences. Turns out, some of us study thinking errors and stereotyping. When we see it in the wild... well, it's kind of cute.",bobbyfiend,2013-11-19 18:23:05
"I'm not the [only one](http://xkcd.com/451/) who thinks social sciences are at the bad end of the ""softness"" spectrum while still defending its place among the sciences albeit by the skin of its teeth. I'd give psychology that status, but cognitive psychology saves it (although that one is trying to escape and re-brand itself ""cognitive science"", presumably to disassociate itself from the woolly reputation of psychology).",Coffee2theorems,2013-11-19 23:02:13
"[Image](http://imgs.xkcd.com/comics/impostor.png)

**Title:** Impostor

**Alt-text:** If you think this is too hard on literary criticism, read the Wikipedia article on deconstruction.

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=451#Explanation)

**Stats:** This comic has been referenced 6 time(s), representing 0.204359673025% of referenced xkcds.

---
^[Questions|Stats|Problems](http://www.reddit.com/r/xkcd_transcriber/)",xkcd_transcriber,2013-11-19 23:03:03
Great article. It's also important to note that big data systems application (Hadoop/GreenPlum) have strong Python and R support over proprietary solutions like SAS. Python takes things one step further in these systems by their usage as a flow control language and as a strong text parsing tool at large scale... a constant issue with huge logs or other unstructured data.,a2love,2013-11-18 20:17:16
"I [last submitted](http://www.reddit.com/r/statistics/comments/xf2w3/statistics_done_wrong_an_introduction_to/) Statistics Done Wrong six months ago, back when I first finished it. I expanded it last month with a new example: [survey overestimates of gun use in self-defense](http://www.refsmmat.com/articles/statistics.html#taking-up-arms-against-the-base-rate-fallacy). (Short version: it's hard to estimate the occurrence of rare events, when even if 1% of people falsely report an event, you will overestimate the occurrence rate by an order of magnitude. So more people report using a gun to defend themselves against criminals than have actually been attacked by criminals.)

I'm looking for more topics to cover. What other common statistical traps are there? Or are there errors I talk about that crop up in other places? I'm almost tempted to make a pamphlet out of this, for use in slapping authors of terrible papers.

(Also, I'll probably be revising the guide in a few weeks, since I'm a teaching assistant for the statistics course I took before first writing this, and I'll be inflicting it on my students.)",capnrefsmmat,2013-02-21 20:18:30
"I'm curious as to whether using linear regression for any sort of data is worth discussion.  I see it lots, people doing linear regression on proportions, count data, etc.  Is it worth discussing when simple linear regression is inappropriate?",daledinkler,2013-02-22 07:19:34
"Of course there is.  Linear regression is a profoundly simple statistic.  Discussing it among those that have no clue that it has profound flaws is problematic.  Discussing it among those that have no clue what it is when it is used poorly is what i believe you concerned about.  When your data have potentially hundreds of hidden variables (or even a handful of hidden variables) and you have only measured two, linear regression is a start to more thorough examinations but nothing more.  Even if your slope is near 1, hidden variables will kill you.",homercles337,2013-02-22 09:18:45
"So I've read papers on, say, the unjustified overproliferation of power law models (""everything follows a power law!!!""), but not much about the abuse of linear models in published papers. Do you know any good examples of botched papers using linear regression, or reviews of its abuse?",capnrefsmmat,2013-02-22 11:52:42
"I agree with that point, I'm thinking more of cases such as when OLS is used with proportion data as the dependent variable.  We know that proportions should be bounded at 0 and 1, but OLS would allow us to predict values much beyond those bounds.  The thing is, I suspect that you can get away with it sometimes (and there's a lot of papers that do!) as long as your data don't have too many edge cases.

I guess that's my question for capnrefsmmat, is this worth getting into as far as s/he is concerned?",daledinkler,2013-02-23 18:37:32
"Could be. Most of my other topics were added after I found a review paper detailing the error and its common misuse in science (""43% of neuroscience papers commit this error""). If I see any on the abuse of OLS I may add a new section.",capnrefsmmat,2013-02-24 16:15:03
"I can't really follow the numbers in this paragraph:

&gt;Such surveys have been done, with interesting results. One 1992 telephone survey estimated that American civilians use guns in self-defense up to 2.5 million times every year – that is, about 1% of American adults have defended themselves with firearms. Now, 34% of these cases were in burglaries. But most people are not at home when their houses are burgled, and most people do not own guns, so we reach the inescapable conclusion that burglary victims in gun-owning households use their guns in self-defense more than 100% of the time when burgled. There are just more reported cases of gun self-defense than there are burglaries of occupied gun-owning residences.

Google tells me the USA population size is 313,914,040, so 2.5 million is just under 1%. OK. 34% of these are in cases of burglaries, so the survey implies that there are 800,000 or so burglaries per year where a gun is used in self defence.

How do we, from this, conclude that the 800,000 burglaries makes up more than 100%? I think there's a missing number here, the actual number of reported burglaries. Or did I just misunderstand something?",Ahhhhrg,2013-02-22 07:34:54
"There are (allegedly) 800,000 burglaries where a gun is used in self-defense, and about a million burglaries where someone is home (as beatle42 noted). But most of those people don't own guns. Somehow, gun-owners would have to use their guns in self-defense more times than gun-owners are ever burgled.

I agree the paragraph is confusing; it spent less time stewing and being edited than the rest of the guide. I'll try to rework it.

edit: I've just pushed an updated version. Is that clearer?",capnrefsmmat,2013-02-22 11:55:05
"Definitely. Again, I really like the text, very clear explanations. I've never really understood the power of a test before, for example. I do now, so thank you for that.",Ahhhhrg,2013-02-22 12:57:28
"No, still not clear. 

&gt;Such surveys have been done, with interesting results. One 1992 telephone survey estimated that American civilians use guns in self-defense up to 2.5 million times every year – that is, about 1% of American adults have defended themselves with firearms. Now, 34% of these cases were in burglaries. But most people are not at home when their houses are burgled, and most people do not own guns, so we reach the inescapable conclusion that burglary victims in gun-owning households use their guns in self-defense more than 100% of the time when burgled.10 There are just more reported cases of gun self-defense than there are burglaries of occupied gun-owning residences.

There aren't enough numbers cited to support your conclusion. How many burglaries were there? How did you jump to the &gt; 100% conclusion? Please be more explicit about cited #'s and your math so we can double check it. As it reads now, I'm left scratching my head. ",mthoody,2013-02-22 12:59:10
"How about now?

&gt; Such surveys have been done, with interesting results. One 1992 telephone survey estimated that American civilians use guns in self-defense up to 2.5 million times every year – that is, about 1% of American adults have defended themselves with firearms. Now, 34% of these cases were in burglaries, giving us 845,000 burglaries stymied by gun owners. But in 1992, there were only 1.3 million burglaries committed while someone was at home, and 42% of American households own guns. So of 546,000 burglaries of gun-owning households, 845,000 of them were stymied by gun-toting residents! We reach the inescapable conclusion that there were more uses of guns to interrupt burglaries than there were burglaries.",capnrefsmmat,2013-02-22 13:42:10
"Much better, thanks. But it isn't inescapable yet. Maybe households in high crime areas are more likely to have guns. Maybe stymied burglaries aren't included in the burglary stats.",mthoody,2013-02-22 13:54:35
"Two thirds of burglary victims in occupied dwellings are asleep when the burglary happens and only notice later. So that's 182,000 burglaries where they have a chance to use a gun to stop the burglary, and 845,000 reported cases -- nearly a factor of five difference. The unequal gun distribution would have to be severe, or the burglary underreporting massive.

This is why I also quote the National Crime Victimization Survey results. With better methodology they show that the number is simply wrong.",capnrefsmmat,2013-02-22 14:01:20
"Hey, I'm just trying to help you write a more bullet-proof analysis. As written, the conclusion isn't ""inescapable"" on the merits of the paragraph alone. I like the premise of the example...I just squirm a bit on the burglary contradiction being an oversimplified napkin calculation.

It might be more persuasive to compare Gleck to NCVS estimate disparities and speak to why they could be so different without the debatable burglary contradiction. Readers (like me) get hung up on the example and miss your main point (which you do explain well). 

Just a random Internet opinion. ",mthoody,2013-02-22 14:58:11
"Hmm. I'll let this one simmer in my head for a while to see if I can devise a better explanation. I do think the back-of-the-envelope math is useful, if just to set up readers to trust the NVCS numbers more than Kleck.

Given how much editing the rest of this went through, I'm not surprised the new section could use some rejiggering. I think it was three months between writing the basics and finally posting on Reddit.",capnrefsmmat,2013-02-22 17:02:04
"I assume that some of the time when the gun is used in self defense, the burglar runs off, and the burglary isn't reported. ",,2013-02-22 08:52:18
"That's true. Studies have shown loads of differences between the two most used crime databases in the US: the uniform crime report, compiled by the FBI from crimes reported to local precincts, and the national crime victimization survey, which is a random phone dial survey by the department of justice that asks about recipient's experience with crime.",jiggajiggawatts,2013-02-22 08:57:01
"I'm not quite sure how to put all of the pieces together, but [here](http://bjs.ojp.usdoj.gov/content/pub/pdf/vdhb.pdf) is a report talking about home burglaries.  It's not for the same years (phone survey was for '92, this is for 2003-2007), which says that each year there are 1,024,788 burglaries where someone is home.  That's certainly more than the 800,000, though I'm not sure there's a breakdown of how many of those homes were gun owning homes, but it seems like it's about [50%](http://www.gallup.com/poll/150353/self-reported-gun-ownership-highest-1993.aspx) of homes that report having a gun in them, so if the burglaries are anywhere close to equally distributed then that would mean only about 500,000 homes a year that own guns are burglarized.  That suggests to me that homes with guns would have to be far more likely to be the targets of burglary than non-gun homes to make the 800,000 number work.

Of course, none of that is in the article so your point stands (though I didn't try to read the reference it provides for it, so perhaps it's laid out more there).",beatle42,2013-02-22 08:12:14
I'm with you here.  In the preceding paragraph to the one you quote it states two scenarios in which guns are used in self defense.  Then in the paragraph you quote it only gives numbers for one of the scenarios.,youdneverthink,2013-02-22 09:39:51
"Confounding and the application of NHST in observational data.

The problem is that the explanation for rejecting a point-null is not identified. Rejecting your null doesn't mean you can interpret _why_ it was rejected in these contexts. Given the complexity of the systems examined (country-level comparisons, population incomes, patient populations, whatever) in observational studies, it's probably a safe bet that it isn't because of a direct effect of the independent variable that's used. Put another way, just from an entropic argument, confounding is much more probable than whatever interpretation you want to put on your correlation.

You see this all the time with applications of fisher exact tests / chi squared tests / linear regression etc. on observational data.",,2013-02-22 12:07:04
"A statistical trap I see regularly at work is related to *population replacement*.

Experience rating for setting health insurance rates for small employers looks at past claims to set future rates. High past claims = high prospective rates. But often the high claims are associated with a now deceased person. Insurers typically remove a portion of the actual high claims and charges all groups a pooling charge. However, in the case of deceased or terminated insureds, they really ought to remove all of the claims and charge a slightly higher pooling charge to everyone. 

TL;DR stats about a population lose predictive power as the population changes. ",mthoody,2013-02-22 11:09:27
"Given this topic, i am curious how one corrects or even estimates bias in reporting for such a ""hot-button"" topic?  I am a computational scientist, so these topics in survey data seem very difficult to analyze.  In this case its clear there is a massive bias toward one ideological perspective.  So much so that the data is unusable, or is it?",homercles337,2013-02-22 09:25:29
"A related issue is recollection accuracy. If a survey asks ""did you have X experience in the past 12 months"" often gets answered yes if the event happened in the last 24 months. Important life events always *seem* more recent than trivial ones. 

Edit: aka ""telescoping"".",mthoody,2013-02-22 10:43:53
"FWIW, I think you mean ""robbery"" or ""home invasion"" when you say ""burglary"". If you come face to face with a burglar who does not immediately flee, he becomes a robber.

Burglarly implies (usually) the victim is never is face to face with the burglar.",mthoody,2013-02-22 11:00:24
I'm just following the usage of the papers I cited. I expect the robbers merely intended to be burglars in most of the cases.,capnrefsmmat,2013-02-22 11:57:52
"&gt;  One 1992 telephone survey estimated that American civilians use guns in self-defense up to 2.5 million times every year – that is, about 1% of American adults have defended themselves with firearms.

This sentence seems like a misstatement.  Perhaps:  ""One 1992 telephone survey estimated that American civilians use guns in self-defense up to 2.5 million times every year – that is, about 1% of American adults defended themselves with firearms every year.""

EDIT:  Also, the 1992 US population was approximately 250 million, with approximately 75% of them age 18 or older.  That is 187.5M adults in the U.S. in 1992.  Anecdotally, I defended myself from a burglar at age 16 using a shotgun.  (Or, more accurately, the *sound* of a shotgun.)  Therefore I think you should simply change ""American adults"" to ""Americans.""
",Heath_Hunnicutt,2013-02-24 13:24:01
"&gt; Unfortunately, many trials conclude with There was no statistically significant difference in adverse effects between groups without noting that there was insufficient data to detect any but the largest differences. *And so doctors erroneously think the medications are equally safe, when one could well be much more dangerous than the other.*

How sure are you that doctors think that?",AxiomL,2013-02-24 16:31:40
"Check out the paper I cited in that paragraph.

&gt;  In those studies where there was no statistically significant difference in serious adverse events despite a twofold difference in rates, the power of the study to detect the reported adverse event rates was not reported, nor was the possibility that the study may have been underpowered to detect these differences even discussed despite a maximum power of 0.37 in these studies. Based on the reported results, the authors of all three studies drew both statistical and clinical inferences and made conclusions regarding the use of a therapy based on no difference in risk.

https://www.sciencedirect.com/science/article/pii/S0895435608002217

So yes, doctors erroneously make conclusions about the safety of drugs when their studies do not support them.",capnrefsmmat,2013-02-24 17:26:54
This is *very* comprehensive and I like it.,featherfooted,2013-02-22 10:09:51
"Interesting analysis. One question came to mind:

isn't your assumption of each generation having the same hazard function as those of the same age 10 years before a bit of circular reasoning? You're concluding that the overall probability of getting married by the 4th decade of life is about the same regardless of the calendar birth decade, but that conclusion is based on assumed similarities between people in their 30s regardless of when they were born. In fact, the existing data may show that this assumption is invalid for the decades we do have. 

You could see how this assumption measures up by running the simulation as if you were in the 1990s, 2000s, etc. and see how well your predicted curves matched the real data. That is, try running it year by year and see if they converge or not.

ETA: It'd also be nice to see the hazard coefficients plotted by cohort over time as was done in your intro slides. ",BillyBuckets,2015-02-26 10:39:29
"I see your point -- it does seem circular.  But I think the conclusion is legitimate, probably even understating the effect.  Here's why.

1) There seem to be two trends happening: successive cohorts have lower hazard rates when they are young (less likely to get married young) and higher hazard rates when they are older.

2) Taking the curve for the 80s cohort as an example, I am using the low hazard rate from their youth (which was actually observed) and extending it using the hazard rate from the 70s cohort.  But if the second trend (more late marriage) continues, we actually expect the hazard rate for the 80s cohort to be higher when they are older (than it was for the previous cohort).

So the prediction is probably conservative in the sense of underestimating the fraction of Millennials that will ever marry.  If current trends continue, even more of them will marry.  They will only underperform the prediction if current trends reverse.

Your suggestion for validating the predictions is a good one, and I might have a chance to implement it.",AllenDowney,2015-02-26 11:24:56
"My understanding is not based on the US so may be slightly off in terms of data but:

&gt; There seem to be two trends happening: successive cohorts have lower hazard rates when they are young (less likely to get married young) and higher hazard rates when they are older.

This is in general true, but seems inconsistent with actual data manifestations (the proportion of women never marrying by age 45 seems to be increasing across cohorts). I would be really surprised if recuperation effects are really that strong.",lionmoose,2015-02-26 15:41:02
"I support anything published as an IPython notebook.

Also, very interesting work!",PhaethonPrime,2015-02-26 10:50:11
Thanks!,AllenDowney,2015-02-26 11:25:17
"A few comments on the slides:
*1. Re. Survival analysis, you may want to explain it in terms of ""time to event"" which is, I think, easier to grasp outside of specific application context (survival). This way the vocabulary is no longer ackward.  
*2. On slide 12, the hazard displays quite unnatural blips, peaks and valleys that are hard to assume. This sort of problem has been addressed by Royston, Parmar et al with flexible parametric survival which uses splines to model the hazard function. The framework is nicely implemented in Stata. Not sure about R (and I know close to nothing of Python)  
*3. Slide 34: why categorize?  You could show point estimates for average persons born in each decade instead.",ThomasSpeidel,2015-02-26 17:16:27
"Thanks for these comments and suggestions:

*1 Good idea, although it cuts one of the funny parts.

*2 I could smooth it, but it gets smoothed anyway when you compute the survival function, so I decided not to solve a non-problem.

*3 The reason I split the dataset into cohorts is that, once I had implemented the analysis, it only took two lines of code to split up the dataset and run the analysis on the cohorts.

Would the approach you suggest be better?
",AllenDowney,2015-02-26 17:55:46
"I thought you were using Cox PH...

See p.363 of this (it's in R)
http://biostat.mc.vanderbilt.edu/tmp/course.pdf",ThomasSpeidel,2015-02-26 21:01:51
"Ah, right. If I were using a regression model I would just include date of birth as an explanatory variable, rather than group by decade of birth.",AllenDowney,2015-02-27 06:15:08
"The slides are nice. I don't understand all of them but that's due to my beginner status. The email, twitter, site slide is very creative and required good thought.",theoriginalauthor,2015-02-26 08:40:20
"All of this is based on Chapter 13 of Think Stats, 2nd edition, so if the slides are not sufficient, you can read it at thinkstats2.com",AllenDowney,2015-02-26 09:02:39
Thanks - I really enjoyed reading that!,GrynetMolvin,2014-08-26 13:18:26
"Wonderful!

I think experienced statisticians could benefit from reading that also, if only because it gives a simple example that they could teach to their own students.

It should be noted that die-hard frequentists are still allowed priors. This may come as a surprise, but bear with me! In general, with more complex models beyond the cookie-jar model there, there is no ""recipe"" that will allow a frequentist to create an estimator or interval with the desired properties. They need to simply start experimenting, writing down many estimators and seeing which are good and which are not so good. A good place to start that search is often to write down a prior and compute a posterior. In fact, there are many cases where the Bayesian estimator ticks all the right frequentist boxes. There always is a Bayesian ""recipe"", if you're prepared to decide on a prior.

This is why I think that a lot of people that call themselves Bayesians are actually frequentist, and vice versa. In fact, I don't really know what I am! And I don't care!!",SkepticalEmpiricist,2014-08-27 11:01:07
"What's with the comment from Hongyuan He?

I've never seen a more coherent and elegantly crafted response that is so completely incorrect.",imherejusttodownvote,2014-04-03 13:54:27
You must be new to reddit then :),,2014-04-03 13:56:56
"Would it be possible if you could point out the points where he's wrong? My stats isn't very strong, so it would be really helpful to me if you had the time to point them out.",truckbot101,2014-04-03 18:20:55
"Linear regression and ANOVA fall under the 'general linear model' framework. The original poster's trying to demonstrate an example similar to slides 11-13 [here](http://courses.education.illinois.edu/EdPsy580/lectures/11glm_anova_ha_online.pdf). The only difference between the linear model and the ANOVA model is how they treat the baseline mean-- the 'grand mean'.

From what I gather, He's nitpicking over error decomposition. ANOVA decomposes observations into: Y_ij = grand mean + between group error + within group error (see the equation on slide 47, i being the category). He's saying the post is wrong because a ""critically determined"" equation shifts all the errors from between group error to within group error, and ?? not sure what point he was trying to make.

It turns out the least squares approach handles this error decomposition implicitly (std errors for base mean (intercept)) but yields the same results, which in this case is the mean effect for each category. You just need to interpret the intercept as the reference group mean rather than the grand mean.

To see how ANOVA and linear regression are related for categorical variables, slides 40 to 45 show a full derivation. In short, the original poster is correct and while He had valid points, they weren't relevant to the post's topic.",vbs_redlof,2014-04-03 19:42:17
Thank you!,truckbot101,2014-04-03 20:57:08
"&gt; If your graduate statistical training was anything like mine, you learned ANOVA in one class and Linear Regression in another.

Is this true for other people?  Did people really get through graduate training without knowing that they're the same thing?  This was third year undergraduate stuff for us, how does it not come up in regression classes?

I've stopped teaching ANOVA as a subject, instead teaching it as a model-evaluation of linear regression.  It also helps avoid all the MANOVA, MANCOVA nonsense.",slammaster,2014-04-03 19:58:58
During my master program we had ANOVA and regression in the same statistics classes and we were taught that they are equivalent.,xediii,2014-04-04 06:48:11
"&gt; Is this true for other people? Did people really get through graduate training without knowing that they're the same thing? This was third year undergraduate stuff for us, how does it not come up in regression classes?

For years I felt inadequate, as I was never really taught ANOVA per se, and every time I tried to learn it on my own I'd get bored and just run the regressions. Learning all the extra jargon did not seem to add anything. Giving up certainly reflects a weakness of mind, but it also never seemed to have a purpose in itself.",,2014-04-03 20:49:22
I am in the third year of stats undergrad and we did ANOVA in linear models class as special case of linear regression.,snork_maiden,2014-04-03 23:54:37
Not too well-trained in stats; what do you mean by MANOVA and MANCOVA nonsense? ,DoorGuote,2014-04-04 04:28:50
"Nonsense in that they're not really anything, they're just special cases of linear regression.  Presenting them as separate subjects makes them more difficult to remember and process, when they're all really just multiple linear regressions",slammaster,2014-04-04 11:50:51
"The M is ""multiple"" so ANOVA = 1 var MANOVA = N var


ANCOVA is analysis of covariance, MANCOVA is the multivariable one.",dza76wutang,2014-04-04 07:10:21
"Got it. Why exactly are they nonsense, though?",DoorGuote,2014-04-04 07:11:54
"Not sure about the nonsense, unless the mechanics get ugly. Once I ""learned"" ANOVA = Regression we stopped ANOVA and moved on to various regression things. I have never formally dealt with MANOVA or MANCOVA so I do not have any knowledge of why they may be nonsense.",dza76wutang,2014-04-04 09:29:21
"The first article I have read that made me bust out SPSS and check the results.

Custodial workers had an average of 298 months of experience? How old is the average custodian? We really have to figure out what is going on to account for such a high average number of years of experience among our cleaning staff.
",Mockingbird42,2014-04-03 13:22:36
"I have a friend who considers ANOVA appropriate to 'test hypotheses' whereas linear regression is 'hypothesis generating', in my friend's mind if there was not a clear experiment with clear hypotheses then a linear regression is MORE appropriate than ANOVA. I can't understand that point of view when I've been taught something very similar to this article. Enlighten me?",tiii,2014-04-03 16:02:51
Because model selection is frequently performed with linear regression. This is typically not done with ANOVA.,rottenborough,2014-04-03 17:29:07
"This is my impression too. To expand, ANOVA is generally taught in the context of finding differences between groups (i.e. We have several group means, can we test and see if they are different?). On the other hand, linear regression is couched in the context of finding a relationship between predictors and a response. The secondary step after fitting a model is then determining which predictors we deem significant, which seems to fit the step of ""hypothesis generating"".",h4ri,2014-04-03 17:31:43
"I'd say determining the type of relationship between the predictors and the response is the exploratory part. In fact many selection methods don't even pretend they have a ""p-value.""

If you start with a pre-specified model and simply test for predictor ""significance"", it's actually confirmatory. That's also usually what people do in ANOVA.

So while the underlying math is the same, the application and education are quite distinct for the two approach.",rottenborough,2014-04-03 18:02:06
"This is a good way of looking at it, and I agree. Thanks! ",h4ri,2014-04-03 18:46:54
"Hmm interesting, yep I can see how he would translate that into his understanding that they are completely different.",tiii,2014-04-03 19:35:11
Can't a clearer demonstrations of this be done by going through the linear algebraic formulation of a multiple linear regression model and stripping it of all but a single categorical predictor?,Neurokeen,2014-04-03 17:53:52
"Either I am missing something, or this article is missing something, but there are at least two big difference between ANOVA and linear regression:

1) Linear regression estimates a parameter associated with each explanatory variable, ANOVA doesn't.

2) ANOVA includes interaction terms, linear regression doesn't.

So while they are related ideas with some elements in common, the conclusion that they are the same thing is just wrong.",AllenDowney,2014-04-04 08:52:05
Um .. am I mistaken? aren't the SE's calculated differently? ,fake_belmondo,2014-04-03 20:33:34
"The models are linear and they both assume normal distributions, no reason why SEs should be different.",YaoPau,2014-04-04 05:32:06
These are awesome! What software did you use to create the videos? ,t_rex_tullis,2014-01-16 07:32:40
Thanks!  In the videos I'm working on a tablet with SMART Notebook installed.  I use the SMART Recorder to capture my work on the tablet and Mediasite to capture the video of me standing in front of a green screen.  Then I use Adobe Premiere to sync the two videos and replace the green screen with the content from my tablet.,sincosine,2014-01-16 10:35:58
"Nice! Where do you make space to get a green screen, etc from? Like what's your set up? ",Ayakalam,2014-01-16 14:49:00
"I'm definitely no expert on video production, so my setup was pretty simple.  The green screen is just a hanging sheet of felt.  I don't move around much while filming so it didn't need to be very large.

I placed a shotgun mic in front of me and three cheap lights- one on either side to kill shadows on the green screen and one just to the left of the camera pointed straight at my face.",sincosine,2014-01-16 17:50:48
"Thanks so much! I ask because I am actually trying to make some pictorial -language videos for language retention in the future and was curious about the setup. I feel there are many sites that make you learn say, French, but not many to help you _retain_ it. 

&gt; I use the SMART Recorded to capture my work on the tablet

I have, (like you, I think), an iPad, and I have the same SMART Notebook, but I couldnt find this ""SMART Recorded"" you mentioned... is it on the app store or am I bone-heading something?

Thanks so much! ",Ayakalam,2014-01-17 05:11:26
"Wow, excellent! Thank you.",carlEdwards,2014-01-16 08:14:24
"Nice. Are you planning episodes on Bayesian Testing? I see you have covered Bayes Theorem.

I have been looking for a decent Bayes Testing for a long time.",dnst,2014-01-16 13:29:03
"Probably not in the short term.  First I want to see whether people will actually get some use out of these videos.  I also wanted to focus the first set of videos on all of the most basic techniques to avoid the potential for confusing information.  

Later on, creating a video that provides an overview of Bayesian Statistics could be interesting, but I'm probably not the best candidate to go really in-depth into the subject.",sincosine,2014-01-16 18:00:18
Very cool.  I'm all about making math/statistics more comprehensible to other people.,HughManatee,2014-01-16 15:28:06
Great stuff!,crabpot8,2014-01-30 17:16:17
A lot of these are private only. Was that intentional?,Entaroadun,2014-02-08 20:27:48
"The paper is actually in Nature's new open-access spinoff, Scientific Reports:

Neil Johnson, Guannan Zhao, Eric Hunsader, Hong Qi, Nicholas Johnson, Jing Meng &amp; Brian Tivnan (2013)  [Abrupt rise of new machine ecology beyond human response time](http://www.nature.com/srep/2013/130911/srep02627/full/srep02627.html). _Scientific Reports_ 3:: 2627 ",DevFRus,2013-10-17 21:01:34
I would like to point out that the p-valley 'fallacy' here is largely pedantic and in no way undermines any of the conclusions of the work as was pointed out in the [early comments](http://bayesianbiologist.com/2013/10/17/p-value-fallacy-alive-and-well-latest-case-in-the-journal-nature/#comment-2188) and by the [paper authors](http://bayesianbiologist.com/2013/10/17/p-value-fallacy-alive-and-well-latest-case-in-the-journal-nature/#comment-2213). The blog post author [has apologized](http://bayesianbiologist.com/2013/10/20/follow-up-to-johnson-et-al-post/) for his original post.,DevFRus,2013-10-21 03:42:25
What is the p value fallacy?,robbyroo,2013-10-17 21:10:47
He explains it in [this post](http://bayesianbiologist.com/2011/08/21/p-value-fallacy-on-more-or-less/).,DevFRus,2013-10-17 21:17:21
"That failing to reject the null hypothesis is the same as accepting the null hypothesis as true. Or, as he notes in his post, ""A lack of evidence for something is not a stack of evidence against it.""",Palmsiepoo,2013-10-17 22:53:41
"I like the snappy version in the comments: ""Absence of evidence is not evidence of absence.""",DevFRus,2013-10-18 00:15:57
"This is /r/statistics, let's not gloss over the details.

You can have the evidence of absence if you have designed your study and test procedures appropriately. With a good estimate of the effect size beforehand, you can calculate the sample size you need to be able to obtain the null. If you reach that sample size, you have the power necessary to obtain the null, with the small caveat that it does not detect below the target effect size.

The problem with p value threshold is that people see it as a magical property that manifests itself in the data, not a property of a test procedure that they have to design.",rottenborough,2013-10-18 07:58:28
"But isn't it? Isn't the saying that ""absence of proof is not proof of absense, but it sure is a hint"", which you could formulate as ""absence of evidence is evidence of absence""?

If I give you an urn and you start drawing balls from it, the more black balls you draw, the more likely it will appear to you that there are no white balls in the urn.",Bromskloss,2013-10-18 06:33:17
"Yes. It is evidence, but not proof. Sometimes people who say ""absence of evidence is not evidence of absence"" are confusing the two.",jmmcd,2013-10-18 06:36:33
"Yes, but unless you draw *all* the balls out of the urn without hitting a white ball, you can't 100% say ""there's no white balls in there."" And furthermore, this isn't necessarily the greatest example to use to elucidate the more fundamental point, because in general it's impossible to prove a negative. This is a special case where you can do so (since if you empty the urn and there's no white balls in it, then you have definitively proven that there were in fact no white balls in it).",Eurynom0s,2013-10-18 12:12:12
"&gt; Yes, but unless you draw *all* the balls out of the urn without hitting a white ball, you can't 100% say ""there's no white balls in there.""

I fully agree.

I'm not talking about proof, I'm talking about the weaker form: evidence. Hints, if you will. Pieces of information that adjusts one's probability one way or the other, without necessarily driving it all the way to zero or one.",Bromskloss,2013-10-18 12:29:42
"
Reverend Bayes says ""Oh yes, 'tis.""  -- since absence of evidence when it would expected to be present will tend to increase the Bayes factor toward absence of the thing.

It's succinct, perhaps, but appears to overstate; usually there's at least some evidence of absence associated with absence of evidence.
",efrique,2013-10-18 17:58:54
So type II errors? Do we really need another term for this?,ajmarks,2013-10-18 03:35:25
"I'd say this is a specific fallacy that's more succinctly described as a fallacy. It means interpreting P(data | hypothesis) as P(hypothesis|data).

This is one common error in applied sciences that do frequentist statistics. But we're not even touching on the mountain of problems that constitutes the NHST paradigm here... There are plenty of theoretical problems too.",,2013-10-18 04:37:42
Like what? I'm curious. :o,Monombo,2013-10-18 05:56:17
"Certainly. I don't know if you're comfortable with the mathematical side of things, so if something isn't clear just let me know.

For example, the p-value is sometimes used as a measure of evidence against the null-hypothesis (I believe this is how Fisher intended to use the p-value... I could be wrong, though). But it can be shown that, under reasonable criteria, the p-value is a very bad measure of evidence ([reference](http://arxiv.org/abs/1111.4821))

A nice demonstration by Australian statistician Geoff Cummings shows that, even if the alternative hypothesis is true, the p-value can easily be either &lt; or &gt; 0.05. See [this video](http://www.youtube.com/watch?v=ez4DgdurRPg) (which you can repeat for yourself by downloading his Excel files). In other words, even if the null hypothesis is false, the p-value does a poor job of ensuring it will show this.

Another problem with null hypothesis significance testing, especially in sequental experiment designs, is it's dependence on stopping rules. Let me give you an example. Suppose you have a coin and you are interested in figuring out if the coin is fair. In other words, you are interested in investigating whether the probability of throwing heads, denoted by [;\theta;], is equal to 0.5. After throwing the coin a number of times, say, [;n=12;], we observe [;k=3;] heads. Let's define the random variable [;X;] as the number of heads. A frequentist might interpret the experiment in two ways:

 * we collected the data by deciding a priori that we throw the coin 12 times. In this case, [;X;] follows what is called a binomial distribution, and the corresponding p-value can be shown to be 0.073.  The frequentist would conclude that the null hypothesis cannot be rejected.

 * we collected the data by deciding a priori that we will throw the coin until we get 3 heads. In this case, [;X;] has a negative binomial distribution and the p-value can be calculated to be 0.0327. The frequentist would reject the null hypothesis (possibly in favor of an alternative hypothesis, if you're a Pearson/Neyman-frequentist-kinda cat).

So, although the data is exactly the same in both cases, depending on what the statistician decides to do, the conclusion can be completely opposite. We are basing our inference not only on the data, but what is going on inside the statistician's mind! You could imagine how this makes it more tempting for scientists to commit fraud or bad science: is the p-value not doing what you want it to? Just decide post-hoc that your sampling distribution is different.

Actually, this is not the only way p-values can be manipulated. If you're collecting data and 'trying to overthrow the null-hypothesis' (after all, you're a scientist, and you have to publish an interesting positive result, right?), you can just keep on sampling data until the p-value is &lt; 0.05. In fact, you can *prove* that, given some null-hypothesis, with probability one, the p-value will cross your significance level *at some point*. 

Should you be interested in all of this and more, you could try reading [this overview](http://www.socsci.uci.edu/~mdlee/WagenmakersEtAl2008.pdf) or this [summary and collection of pointers to other relevant articles](http://www.stats.org.uk/statistical-inference/).",,2013-10-18 06:37:30
"That was fascinating, and I'll definitely be having a read through those articles later! How would you go about proving that if you increase tour sample, p eventually becomes significant?",Monombo,2013-10-18 06:47:58
"Reading some of your second source. I have always been interested in Bayesian methods, but I always struggle to find a research problem outside of clinical trials that would be amenable to it. Are their simple ways of including Bayesian methods in observational studies?",medstudent22,2013-10-18 12:26:18
Bayesian statistics is used in almost every quantitative field of science (and even some non-quantitative ones). Do you want to know about a specific case? I could probably find you some relevant resources.,,2013-10-18 17:34:08
"I end up doing a lot of analysis on pre-existing databases with n = 100-400. So I'm typically making use of Chi-square/Fisher, Wilcoxon/t-test, Spearman/Pearson r, some log-rank for survival, logistic/multiple linear regression, etc. Usually looking to see how associated one factor is with survival or some prognostic thing like having positive lymph nodes and so on. Are there ways I could introduce Bayesian inference in a scenario where I would have used any of these tests?",medstudent22,2013-10-19 06:36:07
"Yes. Bayesian statistics encompasses all these tests and more. Basically, every test that you know from frequentist statistics is also available in a Bayesian variant (actually, there are cases where the Bayesian counterpart performs better than the standard frequentist one, see [here](http://www.indiana.edu/~kruschke/BEST/BEST.pdf) for instance). The only difference is in the way inference is done. A frequentist does it by calculating p-values, a Bayesian does it by obtaining a posterior distribution of the parameter of interest.",,2013-10-19 09:32:58
Thanks for the response. Do you know of any books that break this down by frequentist test and bayesian alternative? I would especially be interested if such a book had examples of how to do such problems in R or python.,medstudent22,2013-10-19 09:39:41
"Not quite. The p value fallacy is about drawing the wrong conclusions from a large p value, type II error is about failing to reject the null when you should. 

Type II error is a numerical issue, p value fallacy is about education ",slammaster,2013-10-18 06:24:16
It's this sort of thing that makes it unsurprising to me that a large number of papers report results that cannot be replicated. ,WallyMetropolis,2013-10-18 10:44:03
Was I the only one who was shocked that such an influential paper was done with Excel? ,sn0wdizzle,2013-04-16 21:35:55
I thought most financial analysis was done with Excel.,canteloupy,2013-04-17 05:20:17
"I think that's true, but the important difference is that academic work should be easily replicatable and documented. Excel doesn't have logs.",jiggajiggawatts,2013-04-17 15:15:57
"I'm not shocked. Relevant [comic](http://xkcd.com/974/). If you have a simple analysis to do, a quick spreadsheet may do the job. ",Gymrat777,2013-04-17 09:36:59
would this count as a simple calculation? ,Zeurpiet,2013-04-17 09:52:02
"Yeah, pretty shocking in a way, that two such influential economists apparently don't have the wherewithal to do a statistical analysis with some software suited for it.",Revontulet,2013-04-17 06:07:30
"When I was in graduate school I was taking a lot of doctoral coursework and I noticed that economists (generally, but not all) use *very*, *very* simple statistical methods (e.g., two stage least-squares, linear probability models [[this](http://pricetheory.uchicago.edu/levitt/Papers/schoolchoicelottery.pdf) one's the most insulting, given that *Econometrica* is one of the most prestigious journals in empirical economics]). As much as I love economics and econometrics, after taking the coursework, I found myself being much less confident in their methods. Although, there are many economists that stand as a beacon of hope; James Heckman is a wonderful example of that (he's recently been incorporating much more advanced statistical methods, rather than purely advanced mathematics). ",econometrician,2013-04-17 07:45:44
"Statistical methods take awhile to diffuse to other fields of research. For example, a health economist could use a really cool (and useful) statistical/econometric technique, but then the economist has to teach reviewers and conference attendees what is going on instead of talking about results. Also, new techniques need time to be vetted in their respective fields before they are ready for public consumption.

I'm not saying this is the right thing way to go about things, but it may explain why economists use simple methods.",Gymrat777,2013-04-17 09:34:24
"I'm not sure that I necessarily agree with that. There are several economists that use newer methods, but many economists see statistics as a means to an end and not a means; that's where I think the big issue is. But, that's just my opinion.",econometrician,2013-04-17 10:04:01
"It really depends on the problems that are to be solved, doesn't it? If the problem can be solved with simple statistical methods, why should more complicated ones be employed?

Sorry if this sounds more combative than I intended. Would be great if you can elaborate on why 2SLS is too simplistic for what you observed.",zynik,2013-04-18 01:57:40
"David Card published a great paper on the interpretation of Instrumental Variables. Mroz also published a great paper on how small changes in the first stage equation (i.e., implicit changes your assumptions for identifying your endogenous variable) causes very dramatic differences in your parameter estimates. 2SLS can be a very, *very* dangerous tool when improperly interpreted. It's not inherently a bad method, but it can be--such is the case with many methods though.",econometrician,2013-04-18 04:02:44
"As a physicist, the thing that shocks me the most is that a lot of economics equations are not dimensionally consistent. And worse, even if economists are aware of this, a lot of them will argue that it's not a problem, or even somehow desirable.",Eurynom0s,2013-04-17 13:05:33
Can you give some examples? I'm always on the lookout for cases of e-con art.,jiggajiggawatts,2013-04-17 15:19:16
"[Here's a PDF link to a short piece about a guy talking about mainstream economics being dimensionally deficient.](http://mises.org/journals/qjae/pdf/qjae7_1_10.pdf)

The [equation of exchange](http://en.wikipedia.org/wiki/Equation_of_exchange) is an example of a dimensionally deficient equation (apparently money per time = price level \* real expenditures).",Eurynom0s,2013-04-17 18:13:06
Thanks!,jiggajiggawatts,2013-04-24 01:17:59
Yea I was pretty confused by that. I guess old habits die hard?,GRANITO,2013-04-16 22:05:23
I thought most economics work was done in Stata.,Neurokeen,2013-04-17 05:22:52
first thought is to use some machine learning: can plays be used to classify game victory? Combinations of plays?,zdk,2013-01-08 18:30:41
This would be quite interesting.,ilikebluepens,2013-01-10 07:15:53
"I've been playing with something similar.  In my opinion, however, Andrew Gallant's data is in a much better format (http://blog.burntsushi.net/nfl-live-statistics-with-python).  He built an entire API to pull the data in any format desired.  I did some work over the winter break to pull play by play and game (per player) data in CSV.  Instead of having to parse that description field, almost everything that someone would want to quantify has its own field.  It's quite a well put together API!  

Edit:  I forgot that I also have the same data as an R data set now.",chabonga,2013-01-09 07:07:13
Could you share the R data set?  I don't know python and I'm too lazy to figure out how to extract the data.,flyingbrotus,2013-01-15 06:14:59
I'd be happy to.  Do any of you have a suggestion on a method or location that would be somewhat persistent?,chabonga,2013-01-15 13:56:48
"Not really, I don't use public datasets that often so I'm not sure about a good place to leave it other than in /r/statistics or /r/datasets.",flyingbrotus,2013-01-16 06:18:41
Sounds like a job for some machine learning. I've been telling myself that I'll do some work with retrosheet.org 's data sometime...it keeps not happening.,,2013-01-08 15:07:25
Amazing... The potential applications are incredible. ,sigma_noise,2013-01-08 15:09:07
"The data here isn't as good as what you can buy on Football Outsiders, but it's free and there's more of it, so that's a *huge* bonus. I'm planning on playing with it some myself, but I think it'll be most useful for getting my data management skills in R a little better.",HelloMcFly,2013-01-08 15:56:06
"I know absolutely nothing about American Football, but I know a good deal about stats and love programming in R (especially graphics!) Are there any interesting little ideas floating around?",Eeples,2013-01-09 00:49:42
Growth curves in play type? ,ilikebluepens,2013-01-10 07:14:05
Related Hacker News comments: http://news.ycombinator.com/item?id=5002974,jitty,2013-01-08 12:56:47
Let me know if you want to collaborate on a project. I was just mentioning in Nate Silver's AMA how I have a bunch of stats related to the English Premier League and wanted to collaborate with someone on some kind of project. This is just spectacular.,SlySpyder13,2013-01-08 14:25:03
"I would be up for collaborating but I'll admit my stats knowledge may not be as extensive, I just finished my second business stats course. I know little about football as well. However, I'm currently learning programming so I could be of help cleaning and transforming the data.",jitty,2013-01-08 14:44:32
"So I am a stats/analytics guy - (I've been foaming at the mouth a bit today with Nate Silver's AMA) so that'll be great. My football knowledge is weak though, so maybe with our powers combined and if another one or two people, we could end up with Captain NFL?",SlySpyder13,2013-01-08 14:51:49
"Yeah I'd be up for that, it would be nice to learn more about football and stats as well. Sounds like I may have to check out that AMA too. On my mobile, but ill check back later and exchange info",jitty,2013-01-08 15:00:53
"Hey. Love football and my sql skills are great. Stats skills are good (bs in applied maths) and I  would love to do something with this data and wouldn't mind a partner. Just have to figure out what the interesting information would be... 

Hit me with a PM ",satnightride,2013-01-08 20:29:25
[deleted],,2012-11-08 14:31:56
"That is really the killer feature of R. The language itself has some warts, but it's hard to argue with the quality of the community and the epic number of R packages.",kiwipete,2012-11-08 15:17:48
"Packages like this and ggplot2 have inspired me to really double down and learn R (as opposed to adapting found code to my needs).

Unfortunately for me, R has been tricky to understand coming from mostly programming in python. Holy hell can example code be hard to read. Why on earth are R coders so obsessed with 1 or 2 character variable/object names, and magic numbers? &lt;/rant&gt;",adamc83,2012-11-09 00:18:26
"I'm kind of partial to R. I spend a lot of time in PHP and javascript, and in comparison, R just sort of works.",firstcity_thirdcoast,2012-11-08 17:27:38
"As a side note, check out [this comparison of the numerical performance of javascript (V8) versus R and Python](http://julialang.org/). Suffice it to say that I was more than a little surprised. I guess Google's been killing it with V8 optimizations. The only slow number in that list can likely be attributed to not using highly optimized fortran BLAS/LAPACK libraries.",kiwipete,2012-11-08 17:42:40
"Holy shit. I'm not surprised that R is orders of magnitude faster than Matlab, but the difference in performance between javascript and R is just astounding.

Still, I'll posit that the time spent coding and troubleshooting in javascript makes up for a large portion of the difference, considering R is much 'simpler' in its syntax and (in my opinion) easier to use.",firstcity_thirdcoast,2012-11-08 17:59:46
"My disposition toward Julia is mostly ""screw you, the world doesn't need another purpose-specific science / math language."" HOWEVER, I did a small toy task the other day just to prove to myself that it sucked. Yeah, it was pretty intuitive and extremely fast. I was disenheartened to say the least :-)

EDIT (my point): Julia is definitely one to keep an eye on.",kiwipete,2012-11-08 18:43:29
"That's all anyone needs is for their professional skillset to become outmoded. Still, if it's that beneficial, Julia seems like it's worthwhile.",firstcity_thirdcoast,2012-11-08 20:50:23
Seems like this would have awesome applications in teaching stats. Could build sites that let students play around with some data/parameters to really get a feel visually for the in class concepts.,Case_Control,2012-11-08 16:46:21
"This looks very cool! Has anyone successfully tried it? I'm attempting to download and install it using the commands they provide on the main Shiny page, but I'm getting a warning saying it's not available. I just installed a completely separate package using the same functions, so I'm really stumped. ",twisterase,2012-11-08 15:50:42
Can't tell if the makers are fans of Pokemon games or Firefly. But looking forward to playing around with it!,Copse_Of_Trees,2012-11-08 15:43:39
Anybody know if this is compatible with googleVis package for making interactive charts?,zdk,2012-11-08 17:14:19
I just started playing with r today. I'm amazed how easy it was to integration with ssms. This is definitely going to be my go to prototyping environment. ,satnightride,2012-11-08 20:35:33
I don't get it... ,Adamworks,2015-02-27 06:16:35
"I'm curious as to what this actually means. I'm guessing my issue is probably simplified notation used in a lower level stats class that's confusing me. 

My guess for R is... correlation coefficient?

I have no idea what I is.

np is usually the number of observations times the probability of success, I feel setting that as &gt;10 could be dangerous for small n or p.

nq I'm guessing is n(1-p) (usually q = 1-p) and my same concerns apply.

Can anyone shed some light on this, am I way off base?",radiantthought,2015-02-27 06:48:23
"Yes, it is simplified notation so I could fit it all in one stamp.

**R** stands for Random

**I** stands for Independent

**&amp;lt;10%** means the sample is less than 10% of the population

**np≥10** means that the sample size multiplied by the rate of success is greater than or equal to 10

**nq≥10** means the sample size multiplied by the rate of failure is greater than or equal to 10

It's very tedious to write these conditions over and over for every problem, so this made things a little easier, in a comical way.

Edit: or equal to",Obliterative_hippo,2015-02-27 07:01:38
"I'm a senior Statistics major, and it is extremly frustrating having to type assignments in Microsoft Word. I applaud your ingenuity.",YouShallSmokeGrass,2015-02-27 08:33:27
"Microsoft Word? Do they make you use it?

One trick is to type the equations you need into wolfram alpha and have them converted.

If you can, learning to use TeX/LaTeX for maths would be useful if you plan to continue on in education. ",ColorsMayInTimeFade,2015-02-27 09:39:07
"You can have word auto replace certain things to make it insert equations, just gotta set it up first. ",nsgiad,2015-02-27 13:16:22
"If you know how to use R*, you should learn R markdown and use the built in latex interpreter then just knit to a word document. You can then have your code inline with your text, and have built-in latex rendering.

[Here's a super simple tutorial](http://rmarkdown.rstudio.com/) that tells you the basics if you're using R-Studio (if you're using R but not R-studio, you REALLY need to give it a shot).

I've been using R for awhile now, but only just found out about markdown recently, it's honestly a game-changer for me.

*If you're a stats major and don't know how to use R, you really, really should give it a shot.",radiantthought,2015-02-27 13:06:13
"RMarkdown is fantastic, but for those who still want to use word but want a better equation editor, try [LaTeXiT](http://www.chachatelier.fr/latexit/).

It allows you to use just the math mode of latex, and generate images from them.",guesswho135,2015-02-28 17:02:03
[Equation editor shorthand](http://www.iun.edu/~mathiho/useful/Equation%20Editor%20Shortcut%20Commands.pdf),Icamehereto__,2015-02-27 11:48:22
[Even better](http://www.latex-project.org/),jonthawk,2015-02-28 17:57:50
"Interesting, thank you.",radiantthought,2015-02-27 07:14:30
"What does 'random' mean in this context?

Presumably this is just applying the CLT to a sequence of iid bernoilli p variables... so what does random mean... here",anonemouse2010,2015-02-27 13:02:36
"I have a rubber stamp that says ""CORRELATION DOES NOT IMPLY CAUSATION"", I get a ton of milage out of it grading indroductory statistics homework.",vuxra,2015-02-27 06:08:44
Probably get tennis elbow using that. ,nsgiad,2015-02-27 13:14:51
"I would expect a person who teaches statistics to go a bit further than just correlation doesn't imply causation, since that is a pretty popular bandwagon nowadays.",somkoala,2015-02-27 23:51:51
"I go over it in detail in my lecture and make a big deal about why it isn't true. When I'm grading I have over 100 papers to grade so I just stamp the mistake and move on. 

Also the problems in the book are fairly obvious examples, one of them was something like  ""The correlation coefficient between the percentage of smartphone use in a country and the average life expectancy of its citizens is .9. Does this mean smartphones are good for your health? Why or Why not?",vuxra,2015-02-28 08:44:15
"Fair enough, I just sometimes feel all kinds of people use the correlation =/= causation thing to discredit anything based on statistics, since that is all they've learned.",somkoala,2015-02-28 11:56:03
"Goddamn, I need this for my stat methods class.",ToughSpaghetti,2015-02-26 19:55:50
I don't understand. Is that small string hard to memorise?,Te3k,2015-02-26 23:14:38
"If a class has you performing the same test on every problem, certain things get tedious to write.",lysker,2015-02-27 00:45:29
Normal approximation to the binomial! CLT!,trailblazery,2015-02-27 15:51:13
"I'd been looking for software to replace SPSS for a while and found [GNU PSPP](https://www.gnu.org/software/pspp/), a [free software](https://www.gnu.org/philosophy/free-sw.html) project with just such an aim. While looking at some of the information about it, I noticed they recommended this online book for anyone not familiar with statistics (but who might be interested in the project, for example). So not only am I excited about having our data team implement PSPP instead of SPSS but this book project looks like a really great resource for directing some of our sales people to.",ElDiablo666,2015-01-05 03:08:17
Why not using R?,Rylick,2015-01-05 07:18:17
R with point and click GUIs are somehow a secret to people who buy SPSS licenses. But hard to have an advertising budget with no income!,cuginhamer,2015-01-05 09:36:17
I'm not sure what advertising budgets have to do with it but I know that the data team does use R as well as SPSS. R is insufficient for all of the stuff they do. That's why I mentioned being excited about finding PSPP: I am hoping to move us over to all free software.,ElDiablo666,2015-01-05 12:35:32
"&gt; R is insufficient for all of the stuff they do

Do you know which SPSS analyses that your team uses are missing in R?",DavidJayHarris,2015-01-05 15:14:49
"I do not. I just know they use R for some stuff and SPSS for others. My colleague who just left was in the process of learning more R, so it's possible there was some investigation.

Are you aware of any comparisons out there? I'd love to help shed those licenses any way I can!",ElDiablo666,2015-01-05 15:21:34
"Until you figure out what kind of specialized supposedly-unavailable-in-R analyses they need, it will be hard to narrow in on the sources that will provide the proper comparison. First line comparisons usually look like this: http://blog.revolutionanalytics.com/2014/06/an-infographic-comparing-r-sas-and-spss.html or get summarized like this http://stackoverflow.com/questions/3787231/r-and-spss-difference . ",cuginhamer,2015-01-05 18:04:30
"&gt; R is insufficient for all of the stuff they do.

Bet you a case of beer that's wrong. Ask the name of the analyses they do with SPSS, then post in the R forums asking how to do it and somebody will help. Warm hearted bunch, the R community.  ",cuginhamer,2015-01-05 18:01:46
Can I also get in on this bet? I like beer and R too.,GoldFisherman,2015-01-05 20:05:15
"I know you're just trying to be helpful, and I appreciate it, but you're having an argument and making a bet with people you're not talking to. :)

I'll see if I can get someone to investigate using R in place of SPSS but like I said, the boss already stated that R is insufficient after the last few audits. Remember, they currently already do use R for a bunch of stuff; I find it hard to believe that they would branch their workflow unnecessarily. Lots and lots of money at stake here, so we do take these things quite seriously.

By the way, does R have any problems opening sav or sps files? I need to look that up because I've got the sales folks and my programming team using PSPP for that.",ElDiablo666,2015-01-07 15:09:53
"People are surprised how much R can do. 

As for opening .sav yes that can be done...
http://www.r-bloggers.com/how-to-open-an-spss-file-into-r/",cuginhamer,2015-01-07 16:33:13
"Are you referring to R-cmdr or other similar products? They're fine, but far more limited than SPSS unless you're also using R-code which defeats the purpose of a GUI. Unless there's a new and improved R GUI out there that I'm unaware of.",Gastronomicus,2015-01-05 16:18:28
"Yeah, just those, nothing new and comprehensive yet, unfortunately. But most SPSS users are not using very fancy analyses, and those serious statisticians/analysts that use higher level functions are generally capable enough to use the code directly. I'm sure there are exceptions, but I think they are few and far between, and R would eat into SPSS's market share drastically if there were money to advertise that argument to the people who currently buy SPSS licenses. ",cuginhamer,2015-01-05 17:58:03
"To some extent, but R is hardly a secret. Don't forget R is adapted from the commercial software S+. I guess also it depends on a lot on the circles one operates in. Some of the most well known statisticians in ecology use and contribute to R, and I've yet to meet an ecologist that does any serious stats work or modelling that doesn't also use R in some capacity. Psychology and social sciences seem stuck on SPSS I think, perhaps out of tradition. I admit to mostly using SPSS on the basis of being a bit lazy about learning R and enjoying the benefits of a GUI, though I've dabbled a bit in R. Plus I find SPSS syntax much more intuitive than R code, but I don't have a programming background so that's probably why.
",Gastronomicus,2015-01-05 18:26:37
The team does use R.,ElDiablo666,2015-01-05 12:33:48
See also [OpenIntro Statistics.](https://www.openintro.org/stat/textbook.php),Astrapto,2015-01-10 15:06:36
"I am on the theoretical side of things, and not even a true statistician, but here goes, roughly in order of difficulty and/or inverse order of interest for the non-mathematically inclined:

* [Hoff's methods course at UW](http://www.stat.washington.edu/hoff/courses/stat421-502/LectureNotes/notes.pdf). This is essentially a sampling theory/experimental design course, covering what you need to understand/guess the analysis in most science papers. By the way, Hoff also has some [math stat notes](http://www.stat.washington.edu/~hoff/courses/581/LectureNotes/).

* [Shalizi's data analysis book](http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf) is light on theory, but it gives you all the right ideas, with some examples in R. It can be read quickly and although it might not teach you the details, it gives you a sense of the bread and butter tools you *should* be familiar with as a statistician today. Once you know the words, can get the [theory](http://data.princeton.edu/wws509/notes/) and details [elsewhere](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf). 

* [An MIT course by Panchenko](http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2003/lecture-notes/). He rigorously develops and analyses all the basic (frequentist) concepts and procedures you typically see in a first class: consistency, sufficiency, asymptotic MLE theory, Neyman-Pearson, Pearson's chi-squared, analysis of OLS, etc. He also says a bit about the Bayesian approach. No measure theory required. 

* [Bayesian theory](https://www.ceremade.dauphine.fr/~xian/coursBC.pdf) by Christian Robert. Very detailed slides rather than notes. You should have no problem following this if you the basic idea in bayesian stats. Some very insightful examples in there. Seriously working through those slides and stopping to think about his (sometimes opinionated) comments will be very beneficial if you have the time. This is based on [his book](http://www.springer.com/mathematics/probability/book/978-0-387-95231-4).

* [Watkin's theory course](http://math.arizona.edu/~jwatkins/notests.pdf). A measure theoretic treatment of the basics. This roughly follows Shervisch's [Theory of Statistics](http://www.amazon.com/Theory-Statistics-Springer-Series/dp/0387945466) book. If you really can't get enough of this type of thing, you can look at [this](http://mason.gmu.edu/~jgentle/books/MathStat.pdf) for a 1000 page book in a similar vein.

* [Nickl's Cambridge course](http://www.statslab.cam.ac.uk/~nickl/Site/__files/stat2013.pdf). The first part is perhaps a lighter version of Dudley's course, going through the asymptotic of M-estimators (a simple generalization of ML estimators), and moves on to some more interesting non-parametric stuff.

* [Bayesian nonparametric theory by Orbanz](http://stat.columbia.edu/~porbanz/reports/porbanz_BNP_draft.pdf).

* [An advanced math. stats. course by Dudley](http://ocw.mit.edu/courses/mathematics/18-466-mathematical-statistics-spring-2003/lecture-notes/). This is more mathematical and specialized than your typical grad math. stats. class. He goes in detail through the mathematical analysis of Neyman-Pearson theory, some Bayes decision theory, and the asymptotic theory of estimators. You probably only want to look at this if you already understand the concepts and really want careful proofs.

",kohatsootsich,2014-09-10 18:41:53
nice. Do you have more on Bayesian Theory?,lustikus,2014-09-10 23:54:52
"Wow, there's a lot to look through. Thanks!",WhoooAREyooou,2014-09-11 12:31:25
"Great list of resources, thank you!",firebase,2014-09-20 16:39:39
"I have a complete econometrics course on YouTube, with the videos listed here: http://www.burkeyacademy.com/home/statistics-econometrics

It is definitely an undergraduate-level course, but if you haven't had a good course in econometrics/linear modeling, give it a try!  While it isn't a slick as some of the MOOCs out there, I have lots of handouts, datasets, and other things for download, and don't mind answering questions from time to time, either.",BurkeyAcademy,2014-09-10 18:26:51
"Here is also an online book on quantitative economics.

http://quant-econ.net/",,2014-09-11 05:50:00
"Any specific area you'd like to branch into? Others have posted some great resources. I'll add a couple more.

* If you're looking to get into *time series* stuff [this financial econometrics MOOC](https://www.coursera.org/course/compfinance) should be great. You can also go through all the lab exercises [here](https://www.datacamp.com/courses/introduction-to-computational-finance-and-financial-econometrics)

* if you wanna do *machine learning*, then [this specialization](https://www.coursera.org/specialization/jhudatascience/1) (a collection of 9 classes) would be a great start..or you can just pick and choose any of the 9 classes and dive into that. For example the one on [regressions](https://www.coursera.org/course/regmods) or [practical machine learning](https://www.coursera.org/course/predmachlearn)

Fair warning though, all these MOOCs assume a knowledge of [R](https://en.wikipedia.org/wiki/R_\(programming_language\)), for better or worse!

",tom-waits,2014-09-11 05:34:28
I am not entirely sure what I want to branch into to be honest. I think ideally it would be analysis that I could apply to my work; so maybe a course on developing predictive models or something of that nature?,WhoooAREyooou,2014-09-11 12:44:10
"If you're interested in psychometrics (the statistical properties of tests and measurements), [Joel Schneider's blog](http://assessingpsyche.wordpress.com/) is an excellent resource.",nezumipi,2014-09-11 09:46:23
Thanks I'll take a look.,WhoooAREyooou,2014-09-11 12:44:32
good thread.  commenting to save.,drsxr,2014-09-11 06:43:40
"Check out the [Swirl Package.](http://swirlstats.com/)  They just came out with a new version, and I've been telling friends to use it.  It has a lot of promise, but right now it's just introductory programming, basic stats, and data management.  It also links to videos for pertinent information.  Great concept.  You can also find free pdfs of ""The R Book"" online from different universities. ",Tankbean,2014-02-03 08:27:59
"I would highly recommend Swirl.  While still in development it's a great way to self teach R.  I've been trialing it with my students for about 6 months now. All passed their finals so far!
",southpaw1983,2014-02-03 09:18:51
Seconded!,Iamnotanorange,2014-02-03 13:20:23
This looks great. Thanks!,thenecrophagist,2014-02-03 21:01:31
"The two Coursera courses by Johns Hopkins seem to have enough content to get your feet wet in R. [Computing for Data Analysis](https://www.coursera.org/course/compdata) started a month ago (not sure if you can enrol now), then there's [Data Analysis](https://www.coursera.org/course/dataanalysis) starts in October. In both cases I think you can register and view the materials from the previous runs.",panda_burgers,2014-02-03 11:10:48
"Don't think you can see the material by registering, but the videos are on youtube and code snippets are on github afaik.",exxplicit,2014-02-03 23:15:14
"Well, it's not really 'Intro', but Stanford has a Intro to Statistical Learning that just started a couple of weeks ago. Plenty of time to catch up.
https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/info",quieromas,2014-02-03 08:23:14
"If you're a self-starter, I suggest working your way through IPSUR.

http://cran.r-project.org/web/packages/IPSUR/vignettes/IPSUR.pdf",xcallstar,2014-02-03 12:28:01
Lynda.com has one I believe ,InvolvingSalmon,2014-02-03 11:59:06
[http://data.princeton.edu/R/](http://data.princeton.edu/R/),SupaFurry,2014-02-03 15:28:36
Coursera also has [Data Analysis and Statistical Inference](https://www.coursera.org/course/statistics) coming up. ,DebtOn,2014-02-03 17:57:08
There is one which starts in about 2 weeks time in coursera from Duke University. https://www.coursera.org/course/statistics,Kiddie_Brave,2014-02-04 02:58:26
"This is the one I ended up doing and it has been really good.

Its full name is ""Data Analysis and Statistical Inference"" led by Dr. Mine Çetinkaya-Rundel of Duke University.

Definitely an intense class but it provides a great statistics overview, R labs that go into how to apply the material, and challenging material.",xxshteviexx,2014-03-27 13:44:43
"edX has an incoming course:
https://www.edx.org/course/kix/kix-kiexplorx-explore-statistics-r-1524#.UzSNE4V4Uwc",jairgs,2014-03-27 13:42:43
"Why constrain yourself to R?  R has some powerful advanced packages, but the language itself is rubbish.  If you are trying to learn statistics, why not learn python?",homercles337,2014-02-03 11:26:20
I'd actually like to do both. But the data scientists I work with all use R and if I start with that I will be surrounded by people I can ask questions of and get help from. I would like to follow that up with Python though!,xxshteviexx,2014-02-03 11:36:19
"If you want to start on Python, use http://www.codecademy.com/ I am an avid R user but found learning python through codecademy is very easy. ",themasont,2014-02-03 12:17:31
"I took Udacity's Intro to Computer Science a while ago, and I just finished Coursera's Computing for Data Analysis. I understand a lot of people use Python for this kind of work, but the Udacity course didn't really prepare me for data analysis and while I'd prefer to use Python because the syntax doesn't drive me nearly as crazy as R, I don't feel as comfortable doing data analysis with Python now. It doesn't look like Code Academy really goes beyond what Udacity offers. I'd really like to find a course geared toward data with Python but haven't found anything that compelling yet.",DebtOn,2014-02-03 17:55:09
"I'm interested on Python resources, could you suggest something good that deals with learning statistics using Python?",rogerology,2014-02-03 14:00:42
"There are tons of tutorials for numpy, scipy, and scikit-learn.  You can also look at pandas, but i dont recommend it.  Sorry if i cant be more helpful.",homercles337,2014-02-03 14:44:16
"That was helpful, thanks.",rogerology,2014-02-03 14:49:50
Never worry about Type II error again! ,misanthrope237,2014-01-29 09:32:12
I don't even... Is there anything we can do about this?,TheDrownedKraken,2014-01-29 09:29:15
Where's Andrew Gelman when you need him?,econometrician,2014-01-29 11:34:16
"They should advertise ""500+ significant correlations per minute!""

(joke stolen from a comment here: http://andrewgelman.com/2014/01/27/disappointed-results-boost-scientific-paper/)",ben3141,2014-01-29 10:30:31
"Non-statistician here.  Can someone explain why this is being ridiculed?

I guess I'm envisioning a scenario where I have a large number of variables - gene expression data, for example.  Would it not be reasonable to begin some exploratory data analysis by looking into all correlations between the gene expression and a clinical outcome of interest?  I'm aware of issues with type I error, but can't this be addressed with multiple testing correction?",glinsky,2014-01-29 10:57:28
"Its just that if you test every possible comparison, you're increasing the probability of a type I error. Related to that, when you test multiple comparisons, you have to account for the increase in probability of a significant result, for example with a Bonferoni correction where your new alpha level is a/n. When you test 10000 correlations, alpha becomes 0.05/10000, or 5x10^-6, which is very small and could keep an actual significant result from showing as significant.

Edit: a mistake.",icantfindadangsn,2014-01-29 11:12:01
"Somehow, I suspect they are not Bonferoni correcting.",ajmarks,2014-01-29 11:27:16
Lol. Probably not. And then people wonder why they always have around 5% of their tests turn out significant...,icantfindadangsn,2014-01-29 13:58:52
"At Butler Scientifics, we guarantee 5% significant results 100% of the time",brainbombs,2014-01-29 17:55:06
"5% of the time, it works, every time.",mandelbrony,2014-01-29 22:55:22
"Thanks, that makes sense.

I can see it's obvious that this would not be a suitable tool for publishing a result, but would it make sense to use this technique for initial exploration?  Suppose I had no idea which variables were relevant to my outcome.  Could I start by considering the list of the top ranked correlations?",glinsky,2014-01-29 11:47:01
"Speaking from the standpoint of science, in some circumstances, this kind of exploratory analysis without an a priori hypothesis is not good science. Certainly, there might be some kind of data set that would benefit from lots of comparisons.",icantfindadangsn,2014-01-29 12:38:57
Functional genomics and high throughput sequencing studies are all about exploratory data analysis. The question becomes more about the level of evidence demanded and follow up experiments.,canteloupy,2014-01-30 00:44:04
"You mentioned gene chips in your previous post.

The common method of statistical hunting with gene expression chips is using FDR correction. It is a different question than an alpha level, you set the level of false discoveries you are willing to have in your results rather than the overall false discovery rate. Then you can take the results from that and do a second separate experiment to test just those specific genes.

The best thing you can do there, and one of the reasons these automated systems are dangerous, is reduce the number of potential predictors in a reasonable way. For example, a previous experiment to remove bad probesets, or restrict your search to just genes from a specific pathway.

Whatever you do, plan it out BEFORE you look at the data. If you try analyzing the data and adjusting parameters at the same time you are on dangerous ground: you are violating the assumptions your significance statistics rely on.

Most of these methods can be dangerous and finicky. They are best done after carefully tailoring them to your specific dataset and needs. Having an automated system is just asking for trouble because it will blindly apply the same methods to data that breaks the assumptions. Even little things: Does it take batch into account? Day of experiment? Who was running the experiment? Which lot of RNA where you using? Any of these things could be a confounding variable which a statistician should look for and ask about. Computers just blindly analyze.",Bishops_Guest,2014-01-29 15:56:32
"Data mining as an approach for hypothesis generation isn't entirely unjustifiable. That said, to claim that your results are ""statistically significant"" (that is to say, publishable), you have to collect a new batch of data with proper error control in order to confirm/disconfirm what you found during data mining. There's always the risk of finding out that your previous results were simply accidental, but to not control for error properly and claim ""statistical significance"" is tantamount to fraud.",rottenborough,2014-01-29 17:50:43
"I think you mean ""type I error"" in the first sentence. ",dY_dX,2014-01-30 08:52:44
Yep. No idea why I put type II.,icantfindadangsn,2014-01-30 09:00:22
"in plain English, if you perform 100000 correlations by *pure chance* you end up with many correlations that seem good but are there just randomly.

You can not perform an experiment like this. Formulating hypothesis out of data correlations is a sick un-scientific practice. It is not only cheating, it is properly *lying* (in most cases).

First one formulates an hypothesis based on prior research/work/knowledge + intuition, and then he goes and checks what the data say. Reversing the process is ultimately only an exercise in ""rationalisation"" of a result i.e. you find a number you like and try to justify in English why it is reasonable. 

Unfortunately numbers are just numbers and wording can be very misleading, to such a point that for an *external reader* it is hard to figure out that the conclusions may make no sense at all.

That's why this is an awful practice. There's not only no honour in such ""findings"", but only ""dishonour"" in calling oneself a ""scientist"".",mailor,2014-01-29 11:36:40
"&gt; Formulating hypothesis out of data correlations is a sick un-scientific practice.

Hypotheses of an experiment should be formulated *before* you get the data, and not *from* the data, else that defeats the purpose of hypotheses in the first place. Of course, different people suggest different methodologies of science, but if we are to abide by a process of falsification, the formulation of a hypothesis *from* the data (which would constitute a form of inductive reasoning, as opposed to the falsification mechanism in science which is held to be deductive) would only be acceptable if we are to take it at a starting point to further testing.

Here, we're just throwing stuff at a wall and seeing what sticks. That's not very good, *unless* its goal is to identify worthwhile trends of inquiry which we then formulate as a hypothesis and more specifically test. If that's what it is used for, then I'm all for it - the problem is, we know it will be abused, don't we?",Naejard,2014-01-29 14:05:23
"&gt; Hypotheses of an experiment should be formulated before you get the data, and not from the data.

I disagree. It is perfectly fine to generate hypotheses from data, but then you need to find independent data to test that hypotheses. 

""I noticed this funny thing in my experiment, so I designed a new experiment to test it."" is how science should work.",Bishops_Guest,2014-01-29 16:01:59
"&gt; It is perfectly fine to generate hypotheses from data, but then you need to find independent data to test that hypotheses.

Yes, though if you read the rest of what I wrote, I clarify what I meant by that, i.e. that the hypothesis of experiment X should not be formulated from the data of experiment X, but that the hypothesis for experiment Y can be formulated from the data of experiment X.

&gt;""I noticed this funny thing in my experiment, so I designed a new experiment to test it."" is how science should work.

According to certain people, say Popper, that's inconsequential. As long as you have hypotheses, science doesn't *really* care how you got them. Then again, other philosophers of science would disagree about whether the way in which hypotheses are arrived at is a definite part of science rather than just a secondary concern of scientists.",Naejard,2014-01-29 16:15:11
"I specially agree with your last paragraph, Naejard! ",raygbutler,2014-01-29 16:17:55
"But many published papers are to be taken as hints for future testing. You can very well publish correlations in datamining as long as you don't make huge claims that you proved something.

One problem of course is that journal editors want you to make huge claims because it's sexier.",canteloupy,2014-01-30 00:47:15
"&gt; You can very well publish correlations in datamining as long as you don't make huge claims that you proved something.

People, including scientists, don't know how to read. And if you want people to read your paper, you better make some massive claim.",Naejard,2014-01-30 05:53:24
"Yep which is pretty stupid.

I'm studying something genome-wide and quite comprehensively that people have studied already in smaller and different ways.

I'm finding what was reported before but doing so with more data and more regions studied. And may correct some misconceptions but no breakthrough finding.

It's not really publishable to a high level, it was expensive so profs. want to add on more and more to hope to get a better journal. ""Resource papers"" are looked down on.

It sucks.",canteloupy,2014-01-30 05:55:32
One of the biggest deficiencies in science as an institution is the men that make it alive.,Naejard,2014-01-30 05:58:50
"Relevant XKCD.


http://xkcd.com/882/",,2014-01-29 19:34:12
"[Image](http://imgs.xkcd.com/comics/significant.png)

**Title:** Significant

**Title-text:** 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=882#Explanation)

**Stats:** This comic has been referenced 32 time(s), representing 0.299% of referenced xkcds.

---
^[Questions/Problems](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Website](http://xkcdref.info/statistics/)",xkcd_transcriber,2014-01-29 19:34:29
"My name is Ray G. Butler and I am the one responsible for the development of this software (AutoDiscovery). If I may, I would appreciate you giving me opportunity to explain my views.

It goes without saying that we at Butler Scientifics and I personally are really glad to participate in so rigorous and professional discussions like this.

As Glinsky points out in the first comment, the ultimate purpose of AutoDiscovery is to help understand a little bit better the complex relationships there may be amongst variables in a given experiment. To do that, it does simply use a most common statistical tool (correlation) as a mean to that end, not as end in itself. 

The idea is to have an overall vision of what is ""cooking"" there (outlines, hot spots, etc.).

We are well aware that thousands of spurious results without any kind of pre-requisites and post-processing is worse than any complex data file. 

For which reason:

* AutoDiscovery **MUST NOT** be applied without a rigorous experimental design, a-priori hypothesis and a set of advanced statistic packages in the lab's toolbox,

* AutoDiscovery post-analyzes the ""degree of exclusivity"" of each significant correlation registered (which is a kind of measure closely related to the quantity of information every correlation provides in the whole context),

* AutoDiscovery provides a graphical way of browsing the results to indirectly reject the correlations that are far from being consistent with the initial theories.

This way of “knowledge discovering” (rather than “data mining”) has proven to be useful at different stages of a given experiment, as it would at the beginning by helping the user designing the experiment, or at the end of it by helping the user reaching to more sound and meaning conclusions.

We are also aware that such a “discovery” process is a very complex task itself, which requires a great deal of intellectual effort. No software could replace that human capacity. Needless to say, neither could AutoDiscovery.

We simply try to keep up with the challenge: there must be other ways to help speed up the research process in order to discover more, faster and better.

Along our collaborative development with Dr. Trejo's lab of ""Neurogenesis In Adult Animal"", we did some tests implementing Šidàk and Bonferonni correction of p and, although that undoubtedly provided more precise and statistically significant results, they prevented us to find out the key relationship that perfectly complemented and reinforced the original a-priori hypothesis. 

Of course, that little ""gem"" was further analyzed in depth by increasing the number of samples in a new phase of the experiment (specifically designed for assessing the involved parameters), validating that it was a true cause-effect relation and ultimately improving **the conclusions they reached before starting using AutoDiscovery**.

For those who would be interested in learning what we have to say through AutoDiscovery, I would be glad and willing to demonstrate the capabilities of the software, and do it through a real case.

I would really appreciate an opportunity to talk with you at your convenience. Please find below links to our contact channels:

- [Contact form](http://autodiscovery.butlerscientifics.com/wesupportyou.html)
- [Live chat](http://autodiscovery.butlerscientifics.com/wesupportyou.html)
- [Personalized online demonstrations](http://autodiscovery.butlerscientifics.com/bookademo.html)
- Our e-mail (autodiscovery@butlerscientifics.com)
- Our twitter (@butlersci)
- [My ResearchGate profile](https://www.researchgate.net/profile/Ray_G_Butler)
- [My LinkedIn profile](http://es.linkedin.com/in/raygbutler)

Thank you,

Ray",raygbutler,2014-01-29 15:38:25
"Hi Ray,

It's nice of you (and somewhat brave) to show up and tell your side. While your tool does appear to have some potential for efficiency in the hands of someone with the right statistical expertise to understand the results and know what to do with them, it also seems that it would have a greater appeal to the less skilled statistician. Does it come with educational materials and guidance to help that class of use avoid misinterpretations?

For the more experienced user, what does your program offer that I wouldn't get from running blind rcorr() and pairs() functions in R on my entire data set to get exhaustive lists of correlation coefficients and scatter plots?

Thanks!",fat_genius,2014-01-29 18:58:12
"Hi @fat_genius,
I'm sorry for the late answer!

Avoiding misinterpretations is precisely the pitfall we try to avoid from the beginning. But as you can see, it is not being easy! :) 

Guidance is so important to us that right from the beginning it is an essential part of AutoDiscovery: not only we have a live-chat and perform online customized presentations, but also and if you get the chance to check out our plans grid, you will see that three “consultancy sessions” are included (somehow mandatory) with each license sold. 

The idea is to make sure people use the software the right way and for the right purpose.

Regarding your second question, R is undoubtedly an excellent choice to automate statistical calculations. 

But once again: AutoDiscovery is not an statistical package. 

And this is so because the software does not only automate a ""correlation calculator"" but it uses correlations as a mean to get a kind of ""thumbnail sketch at the crime scene"", that is, a practical view of the data of your experiments to understand better what's happening and act accordingly.

Generating that model in every possible experimental design, supporting different data structures and interpretations, discovering relations apart from ""basic A-&gt;B correlations"" and having it safely running on your own in a matter of minutes once you identify the potential requires a variety of additional features such as an easy-to-use graphical user interface, inline help systems, configuration dialogs, data persistency and security, results filtering and prioritization, specific data visualization to browse the results efficiently, multi-site installation and login, etc.",raygbutler,2014-02-13 00:41:07
So you automate a basic function and call it data discovery process rofl,,2014-01-31 01:09:07
It's like someone made a company based off this xkcd comic: http://xkcd.com/882/,WallyMetropolis,2014-01-29 17:46:33
"[Image](http://imgs.xkcd.com/comics/significant.png)

**Title:** Significant

**Title-text:** 'So, uh, we did the green study again and got no link. It was probably a--' 'RESEARCH CONFLICTED ON GREEN JELLY BEAN/ACNE LINK; MORE STUDY RECOMMENDED!'

[Comic Explanation](http://www.explainxkcd.com/wiki/index.php?title=882#Explanation)

**Stats:** This comic has been referenced 31 time(s), representing 0.290% of referenced xkcds.

---
^[Questions/Problems](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Website](http://xkcdref.info/statistics/)",xkcd_transcriber,2014-01-29 17:47:17
"I feel like this software is being ridiculed by people who don't spend a lot of time in the lab performing experiments, and are forgetting the role of serendipity in a lot of interesting discoveries.

Yes, the marketing for the software is a bit absurd... as if a figure for your next paper is going to pop out when you click ""analyze"".

But, there are always side observations and unusual results popping up in the lab, that don't make it to the paper or to the data tables.  Sometimes, however, following up on these can lead to interesting discoveries (after further experimentation and testing).  

It's entirely reasonable to think that there are small blips of correlation in the data that go unseen to humans and to the statistical tests that were decided up on prior to performing the experiment. I'm having trouble seeing why this software wouldn't be useful for that finding possible new leads that could lead to new hypotheses, as /u/glinsky pointed out.",journalofassociation,2014-01-29 18:31:48
"Yeah and the level of righteousness about not forming hypotheses from data is overwhelming. With the money we are throwing at high throughput experiments these days, it's all about generating testable hypotheses from data.

And low throughput has also been used overwhelmingly to support false conclusions. I am in functional genomics and the number of times a transcription factor's role has been concluded from a study of a handful of targets or the number of times a dynamic phenomenom hasn't been properly assessed because of lack of time resolution are high. So you need both approaches. Ideally you could test a large number of genes or regions with targeted hypotheses but it's not really realistic.",canteloupy,2014-01-30 00:54:09
"Wait, do I understand this correctly? Are they just computing correlations between data that you provide (with some visualizations) and selling that as a product?",ComicFoil,2014-01-29 09:23:09
"I think they're calling it a correlation, but it might actually be an SEM/ graphical model. But still...",Iamnotanorange,2014-01-29 09:28:53
haha oh wow,Humppis,2014-01-29 10:25:43
&gt; Ratio variables dramatically increase the probability of identifying a relevant discovery so make sure to select the adequate version of AutoDiscovery to maximize your success!,Quant_Liz_Lemon,2014-01-29 20:03:28
"&gt; I was seriously thinking of just titling this submission ""Lol""...

But instead *deliberately* made a much longer title that was not one iota more informative that ""Lol""?

Deliberately uninformative titles get my downvote.
",efrique,2014-01-29 18:01:12
"I was about to deride this for being petty, but I think the plot in Excel won me over.",Oxonium,2014-01-24 15:07:58
"I get a little thrill every time I see ggplot2 graphics ""in the wild"". How nerdy is that?",w1nt3rmut3,2014-01-24 21:52:43
"""in the wild"" made my day",fittel,2014-01-25 04:52:58
"The ""Princeton researchers"" in question are two graduate students in material sciences FWIW. I didn't find the paper particularly convincing since it feels like they picked a model that, by its nature, predicts sharp crashes and then validated it by applying it to a *single* site that they knew a priori experienced a sharp drop (MySpace).",NOTWorthless,2014-01-25 10:14:45
"I'm confused; when did an r^2 score of 0.545 denote ""strong correlation""? The plot of search index ~ enrollment likely would have benefited from a log transformation given the volatility of enrollment rates as search index increases. Either way, this is adorable.",wingsntexans,2014-01-24 15:11:42
"|r| &gt; 0.7 is a common rule of thumb, fwiw",1337bruin,2014-01-24 15:25:45
Until the last sentence I was going to say clearly the article is a satire and not meant to be taken seriously. Nice save.,PixelLight,2014-01-25 00:31:18
It is now official. Netcraft has confirmed: Princeton is dying .,giziti,2014-01-25 07:06:53
Not even one comment here on the concept of statistics as an inaccurate tool for predicting the future.,IndustrialstrengthX,2014-01-25 07:51:59
"I understand that it's tongue in cheek, but it comes across as terribly unprofessional from a FB employee posting from FBs website. ",Palmsiepoo,2014-01-24 12:59:49
"I don't think it's that unprofessional. Especially coming from an organization whose CEO is pretty well known for wearing hoodies when everyone else is wearing a suit. 

The Princeton paper was kind of ridiculous. How it was reported (or the fact that it got play at all, given that it's an unpublished paper written by grad students) was kind of ridiculous. If anything Facebook use is more like a parasite than a disease.",hotandtiredanddry,2014-01-24 14:08:06
I think they were trying to do what Tesla did after that one bad review.  I don't think he makes a strong enough case however.  ,FullSharkAlligator,2014-01-24 13:27:01
"What did Tesla try to do? But whatever it is I think it succeeded in doing what he intended, satirising the Princeton paper. /u/hotandtiredanddry put it well.",PixelLight,2014-01-25 00:34:35
"http://www.teslamotors.com/blog/most-peculiar-test-drive

An negative article came out from a guy who test drove a Tesla.  There were on board computers so Tesla was able to look at the data of his vehicle and claimed he either exaggerated or outright fabricated his negative experiences.",FullSharkAlligator,2014-01-25 08:55:06
"What's said about Fisher regarding his use of p-values is not true. He helped introduce them to science for us, but he himself said the same thing as what everyone now tries to complain about: don't rely strictly on them. 

I shall double quote from ""The Lady Tasting Tea"":

""To Fisher, there was never a presumption that the failure to find a significance meant that the tested hypothesis was true. To quote him:

&gt;For the logical fallacy of believing that a hypothesis has been proved to be true, merely because it is not contradicted by the available facts, has no more right to insinuate itself in a statistical than other kinds of scientific reasoning... It would, therefore, add greatly to the clarity with which the tests of significance are regarded if it were generally understood that tests of significance, when used correctly, are capable of rejecting or invalidated hypotheses, in so far as they are contradicted by the data: but that they are never capable of establishing them as certainly true...""


Another quote from this book, which really, truly is the intent of what Fisher gave us is this:

""The closest he came to defining a specific p-value that would be significant in all circumstances occurred in an article printed in the Proceedings of the Society for Psychical Research in 1929 [...] In this article, Fisher condemns some writers for failing to use significance tests properly. He then states:

&gt; [...] It is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained. He should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation.""


People need to stop blaming Fisher and start blaming themselves. Who cares what has proliferated through th years dictated as ""what Fisher gave us""? The rules are laid out in hypothesis tests and the meaning of p-values are written ad nauseum in books. Just because they aren't used that way doesn't make it Fisher's fault.


And seriously, who hasn't been looking at effect sizes in the past 10 years? That's a huge part of what many different organizations and societies want now. They want effect, intervals, and ps. They _want the whole story_.

In sum, Fisher's goals were not p-values. He was all about _excellent_ experimental design. While he was a mathematical genius, the dude had an intuition for experimental design that is relatively unprecedented. If you know your field well enough, and you know enough about experiments, Fisher claims (as would many others), you are then competent enough to create an experiment that will be a success -- you have a sound hypothesis, enough samples, the right method. And thus, a robust result. Stop blaming Fisher.",dearsomething,2013-10-11 15:14:01
"&gt; Notice that there’s zero mention of effect size?

But isn't the t-statistic, which they do state, often just a scaled standardized effect size? (Depending on the context.)

Anyway. The view of the author seems awfully limited. If I were in a bad mood, I'd claim he doesn't know what he is talking about. 

The discussion spurred by the [the Breiman (not Brieman) paper](http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.ss/1009213726) from 2001(!) that he refers to have been settled for quite some time. It seems he has not bothered to read the comments from Cox, Efron, the others, and the rejoiner.",AlpLyr,2013-10-11 15:58:31
"My impression from reading that snippet with t-tests is that they actually performed correlations and are only reporting the _t_ as the test of the null (correlation != 0). It's not necessarily true that those authors report _p_ and move on ""because that's how it is"", rather, they don't know how to report the statistics properly (which could also be the fault of the journal or organization). 

In those cases, though, that might be an impressive _t_ (and ergo, effect size) if we knew the sample size.",dearsomething,2013-10-11 16:06:32
"Agreed.

But correctly presented or not, I fail to see how this is a fault of the concept of p-values at he makes it out to be. That lots of non-statisticians (and perhaps statisticians) use statistical concepts improperly does not mean that the statistical methods are wrong or bad.

I think that few statistics teachers do not make it painfully clear, that statistically significant results does not imply practically significant results.",AlpLyr,2013-10-11 16:18:02
"If you're wondering why hypothesis testing is still extremely valuable in the current day and age, have a read at this: http://stats.stackexchange.com/questions/6966/why-continue-to-teach-and-use-hypothesis-testing",Andican,2013-10-11 14:23:49
Is there anything particular from there you'd like to share with us?,Bromskloss,2013-10-11 15:41:48
"It's not my comment, but this is my perspective. Bayesian analyses rely on priors. A lot of fields don't have solid priors to work from, especially as new types of data or new sub-divisions within fields begin to appear. If you can't apply a (kind of) objective prior -- you're doing Bayesian wrong, too. 

The nice thing about Frequentist is that you go in and say ""Well, if there's an effect based on certain parametric assumptions, it will appear -- I don't have to do anything."" It's a more objective approach when it comes to fields that have no real priors. 


What apparently gets lots in these arguments is everything Efron, Tibshirani (both of them!), Hastie, and that tribe of modern statisticians have given us: ways of combining the two different philosophical underpinnings of stats. But they gave us this stuff.

With regard to, say, the bootstrap: that is absolutely genius. You generate distributions to approach a population and apply ""frequentist"" cutoffs (i.e., p &lt; .05; 95% confidence intervals). But, you're using the data you're given and the properties of it. You're building a distribution (kind of like a prior) and then testing. 

[This basically sums it up](http://www.sciencemag.org/content/340/6137/1177.full). The ""two-sides"" of this argument is a bit silly when we have the tools, now, to literally meet in the middle.



**EDIT:**  It's important that people understand that the tools they are using are, in essence, out of date. And it's best to stop arguing and fighting and being contentious when we literally have a ""better fit"" set of tools smack in the middle of the Bayes vs. Frequentist fight. 

The bootstrap was provided as a tool to us 34 years ago (Efron). But even that work is based on prior resampling work, the jackknife, which dates to Tukey in '58 and Quenouille in '56 -- ~56 years ago. And just 30 years prior to that was what Fisher, Gosset, and Pearson gave us. But, for some history, Fisher and Gosset basically did permutation and other resampling procedures, too. But they did it in order to _give us_ parametric distributions to test against because, it used to be completely infeasible to do that on your own. Not anymore, though. 

What Fisher and Gosset did for us was what we can now do for ourselves. They'd be furious at how luxurious our lives are, but thrilled and joyed to know that, for example, I just ran a battery of resampling tests (6 permutation tests, 2 bootstrap tests) for 2000 iterations (created distributions) on _N_= 300 with ~150 variables, in .73 minutes.",dearsomething,2013-10-11 15:55:22
"I love it when my education is delivered with enthusiasm, thanks!",naught101,2013-10-11 16:14:55
"It's a mix of enthusiasm and frustration. I can't stand to hear how Frequentists are wrong or Bayesianists are wrong. They both are. And, depending on the situation, they're both right. 

I really just don't get why permutation, bootstrap, and other resampling methods are not yet standard for nearly everything. They are such a lovely balance between the two sides where in you can even apply more Bayesian or Frequentist properties to generating these distributions. 

We have the tools... so, let's just use them!",dearsomething,2013-10-11 16:17:23
"I think these things aren't standard because they aren't taught to applied researchers. Most of the PIs (doctors or epidemiologists) I work for understand three things: gaussian regression, logistic regression and cox regression, and of these things, they barely understand logistic or cox. They bristle at the suggestion of things like mixed effects models.

I would love to be able to take more modern approaches, but my PIs don't understand them and don't seem to want to take the time to understand them if they aren't obviously better than the approaches they're familiar with.",hotandtiredanddry,2013-10-11 18:22:56
"&gt;I would love to be able to take more modern approaches, but my PIs don't understand them and don't seem to want to take the time to understand them if they aren't obviously better than the approaches they're familiar with.

You'll be a PI someday. Be the course of change.",dearsomething,2013-10-11 20:05:34
"As someone with an MS and zero interest in a PhD, I doubt I'll ever be a PI in academia. This is totally cool with me. ",hotandtiredanddry,2013-10-13 14:28:26
[deleted],,2013-10-12 09:26:53
"I hope this is the start of a trend. In my program, the only mention of resampling was in a statistical computing class, and there was only one lecture on it.",hotandtiredanddry,2013-10-13 14:30:59
"[is this the same paper?](http://www-stat.stanford.edu/~ckirby/brad/other/2013Perspective.pdf)

",dabreaks,2013-10-12 09:25:25
Yes.,dearsomething,2013-10-12 10:52:55
"Regarding lack of solid priors, I'm thinking that a Bayesian viewpoint makes clear, on the one hand, to what extent our conclusions are shaped by the data we just observed and, on the other, how sensitive they are to prior knowledge. Using frequentist methods, with whatever priors that implicitly involves, is just sweeping the whole thing under the rug. This would be _especially_ dangerous when we don't have satisfactory priors, _i.e._ when the data alone are not unambiguous enough.",Bromskloss,2013-10-11 16:45:24
"&gt;Using frequentist methods, with whatever priors that implicitly involves, is just sweeping the whole thing under the rug.

That's not true. There _is_ a prior in frequentists methods: the central limit theorem vis-a-vis the normal distribution. A frequentist uses the prior of the normal to find out if an effect is, _well enough in the tail(s)_ to suggest a real result. It's objective and pretty robust.

&gt; I'm thinking that a Bayesian viewpoint makes clear, on the one hand, to what extent our conclusions are shaped by the data we just observed and, on the other, how sensitive they are to prior knowledge.

I would disagree because you have to define priors that, at times, may make no sense. It's no clearer. And when it comes to Bayes's -- how does one objectively define appropriate priors? It has to come from somewhere. That somewhere is usually after enough evidence suggests some property that _requires_ a prior. And that, for now, is done with frequentist methods.

And one more important point:

&gt;our conclusions are shaped by the data we just observed and

That can be really dangerous in Bayes without an appropriate prior. It's less so in frequentist because what you observe is being compared against a normal distribution (**EDIT**: I'm using normal in a general sense and this is not true for some distributions, obviously, but the point is the same, frequentists have a prior and it is based on distributions. Some of the most popular (F, t) were painfully (literally, Fisher ended up with a bad shoulder because of this) created to find a best generalized set of parameters for distributions).


This is why I prefer to live in the middle with my resampling methods and let everyone argue around me. We have the tools to not be strict on one side or the other. It makes little sense to argue about it these days. So I literally just sit back and go ""uh-huh"", and ""your points are valid"", nod politely (except for now, obviously), and get back to testing all of my stats via bootstrap, k-folds, permutation, and jackknifes. ",dearsomething,2013-10-11 16:50:33
"&gt; That's not true. There _is_ a prior in frequentists methods: the central limit theorem vis-a-vis the normal distribution. A frequentist uses the prior of the normal to find out if an effect is, _well enough in the tail(s)_ to suggest a real result.

By prior, I mean a probability distribution over possible values for parameters of a model. Do you mean something different?

When talking about implicit priors in a frequentist procedure, I'm thinking that one could recast the frequentist calculation into a Bayesian one by finding a model and a prior over its parameters that, when subjected to a Bayesian analysis, yields the same results as the frequentist one. _That_ prior would then be what was underlying the frequentist calculation all along, even though we never saw it.

&gt; I would disagree because you have to define priors that, at times, may make no sense. It's no clearer.

If no meaningful prior can be determined, I take that as a warning sign that the situation is poorly understood and that any conclusions will be unreliable.

There would still, in my mind, exist a correct prior for the situation, even though we don't know about it yet. That prior, when subjected to Bayes' rule, dogmatic me thinks, would yield _the_ correct answer. Any other method had better closely approximate that answer. Trying to avoid the prior altogether, then, doesn't seem like the way forward.

&gt; And when it comes to Bayes's -- how does one objectively define appropriate priors?

Well, the simplest case is of course that if a parameter can take on any of _n_ values, probability 1/_n_ is ascribed to each of those possibilities. In most other cases, I don't know, which I speculate is due to my own limited insight, rather than due to the prior not existing.

Edit: Spelling.",Bromskloss,2013-10-11 17:22:32
"&gt;In most other cases, I don't know, which I speculate is due to my own limited insight, rather than due to the prior not existing.

Both of you might be interested in the Jeffreys prior.",,2013-10-12 09:58:23
Indeed. I was just not sure in what cases it is unproblematically applicable.,Bromskloss,2013-10-12 14:27:05
"This is a serious question.  I'm the type of guy that gives rats some drugs and runs them through a maze.  Say I have 4 randomly assigned groups of rats, n=15 per group, 1 control, 3 increasing doses of drug and I want to know if the drug improves learning of the maze.  We teach undergrads to run an ANOVA followed by at post-hoc like Tukey.  How can this be done better?  I know the pitfalls of interpretation.  I know p&lt;.001 is not very significant and p=.06 is not almost significant.  But I do want to learn more.  Is there a better way to approach this type of data analysis?",manova,2013-10-11 22:02:54
"&gt;I'm the type of guy that gives rats some drugs and runs them through a maze. Say I have 4 randomly assigned groups of rats, n=15 per group, 1 control, 3 increasing doses of drug and I want to know if the drug improves learning of the maze.

This is what Fisher would really like. He loved randomization (seriously, read ""The Lady Tasting Tea"", as well as ""Unfinished Game"" about the discovery of probability [hint: it was discovered by serious gambling addicts]). 

The design is, without more info, good so far. If you know something about the drug dosage, then you can more accurately design an experiment. But... this is actually a place where Bayesian stats can become quite useful. _If_ you know how these drugs act, you can account for that. 

&gt;We teach undergrads to run an ANOVA followed by at post-hoc like Tukey.

That's a good standard. 

&gt;I know p&lt;.001 is not very significant and p=.06 is not almost significant. But I do want to learn more.

Effect size. Especially in animal models. Effect sizes are quite important. 


If you think your data are non-normal, or would benefit from non-parametric stats, then use permutation (to test the null), and bootstrap (to create confidence intervals). Resampling methods tend to be conservative approaches (see Chernick's 2008 book, chapter 8 (.2 or .3, I can't recall at the moment). 


&gt;I know p&lt;.001 is not very significant and p=.06 is not almost significant. 

This is good. Especially if you're teaching this. When it comes to rat studies, just [don't be one of these people](http://www.nature.com/neuro/journal/v14/n9/full/nn.2886.html). 

The best practice is to fully understand your design before you start the experiment. When you do, that's when you can best decide the approach. ",dearsomething,2013-10-11 22:09:04
"So assuming an experimental drug where we do not know much about how it will interact and that the data fits the assumptions of an ANOVA, this is still the best approach?

You're right about effect size, forgot to mention that.  But it is funny/sad how many people equate p with effect size.  I went through 3 rounds of reviews on a paper a few months ago written by some veterinarians that did this.  Finally I told the editor to stop sending me the paper because I was never going to approve those stats (and a poor repeated measures design).  It was still published. 

I wish I had that Nature-Neuro paper about three weeks ago.  I reviewed a paper that did exactly that and I would have sent them the citation.  I actually had my lab meeting centered around that paper to make sure my students understood why the paper's stats were screwed up. 

&gt;The best practice is to fully understand your design before you start the experiment. When you do, that's when you can best decide the approach. 

You took me right back to my first year graduate research methods course.  And what I want to tell every medical resident I have ever worked with that wanted to collect some data from their clinic and try to get a quick publication out without any plan about how that data would be analyzed before hand. ",manova,2013-10-11 23:05:17
"&gt;So assuming an experimental drug where we do not know much about how it will interact and that the data fits the assumptions of an ANOVA, this is still the best approach?

Yes, but the keyword (which you may not be using statistically) is interact. If you have a suspicion of an interaction, you really need to design for one. 

You sound like you're heading in the right direction. You know your field and you know designs are important. As a personal/professional venture, try to find alternate methods to what you do for your designs. It never hurts to know and understand, for example, parametric vs. non-parametric resampling in frequentist domains, or frequentist vs. bayesian approaches to cut-and-dry ANOVA designs, or alternate methods of post-hoc corrections (as well as designing elegant a priori contrasts). ",dearsomething,2013-10-11 23:19:01
"I did not mean interact in a statistical way.

&gt;as well as designing elegant a priori contrasts

The analysis of my most cited paper was (at least I think) a very cleaver design that utilized an a priori Methods of Orthogonal Contrasts.  A reviewer, though, did not believe that we actually made a priori hypotheses and thought we were just trying to increase power.  I had another a priori planned comparison that I used in a paper from grad school (that I checked out with 2 statisticians at my school) and when I presented that data at a post-doc interview, the PI slammed me and said such analysis would not pass the statistical muster in her lab.

That being said, I know there are always more techniques to learn. ",manova,2013-10-11 23:35:11
Is anyone involved heavily in Business Stat and do they focus on p-values? From the very light use of stat in my business analyst role I've never really focused on p-values. Not because I was focusing on effect size but because I could never find enough data points to get a p-value of less than .05. I wanted to see if anyone else had similar problems.,2bfersher,2013-10-11 12:46:20
Yes! And you're really better off doing Bayesian analysis given that. It's good that your gut tells you that the non significant p value is not the end of analysis.,RA_Fisher,2013-10-11 13:28:22
"But the flip side, especially of business stats or anything involving motivation in which experimentation = $$$, is that you need to define priors. Bayesian analyses can be just as insane, and inane, as frequentist.

It boils down to the misuse and misunderstanding of statistics within domains.",dearsomething,2013-10-11 15:18:53
My results are generally really robust to choice of priors. I already know the distribution ahead of time. ,RA_Fisher,2013-10-11 15:20:18
"For you, maybe. But to say that someone is better off with Bayesian or Frequentists methods is untrue. The type of data, the experimental approach, the design, the scales of the values, etc... all factor into which method should be most appropriate. It might be a Bayesian, it might be a frequentist. It doesn't matter, because, if you pick the the most appropriate approach, you should attain the most appropriate, and often robust, result.",dearsomething,2013-10-11 15:23:33
"&gt; I already know the distribution ahead of time. 

Sounds your prior follows a Dirac distribution. In which case your results will be robust to the outcome of the experiment.
",derwisch,2013-10-11 22:33:43
Maybe I am naive but I don't know of anyone with an advanced degree in statistics that focuses exclusively on statistical significance. I feel like this is something that people in applied fields that are heavily quantitive went through.,zmjones,2013-10-11 13:40:39
"No, but there are millions of scientists who do..",naught101,2013-10-11 16:20:40
This article is ostensibly about statisticians though.,zmjones,2013-10-11 16:30:48
The article doesn't question the use of p-values in science.,derwisch,2013-10-11 22:34:56
So is your user name ironic?,giror,2013-10-11 13:38:37
"Fisher used to be my hero! Now that's Deming, Gosset, anfd Neyman.",RA_Fisher,2013-10-11 15:21:53
"How can Fisher not be lumped in with Gosset? Gosset himself was blown away with Fisher's work, and at times, couldn't understand it. Fisher had generalized Gosset's work. Gosset and Pearson (and several others) kind of didn't get it. ",dearsomething,2013-10-11 15:28:12
"My last job I didn't even think to get into significance testing, they were looking at costs trends, and they didn't much care whether they were accurate, they just needed something to go off of. 

The actual data was of course not going to come in exactly at the cost projections, but that's not at all what they cared about, they just needed an idea of where things were going.",MipSuperK,2013-10-11 14:09:23
"Statisticians lost their business mojo when they told their own people that  research in ""high dimensional data analysis on complex data-sets"" is pointless back in the *60's. When this stuff started getting hot, people found out they were rediscovering methods statisticians already wrote about decades ago, but the American Statistical Association decided that ""it was not worth pursuing"". Now they're in a world of regret. 

*Not sure which decade.",Andican,2013-10-11 14:17:03
"I don't think statisticians are willing to violate basic assumptions. For example, I use Pearson correlations to to make predictive models on non-parametric data.

Check-mate Statisticians. ",chaoticneutral,2013-10-11 15:07:54
"You use correlation to make predictions?? Autocorrelation, maybe?",naught101,2013-10-11 16:23:49
Straight up correlation. It sucks we all know it. But it is easiest to explain to non-savvy executives.,chaoticneutral,2013-10-11 18:27:33
"But.. correlation isn't predictive. It only tells you how strong the linear relationship between two variables, not what that relationship is. Are you sure you're not talking about linear repression?",naught101,2013-10-11 18:44:24
"Nope nope, it isn't regression. my boss believes that correlation and regression look very similar, so might as well do correlation which is more ""Straight Forward"". It is bad. I know.",chaoticneutral,2013-10-11 19:43:34
Oh my god,Ayakalam,2013-10-11 20:50:33
Dear god.  And how has your company stayed afloat?,beaverteeth92,2013-10-13 14:43:48
Well most of our services are not statistical in nature. ,chaoticneutral,2013-10-13 15:28:10
"That is true, but if you're responsible for decision making...",beaverteeth92,2013-10-13 16:25:19
"BTW I have a boss who is EXACTLY like this - not dumb - dumb I can handle. The problem is he is too dumb _to even realize he doesn't know something_. How do you handle this, brother?",Ayakalam,2013-10-23 21:04:59
"I dunno, I guess you pick your battles. Fight the ones that really matter.",chaoticneutral,2013-10-24 11:28:55
"Business is focused on the $-value. So anything that increases the $-value will be given more credibility than any other metric.

P-values are good for what they're good for. If you want to predict things, then who cares about p-values? You want your models tuned for minimum error and you don't necessarily care all that much about inference. Parsimony shmarsimony.",hotandtiredanddry,2013-10-11 13:46:11
The problem we grapple with is too little emphasis on meaning in a world awash in too much information.,mstruck,2012-02-12 20:02:40
"I'm sure many of you who browse r/statistics have heard of most of the applications mentioned in the article, but some were new to me -- particularly the use of Google searches.

Are data on Google search trends freely available the public?",Here4TheCatPics,2012-02-12 08:58:38
Google [Insights for Search](http://www.google.com/insights/search/) also Google [Trends](http://www.google.com/trends/),bharder,2012-02-12 09:32:07
"While search data are useful, they must be interpreted with caution. 

 http://blogs.sas.com/content/iml/2011/08/19/estimating-popularity-based-on-google-searches-why-its-a-bad-idea/",alien8r,2012-02-13 09:42:11
Great article.  I think a person who can add statistics (and data mining) to another discipline really has an edge.,prhodes,2012-02-13 20:41:50
This would sure be nice. I'm waiting for the day when cran packages have better syntax. ,,2011-06-20 08:53:46
Better syntax in what way?,talgalili,2011-06-20 09:55:19
"I'm coming from a Python background. It would be great with less symbols and more text. Also, cran packahes have a huge variety in quality and ease of use. ",,2011-06-20 10:56:18
"I am not sure about the symbols thing (I need examples).  Regarding the packages - it is a known issue, but I don't see how you fix this without blocking many smart contributors (who are just not the best programmers)",talgalili,2011-06-20 14:17:22
"Coming from a java/c++ background, the syntax in R is very familiar to me. I suppose someone could write a frontend for R that makes things more python-like.",,2011-06-23 10:08:44
"Do you mean we need better documentation, especially for what the parameters are? I agree to some degree. However, often times if you can't read the ""symbols"" you really shouldn't be running those tests anyway. It also helps to read the references provided.",rottenborough,2011-06-20 15:26:42
"I'm comparing it with the ease of use of Python in one sense. Clear libraries, unified development teams, and nice syntax. ",,2011-06-20 15:54:09
In my own experience it's actually quite easy to use if you know what the tests are exactly.,rottenborough,2011-06-20 16:09:20
I thought for sure it was going to be ARRRRRRRRR,TraptInaCommentFctry,2011-05-21 16:23:55
"Well, jackknife me bootstrap and blow the man down.",,2011-05-21 19:02:59
"What's a [creeper's](http://www.minecraftwiki.net/wiki/Creeper) favorite stats program?

SSSSSSSSSSSSSSSSssssssssssssssss.......plus",therealprotonk,2011-05-21 21:41:01
"You SASsing me, boy?",slacker22,2011-05-22 01:17:51
"Well, you could technically pirate Revolution R.",rialtund,2011-05-21 16:24:30
"If, by pirate, you mean reprobrate plunderer sailing the high seas, then I would say MATLAB. However, if you are implicitly advocating illicit procurement of stats software, I would say nothing. Nothing at all.",golden-boy,2011-05-28 21:10:19
The one time I've encountered this in the wild was [here](http://www.reddit.com/r/funny/comments/23vd8v/one_of_my_favorite_subtle_jokes_from_community/ch1caxe).  Black fathers who live with their kids spend more time with their kids than white fathers who live with their kids. Black fathers who live apart from their kids spend more time with their kids than white fathers who live apart from their kids. White fathers spend more time on average with their kids than black fathers.,electricfistula,2015-02-16 14:49:28
"I think I understand this, but I think you have a typo in your answer?  ""Black fathers who live apart from their kids spend more time with their kids than white fathers **who live with their kids**.""  Is that supposed to be who live apart from their kids?",wichitagnome,2015-02-16 15:45:10
"Yes, that was a typo. I've corrected it. Thanks for pointing it out. ",electricfistula,2015-02-16 15:48:34
Man. I was just looking back at the comment you linked to. It's a pity that you got down voted for explaining the paradox. ,acqua_panna,2015-03-05 06:04:12
"Average marathon completion times are faster in the winter. Amateurs bring up the average in good weather, while only the most dedicated runners will do a marathon in the winter.",srs_jon_is_srs,2015-02-16 13:45:01
"Very interesting. I shouldn't wonder that this happens with lots of sports and, by association, sport injuries.",happyhorse_g,2015-02-16 17:54:39
"White people on trial for murder are more likely to be convicted than black people. However, when conditioned on the race of the *victim*, the association swaps. White people are typically on trial for white-on-white crime, and those with white victims are more likely to be convicted, whereas black people are typically on trial for black-on-black crime and crimes with black victims are less likely to result in convictions. Black-on-white crime has the highest conviction rates, and I believe white-on-black has the lowest (this last one is from memory). 

This example comes from [this book by Alan Agresti](http://www.amazon.com/gp/product/0471226181/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687702&amp;pf_rd_s=lpo-top-stripe-1&amp;pf_rd_t=201&amp;pf_rd_i=0470463635&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_r=1NAVJ3DCHH94D9EXMC3P). ",NOTWorthless,2015-02-16 14:42:40
"A politically relevant case is the gender wage gap:

http://blogs.wsj.com/numbers/calculating-pay-inequity-919/

http://www.unshieldedcolliders.net/2011/09/gender-wage-gap-and-simpsons-paradox.html",carmichael561,2015-02-16 14:57:53
"Where I work, we've seen our overall website ""Conversion Rate"" (Visitors that buy something) steadily decline in the last year.  However, looking at Desktop traffic and Mobile traffic individually we see the opposite trends, both increasing.  The reason for the overall downward trend is that mobile traffic converts at a much lower rate, and we have seen a significant increase with mobile traffic compared to desktop.

Interesting to me, most people in Marketing are still confused.",zdriddle,2015-02-17 11:49:28
I was very confused when I clicked your links because I also subscribe to /r/thesimpsons. ,metagloria,2015-02-16 13:52:22
"Man loses pants, life.",dizznik,2015-02-16 21:50:43
cool stuff just had a guest lecturer give a special topic on simpsons paradox today,I-want-to-be-better,2015-02-16 18:25:57
"This sounds like a very exaggerated use of the word ""paradox"". ",cooked23,2015-02-16 12:31:38
"""A statement that is seemingly contradictory or opposed to common sense and yet is perhaps true.""

http://i.word.com/idictionary/paradox

It's paradoxical because it is counterintuitive that a trend present is several groups of data could reverse when combining them.",UnadornedBeef,2015-02-16 12:41:21
"&gt; opposed to common sense

This criteria is not met, once an explanation for it is understood",cooked23,2015-02-16 12:48:29
That is how paradoxes work.,WallyMetropolis,2015-02-16 13:57:21
"I didn't think being able to explain something disqualified it as a paradox. I thought it only required that a situation appear to be contradictory on the face of it. 

Are zenos paradoxes no longer paradoxes now that we have calculus?",UnadornedBeef,2015-02-16 12:55:26
"I would add I don't think being able to explain something makes it common sense, let alone not a paradox.",bigmansam45,2015-02-17 10:49:33
"I think Zeno's thing is just a name. In my view, it's been used too long to change. I don't view it as a literal paradox.

If I was hungry 5 minutes ago but I ate food and now I'm not hungry, is that a paradox?",cooked23,2015-02-16 13:00:35
"No, because it's common sense that eating food sates hunger.

Something being a paradox does not require that we cannot explain it. Only that it seems to be contradictory without being given the explanation or working out it yourself.",UnadornedBeef,2015-02-16 13:08:22
I think the Greek thing with the mended ship* is a paradox. Simpson is just 'forgetting' to mention a crucial aspect,jeroenemans,2015-02-17 03:36:41
"Paradox of the mind, rather than application.

I.e. it looks paradoxical that 1+1=-2, but the issue is actually that 1+1×Y =-2.

The lurking confounder as the literature seemed to refer to it.",bigmansam45,2015-02-17 10:47:30
The keyword is actually seemingly,Folmer,2015-02-16 12:57:58
What would be a better example of a paradox? This certainly seems to fit the definition in my opinion.,mrestko,2015-02-16 12:34:57
"/u/cooked23's point is that none of the examples of Simpson's ""paradox"" are paradoxical if more diligent research is done. Essentially, the 'paradox' only occurs when valuable data has been distilled away by oversimplification of the presented statistics. 

**TL;DR** - missing the trees for the forest. ",probablyreasonable,2015-02-16 12:41:51
"Well real paradoxes can't exist, at least not in the physical world. So we know that all paradoxes are really ""apparent paradoxes"" and for this reason the two definitions are essentially equivalent. I think it borders on pedantic to complain about this use of the word.",mrestko,2015-02-16 12:59:58
"Both sides of this thread are being pedantic about the definition. Certainly some of these examples would be 'paradoxical' to the layperson. 

However, given that we're in /r/statistics, I don't think it's particularly pedantic to point out that these are not paradoxical to statisticians or anyone generally familiar with the typical rigor of this field. 

^(*Edit: spelling*)",probablyreasonable,2015-02-16 13:05:22
"&gt; missing the trees for the forest. 

I would be a little harsher and call it a lack of analysis or just plain poor/faulty analysis.",cooked23,2015-02-16 12:49:24
"I think the primary trait of a paradox is that there are two conflicting ""things"" AND there is no understood reasoning for it. There's no way to explain it.

With data that falls into the category of what's described on that wiki page, or even just looking at that picture example on the page with the red and blue scatter points, I think almost all the time this comes up in some applied stats project it can be explained by conditioning on some factor variable. Like those two colors could represent males and females, for example. A stats project is done precisely to explain it, and the process of doing so is just the routine practice of stats.

The baby birthweight thing had an explanation too. What's left as the paradox?

As another example, with the popular quip about the occurnece of rapes and ice cream, I wouldn't call it a paradox that they are highly correlated. It's easily explained away.",cooked23,2015-02-16 12:46:56
Paradox doesn't mean 'unexplained phenomenon.' ,WallyMetropolis,2015-02-16 13:58:35
"Which is why my ""AND"" statement had two parts to it.",cooked23,2015-02-16 15:44:58
"A paradox merely means a proposition whose resolution appears to defy logic or logical intuition. It does not require or even suggest that the proposition is unresolved.  

The vast majority listed here http://en.wikipedia.org/wiki/List_of_paradoxes have known resolutions. ",WallyMetropolis,2015-02-16 15:53:48
"Related remark: I also want to add, this is a big beef I have with using wikipedia for math or stats related articles. I think there is such a huge desire on the authors' part to make things seem so complicated and fancy, as quickly as possible. It's like a freshman math major showing off to the hot girl at a party to sound smart. I always try to avoid wikipedia for math or stats related summaries. 

I think this is another example of taking something mundane and dressing it up as a fancy complicated sounding thing just because. Kind of stupid, really bothers me.",cooked23,2015-02-16 12:52:32
"The SEoP also refers to it as simpsons paradox.

http://plato.stanford.edu/entries/paradox-simpson/",UnadornedBeef,2015-02-16 13:02:13
"Yeah, philosophers and other non-stats people probably would. Gotta make stuff sound fancy.",cooked23,2015-02-16 13:06:23
"Then how about the Journal of the American Statistical Association?

 http://www.jstor.org/stable/2284382?seq=1#page_scan_tab_contents",UnadornedBeef,2015-02-16 13:12:31
I agree that example is better than anything else mentioned. I'd also still rather just describe it by what that particular example is and not give it a fun sounding name.,cooked23,2015-02-16 13:46:46
"This thread has been linked to from elsewhere on reddit.


 - [/r/badphilosophy] [r/statistics on paradoxes](http://np.reddit.com/r/badphilosophy/comments/2w9yrl/rstatistics_on_paradoxes/)


*^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)*

",totes_meta_bot,2015-02-17 20:17:41
"I understand the confusion. But I think the statistical paradox comes from the fact that when we introduce more information, our initial conclusion seems to have been contradicted. However, the fact is, the extra information does not invalidate the truthfulness of the original conclusion. A medicine might be beneficial for both males and females, but detrimental when looking at the population as a whole. So without any context, what the data is saying to us is that, *if* we know the gender, we will give the medicine. But if we don't we will not give the medicine. 

However, intuitively it make sense that more information is better, but if different aggregation will lead to opposite conclusions, do you know which level of aggregation is correct? A subpopulation of male and female with different composition might be short males, tall males, short females or tall females, which also exhibit the Simpson's paradox. Will you then use the medicine based on the data for just males and females? And that's a problem. Statistical analysis is limited to data resources, and in the presence of the Simpson's paradox there are no statistical tests that can indicate to us whether we are on the correct level of aggregation or not.",Noetherville,2015-02-17 09:04:41
"Never mind the downvotes, I'm with you. This is just partial correlation",jeroenemans,2015-02-17 03:34:33
"Thanks, some people just love making stuff sound fancy for no reason.",cooked23,2015-02-17 04:19:02
"This thread has been linked to from elsewhere on reddit.


 - [/r/SubredditDrama] [An argument over the definition of paradox confounds /r/statistics.](http://np.reddit.com/r/SubredditDrama/comments/2w7kwu/an_argument_over_the_definition_of_paradox/)


*^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)*

",totes_meta_bot,2015-02-17 09:20:36
This is excellent. I need to carve out some time and play with this a bit.,kerosion,2015-02-01 10:23:37
"That is freaking cool, man.",kanagawa,2015-02-01 09:45:43
"PyMC looks cool, any idea on how it works in practice? Is convergence relatively quick?",put_it_down,2015-02-01 13:28:43
"I've only used PyMC so I can't really compare it to anything.  I do know that it's under active development, and that an updated version ([PyMC3](https://github.com/pymc-devs/pymc/tree/pymc3)) is essentially done, except for documentation.  That said, I'd refer to this excerpt from the PyMC docs:

&gt; PyMC’s number-crunching is done using a combination of industry-standard libraries (NumPy and the linear algebra libraries on which it depends) and hand-optimized Fortran routines. For models that are composed of variables valued as large arrays, PyMC will spend most of its time in these fast routines. In that case, it will be roughly as fast as packages written entirely in C and faster than WinBUGS. For finer-grained models containing mostly scalar variables, it will spend most of its time in coordinating Python code. In that case, despite our best efforts at optimization, PyMC will be significantly slower than packages written in C and on par with or slower than WinBUGS. However, as fine-grained models are often small and simple, the total time required for sampling is often quite reasonable despite this poorer performance.
",TraptInaCommentFctry,2015-02-02 07:36:35
Very cool model implementation. Would be interested to see it's accuracy on a dataset with 20+ seasons. ,thunderdome,2015-02-01 17:05:42
I'd prefer just a stickied mega thread at the top. Doing a weekly thread won't keep the posts from cropping up during the week. ,Case_Control,2014-04-13 04:48:45
I like this idea better.,Palmsiepoo,2014-04-13 10:13:50
"&gt;so why not have a place where we can bundle them up?

Is there something wrong with /r/AskStatistics?",dearsomething,2014-04-12 21:02:44
"They all get asked here on a regular basis. I have no problem helping where I can, and I feel that others don't mind helping either (correct me if I'm wrong). But we see posts on a regular basis and it's cluttering more diverse sets of questions.",Palmsiepoo,2014-04-12 21:29:03
Can we get a mod's input on this?,Palmsiepoo,2014-04-12 18:08:51
I like this idea.  ,sir_punsworth,2014-04-12 20:43:27
"This kinda reminds me of work by [Diaconis and Freedman](http://projecteuclid.org/euclid.aos/1176349830). That's from 1986, so there is nothing new in the observation that the posterior distribution (as a whole) is not *consistent*, and that priors can have a large effect even after arbitrarily large amounts of data. (As far as I know however, the posterior *mode* is still *consistent*.)

Anyway, the point is that this doesn't really change anything. Not all ""Bayesians"" have the same philosophy and therefore this 'objection' doesn't matter to them.

(But I need to read more of this article...)",SkepticalEmpiricist,2014-03-13 09:54:41
Interesting... Am looking forward to reading more about this paper. Anyone here with expertise willing to weigh in?,,2014-03-12 23:24:01
Seriously.  This is the first strong critique of Bayesian inference I've seen that actually discusses the math behind it.,beaverteeth92,2014-03-13 06:08:47
"In the comment linked below, Dave Higdon suggests that there's nothing deep going on with these brittleness theorems. His money line: ""So the conditions of theorem 2 seem to say that if you are allowed to make the observed data impossible for most values of theta, while allowing some probability for the data for extreme values of theta, you can get extreme results.""

OSS say that you can make an arbitrarily small change to your model and radically alter the answer. Higdon's example suggest that they are allowing small changes that are sort of silly. 

Also, I'm not sure why the OSS critique applies only to Bayes, except that it might make the math easier to have everything be random. 

http://xianblog.wordpress.com/2013/09/11/bayesian-brittleness-again/comment-page-1/#comment-38525",brccli,2014-03-13 20:22:09
"I don't know. Rather than finding any fundamental problem with probabilistic reasoning, it looks like they are demonstrating situations where a small change in your model can cause a big change in the result (_i.e._, in the posterior probability that you calculate). A _fundamental_ problem, to me, would be if picking the correct model (_i.e._ the one which exactly captures the available information) and then performing inference yields the wrong result. I'm not sure what I mean by ""wrong"" here. I mean, what should we compare it to? Experiments?

More fundamentally serious, then, seems to be their reference 15 ([A Counter Example to Theorems of Cox and Fine](http://arxiv.org/abs/1105.5450)), which claims to have found that Cox's theorem is wrong.

I should say that I have not yet read the two articles in full.",Bromskloss,2014-03-13 07:23:59
"In short, wrong here means an arbitrarily different result than than what the prior you started with suggests.  Specifically, if I'm reading it right, the last several paragraphs are saying that if you choose a prior and generate a conclusion, then there is a almost indistinguishable prior (in some metric) that will yield drastically different results.

The impact is that you effectively cannot draw any conclusions from Bayesian inference if you fall under their purview.

I guess I would think of it as an essential singularity of a singular complex function, where you can get any arbitrary number out by feeding in a different converging sequence.",anotherbloyfish,2014-03-13 09:25:01
"That's a good explanation.

Actually, where I mean that the meaning of _wrong_ is unclear (to me) is in the situation I referred to where you start with just a single prior (and no other prior to compare it to) which perfectly describes your prior knowledge, and calculate a posterior from that prior and your data. Then, other than by believing your calculation, how can you say that the result is right or wrong? That was what I was unsure about.",Bromskloss,2014-03-13 09:46:59
"Now do it for Bayes.

I'd love to see a raps battles of history for Fisher and Bayes. ",TDaltonC,2013-10-27 14:00:59
so what are the implications of all the p values being uniformly distributed?,alhear,2013-10-27 18:44:47
https://www.youtube.com/watch?v=ez4DgdurRPg,displacingtime,2013-10-27 18:52:57
thank you good fella  ,alhear,2013-10-27 20:14:35
my whole life has been a lie,alhear,2013-10-27 21:16:57
This was awesome.,econometrician,2013-10-27 09:08:00
[deleted],,2013-10-27 10:30:54
"&gt; Makes it a lot less clear ...

It'll never be very clear. :P

The more I get into the philosophy of stats, and the more curious I get about *some* aspects of frequentism, the more negatively I feel about p-values.  I still can't really see what is 'frequentist' about it (and I'm not the only person a bit mystified by it).

Anyway, there's no point debating the merits of it here.  This video does have some value.",SkepticalEmpiricist,2013-10-27 11:07:49
"OMG. That's my old house. And my old roommate!

wooooooooow Jeff! Jesse!",slashcom,2013-10-27 21:14:44
"""This is intended to be the final round-up release of the 2.15 series, and in fact of the entire 2.x.y series which started 2004-10-04.""

Does this mean there is a major change coming, or not much change coming? Version 3 coming next, or version 2 forever?",WasteCadet88,2013-03-01 14:21:11
"R 3 is coming in a month:

http://blog.revolutionanalytics.com/2013/01/r-version-3-scheduled-for-april.html

&gt;Version 3.0.0 is scheduled for April 1 this year, and reflects not a major departure from the current 2.15 version, but instead recognizes the establishment of major functionality introduced during the 2.x series. 

Not sure why they chose april fools day to make such a major change... 
",rm999,2013-03-01 15:35:48
"That was a projected release date. Currently, it is slated for April 3. See the main R and CRAN sites for details. ",dearsomething,2013-03-01 16:02:04
"Next is v3, but it's not a particularly major change from 2.15.3 - more a recognition that R *has* changed a lot, accompanied by a change in update frequency.
",efrique,2013-03-01 15:29:05
Just started learning R this week. Steep learning curve. ,misanthrope237,2013-03-01 21:45:21
"Hang on, it's worth it",e2c2,2013-03-06 15:26:24
"[Comment from Andrew Gelman](http://andrewgelman.com/2013/01/if-youre-already-using-sophisticated-non-bayesian-methods-such-as-those-of-tibshirani-efron-and-others-that-bayes-is-more-of-an-option-than-a-revolution-but-if-youre-coming-out-of-a-pure-hypo/), mostly agreeing with a few criticisms. (For those that don't know, Andrew Gelman literally wrote the book on Bayesian data analysis)",johnmcdonnell,2013-01-27 15:20:05
[deleted],,2013-01-28 09:37:32
"I think you're confused - you're probably thinking of geman and geman. And that's the gibbs sampling paper, not the ""first mcmc paper"".",,2013-01-28 11:09:15
"&gt; But the Bayesian approach is much less helpful when there is no consensus about what the prior probabilities should be.

False, you can use uninformative priors in cases where there is little or unreliable knowledge of the phenomenon. 

&gt;In actual practice, the method of evaluation most scientists use most of the time is a variant of a technique proposed by the statistician Ronald Fisher in the early 1900s.

Misleading argument, while scientists with little statistical background still use frequentist statistics in their research, the scientific community, specially in fields where precision is essencial such as pharmacology and biostatistics, has been adopting bayesian methods in their analysis in the past few years. Also, I have **NO IDEA** how he leaps from bayesian inference to hypothesis testing.


&gt;The advantage of Fisher’s approach (which is by no means perfect) is that to some degree it sidesteps the problem of estimating priors where no sufficient advance information exists.

Not only does Bayesian hypothesis testing exists, it is far more flexible than the frequentist approach since it allows more than two hypothesis and they don't even need to have an asymmetric relationship between them. Furthermore, Bayesian hypothesis testing does not have the issue of trying to interpret what the hell does **confidence** means in a real world setting. 


&gt; Unfortunately, ~~Silver’s~~ Gary Marcus' and Ernest David's discussion of ~~alternatives to~~ the Bayesian approach is dismissive, incomplete, and misleading.

FTFY
",Don_Ditto,2013-01-27 10:28:37
"&gt;Bayesian hypothesis testing does not have the issue of trying to interpret what the hell does confidence means in a real world setting.

Quote for truth.

The old Fisherian vs Bayesian camp again? Why does it need to be an all or nothing? I never understood why anyone advocates only for one. They both have uses, and abuses.

Gary Marcus is a psychologist, and I don't really see anything in his publications to imply he is an expert on either stats in general, or these two methods, or even non-parametric stats, for that matter.

Ernest Davis does however appear to have the experience, B.Sc. in math, and Ph.D. in CS, which tells me he's firmly planted in the frequentist camp, which is surprising. I'd imagine those in CS would actually understand Bayesian concepts. I guess it's easier to dismiss than to investigate.

I guess all phylogeneticists are completely wrong using markov chain Monte Carlos, according to Davis.",SigmaStigma,2013-01-27 10:48:18
"&gt; The old Fisherian vs Bayesian camp again? Why does it need to be an all or nothing?

As I see it, they build upon different conceptions of _probability_. The Bayesian probability is used to describe a state of knowledge. Wouldn't the Fisherian probability rather be something like a propensity of an experiment to yield a certain outcome?

I can't see them as ""just different tools"" and that one would be just as good as another. Like [David MacKay](http://www.cs.toronto.edu/~mackay/itprnn/ps/459.468.pdf) , ""I have no problem with the idea that there is only _one_ answer to a well-posed problem"" and stick to the Bayesian view. It's not just another tool; it's the law.",Bromskloss,2013-01-27 13:15:04
"Perhaps I'm a bit buzzed, but are you making the point that the frequentist approach is wholly inferior to Bayesian approaches, and the latter is the better solution in all cases? Let's not be so dogmatic. 

&gt; As I see it, they build upon different conceptions of probability. 

Well yes, of course, that's their main distinction. Fisher's is P(D|H), and Bayes' is P(H|D), where D = Data and H = Hypothesis. 

&gt; Wouldn't the Fisherian probability rather be something like a propensity of an experiment to yield a certain outcome?

Well kind of, but I wouldn't word it that way. It's the propensity of the observed data from the experiment (or quasi-experiment, or whatever) to exist if a given hypothesis is true. 

It's not necessarily that ""one would be just as good as another"" because that just isn't true, but each has their place and is more appropriate in some situations. Too often individuals that espouse one as the ""one true method"" have gone too far down the philosophical path. Having said that, Bayes is at the very least under-used, and most probably the more appropriate method for more situations than not; that does not mean it's the ""one true method"" though.

Or maybe I've just got it all wrong. I'm mostly self-taught, so perhaps I'm a fool. I don't think so though, and given that smarter people on both ""sides"" argue each has their place, I think we should abandon the dogma.",HelloMcFly,2013-01-28 18:01:05
"&gt; Perhaps I'm a bit buzzed, but are you making the point that the frequentist approach is wholly inferior to Bayesian approaches, and the latter is the better solution in all cases? Let's not be so dogmatic.

It's not only that one is inferior to the other, but rather that one is wrong and the other is right. :-)

&gt; Well yes, of course, that's their main distinction. Fisher's is P(D|H), and Bayes' is P(H|D), where D = Data and H = Hypothesis.

I'm not sure if we're talking about the same thing now. Both of these would be valid Bayesian probabilities.

&gt; It's the propensity of the observed data from the experiment (or quasi-experiment, or whatever) to exist if a given hypothesis is true.

What you refer to, I would rather see as a property of the experiment, because the data hasn't come out yet. When the data is out, it's fixed, and has no propensity for anything else than being what it is.

&gt; It's not necessarily that ""one would be just as good as another"" because that just isn't true, but each has their place and is more appropriate in some situations. Too often individuals that espouse one as the ""one true method"" have gone too far down the philosophical path.

I'm afraid I don't agree that each has its place. I think there is one true method. As above, I embrace the quote ""I have no problem with the idea that there is only one answer to a well-posed problem"". It's similar, really, to how we reject Aristotelian physics in favour of Newton and deny that it ever has it's place. (It's an imperfect analogy, since it concerns physics and therefore always a matter of approximations.)

I could be wrong, but through reading and thinking I have repeatedly updated my beliefs and have now reached the point where I am confident enough to say out loud that I think the Bayesian concept of _probability_ is the reasonable one.",Bromskloss,2013-01-29 08:04:32
"&gt;&gt; Well yes, of course, that's their main distinction. Fisher's is P(D|H), and Bayes' is P(H|D), where D = Data and H = Hypothesis.

&gt; I'm not sure if we're talking about the same thing now. Both of these would be valid Bayesian probabilities.

I don't think so, unless I'm really missing the mark when painting with broad strokes (certainly not impossible). P(D|H) treats the data as random (i.e., the data may change if you repeat the circumstances) and the hypothesis as fixed (i.e., it's either true or false, you just don't know which). The p values reported in studies are typically the probability of H, the null hypothesis, being true. Bayes is the opposite and views the data as fixed and the hypothesis as random taking a value somewhere between 1 or 0. That's a substantial difference. 

In other words a frequentist says ""I don't know how X works. I can collect data about X, but because data is messy and unreliable I'll use stats to rule out alternative possibilities about X."" A Bayesian says ""I don't know about X, so I'll use stats to infer the probability of different states of X."" 

At any rate, I certainly don't believe I can change your mind, and I don't think I'm the right person to try (perhaps you've read it, but [this book](http://www.amazon.com/Experimental-Knowledge-Science-Conceptual-Foundations/dp/0226511987) brought me back from a one-or-the-other mindset to some degree).  If you find Bayes is the best solution for you in every situation then so be it, but I think you're throwing the baby out with the bath water.",HelloMcFly,2013-01-29 09:28:29
"&gt; I don't think so

Do you mean that you don't think P(D|H) and P(H|D) are both valid probabilities to a Bayesian? I'm sure they are.

&gt; P(D|H) treats the data as random (i.e., the data may change if you repeat the circumstances) and the hypothesis as fixed (i.e., it's either true or false, you just don't know which).

This classification into _random_ and _non-random_ seems a bit off, at least in a Bayesian view, because anything ""random"" there just means that we have incomplete knowledge about it, not that the quantity itself has any inherent ""randomness"".

Before the experiment, both D and H are unknown and we start out with a probability distribution over the pairs (D,H). After the experiment, we restrict ourselves to the now known D and thereby refine our knowledge about H.

In Bayesian probability, thus, P(D), P(H), P(D,H), P(D|H) and P(H|D) are all well-defined.

&gt; The p values reported in studies are typically the probability of H, the null hypothesis, being true.

I don't think that is so, actually. It's mentioned by Wikipedia as a [common misunderstanding](https://en.wikipedia.org/wiki/P-value#Misunderstandings) (number 1 on list). (Though, I don't know what you mean by ""typically"", so I might misinterpret you.)",Bromskloss,2013-01-29 10:19:36
"&gt; Do you mean that you don't think P(D|H) and P(H|D) are both valid probabilities to a Bayesian? I'm sure they are.

I'm not saying Bayes' has no nothing to do with P(D|H) because that would be nonsense, I'm saying the primary focus of Bayes' (i.e., the left-hand side of the equation) is P(H|D), and that everything is about getting to that point which is why it is so desirable!. For frequentist terms the outcome, or left-hand side of the equation, is P(D|H).  

I don't really have anyone to discuss this stuff with, so I think I'm not great at talking about it. Perhaps [Wikipedia's explanation](http://rationalwiki.org/wiki/Bayesian#Probability_of_the_hypothesis_versus_the_data) will be better than mine, or perhaps you'll teach me how I misunderstand.

&gt; This classification into random and non-random seems a bit off, at least in a Bayesian view, because anything ""random"" there just means that we have incomplete knowledge about it, not that the quantity itself has any inherent ""randomness"".

You're right, I don't mean ""random"" like random number generator, but I couldn't think of a better way to put it, and that's how I first came to read about. It gets the point across adequately, I think, even if doing so sub-optimally.

&gt; I don't think that is so, actually. It's mentioned by Wikipedia as a common misunderstanding (number 1 on list). (Though, I don't know what you mean by ""typically"", so I might misinterpret you.)

You caught me in a moment of lazy writing I'm ashamed to admit. It's the probability of obtaining at least as extreme of result (statistic) if the null hypothesis is true, I believe.",HelloMcFly,2013-01-29 12:09:33
But we often use Newton as an approximation now even though we know general relativity...,BullNiro,2013-01-29 18:04:32
"That's why I confessed it is an imperfect analogy. My message was that a compromise is not always a good thing. Sometimes, one really is wrong and the other really is correct.",Bromskloss,2013-01-30 02:16:47
I was just pointing out (that whilst I upvoted you because I feel you are right on Bayes/Fisher) that it was a really bad analogy because we use a system that we know is not correct but is still useful in the same field and the thing you pointed to as correct is actually an incorrect approximation that we use.,BullNiro,2013-01-30 02:36:17
"Your second point: while it is a misleading argument, plenty of top notch scientists and statisticians use frequentist statistics successfully. Whether it is the best way they could do it is another matter.

Your third point: While, omfg, it's sexy flexible, until we get quantum, or a better algo than MCMC, the flexibility is lost in the bloody tedium and difficulty of doing it in many applications.

",,2013-01-27 13:40:16
"""uninformative"" priors are nonsense.  *All* priors are informative.  Being more or less flat on one parameterization does not make you more or less flat on another parameterization.  Putting nearly all of the prior probability ""near infinity"" or some other ridiculous value sometimes does no harm and sometimes leads to ridiculous results and *nearly all users are unaware of the difference* because this is a really tricky theoretical question.",berf,2013-01-27 13:29:10
I am just starting to learn more about Bayesian inference. Could you please refer me to some material on what you mean by this? ,RSeafood,2013-01-27 19:54:43
"Most of the difficulty arises with improper priors.  But you do not help yourself by using a proper prior like uniform (-R, R) where R is 10^10 or something of the sort.  Yes you are not *technically* using an improper prior, but you can expect to have more or less all the same problems.  So on to improper priors.

The first bit of literature starts with ""marginalization paradoxes"" for which see Dawid, Stone and Zidek (*JRSSB*, 1973).  Since improper priors are not really probability distributions, Bayes rule isn't really doing conditional probability, and ignoring this can lead to mathematical nonsense.  This can be avoided by only using finitely additive proper priors (which can mimic some but not all countably additive improper priors) (Sudderth, *JRSSB*, 1980), but although this is a complete solution to the problem, nobody wants to learn finitely additive probability theory, so it has not caught on.  Then there is the problem that improper priors can lead to inadmissible estimators (which proper priors never can); see Eaton (*Annals of Statistics*, 1992) and the fairly large literature that this paper cites or that cites this paper for that can of worms (the math is really difficult, every theorem a PhD thesis).  Then there is the issue that improper priors can lead to ""strongly inconsistent"" estimators in the sense of Eaton and Sudderth (*Bernoulli*, 1999) and that math is hard too.  Finally there is the notion of so-called ""reference priors"" of Berger and Bernardo (I'm not sure what's a good reference for this, Google scholar has lots) which are basically ""frequentist envy"", that is, choosing the prior so the resulting posterior agrees with frequentists and is also mathematically difficult (they are hard to define in multiparameter problems).  Lastly there is the issue that improper priors do not always lead to proper posteriors, and when they don't total nonsense results, and authors sometimes miss this (I did once), and that too is too hard to check for most naive users.  The only ""noninformative"" priors that have any mathematical simplicity are Jeffreys priors, and they are often improper so can run into any of the difficulties discussed above (or may not, and it can be very difficult to prove which).

tl;dr improper priors are a *mess* and *much too difficult* for naive users

Conclusion: Bayesians should always use *informative* proper priors.  Anything else may be total nonsense and you will never know.",berf,2013-01-28 05:54:24
Silver has an entite chapter in The Signal and the Noise explaining why Bayesian is better than Fisher and explaining how so many people (unfortunately) still adhere strongly to Fisher's methods.,,2013-01-27 11:59:48
I've never read a paper in pharmacology that uses Beyesian statistics. Do you have an example? Or know of a particular subfield that primarily uses it?,,2013-01-27 11:47:30
"My understanding is that Bayesian methods are commonly used in pharmacokinetics/pharmacodynamics stuff. I don't directly do any of that and so don't have many references for you, but what I gather from my peers is that is it fairly commonly used in that area. 

A quick Google search of ""pharmacokinetics and bayesian"" brings up http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1885149/ so there is something to it. ",iacobus42,2013-01-27 12:47:44
"This isn't pharmacology proper, but I know CRM (continuous reassessment method) dose-finding studies (Phase I) are Bayesian flavored, and there are some other Bayes-flavored adaptive trial designs (Phase I-II) that allocation incoming participants to treatment arms (differing either by dose or treatment) based on information gathered during the trial.",Neurokeen,2013-01-27 15:47:45
"[Page load issue](http://imgur.com/aKwk3S8), or silly joke by New York Times?",mail124,2013-01-27 10:08:49
The authors argument against Bayesian inference is very weak. He talks about Bayesian prediction and then he switches and starts to talk about hypothesis testing. ,quiteamess,2013-01-27 08:51:50
"Yeah, it's pretty clear that the author doesn't have a very good understanding of Bayesian inference. I'm not going to take an argument seriously if part of it is based on:
&gt;But the Bayesian approach is much less helpful when there is no consensus about what the prior probabilities should be",,2013-01-27 09:34:17
"&gt; Lost until Chapter 8 is the fact that the approach Silver lobbies for is hardly an innovation; instead (as he ultimately acknowledges), it is built around a two-hundred-fifty-year-old theorem that is usually taught in the first weeks of college probability courses.

A philosophy based on some old formula must of course be shitty. Next thing is that somebody comes around with this old dull razor from that occam guy. ",quiteamess,2013-01-27 09:47:56
"God, this physics shit is built on calculus. Do you have any idea how OLD that is?",,2013-01-27 09:54:16
"Why should I believe this so-called theorem from this Pythagoras guy, he lived twenty-five hundred years ago! ",mickey_kneecaps,2013-01-27 17:52:06
"You don't see this bias of appealing to ""being old"" used very often.",leonardicus,2013-01-27 16:32:57
"He fails to take into account that the prior is swamped rapidly by the likelihood. Also, while Bayes' theorem is taught in the first few weeks of probability courses, Bayesian Inference is not. This is not from a person who knows much about Statistics.

On the other hand, Silver's presentation of why he thinks the toad study is wrong is utterly penile.",,2013-01-27 13:36:28
"Frequentism is just better at hiding their priors.

This is a bug, not a feature.",ThrustVectoring,2013-01-27 17:50:03
This reads like a creationist criticism of evolution. ,,2013-01-27 09:56:42
I would love to see this notion expanded on.,Still_Wind,2013-01-27 21:26:55
"&gt;The advantage of Fisher’s approach (which is by no means perfect) is that to some degree it ~~sidesteps~~ **completely ignores** the problem of estimating priors where no sufficient advance information exists.

How is that an advantage? Just because you don't want to talk about your flat priors, doesn't mean they aren't there.",BanachSpaced,2013-01-27 19:29:20
"It means that you're not incorporating them into your inference. Fisher specifically wanted his system of inference to incorporate prior information into how the experiment is designed and/or what type of analysis would be done. 

As an example, if you're trying to measure an ESP phenomenon, you know quite blatantly that a binomial process isn't going to be sufficient, you need to control for the multiple things which may have explained away the lack of ""psychic powers"" in previous experiments, as discussed in the paper below.
http://www.phil.vt.edu/dmayo/conference_2010/Diaconis%20on%20stats%20in%20ESP%20%281%29ed.pdf

Analysis-wise, Fisher wanted it so that no matter what with frequentist inference, the analysis would come out to have the same answer, it might be a little more inefficient in how it got there if you weren't using the most efficient analysis, but it would get there.

Of course, all of this is not to say that I dislike Bayesianism, I actually really like Gelman's approach to things, but I just thought I'd clear up a common misconception about frequentism.

EDITED: For clarity",TobyPolaris,2013-01-28 01:21:39
This is just the usual anti-Bayesian claptrap from a Frequentist. ,FullSharkAlligator,2013-01-27 16:06:53
Is there a separate subreddit for maths and stats related webcomics?,samclifford,2012-12-19 03:41:21
That's the dream. But at the moment it would just be SMBC and XKCD reposts.,bdobba,2012-12-19 06:36:38
"And Abstruse Goose, and [Irregular Webcomic](http://www.irregularwebcomic.net/2339.html). I'm sure somebody else will be along to suggest some more I've missed. ",isarl,2012-12-19 07:58:37
"""for mysterious non-baby reasons""",blueblank,2012-12-19 06:30:26
I took a class with Stu Geman (he's one of the two Gemans on the original Gibbs sampling paper). Such an awesome teacher and dude. ,mezinadour,2012-06-10 20:33:34
"OMG! My professor just told me last week that I should learn MCMC ASAP, along with a bunch of other stuff, so this is perfect! Thanks a bundle.",jebiv,2012-06-10 18:50:51
[Here's another document](http://web.mit.edu/~wingated/www/introductions/intro_to_mcmc_mackay.pdf) that really gives some good motivation for the method.,roger_,2012-06-10 19:14:30
Thanks!,jebiv,2012-06-10 19:25:54
I wish all statistical papers could be as well-written as this. Thanks!,CauchyDistributedRV,2012-06-14 11:43:59
"The linked post has the right message, but the argument is ridiculous.  You shouldn't use simulations to ""prove"" a theoretical point.  Simulations never prove anything because they can be fudged to prove any point by picking the cases carefully.

There are three theoretical points here.

 * if you assume equality of population variances and your assumption is false, you may get the wrong answer.  Isn't this obvious, even to the theoretically illiterate?

 * The one-sample t test is robust in the sense of being asymptotically nonparametric distribution free: it does almost the same as one-sample z test for large n.  The two-sample t test does not have this property.  It does the wrong thing (when variances are not equal) no matter how large the sample sizes are.  It's wrongness does not go to zero as sample sizes go to infinity.

 * Pretests are bogus.  All multistage procedures that produce a result that is the same as doing the last stage by itself (with no preprocessing) are bogus.  Doing a pretest *changes the sampling distribution of the posttest*.  The only way to correctly do the pretest-posttest is to bootstrap the *combined procedure*.  But then the whole mess is more complicated than just doing Welch's procedure.  This last criticism is hard for people to understand because there is lots of multistage bogosity in the refereed literature.  So much that experienced users of statistics cannot bring themselves to think that all of that refereed literature can be wrong.  But it is.",berf,2015-01-28 05:41:06
"Can't you do a pre test that is independent of the actual test statistic? So that the joint distribution of pretest and stat test is just their product? Like f(x,y) = f(x)*f(y) where x and y are the test statistic?Edit. In point 2, this is not really clear. What do you mean by asymptotically nonpar distn free?In point 1: How can this be true? If i have a pre test that determines me choosing between wilcoxon signed rank and ttest and compare the result with ttest alone and you are saying these 2 are the same in power?",boogahwoogah,2015-01-28 12:42:15
"But in practice they won't be independent.

Asymptotic nonparametric distribution-free means the asymptotic distribution does not does not depend on assumptions (at least some assumptions).  Here it does not depend on any properties of the distribution of the data so long as the variance is finite.

In you point three which is, I think, about my point 3 not my point 1, I did not say anything about power.  I said the test is not valid,  it does not have the claimed significance level.  So we do not even get to power.

Just do the rank sum test (we are talking about tests concerning two independent samples here, so Welch's test is appropriate) if you don't believe the normality assumptions that Welch requires.",berf,2015-01-28 14:22:33
"You're still very unclear dude.

Can you give me an example that it is not independent? Do you mean like a test of equality of variance and ttest?

With your argument right now, almost anything has 'nonparametric' asymptotic property then?? Since most asymptotic distribution is somehow not related to the original distribution. Makes sense?

How do you 'believe' that data does not come from normal then or how do you believe that assumptions are not met? By experience ('ohh the data doesn't look normal'), or formally by using some pretest right?",boogahwoogah,2015-01-28 21:22:21
"I have never heard of a pretest that is independent of the posttest.  And yes your example definitely is not.

No.  Your argument about nonparametric asymptotic distribution-free misses the point.

It isn't an issue of what you believe.  The data has the distribution it has whether you believe it or not.  The question is what assumptions are required for the procedure you intend to use to be valid.  And pretesting to attempt to make the two-sample t test that assumes equality of variances work when the variances are not equal is a totally broken idea.  It just does not work.  Whether the data are normal or not has nothing to do with that.

The Welch procedure does happen to work for large sample sizes regardless of whether the data are normal or not because it uses the same test statistic as the two sample z test.",berf,2015-01-31 15:18:44
"Sounds like a good idea; except I have taken a very pessimistic approach on the intelligence of people in introductory classes and I don't trust the average at calculating the degrees of freedom correctly.

The introductory classes I have done are for general requirements at a bachelor's degree. We get a lot of people who aren't motivated to learn anything about statistics and wouldn't care for the world if they had barely passed.

However, for research purposes - should be worth it.",Corruptionss,2015-01-28 04:38:42
"You don't have to trust people to calculate the degrees of freedom correctly.  Just implement a correct computer implementation and give it to them.  In R the syntax to do Welch's two-sample t test is

    t.test(x, y)

and the syntax to do the ""exact"" two-sample t test that assumes equality of variance is

    t.test(x, y, var.equal =  TRUE)

so the bozos will do Welch's test and do it correctly.
",berf,2015-01-28 05:29:48
"Yeah... We aren't going to teach them R either... Have you taught a class of 300 introductory students before? I can guarantee that a quarter of the class will somehow mess up what you just told me. They wouldn't even know how to begin inputting the data.

We aren't talking about a population of mathematics majors or statistics majors.

We do have statistics software that they can click buttons to run the test, and yes, they mess that up too. But at some point in the introductory class they need to be tested on their knowledge of how tests connect to different problems, you'd be surprised how many of 300 people know when to use what.",Corruptionss,2015-01-28 06:04:17
The sheer number of TAs needed to get 50% of students conversant in R would be mind boggling. ,tm1087,2015-01-28 08:09:29
"It all depends on what your goal is.  If your goal is to make 50% of the students R ""knowledgeable users"", then you are right.

If you have a more minimal goal -- they can use R to look up tail areas of distributions and to do tests and confidence intervals -- you are wrong.  About 90% of a class of 75 achieved that and there was just the instructor (me) and 1 TA.  Of course we did have both of us answering questions in the lab sections and there were 3 of them a week (25 students at a time in the lab), so I had to spend 3 extra hours of class time that is normally solely the province of the TA.  But it did work.",berf,2015-01-28 14:29:48
"Perhaps but those people will go out into the real world and a proportion might seem analytical positions and benefit from having learned good practice in the first place.  The rest of the class are doing it to bulk out their credits, and as you suggest will have got point-and-click exercises wrong too.  Suggesting dumbing things down to the lowest common denominator is an approach that always disappoints me. 

I'm not disparaging your efforts overall, just the resignation that trying to get people to engage with good practice isn't worth the effort. ",enilkcals,2015-01-28 10:00:40
"If you just lecture at them, you're right.  If you have a lab in which they actually do examples and if you have tests in the lab too so the have to do it on tests, they will learn it.

This is from actual experience, admittedly with only 75 students not 300 students.

What you don't test, you don't teach.  Computing is no different.",berf,2015-01-28 14:25:17
"We do have actual labs where they use minitab (which as far as I know, is one of the simplest ways at running statistical tests). It's kind of like SPSS where you enter the data into a spreadsheet like table and then you click:

analyze --&gt; tests --&gt; two sample t ---&gt; {box pops up} ---&gt; do not click assume equal variances

We give them step by step instructions how to do this

Out of 300 students, I'd say 20 will click two sample Z. 15 of them will click assume equal variance for some reason. 30-40 will not write down the correct values in the correct places.

To be honest, I hate the school for requiring people to take introductory statistics when people are not even capable of following simple basic instructions.


---

Logic makes so much sense to me. I pay attention to details, doing my undergraduate and even graduate work, things just seem to intuitively make sense. I don't understand why so many people in the university system can't understand common sense. Then we complain that the value of degrees drop when the hordes of them flood the graduation ceremony leaving with degrees.",Corruptionss,2015-01-28 14:35:19
"You missed the part where I said you have to actually give tests in the lab.  What you don't test, the students won't learn.  Have you never heard ""Is this going to be on the test""?",berf,2015-01-31 15:20:31
"I did miss that. We do have other classes that actually specialize in statistical software and they do their tests using statistics software. Not feasible for the size of the introductory classes. One introductory class can fill up every computer lab (8+ labs) we have on campus and still have people who wouldn't be able to take the test. This may be a problem with the university system.

I wish we were able to take off points for people who say, ""If this going to be on the test."" Hear it so many times and it is clear indication that their education is more like an obstacle to them and less of a journey of learning.",Corruptionss,2015-01-31 15:30:00
We also do not do that any more.  I was doing this in the late 1990's before statistics became so popular.  But it does work.,berf,2015-02-01 04:25:16
"&gt; Out of 300 students, I'd say 20 will click two sample Z. 15 of them will click assume equal variance for some reason. 30-40 will not write down the correct values in the correct places.

Sounds about right.  I tutor at a university that uses Minitab to teach undergrads statistics also.",beaverteeth92,2015-01-28 16:21:28
"I hate to have a defeatist approach on intelligence, there is legitimately good students; but the number of them that have trouble doing simple things can be quite overwhelming at times. I realize the reason why I am in statistics now is a lot of interest and motivation that may not be shared amongst all of the students, but even in classes I loathed I managed to get by just fine.",Corruptionss,2015-01-28 16:26:27
Me too.  I tutored a student who didn't realize a p-value had to be *low* to reject it until the week before finals.  I don't know how that's possible.,beaverteeth92,2015-01-28 16:53:43
"I've had similar experiences. I often did the lab sections and minitab outputs pvalues. So I told everyone to just use the pvalues as the decision rule and someone came up to me and said, ""The lecturer told us to use the T test"" as if they didn't understand the concept that the p-value was was in fact using the T test.",Corruptionss,2015-01-28 17:07:44
I figure that means critical value?  That's what my university does because our books don't have tables with p-values for t-tests.,beaverteeth92,2015-01-28 17:13:51
"Yeah, well in this class we have:

Z test (for normal data with known sigma or large sample)

2 sample Z test (same - two populations)

T test (unknown sigma, small sample)

2 sample T test (dependent, independent, equal/unequal variances)

Chi square

F test

So the lecturer wasn't telling them they have to use the critical value, she was saying which test to use. Matter of fact, our labs require them to write down the p-value and is encouraged to use p-value during the lab than to manually look it up for timewise purposes. But this particular student, I'm sure was confused that the p-value for the T test was not the T test (she called it the P-test) but I guess it is possible she thought she was supposed to use the critical value.
",Corruptionss,2015-01-28 19:38:03
"&gt; pessimistic approach on the intelligence of people in introductory classes and I don't trust the average at calculating the degrees of freedom correctly

That's what computers are for. I've never calculated a GLM fitted by hand in my life, yet I use them regularly. 

They need to understand that the d.f. in the Welch approximation will be smaller than (n1-1)+(n2-1) but larger than the smaller of (n1-1) and (n2-1) (which means you can also get simple bounds on the p-value if you don't have a computer to hand)

",efrique,2015-01-30 20:38:59
"Absolutely. I agree. But I would argue the reasons why they can't calculate the degrees of freedom correctly is related to reasons why they won't understand the concept of bounds of the degrees of freedom. Sure we can tell them that information, but it won't be solidly built into their logic systems.",Corruptionss,2015-01-30 20:43:26
"But if they're all *soo* incompetent, even the ordinary two sample t-test, with its complicated denominator will be equally beyond them. 

If they're nearly all able to grasp the ordinary two sample test, I don't accept that the Welch test (which test statistic is actually simpler in form and easier to explain, but with a more complex df when using the t-approximation to the distribution of the test statistic) is really beyond them. If they're not following the ordinary t-test either, then it doesn't matter which you teach.",efrique,2015-01-30 22:40:28
You are right. My pessimistic self is showing,Corruptionss,2015-01-31 01:11:37
"Awesome,  just in time for this coursera course I'm starting which works in matlab and octave.  I'm used to R. ",hdooster,2015-01-12 06:53:31
Oh Andrew Ng's course? ,Iamnotanorange,2015-01-12 10:06:02
"University of Illinois at Urbana-Champaign's [Data Mining Specialization](https://www.coursera.org/specialization/datamining/20?utm_medium=courseDescripTop) works with MATLAB.  
  
I'd start at the beginning of that specialization with [Pattern Discovery in Data Mining](https://www.coursera.org/course/patterndiscovery).  
  
I will also be following Andrew Ng's [Machine Learning](https://www.coursera.org/course/ml) this Jan 19th.  
  
I've also done John Hopkins University's [Data Science specialization](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=dashboard). I'd definitely recommend it, at least for obtaining workable knowledge in R and statistics. Some videos are of low quality (the teacher should have revised his slide comments; it's full of 'uh', 'ummm', ...) but overall it's fun to do.",hdooster,2015-01-13 00:29:55
"&gt; it's full of 'uh', 'ummm', ...) but overall it's fun to do.

I think Brian Caffo needs some public speaking lessons.

Those 'uhh' and 'ummmmms' seriously annoyed me. He tried to play them off as nerd-cute-awkwardness, but that just bothered me more. ",Iamnotanorange,2015-01-13 07:12:40
"Well...  Yeah,  him. I'm sure as the course grows so will the quality of the lessons.  
  
If you watch it with subtitles at double speed his lessons go by quicker! ",hdooster,2015-01-13 11:04:33
"I'm glad that it helps, although I'd recommend Python over Matlab/Octave any day.",statmobile,2015-01-12 09:13:22
"I'll be doing a beginner's Python course as well. All the cool kids are using Python around here, apparently it's significantly better at certain things than R.  
  
(What things though?)",hdooster,2015-01-13 00:32:30
Don't you consider Python much more cumbersome than Matlab for a lot of statistics/analysis? I'm not proficient at Python but the way I see it it would take much more time to do stuff with matrices and visualization in Python than Matlab. ,Folmer,2015-01-28 05:01:57
"I've never taken the time to really learn Matlab, myself.  I've only run a few scripts while collaborating with Matlab users.  So I can't say which would be more cumbersome, as I think any language has its barriers of entry.  The question to me is long-term value.  I intentionally only invested my time in open source languages so as not to be locked into a proprietary software system while sharpening my skill set.  

Plus, one of my keys to learning a language is actually diving into the source code, what better way to learn how to really learn how to exploit a programming language's strengths?  Matlab, SAS, SPSS, Splus, don't really give you that amazing learning resource.",statmobile,2015-01-28 10:53:03
You should look into [Spyder](https://code.google.com/p/spyderlib/),statmobile,2015-01-28 11:08:00
This is an awesome resource. I'm forwarding it around my group of coworkers :),lexisasuperhero,2015-01-12 10:35:06
"You're welcome! :)  Although, thank the authors who did it!",statmobile,2015-01-12 11:42:47
http://rosettacode.org/wiki/Rosetta_Code,lolhaibai,2015-01-14 03:15:11
"This is a great book if you are mathematically literate, but are weak in statistics. I use it all the time. I wish I would have known about this Pdf before I purchased it though.",antisyzygy,2014-08-15 10:55:00
"If you knew about the cdf, you could have obtained the pdf from it. :-P",Jimmy_Goose,2014-08-15 15:21:25
FWIW it is available through Springer-Link as a pdf and to me it seems likely to me that this website has it for download illegally (i.e. someone downloaded the pdf legally and it made its way to this site). It isn't like EOSL where the book is free by design. ,NOTWorthless,2014-08-15 11:26:44
"Hmm. That makes sense, I didn't check the URL closely so I assumed it was a legitimate copy. I seem to remember in the past that some professors have made copies (perhaps lacking some sections) of their work available on their websites. 

I have a hard copy anyway, and I prefer reading hard copies so I have no need to download this file.",antisyzygy,2014-08-15 11:30:31
"Tried reading it, it's good but very abstract.  It would be nice with some more easily graspable examples and worked problems. ",AnathemaFan,2014-08-15 15:47:38
"This is a really, really good reference book.",beaverteeth92,2014-08-15 11:48:59
"I hate it. The whole ""here's a bunch of proofs, now do these 50 exercises"" format is horrible.",srkiboy83,2014-08-15 11:37:51
What do you propose as an alternative?,antisyzygy,2014-08-15 11:52:03
"Rice, or Casella &amp; Berger, for starters.",srkiboy83,2014-08-15 11:59:54
"Casella &amp; Berger...I hated that book in my theoretical statistics class ha
",,2014-08-15 14:10:37
What is a reasonable amount of time to work through this book in its entirety?,limit_dne,2014-08-16 07:44:27
The book is essentially a compilation of lecture notes for a semester long class.,1337bruin,2014-08-16 08:56:02
"Thanks, I will try to work through it. After this one, I think I will attempt the very feared Casella &amp; Berger.",limit_dne,2014-08-16 13:47:20
"FWIW, Casella and Burger is the official textbook for the class for which All of Statistics was written, so the material should be pretty redundant.",1337bruin,2014-08-16 14:13:06
Is it true that we are living in the future? Could connections between people possibly be entirely virtual in the next 20-30 years? The oculus rift purchase scares me. Do you think we will be able to live in a virtual world very soon?,Cousyman,2014-05-15 11:18:53
"I'm not LeCun. This is a crosspost, post your q here:





http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/",NOT_BRIAN_POSEHN,2014-05-15 11:34:57
"Did you just say UBC is not a top-school? Is MIT the only top school in your ranking? 

EDIT: I'm not Canadian but it looks like a top school to me: http://www.theguardian.com/news/datablog/2014/mar/06/worlds-top-100-universities-2014-reputations-ranked-times-higher-education





",,2014-03-21 08:35:32
"Came here to say this, I've interviewed several stats job candidates fresh out of UBC or out a few years, and my impression is favorable with regard to their skills and preparation.",wil_dogg,2014-03-21 10:56:30
"Statistics with a graduate degree from a decent school generally pays well and has good career chances. Plus, it's not as mind-numbingly dull as being an actuary. ",giziti,2014-03-21 06:27:52
"To be fair to us actuaries, it is what you make it. I am more of an R&amp;D actuary. I work on long-term projects, running regressions, modelling with copulas, and more true statistical work than other actuaries. But the exam process can very arduous. And yes, there are many actuaries that have a glorified accountant role, but that's what they want - to be far removed from having to use more statistical knowledge than mean and standard deviation.

Edit: [Here](http://paa2013.princeton.edu/papers/132119) is an example of a publication by a respected actuary I have worked with (my company helps fund a lot of research with Duke).",Roytee,2014-03-21 08:31:01
"Yes, there are certainly some opportunities there for very interesting work, I was being a little unfair. But, the average case isn't worth trying to get over the crazy competitive hump for, as it's a lot easier to get into a competitive statistics program and get an MS that lets you have a lot easier time finding interesting work.",giziti,2014-03-21 10:35:47
I personally find insurance extremely exciting/engaging.,dza76wutang,2014-03-21 11:38:06
"I was being serious, I work in insurance and think it's great...",dza76wutang,2014-03-21 16:55:55
"Actuaries have among the highest job satisfaction of any profession. Sure, the glorified accounting positions may suck but catastrophe modelling and quantitative finance analysis is far from boring. ",ProudOppressor,2014-03-21 07:02:41
"That may be true among FSAs and to a lesser extent ASAs, I did the actuarial program at my school before I bailed and got a masters degree in stats, all my friends who stuck with it have not had high job satisfaction. 

Maybe the actuarial route leads to good jobs eventually, but that wasn't the entry-level case for the people in my life. ",MipSuperK,2014-03-21 07:24:30
"&gt; but that wasn't the entry-level case for the people in my life.

that may be the case for most careers.",trashed_culture,2014-03-21 10:24:49
amen,dza76wutang,2014-03-21 11:35:08
[deleted],,2014-03-21 07:59:26
[deleted],,2014-03-21 13:57:49
"This is certainly true, from what I've heard. ",giziti,2014-03-21 07:17:35
"&gt; but catastrophe modelling and quantitative finance analysis is far from boring. 

Personal opinion is far from truth.",homercles337,2014-03-21 20:23:52
"Any good tier 1 automotive supplier would love a quality engineer with a background in statistics to stay on top of gauging data.  Trend analysis is a neglected field in gauging, and we need more people with a healthy knowledge of statistics to put together dimensional data that enforces our company's position in regards to gauge design.",DreamOfTheRood,2014-03-21 08:30:04
"I work at a think tank and while the statisticians tend to get piecemeal work hopping between projects, they're pretty much always in demand.",Eurynom0s,2014-03-21 08:59:36
"I'm a statistician at a P&amp;C insurance company, and work alongside actuaries frequently. I'd disagree that their work is mind-numbing, but mine is definitely more interesting. My role is primarily data work and modeling, using statistics and machine learning (things like GLMs, RFs, GBMs). The work varies a lot and is always engaging.

OP, for what it's worth, I have an MS in stats and everyone I graduated with had offers prior to graduation. But if grad school is unappealing and you have a strong CS background, you could market yourself as a data scientist (which is basically the same thing... but also different).

Start searching for jobs to get a flavor for how you stand on their requirements (many such roles are filed under advanced analytics and business analytics, in addition to the names you'd expect).",derSchuh,2014-03-21 17:40:32
"Are you kidding? Stats jobs are booming right now. Data scientists are the hottest new jobs to have. And the greatest part about that? companies don't even know what data scientists are. Couple stats with literally any other field = employment. 

My own personal background is as a social science researcher - so I know stats and IO psychology - just to give you an example of two less-common fields. The jobs are out there, for sure. University doesn't matter. The most important thing you can demonstrate is that you can solve problems. Most of the time, no one cares that you know some esoteric statistical concept. If you can translate that knowledge into something useful for a company, you can  have any job you like in the stats world. ",Palmsiepoo,2014-03-21 12:32:03
I'm seriously considering IO. Got my masters in clinical psych and I'm just not happy with the research options. I want to be more involved in analysis and number crunching. ,Dreamer06,2014-03-21 16:33:43
"IO MS here - got a bitchin job doing statistics for a business and marketing consulting company. Try for a program that emphasizes quant - a solid quant background with the IO expertise makes you a huge asset. 

Also, the problem solving aspect is so true - I may not understand every theoretical nuance of the analysis I do, but I can sure as shit explain my findings to make them actionable to an executive at a Fortune 500 company",makethelogobigger,2014-03-21 17:27:53
That sounds awesome. ,Dreamer06,2014-03-21 18:01:32
"Learn to code, call yourself a ""data scientist,"" and make $120k starting in the giant metropolis of your choosing.",slashcom,2014-03-21 19:01:53
[deleted],,2014-03-23 19:01:01
"I was mostly being tongue-in-cheek about how using that job title magically ups your pay grade, despite being mostly the same position as an applied statistician.

There are many ways to learn to code. Both official courses at universities and self-education through MOOCs are good ways. They'll really just get you started though. Like anything, the best way to get good at coding is to practice a lot.",slashcom,2014-03-24 07:31:37
"The jobs that you can pick up with a stats degree are extremely varied in terms of type of work you are doing as well as how much you will enjoy them. 

At least for me, it was really hard finding the right job, I'm 4 years out of graduation with an MS in stats, and after about 4 jobs in that time frame, I'm only now in a position where I can say that I enjoy my work and can stick with this for a long time.",MipSuperK,2014-03-21 07:27:11
"Can you briefly describe what you did/ were responsible for in each job?
",tree_man,2014-03-21 15:14:34
http://www.reddit.com/r/statistics/comments/1y6nb9/how_can_i_figure_out_whether_or_not_i_would_enjoy/cfi4ty1,MipSuperK,2014-03-22 00:26:12
"Even though you said you don't want to do finance, I really want to encourage you to. Quants tend to have a different experience than most other finance people, and they're typically valued much more. Combine your stats degree with CS and you'd be a great candidate. 

Anyway, just my $0.02. If you have any questions, feel free to ask. ",dgutty,2014-03-21 06:32:06
"I'm doing a PhD in computational neuroscience (mostly machine learning and signal processing) I think I want to work in data science but I would consider quant work.

How do you become a quant and how do I start learning about finance and economics as it seems like a huge field and I don't know which bits I would be expected to know?",alexgmcm,2014-03-21 08:23:40
"I highly recommend becoming a quant. First steps would be to check out different grad school curriculums from here: www.quantnet.com/mfe-programs-rankings/

Get an idea of the subject matter, see what you need to get familiar with, and then check out the class textbook lists.

That should give you a pretty good idea of what the field covers, and then PM me for further guidance. You probably have a great start with your PhD, so getting familiar with econ/finance shouldn't be too difficult. There are even certificate programs that help transition quants from other fields to finance. I think UW and Baruch have the most popular ones.

All else fails, I love teaching people about finance. I was lucky enough to be mentored by a quant trader, so I think I have a pretty good perspective on how markets flow and their interconnectivity. I'll even trade you econ/finance lessons for math if you're up for it. ",dgutty,2014-03-21 08:53:44
"Thanks, I'm at the start of my PhD so I have a long way to go yet. And my math skills are okay, as I did Physics before but I doubt I can really help that much :P

Ah so computational finance == quant.

That's cool to know as there was a Coursera course on that but I wasn't sure exactly what it meant, and a coursera course helped get me my present PhD position so I am kind of a fan boy.

I guess Salman Khan has some econ/finance videos too? But it's hard to distinguish between like the econ that is useful for working as an academic economist and the econ that is useful for working as a quant in industry.

I mean technically all knowledge is useful but time constraints require some triage.

How important is it to learn C++? I used it before but these days I tend to use MATLAB and I am learning more Python as well simply because of the speed of implementation and discovering things and the wide range of libraries available.

Thanks for being so helpful though - it reminds me there is more to the internet than amusing cat images :P",alexgmcm,2014-03-21 09:11:07
"Well, I only have an undergrad degree in finance and haven't taken calc or stats in years, so my math skills are pretty rusty. I'm preemptively studying for a MFE, but I'm realizing I need to brush up a ton.

Yep, they all cover the entire field, but each focuses a little more on its own nuances: compfin == more the computational systems focus, fe == the mathematical engineering/pricing of securities, qrm == risk management.

I'm a coursera fanboy too, and I've worked with one of the UW profs that host a couple of classes on there. Brilliant dude who works with the guy that created S-Plus, and now they use R almost exclusively. I tend to lean towards R instead of MatLab because it's open source, and there are countless pre-built libraries (like RQuantLib from the C++ package QuantLib) that you can use for free (even stuff like GPU/CUDA processing). It's incredibly powerful.

Khan does have some econ/finance on there, but it's pretty basic. You'd be better off picking up a couple of books or perusing www.quantnet.com, www.wilmott.com, or www.quantstart.com. They all have reading lists posted somewhere on their sites.

As far as time constraints, the best way to learn about it is to just do it, which is where www.quantopian.com comes in handy. It's an algorithmic, web-hosted trading system all run in Python. It's free to run simulations (which I urge everyone to do before putting real money down), and there are forums that people discuss their models on. Pretty damn cool, I think.

C++ is of the utmost importance. It's used by almost every quant firm in some way, and it has incredible capabilities for applications at every level of compfin like risk and sec modeling, microstructure, etc. It can also be interfaced with almost every system a firm uses. I'm slowly picking away at it, along with R. Python is next for me.

And no problem. I've been doing all of this research for myself, so I thought it'd be wasteful if I didn't pass it on. I'm planning on putting together a self-study curriculum because I want to go into my MFE having already gone over most of the material with industry experience. I'm hoping for top of the class, but I'm a terrible student so I want every advantage I can get.",dgutty,2014-03-21 09:40:42
"Cheers. I'll look through that stuff. Quantopian especially looks interesting.

I'm implementing a hidden markov model (well... using Murphy's implementation..) for brain computer interface data atm. So I can try and apply that to some finance stuff as I know it is used in that field and I'll be familiar with the 'black magic' tweaking that's required.

As for being a better student I can recommend caffeine pills - they really helped me to focus and stop wasting time, except reddit :P
",alexgmcm,2014-03-21 10:19:38
"Bit late on this but this is really good information... I am a (pre) stats major at UCLA and was really stuck on deciding whether or not to double major in computer science along with my stats degree and this thread has convinced me to do go through with it!
",redd3t,2014-05-07 03:07:02
"Same story with ""data science"", and it requires a similar skill set.",antisyzygy,2014-03-21 07:23:56
"Very true. I've found that there tends to be a niche in finance for most major fields as well. Data science could include roles like bus. intel, database managers, trading systems developers, etc. Hell, even psychology and sociology have a niche called behavioral econ. Even philosophy majors can land a good salary by applying the concepts they've learned to admin strategy.

I have fun in my field, and I encourage other people to as well.",dgutty,2014-03-21 07:41:16
Don't you have leave your morals at the door? Don't you ever worry that you're mining ever deeper into a fantasy mountain?,tekelili,2014-03-21 13:20:02
"Considering I'm a Marxist that expects to see the collapse of capitalism in his lifetime, not really. The moral issues usually surround upper level guys due to market manipulation. Quants don't necessarily manipulate markets with those tactics, so we're more able to focus on the science of why markets move, hence all of the stochastics.

Is the financial system a figment of our imaginations? Absolutely. As long as you remember that it's all made up and the points don't matter, you can enjoy the other stuff like building a 3D map of market fear (volatility surfaces). ",dgutty,2014-03-21 13:52:47
Good answer. I'll have to save my clever spelling of qunt to insult someone a little less insightful.,tekelili,2014-03-24 13:41:06
"Whoa, I never said I don't enjoy punny quips. Hit me with your best shot. ",dgutty,2014-03-24 13:56:08
"As a postgrad stats student with one foot already in the working world, my viewpoint is that Statistics is a very good career choice if you want to something mathematical and applied, especially if you strengthen your coding skills. I also specifically wanted to stay away from business/financial statistics, and have discovered that since statistical analysis is of paramount importance in every branch of science, you pretty much just have to choose what interests you and you have a good chance of finding something stable to do. That has been my experience so far, hopefully I am not too naive about it.",saruwatari_takumi,2014-03-21 06:43:28
"Well, if you really want job security, you might consider doing something related to Computational Biology for your graduate work. Depends on your project, but a lot of Comp Bio people (myself included) are just statisticians who learned some CS and Biology along the way. Just finished my PhD, and there isn't a single person I know who was having trouble finding a job. They're working all over the place, too -- Wall Street, Google, tech startups, national labs, you name it. Big data scientists are in pretty big demand right now, and Comp Bio is a great training for it. Can't guarantee the job market won't change by the time you finish, but it would be a pretty safe bet. 

Also, science is way more interesting than being an actuary (sorry actuaries).",shiroganeookami,2014-03-21 09:43:27
What field of Bio are you in?,looming_owl,2014-03-21 13:34:58
"My PhD was in Computational Biology, but I'm currently working in an immunology lab doing stuff related to vaccines. I'm mostly the statistics guy who helps out the people who actually collect samples, but I also do some work on my own.",shiroganeookami,2014-03-22 00:08:36
Have you thought about looking into psychometrics? It's a field that requires strong statistical understanding and is woefully understaffed. Several large test design companies and firms provide generous post doc scholarships and grants.,UrAccountabilibuddy,2014-03-21 11:31:44
"I'm an Applied Mathematician, but I play a statistician on TV.

A BS by itself is going to only give you access to, on average, relatively ""meh"" jobs. Getting an MS will open a tremendous number of doors. I would factor a masters into your ""life trajectory"".

Once I got my MS in AM (prob and stats) the ratio of ""interesting work"" to ""uninteresting work"" started to tilt heavily towards interesting.",dza76wutang,2014-03-21 11:34:49
[deleted],,2014-03-23 19:04:55
"I worked for a year then began a masters part time. My family has a bias towards grad school so I was constantly being told to get a masters, I used the intervening year to determine what graduate degree to get.

I have an uncanny knack for finding employers with no education benefits so I have paid for the entirety of my graduate education out of my wages. It can be frustrating/difficult to spend money on an intangible asset like a graduate degree but you have to trust it will pay off eventually.

I finished my MS in 2010 and started an MBA right after that. Not the same desired trajectory as yourself but I want to show you an example of someone doing the graduate school thing. I am also paying for my MBA out of pocket.

If you would like to have an employer pay for graduate school you will need to do one of the following (IME):

1.) Get a job with an employer that lives and dies by the expertise of its employees, e.g. consulting firm

2.) Get a job with an employer that has a ""fellowship"" type of program where they'll send outstanding employees to graduate school in exchange for a commitment after graduation. For example, my employer has a program like that (started after I started my MBA so no dice) where they will pay your tuition + continue your salary in exchange for a 3 year commitment, I think the graduate program has to be 2 years or less though.

Those are the only two ways I have heard of ""getting someone else to pay for it"" in the business world. Obviously you can also get a fellowship type of arrangement with a graduate school if you are good at what you do/want to do.",dza76wutang,2014-03-24 07:34:14
"I'm about two years out from my MS in stats. I work for a small biotech company.

It can be rather lonely, as there are only two other people in the building who speak statistics and I spent quite a bit of time working alone. However it is often new and interesting problems, it so far has not gotten dull. Overall I would say I enjoy it.

Not sure about getting the job in the first place. Though I did not go to a top ranked university, I did have academic and family connections in the industry as well as a number of internships. I only know a few people in the industry who did not get their job through some sort of networking. If you don't already know people in the industry you are looking at do your best to get internships and meet people before you graduate.",Bishops_Guest,2014-03-21 08:08:32
"Statistician working in finance speaking up. I can second many of these comments that it is not as mind numbing as you would think. Financial benefit is usually the last thing you look at in any analysis. But, when you do, the financial estimations are not boring but often can be complicated. I would also encourage you to take a few economics or finance courses to have an understanding of how to translate risks and other factors into real money.",save_the_platypi,2014-03-21 08:29:15
[deleted],,2014-03-23 19:02:51
Most definitely! Especially if you really enjoy time series,save_the_platypi,2014-03-23 20:15:52
[deleted],,2014-03-23 20:21:26
"Regardless of any desire to study time series, I would definitely recommend taking econometrics",save_the_platypi,2014-03-23 21:06:16
"Can anyone speak to what sort of portfolio you would need to get into a top statistics masters program? 

 I don't know if it makes a difference like it does for law to go to a top ranked program, if you are only looking for a professional degree.  I assume I would need to take or retake a few basic prereqs like linear algebra and calc (anyone want to fill in on what would be other necessary prereqs?), but what about GRE scores?  research projects (which I know are important for science grad programs)?",chiropter,2014-03-21 16:48:38
"Hey man, Rutgers Statistics/Mathematics joint graduate here. I only ever got the BA (or BS...whichever it was) and never pursued grad school.  
Business Intelligence jobs look for math and stat backgrounds with no need for advanced degrees. SAS is what got me in the door, but a lot of companies are willing to hire you without any experience at all. ",rutgerswhat,2014-03-21 17:14:36
"Do you mind mentioning any places where I can look for those jobs?  I have literally finished my last final exam today and am graduating with a Stats degree. I planned to go to grad school, but my grades haven't been looking very competitive.  I used to script testing automation at a large tech company in San Jose, but I don't find it very fun (but it has made me very familiar with Python).  I am very interested in testing out the waters of different jobs out there and Business Intelligence sounds kind of cool.",antohneeoh,2014-03-21 21:15:24
"I'm in Minneapolis and there are plenty of business intelligence jobs in minneapolis/st Paul metro area. I moved here from NJ and there were plenty of options in Philadelphia, Princeton/Trenton, and NYC. Can't really speak to the west coast at all, though.",rutgerswhat,2014-03-21 21:21:14
"As far as I know, UBC is a very good school with a good statistics program. I'd say it's one of the top schools in Canada.

There are plenty of statistics job and it's growing field, I don't understand what you're worrying about. It's not true that most are finance related, most of the statistical jobs are in hospitals and health companies. 

At my first statistical job interview, it wasn't even really an interview, my boss just wanted us to talk about stats related to his project and he hired me instantly. And then after only 2 years of work, I already have 2 accepted publications as third-author because of this job. I also am transgender and I have bipolar and both of these were never an issue to my boss and colleagues, they are very accepting. 

I really don't think you should worry at statistics being a bad career choice, because it's the opposite.",leviathanxs,2014-03-22 05:06:55
"I know I'm a bit late to the party, and my advice on long term isn't really worth shit as I'm not much older than you, but learn SQL well enough to put it on your resume.  It is a concrete skill that is useful in just about any position that deals with data and its not all that hard to learn the basics.  And it will make you useful right off the bat, which is one of the most important things when trying to find an entry level job.  I know this isn't the answer you were looking for, but I wish somebody told me while I was in school (BS in math, minor in statistics)

I was a few months out of school with nothing coming from my interviews until I learned SQL and put it on my resume, within a month I had a data analysis job. ",MrBrodoSwaggins,2014-03-31 19:09:43
"This is the thread for me.  I'm at UC Davis right now doubling in Biochemistry and Statistics.  Wondering if this is a good tack to take (don't worry about grad school at the moment, focus on getting an internship at Genentech or Roche or someplace like that in a stats-focused position), or if I should swap over to pure Statistics and focus on applying to grad school.",houseinfinite,2014-07-13 09:41:52
"Yes and no. If you simply graduate with your degree without relevant internships and school activities, then you will likely be hard pressed. 
Im currently a senior, and i got a job offer in December. I will be graduating soon with a degree in statistics, but I coupled that with a double major,  a couple minors (including math), and lots of leadership training programs.  

My friends who previously graduated that did not work as hard and/or attempt to get internships were hard pressed to get interviews for jobs but they eventually got one.  

In sum, it is very possible to get a great job out of undergrad in statistics,  but it is not necessarily easy. Also in lots of stats related fields, if you want to advance in the company you will likely need to get a master's degree anyway.

Also have you considered doing a master's in statistics and teaching yourself to code?",frisbee_hero,2014-03-21 06:09:52
[deleted],,2014-03-21 06:17:40
"If that is your plan, you literally won't have any problems being employed. ""Data science"" (as much as I hate the term) is a lucrative field and appears to be growing. It's just a umbrella term for ""developers or analysts that know statistics and computer science"". I don't think companies in any industry will ever stop needing them.",antisyzygy,2014-03-21 07:26:07
You could likely get a master's degree paid for if you are at a bigger company. ,sansmypants,2014-03-21 06:53:26
The BLS and Census are big statistical powerhouses in the government. Check out USAjobs.gov and see what they are looking for and maybe you can work towards that.,Adamworks,2014-03-21 07:22:21
"If you have computer science under your belt along with statistics, you will have not problem getting a job.  A majority of job posting I saw while applying for positions required or strongly desired someone with some type of coding experience.  Statistics + Computer Science = Gold",frisbee_hero,2014-03-21 11:13:40
[deleted],,2014-03-23 19:08:39
"Don't get me wrong, you can get a job without internships-I had friends that graduated before me do so.  That being said, those I knew without internships (or any relevant work experience) struggled getting interviews and job offers.  Many of my friends were unemployed for months after graduation.  On the other hand, if you can equip yourself with relevant work experience, you can make the job-seeking process so much smoother.  I got job offers a whole semester before I graduated, for example (not meaning to gloat, just an example).

I found that internships specifically for undergraduate statistics majors were hard to come by, so I went into other realms to start.  My first internship during the summer going into my junior year, I did a volunteer sales/marketing internship for a local non-profit that I found on Craigslist.  Was the job glamorous? No. Did it get me some kind of work experience that I could leverage to help get my next job? Yes.  

The next summer, I was really struggling with even getting responses from companies to interview for internships.  I applied to 35 plus places without even one response.  I ended up landing an internship purely through networking with family members and friends.  NETWORK, NETWORK, NETWORK. I cannot stress it enough.  If you school has career fairs, go and show your face.  Talk to the recruiters and express your interests.  If they have a face to tie your resume to, your odds of getting selected for an interview are astronomically higher than if you just blind apply online (as I did with the 35 plus apps).  The job I did was not in the statistics field; however, I was doing a ton of VBA coding in Excel.  The coding experience is something that a lot of companies want, so I was able to leverage this skill later on. 

Lastly, During my senior year, I applied to an internship though the university I attend.  I attend a large university, and there are plenty of opportunities on campus that not many students seek out.  I am unsure if similar trends apply to other schools, though.  This job was basically and event planning job.  Obviously, this was completely unrelated to statistics, but any job experience was better than none.

I ended accepting an Information Analytics position from a Fortune 50 company.  As was evident, I did not have any statistics internships, but I did have internships.  The job experiences I had were a means to getting the job I truly wanted.  I talked a lot about how my positions in sales and event planning helped me develop interpersonal skills.  Even if the job is not directly related to the position you are applying for, be sure to really ponder what skills you developed through your work.  Always ask yourself if the skills you learned can be applied elsewhere. 

Ultimately, the answer to your question is dependent on your career aspirations.  If you want to work for a big company right out of school, then internships are critical.  You have to show you have some sort of relevant job experience.  You can, though, get a job out of college without internships, but it will likely be a much more difficult path ahead. 

If you have any questions, please feel free to contact me further.  I hope I helped!",frisbee_hero,2014-03-27 22:25:55
"&gt; I know that just a BS in statistics is generally not enough to get a job with.

Ughh....not when your direction is Comp Sci.

&gt; I know that just a BS in statistics is generally not enough to get a job with.

Data Science is a hot industry right now.  ""Big Data"" is the buzz word of the decade.

Now, this is purely my opinion, but I would suggest *not* going straight to graduate school.  Since your direction seems to include Computers, I would try and find a job that will give you computer programming experience of some sort.  The programming industry is one of the outliers where experience is king.  Having programming work experience will be invaluable, even if you decide to go back to graduate school.

I was a Applied Math/Stat major at a top 30 American university who ended up washing out of the program because it was abstract heavy (I am decidedly not good at writing proofs).  I ended up taking an Economics degree but have 3 years worth of Mathematics and Statistics courses.  I got a job at a small software firm (since I knew Python and C) and am now working on integrating analytics into our products.  Obviously, I can only go so far, but my point is that you have actually *finished* your statistics degree and would therefore have a better resume than mine in that regard (when fresh out of college).

So, I would try to find a job with a technical bend before going straight into graduate school.  If you can't find one, you can always fall back on your original plan and go straight into graduate school.",,2014-03-21 13:02:34
"Statistics works best if paired with another focus. For example, I got into natural resources, focusing on fisheries. This might be a good fit for you, especially if you're going to UBC. Alaska is hiring three biometrician positions right now. I was offered two jobs for doing ecological statistics (one in Washington State, one in Wyoming). There is more of a push for statistics in these fields, but not surprisingly a lot of the natural resource majors are afraid of math.

These are not boring actuarial jobs. You'd be working on problems that matter. Often they're fun problems to deal with too, like trying to get population estimates when you know you have imperfect detection.",tekelili,2014-03-21 13:17:41
"Statistics student currently working on an MS here. When you say that these states are hiring, do you mean the state governments are hiring or are they university positions? And, do they require a PhD? Ecological statistics is an area that I'd like to get into and I'm just wondering what sort of jobs there are. ",adugan3,2014-03-21 16:55:12
"The state government is hiring in Alaska. The Hawaii position I saw was through a university/government partnership. I've seen several state government positions on the east coast as well. Most of these are MS level positions. 

There are also several ecological consulting firms that are more and more hiring quantitative folks. Check out WEST (Western Ecosystems Technology), they've got their own mini Statistics department there and are hiring MS level positions.

Federal jobs you can get with an MS, but a PhD is much more helpful there. I hear that the Forest Service will be hiring a Statistician (GS12) in the Pacific Southwest region, which basically means any of the eight offices in California or out of Hawaii.

Here are a few job board websites were I see positions posted. AFS in particular I've seen new biometrician postings almost every two weeks or so:
http://biology.duke.edu/jackson/ecophys/positions.html
http://fisheries.org/jobs
http://wfscjobs.tamu.edu/job-board/
",tekelili,2014-03-24 13:39:58
"def learn to code dood. the two are a powerful combination. also, data analysis. many of these roles would benefit from someone with a stats background, even though you may not be practicing statistics",watersign,2014-03-21 21:36:24
"Geologist here. If you really love stats then you might be interested in environmental risk analysis (speciality usually in a civil engineering department) or financial risk analysis for oil and gas companies which use decision trees, reliability functions, etc. to figure out which plays to pursue. ",KomatiiteMeBro,2014-03-22 01:21:49
"Go for it, statistics is the most rewarding path you can follow in the applied fields. Just one clarification, statistics is a language not a field itself, I work as an econometrician and have graduate studies in economics so statistics is the language and techniques I use. You can get as big as you want as a statistician, if you are good no one can outperform you in the business world.",jairgs,2014-03-22 09:30:29
"Learn as much Python and R programming as you can, and you will have some great opportunities.  I'm wrapping up a degree in analytics, and my fellow students who have no work experience are not having trouble getting jobs in the 80-90K range.",HodorNC,2014-03-25 21:13:09
60% of the time it's the right major every time. ,Megatron_Griffin,2014-03-21 16:31:42
"if u were in the US id say look at Census Bureau, which is effectively the largest statistical agency in the world. im sure Canada has their version of such govt entity. good luck u maple leaf faggot",faggaren,2014-03-21 07:49:13
"One reason to write a book is for the glory, but another is the money. Are you able to say how much the average 'For Dummies' author is rewarded?",deadsalle,2014-03-18 17:21:45
"O'Reilly author, here.  One doesn't write a book to make money.",seanpower,2014-03-18 18:12:14
Why do textbooks cost an arm and a leg then ? ,tree_man,2014-03-18 18:58:13
"Publishers are there to make money, whether the authors get anything or not. I know authors of very popular textbooks who have made no money at all, or so close to it it makes no odds, for their published work (there's usually some provision for getting some money, but for some reason the actual money generated often seems to be somewhere between nothing and almost nothing). Sometimes the only real benefit is simply getting a few free copies, which is little reward for all the effort.

(It's no wonder so many authors are just posting their work online now. Like Rob Hyndman's excellent new time series book. I loved his earlier book, but the new one has the advantage of being available to everyone I talk to, and I can link to a relevant subsection online. And the price is right within my budget.)

It's not all (or even mostly) greed, of course. Even the most public-spirited publisher faces some steep costs, and publishing costs seem to face a much higher rate of inflation than the rest of the economy. So just to break even means somewhat expensive books, unless print runs are huge\* -- which these sort of things usually aren't. And if a publisher isn't doing a fair bit better than breaking even, the shareholders will be complaining, and giving the board a hard time.

\* prices per book for say, a Harry Potter book, where you can sell millions and millions of them, are relatively  better for a publisher, but even then, costs like paper and ink and shipping and so on ... well, they're often a bit cheaper if you're buying really large quantities, but they don't change all that much, and costs of things like wages, barely at all (indeed they can even go the wrong way if you need lots in a hurry).
",efrique,2014-03-18 20:30:03
[Some people do](http://www.thestar.com/news/gta/2011/02/04/the_house_that_math_built.html).,deadsalle,2014-03-18 22:33:34
I too used his Calc book!  Its awesome to see he was rewarded for his efforts.,Captain_Filmer,2014-03-19 06:18:20
"Define average. Most of our books are royalty deals, so authors who write books that sell well do very well indeed. Authors who write books that don't sell don't earn out their advance.",Aleph_Alpha_001,2014-03-19 06:34:32
"&gt; authoring or coauthoring books in the For Dummies series on data science, data analytics, big data, R,

There's *already* an R for Dummies, so it would be difficult to actually get to write that one. 

(I thought it was okay, actually, if not quite to my taste) 
",efrique,2014-03-18 20:26:49
"Hi Aleph, this looks very interesting. I've sent you a message.",advait,2014-03-19 07:47:40
Just messaged you :),IllmaticGOAT,2014-03-18 18:03:12
[deleted],,2014-03-19 06:31:14
I don't really use linkedin but I sent you a request. It would probably be better to contact me though either of my email addresses,IllmaticGOAT,2014-03-19 07:05:02
"Hey, FYI- you just gave your email addresses to the whole internet; you did this in a comment response rather than a personal message.",HiddenRisk,2014-03-19 11:52:14
"Hey, FYI- you just gave your full name and linked in information to the whole internet; you did this in a comment response rather than a personal message.

",HiddenRisk,2014-03-19 11:52:39
Is the CLT not limited to cases of finite variance? That's seems to leave out a few too many families of distributions to be considered 'fundamental',Phaedrus85,2014-02-24 10:22:22
"It includes a fair amount of distributions too.  Almost all of ""basic"" statistics is based on the CLT so its got as much claim as anything else to being the fundamental theorem of stats. 

Just my opinion but I think it is ridiculous to even consider fundamental theorems of stats in the same way as the other theorems in the article.  There isn't fundamental theorem of math, right?",,2014-02-24 18:28:51
"There are two ways I look at this. The obvious first one is that the other theorems are rigerous mathematical concepts that are universally true, while the CLT is a generalization of a particular data set. So in that regard it's very different.

But the other way to look at it is that they are all theorems that are truly  fundament to particular branches of mathematics. And I think in this regard the CLT is a big enough part of statistics that it fits in that group and is an appropriate addition.",,2014-02-24 22:02:11
The FTC also leaves out most functions since most functions aren't differentiable; I guess that leaves out too many functions to be considered fundamental?,NOTWorthless,2014-02-26 10:05:27
"A very good point, but mainly a semantic one in relation to my statement. The whole concept of a distribution would fall a bit flat without the FTC. I think that fits the bill of 'fundamental'",Phaedrus85,2014-02-26 11:17:38
I don't see why failing for finite variance distributions makes the CLT not fundamental. It and its cousins power much of the statistical techniques used today.,NOTWorthless,2014-02-26 13:55:25
"I prefer the Generalized Stokes Theorem for calculus, since the [equation](https://upload.wikimedia.org/math/9/6/0/9604aad31a577adc91b462de4ce92cb2.png) is elegant and the FTC is a special case of Stokes.",deanzamo,2014-02-25 07:54:19
"As an academic who has spent the good part of the last 6 months trying to replicate (in various forms) a very popular theory in social psychology, these types of articles ring quite true for me personally. The worst part yet: what do you do with 10-20 studies that have all null results? There simply isn't a forum for it. ",Palmsiepoo,2013-12-16 15:23:49
You wait 8 years until whatever the topic is becomes an accepted fact in the field. Then you publish a paper saying that it is actually a questionable and poorly reproducible theory *and we have known that it was for the past 8 years*.,Bishops_Guest,2013-12-16 16:01:17
"I hear you. I've spent the last 9 months replicating an up and coming theory, only to find the theory is nonsense. But I'm just about to finish a PhD, so do I really want to be involved in an academic dispute with a well known name, and where would I even publish it? ",DrunkDylanThomas,2013-12-16 16:19:15
"No, you bide your time, replicate *your* results, and then make that pub the capstone of your tenure package.",eggplantsforall,2013-12-16 19:09:01
Too bad there are tons of 'old boys clubs' in academia which will strongarm your pub and make sure it doesn't see the light of day in any respectable journal. ,Palmsiepoo,2013-12-16 21:59:47
Which theory?,Palmsiepoo,2013-12-16 16:54:09
The Comprehensive Action Determination Model; wildly inconsistent pathways and basic structure. What theory were you testing?,DrunkDylanThomas,2013-12-17 01:16:32
Construal level theory (psychological distance),Palmsiepoo,2013-12-17 16:45:36
"Interesting, best of luck with publishing!",DrunkDylanThomas,2013-12-18 08:23:01
"[PLOS ONE](http://www.plosone.org/) promises to publish any competently-performed study even if it's completely boring, explicitly including null results.

It's peer reviewed and has quite a few interesting papers (it has to -- it's massive now), but doesn't have quite the same prestige as whatever big-name journals are in your field.",capnrefsmmat,2013-12-16 19:06:37
"Yes, for two to three thousand dollars, they will publish your null findings, which won't be cited by your peers and won't help you towards tenure at most universities.",whereisthecake,2013-12-16 20:51:27
"Sure, but if you've already run the studies...",jwdink,2013-12-16 21:08:40
"It's still quite an investment to write up the paper, polish it, and edit it throughout the review process. ",Palmsiepoo,2013-12-16 21:59:12
"The publication fee is $1,350, not two or three thousand. They have a good impact factor too. Can't speak for tenure decisions, though; I hope that slowly changes as PLOS ONE is accepted and the importance of publishing null results is clearer.",capnrefsmmat,2013-12-17 05:51:18
"Publication fee depends on the topic area and country of origin - first world counties publishing in big areas (medicine, etc.) pay 2,500 as a base rate.  Impact factor is great, but if your results are non significant you probably won't see citations.  

Don't get me wrong - I think open access journals are necessary and that we should publish null findings - I'm just cynical about the reality of doing do in the current academic environment if you want to have a career.",whereisthecake,2013-12-17 10:06:08
"I think you're mixing PLOS ONE with the other PLOS journals (PLOS Biology, Medicine, etc.). The other journals are more selective and more expensive. ONE charges $1,350 and will publish anything with scientific merit. There's a [price list](http://www.plos.org/publish/pricing-policy/publication-fees/) available.

But yes, I suspect your cynicism is right. It will take a long time for academic culture to change.",capnrefsmmat,2013-12-17 10:59:01
"Journal of Negative Results? [1](http://en.wikipedia.org/wiki/Journal_of_Negative_Results_in_Biomedicine), [2](https://www.google.hu/search?q=journal+of+negative+results)",Pas__,2013-12-17 07:25:23
"I work on the biomedical side of things, and I have to agree with the earlier papers on his list - there is a *very* substantial problem with the replicability of big results in biochemistry, cell biology, and a lot of medicine.  It's entirely anecdotal, but of the papers I've had to replicate during my PhD, at least 50% have failed in my hands, even with the assistance of the publishing lab.  It's a huge problem.

It also irks me a little the way that the author dismisses many of the papers.  Particularly #1, which provides an elegant mechanism for the failure rate, #2, which provides an industry perspective on the matter while, admittedly, omitting the precise data due to confidentiality - which is a common behavior in the field, and *especially* #3, #5 and #7 which actually do provide data which he dismisses in favor of his pet favorite, #8.  The 'Many Labs' project that he discusses operates in the psychological sciences, where the major problem tends to be randomization and statistical power, as I understand it, rather than the somewhat more nebulous constellation of confounding factors you encounter in the biomedical sciences.  Discounting issues of fraud, nearly any bench biochemist can tell you that there are reactions that only ever work in one person's hands, and that these results very frequently land very high profile in the literature.",chicken_fried_steak,2013-12-17 08:16:37
"...I'll just leave this here...

http://faculty.washington.edu/agg/pdf/Gwald_PsychBull_1975.OCR.pdf
",wil_dogg,2013-12-17 08:57:45
"Coming from a CS background, I was initially unsure about learning R. I figured everything I can do in R, I'd just be able to do in MATLAB or Python with the right libraries. Although that's pretty much true, I've still grown to love it. Everything is just so _intuitive_. It's just so much faster to write code for it (for the right applications, obviously) once you learn even the basics.",,2012-12-05 19:12:54
that and ggplot is much sexier than matplotlib,bdobba,2012-12-06 04:55:19
"If you are going to spend money, buy the book 'Discovering Statistics Using R' by Andy Field. ",,2012-12-05 16:40:27
"OP, so tells us about this.

I don't want to create an account, verify, and then get spam mail.",,2012-12-05 14:25:15
"It seems to want me to sign up to try a test drive. (Edit: looks like that can be bypassed)

Without more information about what the heck is going on, nope.
",efrique,2012-12-05 14:06:16
"We force students to use R for our intro to econometrics course - which is often these kids' first experience with anything close to programming. (It's pretty painful to get them started) 

This looks like a short and sweet intro to R. So long as this is free, I'll certainly recommend it. ",economicurtis,2012-12-05 22:22:38
"Non-statistician here.  If you gave this to me as homework my first week of class, I may not be predisposed to 'enjoy' econometrics (massive weeder course at my Uni).  But I'd be sure to fondly note in your course evaluation that from the beginning, you made a complex, powerful tool feel very approachable.  

That lesson taught me in 90 minutes what I didn't learn in entire days spent with Data Analysis texts, even introductory ones.",Lady_Beardman,2012-12-08 13:05:46
"lol you want 25/month for something that cna be learned through $1.50 in late fees from any library.

do we get official certification declaring achievements like a diploma, internationally recognized as a champion R coder?

you want us to sign up to even test it?

no.",BahBahTheSheep,2012-12-05 16:11:02
"I just took the intro at OP's suggestion (it's free, actually) and I'm glad I did.  It gave me exactly the set of training wheels I needed to benefit from those library textbooks of which you speak.  Always, for me, it's *""Okay... now that I've loaded mydata.csv, what do I* do *with this?""*  

A lot of textbooks seem to assume that aridly explaining how to create a matrix = teaching the concept of a matrix.  Non-math person here: kindly tell me more about this Matrix and perhaps illustrate its connection to vectors and data frames.

OP's link does a great job of linking the concepts and building comfort with the syntax, but it's Just The Basics.  If you're active on this subreddit you won't learn anything you don't know, aside from maybe some pedagogical tips on how to teach R's basics very rapidly, and painlessly.

**Rating: 9/10**

Loses one star for the opening quote in Chapter 7:
&gt;Modern pirates plunder software, not silver.

...Somalia?

*edit: formatting.  Hey, I have a new appreciation of nested italics now!*",Lady_Beardman,2012-12-08 12:00:54
"&gt; kindly tell me more about this Matrix and perhaps illustrate its connection to vectors and data frames.

FYI a matrix is actually a vector that's been given attributres that tell R where to 'slice' it to turn it into a matrix for operations.",bdobba,2012-12-09 12:58:41
"I've done the backbone.js course at codeschool, and thought it was fantastic compared to other resources out there. 

They'll follow up occasionally when a new course comes out, but they're by no means annoying.",codementum,2012-12-05 15:53:45
[deleted],,2012-12-05 15:40:43
should `n`  be replaced by `trials` or should `trials` be replaced by `n`?,efrique,2012-12-05 17:52:11
"Note that you can add logicals, TRUE would give 1 and FALSE 0.... so m is superfluous

    4*sum(x^2+y^2&lt;=1)/n

works.

Also, c() concatenates things, in your definition of m you had

    m &lt;- (c(x^2+y^2&lt;=1))

But you aren't actually concatenating anything.  So c in this context is superfluous as are the external brackets.",anonemouse2010,2012-12-05 18:06:16
"Speaking of paradoxes, could anyone explain the Banach-Tarski Paradox and how it applies to statistics?

http://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox

It was briefly mentioned in one of my classes and never mentioned again.",,2012-03-05 05:36:13
"I don't quite see how it would apply to statistics. It's basically about messing with the axiom of choice to allow you to construct some very strange sets (strange relative to the real world). It might somehow be connected through the axiom of choice.

Maybe look at the Vitali set for an example of an unmeasurable set? That might be somewhat useful in some way.

http://en.wikipedia.org/wiki/Vitali_set

I don't know much about the axiom of choice, or statistics though. Or the BT paradox.",choochoochoose,2012-03-09 12:21:13
"I have seen that two-girl problem before and find it very irksome.

""In a family with two children, what are the chances, if one of the children is a girl, that both children are girls?""

As far as I can tell,  the actual questions is simply ""what are the chances that a child is a girl?"", with some arbitrary, irrelevant facts thrown in (other formulations state that the sister's name is Florida, or other variations).

Since we are **told** that the child has a sibling, and that the sibling is a girl, the probability of those factors is **100%**. They should have no effect on the probability, but supposedly that information reduces the child's probability of being a girl. Please tell this layperson why I am wrong.


",webbitor,2012-03-05 08:55:18
"If a family has two children, there are a four possibilities;

* A) Girl, Girl
* B) Girl, then Boy (oldest is Girl)
* C) Boy, then Girl (oldest is Boy)
* D) Boy, Boy

Each of those has equal probability; i.e. 25% chance. The chances of having ""at least one Girl"" is therefore 75% = A + B + C.

So, we are told that there is at least one girl. That rules out D. This leaves us with A,B and C. Each of A, B, C were equally likely *a priori*  and now they are still equally likely; 33% each.

* If A, then both children are Girls.
* If B, then there is only one Girl.
* If C, then there is only one Girl.

Therefore, the chance that both are girls is 33%. i.e. the probability of A versus the probability of A+B+C.",SkepticalEmpiricist,2012-03-08 08:27:52
"I still can't quite get my head around it, but this explanation is the best I've heard, thanks.

I guess my problem is with D. Why would we consider that outcome, when we already know that the sibling is a girl? It's like we are including probabilities from some past state before the family had children.",webbitor,2012-03-08 08:46:22
"The 'past state' refers to a time when you knew what the model was, but when you hadn't observed all the relevant data. For example, when the question started with:

  ""In a family with two children, ...""

then you knew then that A,B,C and D were all equally likely. Then, you hear that

  ""... one of the children is a girl, ...""

and this allows you to strike D from the list. Also, this knowledge does *not* change the relative weights of A,B,C; the posterior probability of D has dropped to zero, and the posterior probabilities of the other three options is 33% each.

In the beginning, you have very little information. Then you are told that there are two children, this narrows the options to ABCD. Then, you are told that one of them is a girl, this rules out D.",SkepticalEmpiricist,2012-03-08 09:09:04
Could you link to the abstract instead of the pdf?,carmichael561,2013-08-18 21:39:35
"Not sure why, but [here you go](http://arxiv.org/abs/1205.4446v1).",blindConjecture,2013-08-19 00:03:56
i was on my phone and wanted to see what the post was about without downloading a pdf. thanks.,carmichael561,2013-08-19 20:32:38
"Another thing - you have categories broken out by program, but not by subject matter say like agronomy, marketing, etc. Or by method like Regression, ANOVA, etc.

I don't know what your target audience is but most of my clients do not know (or care) what program is used for analysis, but instead just know what kind of problem they have.  Am I just missing the point of the site?",kinross_19,2013-05-22 23:01:26
"I don't think you've missed the point; that is a valid comment. 

The thinking behind the current layout was to simplify the search process from the freelancer's perspective. In my experience, analysts/statisticians tend to be proficient in 2-3 packages or languages and that is more of a determinant than industry in whether they can complete a project or not. I would love to hear more perspectives, though. 

I agree that job posters sometimes don't care what the platform is. Ideally, when those clients post a job, they will select ""Any or don't know"". Of course, some clients either want something they are familiar with or their company uses, or they want the open source version. My plan was to watch how the first few jobs got classified and determine whether that classification was harder (or less relevant) for job posters than I realized. 

Job posters also select an industry, so that component is there, but probably not as prevalent as it should be. I am still trying to figure out where to display those categories.

I hadn't thought much about adding a category for method. Anecdotally, for many of my clients that would be difficult, but again, I am interested in other perspectives. 

Thanks for checking out the site and offering some things to think about. Also, thanks for enlightening me about a new field - agronomy. There's a bad pun in there somewhere about the field of agronomy, I'm sure. It's now the newest addition to the list of industries on the site.",treedog,2013-05-23 07:25:20
"I think this site looks awesome and I'd be excited to use a service that's not oDesk. 

I agree with kincross - it might be nice to find projects via statistical technique required or maybe field. ",Iamnotanorange,2013-05-23 15:20:00
Thanks for the input. I'll work on getting that set up ASAP. ,treedog,2013-05-23 18:56:30
"I like it, good work. I hope it receives some attention.

What is your profit model like? Is there a % taken or a fee for each project?",jangevaa,2013-05-22 17:56:23
"Thanks for checking it out. The current fee is 3% of the project budget, taken once the project is complete. With Paypal fees included that is a still a good bit less than Elance (around 8%) and oDesk (around 10%). Technically, all the fees are paid by the client, but that's more of a semantic issue given that an efficient market would (theoretically) decrease the average bid by that amount. It's free to register, post a basic job, bid, send pms, pretty much everything else. One thing I dislike about Elance is when I find a job that I want to bid on and then realize I have to spend more money to sign up for a new category to do so. I've tried to avoid that nickle and dime-ing and just set up a fee structure that will bring in enough to keep the site going. 

On a side note, since I've been a freelancer for a while and experienced the pro-client bias that is now the norm on freelancing sites, I'm really interested in making this a site where things are more equitable. Common sense says that if a site can do that it will benefit everyone, as it will attract highly-qualified experts who are now avoiding freelancing because of the lack of respect and adequate compensation it offers and clients will get better work because of it. I have contacted Freelancer's Union to see if they had any recommendations for building such a community, but I haven't yet heard back from them. Any suggestions in that vein are welcome.

Anyway, that's a much longer answer to your question than you were probably expecting, likely attributable to that pot of coffee I probably shouldn't have had so late in the evening.  ",treedog,2013-05-22 18:48:24
"Longer answer than I expected, yup. But I appreciate it. I have never used any sort of freelance sites, but you're making a lot of sense.",jangevaa,2013-05-22 19:24:40
"Looks really cool. Would like the descriptions to be a little more detailed before submitting a bid. Hard to gauge how much time something like ""Need assistance generating random data with R. I will need to add a few variables to a current program I have that creates around 1M rows of data."" is going to take. It could be 20 minutes or it could be 20 hours. Hard to tell. ",satnightride,2013-05-22 14:09:14
"That's a good point. The listings are editable, so if there's a job you need more information on just PM the poster and ask them to add more details or to at least send them to you via PM. On a related note, any suggestions for encouraging/getting more job listings would be appreciated. It's pretty clear that's going to be the most difficult part.

Thanks for taking the time to check it out!",treedog,2013-05-22 16:36:04
The Post-new link on the front page returned me to the homepage. You might want to look into that. ,satnightride,2013-05-22 16:40:41
"That would certainly make it difficult to get more job listings. It works on my end and for my test cases, but I think I know what might have happened and have made a change. Would you mind testing it again? If it still doesn't work, please drop me a pm with any details you can provide about your setup. Thanks again.  ",treedog,2013-05-22 18:01:15
Just introduced this to a few friends.,,2013-05-22 18:47:20
"Thanks, that's much appreciated! ",treedog,2013-05-22 19:39:27
"On your ""Service Provider Search"" the Next button does not advance to the next page (however clicking on the numbers do).",kinross_19,2013-05-22 22:54:44
"Thanks, I'll look into it.",treedog,2013-05-23 06:41:04
I don't think the mean number of sex events per week is high enough to warrant approximating the Poisson distribution with the Normal. I'd also much rather see ECDFs comparing males versus females than Normal approximations because I'm interested in the... tails. \*YEEEAAAHHHHH\*,samclifford,2012-02-24 03:13:28
You want a 7 inflated Poisson distribution?,Zeurpiet,2012-02-24 08:27:11
Giggity.,,2012-02-24 10:55:19
For give my ignorance. Why an F test?  why an alpha of 0.10? and why  the PDF graphs?,chaoticneutral,2012-02-24 18:51:01
"I agree. These were all integers (I don't think anyone would want a ""half"" sex a day or week or whatever), he was comparing means so a t test would have been just fine.",khasiv,2012-02-24 20:36:53
"Good work, but I wish a p-value was reported.",kinross_19,2012-02-24 11:31:05
"Sex and statistics, hmm? Why am I suddenly thinking of George Costanza from Seinfeld trying to do *both at the same time*?",kiwipete,2012-02-24 05:32:27
"Younger girls in particular can also reflect a shifting overall value system, toward isolation and outlash (and thus stronger ""stake"" in the cultural myth of sex.)",JIVEprinting,2012-02-24 16:35:49
http://ggvis.rstudio.com/,h0ruschorus,2015-03-06 15:39:42
"While we're on that topic, which is the best plotting library for making interactive plots to put on a web page?",Bromskloss,2015-03-06 10:07:33
I've been using rshiny recently and it's very nice.,Lycur,2015-03-06 15:59:47
Shiny works well for it.,chrico031,2015-03-06 17:37:36
I've been experimenting with rCharts (and rMaps) and they are great! I can create really really nice interactive graphics through JavaScript libraries without writing any JavaScript (just R). And they can be put onto a website very easily.,ojdp19,2015-03-12 17:54:55
"Why not use [shiny](http://shiny.rstudio.com/) here?  

Don't get me wrong, I love d3. However, the author starts in R, and then switches after taking as an axiom that there is no way to make an R visualization interactive.

The article needs a stronger support for connecting the two and requiring the use of framework that's built in a second language.",p00b,2015-03-06 11:42:25
Click to tweet? Really?,0xfeedcafe,2015-03-04 15:43:10
Medium programmer reporting for duty.,Ghostface_Grillah,2015-03-04 12:31:24
[deleted],,2015-03-04 13:09:23
"I feel the same way...am working as a python web developer now and hope to move into data science soon.

I like the idea of using programming as a means to accomplish something else, but am not really into it enough to learn it inside out and move into a more hardcore programming area of CS.",kindofapigdill,2015-03-04 13:38:05
"it makes a mediocre data scientist. spend your time learning the fundamentals, it will pay off.",everynameisFingtaken,2015-03-04 15:19:18
Which fundamentals are you referring to?,kindofapigdill,2015-03-04 15:52:25
"fundamentals of algorithms, data structures, statistics, system design, and others.",everynameisFingtaken,2015-03-04 17:08:50
"to be fair I learned math/modeling/stats in school and picked up programming later, so it'll be a while until I'm 1337 haxor or whatever",Ghostface_Grillah,2015-03-04 16:48:25
same here. you'll be fine.,everynameisFingtaken,2015-03-04 17:10:28
I genuinely like R and Python so it's been fun!,Ghostface_Grillah,2015-03-04 17:14:22
Aha.Hadoop though.,Hyperbolies,2015-03-04 14:56:08
"As a statistician, I spend most of my time learning to write statistical models from scratch. That said, I echo the characterization. I deeply dislike the fact that my field feels like it's filled with the end-users of software rather than their architects. ",mrdevlar,2015-03-04 14:49:53
"I think it's best to know *how* to architect models, but also be savvy enough to know where to find the best packages to do what you need to do. In a work environment with tight deadlines, it helps tremendously to know how to load up the right library for the job, run your analysis, and move on to interpreting the results.

However, if you don't know how to design your own model, the odds are very high that you won't know how to correctly interpret the results of a model that someone else has written, and that's what separates the good statisticians from the bad ones.",ragnarocka,2015-03-04 16:50:55
"I'll agree with that, but I'd add a caveat.

I genuinely believe the industry is changing away from one-shot analysis. The last half-decade of work I've done has been less, ""Please analyze this data set"" and more ""Please help us integrate this statistical process into our existing data system"". The former allows you to get away with not really understanding what goes on under the hood, the latter less so.",mrdevlar,2015-03-05 00:49:30
"I have interviewed for high level (i have a phd and 10+ years of experience as a computational scientist, data scientist is a bullshit marketing term), and have had to deal with some of the most esoteric C questions.  An example: one interviewer was supposed to talk about the code i wrote (computer vision in python) and C++, immediately he launched into esoteric pointers in C.  **C IS NOT C++**",homercles337,2015-03-04 13:36:46
"Excuse me if I'm mistaken, but don't C++ pointers work exactly the same as in C? I was under the impression that they're the same thing.",steaknsteak,2015-03-04 15:21:27
"u/homercles337 is mostly bluster. Going by his comment history, his PhD is from Berkeley or Stanford, depending on which day of the week it is. But he definitely trained under someone with a Nobel Prize at some point at one of these places, maybe.

He's pret-ty serious business IRL.",sucks_to_yr_assmar,2015-03-04 16:55:40
"without googling I'm going to take a risk, go out on the limb, make a fool of myself and claim that they absolutely are identical.",Simusid,2015-03-04 15:23:41
You dont have to use pointers in C++.  Along with object orientation that is the whole point of C++.,homercles337,2015-03-04 15:32:02
What???  There's plenty of use cases of pointers in C++.,flipstables,2015-03-04 17:14:36
**C++ is NOT C.**,homercles337,2015-03-04 19:21:38
"Yes yes, anybody who's coded in C++ for more than 20 minutes will get some other C++ programmer who will say ""don't do xyz"" in C++ because **C++ is not C**.

That's great until you have to improve performance, allocate large data sets into memory that need to outlive the current scope, or have to use a C library (you know, doing things that might be helpful for statistical analysis of data?).  Then all of a sudden, knowing how to effectively use pointers is very helpful.

So calm down and understand that sometimes we need to use the right (easiest) tools for the job.",flipstables,2015-03-04 19:51:36
No.  C is not C++.  Matlab is not Python.  I have 10+ years post-phd experience and your post is profoundly ignorant. ,homercles337,2015-03-05 11:57:53
"Come on dude, you pimp out your ""PhD"" like someone who doesn't actually have a PhD.

Someone who really had a PhD wouldn't rely so much on ""I have a PhD!"" as a substitute for actual, persuasive argumentation. Someone with a PhD would be like ""Here's why you're wrong."" They wouldn't be like ""I have a PhD, so you're wrong.""",sucks_to_yr_assmar,2015-03-06 16:46:58
X IS NOT Y!,smoovewill,2015-03-06 12:10:28
"What is char* doing in signature of C++ main function then?

    int main(int argc, char* argv[]);",dimview,2015-03-04 22:04:01
You wrote C.,homercles337,2015-03-05 01:10:26
How would it look in C++?,dimview,2015-03-05 07:48:31
"If you claim to know C++, you have to know C. Even though I've used C++ for years, I'm hesitant to claim to be an expert. There's just so much to it. Template metaprogramming, RAII, the C subset, functional style, C++11, Boost, each of these are huge topics/concepts.

Even modern C++ style uses pointers regularly, relying on RAII/smart pointers for lifecycle management. If you think otherwise, you really do not know C++ well enough.",klaxion,2015-03-04 19:24:04
[Khanacademy](https://www.khanacademy.org/math/probability) has free stuff for stats ,Roflsquad,2015-01-13 05:55:49
"* [OpenIntro](http://www.openintro.org/) is a free basic statistics book including R labs on their website.
* Norm Matloff has a free book on statistics [here](http://heather.cs.ucdavis.edu/probstatbook). Even though it's intended for computer scientists, it's contents are really universal and basic (and it uses R too).
* See [this post](http://stats.stackexchange.com/questions/614/open-source-statistical-textbooks) or [this one](http://stats.stackexchange.com/questions/170/free-statistical-textbooks) for a compilation of freely available textbooks on statistics.
 
As you wish to get into applied statistics (i.e. actually analyzing data), you'll need software. I'd strongly recommend learning and using R because it's completely free and incredibly powerful.

Here are some resources for learning statistics using R:

* http://www.r-statistics.com/2009/10/free-statistics-e-books-for-download/
* http://blog.revolutionanalytics.com/2011/11/three-free-books-on-r-for-statistics.html
* http://www.r-project.org/doc/bib/R-books.html

Then, these websites provide very valuable resources for doing statistics with R:

* http://zoonek2.free.fr/UNIX/48_R/all.html
* http://www.cookbook-r.com/
* http://www.statmethods.net/

Hope that helps.
",COOLSerdash,2015-01-13 05:36:00
"These are great. Thanks for the links. I'm told I need to learn R since at the moment I only know SAS and SPSS.

For OP: I don't know your situation, but if you are currently in an intro-level stats class and want really concise videos to review for a quiz or exam or something, try user jbstatistics on youtube:

https://www.youtube.com/user/jbstatistics

This is great for review on basic topics, and much more quick and to-the-point than Khanacademy.  ",Falconhoof-,2015-01-13 12:04:38
"You can find a good collection of free stats tutorials here:
http://introlearn.com/mathematics/statistics/",mightytriangle,2015-01-13 07:17:53
edX is currently running an [introduction to statistics](https://www.edx.org/course/foundations-data-analysis-utaustinx-ut-7-01x#.VLUghCvF-aU) with R course. It is 7-8 weeks in already but you should still be able to join.,toodim,2015-01-13 05:42:21
http://onlinestatbook.com/,nameBrandon,2015-01-13 09:21:42
"My website, beginning stats, plus a lot of linear models with a focus on understanding what you are doing and why (i.e., very light on proofs, etc.).  Dozens of YouTube videos plus a few handouts, and a little Excel and R.  All absolutely free.

http://www.burkeyacademy.com/home/statistics-econometrics",BurkeyAcademy,2015-01-13 10:34:59
"Maybe this one, too?
http://stattrek.com/descriptive-statistics/variables.aspx?tutorial=ap
They seem to have heavy SEO, anyone here is free to reject its quality",gooOpen,2015-01-13 13:03:58
Brandon Foltz' YT channel is great: https://www.youtube.com/channel/UCFrjdcImgcQVyFbK04MBEhA,Rylick,2015-01-13 15:07:54
"Maybe this will be helpfull too: Coursera
It's free unless you want to obtain a specialization certificate which might be helpfull to show your effort and accomplishments on your resume. But that wont cost you much either.

Coursera actually offers alot! So much interesting subjects for free from universities from all over the world.",Forthethirdtime,2015-01-15 03:37:02
"Thank you all for your responses. I'm making a career change and was looking at some jobs that had a statistical analysis element to them.

You've all been very helpful!",becauseIwasInverted,2015-01-13 15:27:20
Took a glance at the Etsy store.  Totaling getting the 'control group' 'treatment group' shirts for when I have kids.,Serdmanczyk,2014-12-15 18:08:08
[Link](https://www.etsy.com/se-en/listing/83311858/control-and-treatment-group-shirts-or?ref=shop_home_active_8),Bromskloss,2014-12-16 03:24:16
You just defined my wish list with this link.,sjgw137,2014-12-15 19:53:15
"So, those posters are awesome, but I'm all about that chisquaratops t-shirt.",viscount16,2014-12-16 02:01:00
"I have to admit, I prefer the steganormalus one!",AccioTheDoctor,2014-12-16 05:06:14
"That's our store, glad you liked them! :D",kinross_19,2014-12-16 09:47:48
That's amazing!! I specifically requested he get them for me because I work with some not so statistically oriented people:-),AccioTheDoctor,2014-12-16 17:13:39
"This thread has been linked to from elsewhere on reddit.


 - [/r/mistyfront] [Awesome gift from my secret Santa arrived today...statistical propaganda posters! (/r/statistics)](http://np.reddit.com/r/mistyfront/comments/2pj5k0/awesome_gift_from_my_secret_santa_arrived/)


*^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)*

",totes_meta_bot,2014-12-16 17:47:16
I think this guy is confusing his dependent and independent variables. He is stating that the language used in the review affects the rating given. Whereas I would have thought that the rating of the review be it positive or negative would affect what people write in the content of the review. ,Ajubbajub,2014-10-29 15:24:26
"Interesting point. However, I actually agree with the author. I think you need to first have something bad to say about a place (review descriptions), and on that basis you are able to rate the stars accordingly. ",ohhmyg,2014-10-29 16:18:05
"But that's not how it works. You usually think about a rating and then write the text.

  I disagree with the author on 3 stars being negative. It is more than half and means ""ok"". 

  I'm not sure what the whole point of the study was other than practicing at analyzing data and making graphs. There is no conclusion as to how one can use the data. Negative words have the same correlation with ratings as positive words, but in opposite directions.

  Now that I think about it, one use could be to analyze all the reviews and detect potentially mistaken reviews (you sometimes but rarely find these positive reviews giving 1 star and vice-versa). Such a tool could help Yelp and similar companies improve (slightly) the reviews database.",Max_Thunder,2014-10-29 19:37:57
"I don't agree. A numerical rating is based on the experience you had. Language describes the experience, not numbers. Humans don't go around the world like a robot, thinking 'that service was a 3', 'that waiter had a 2 attitude'. They think 'that service was amazing/terrible.'
So while you're correct, you log onto the site and have to give the rating first. The rating is just a best-guess, numerical summary, of the underlying experience, which is better described by language.",Elnocho3,2014-10-30 03:24:18
[deleted],,2014-09-07 16:44:43
"For those who are willing to go the distance, they are currently hiring for 3 positions:

* programmer (write scripts to collect data)
* data scientist (provide analysis based on that data)
* data architect (basically data warehousing)",featherfooted,2014-09-07 19:32:41
We have more data on sports teams than ever before - If you have seen the movie Moneyball you can understand why they are using the data.  This is no longer just calculating basic stats it is using supercomputers to calculate every move.  This technology is moving to football and basketball soon: http://www.pritzkergroup.com/venture-capital/how-sportvisions-innovations-have-changed-major-league-baseball/,krystyin,2014-09-08 07:54:50
"I'm not sure I entirely agree, because the underlying assumption is that all minutes are created equal. The value of having a star-player in at the end of the game may make up for lost minutes due to foul trouble. Not sure how to address this empirically, however.

you should cross-post this to /r/nba",zdk,2014-05-08 10:35:33
"The OP's article doesn't address that, you are correct. But the more serious blog post the article is referencing does contain caveats including that which you just mentioned. [Foul Trouble](http://theoryclass.wordpress.com/2010/04/27/foul-trouble/) is the blog post, 5th paragraph down he considers his ""Second caveat: maybe not all minutes are equal"". There he justifies assuming we want to maximize total star play time instead of using them at the supposedly most effective times during the game.",ljcoleslaw,2014-05-08 11:17:37
"There was an interesting argument about this a couple years back, between kenpom and a few game theorists that I know. I'll have to see if I can find it.",llimllib,2014-05-08 11:11:29
"Probably this (or one of the links):

http://erikstuart.com/post/629029612/foul-trouble-and-coach-behavior-incentives",david55555,2014-05-08 15:15:34
"That's the discussion exactly! The post I remember in particular is [this one](https://theoryclass.wordpress.com/2010/04/27/foul-trouble/), followed by [this one](https://theoryclass.wordpress.com/2010/05/17/foul%C2%A0follow-up/).",llimllib,2014-05-08 16:27:39
"Ooh, I'd love to read that, thanks for looking for it!",BadSoles,2014-05-08 11:47:43
"Have there been analyses on shot percentage, say the last minute, compared to the rest of the game? ",VictoriousEgret,2014-05-08 15:09:14
"I haven't seen any, but the data should be there, from the new advanced stats.",zdk,2014-05-08 15:12:18
"The comments on the original article give some good caveats. Another one: Rest. It may make sense to sit the player for a while, because he will be rested when he comes back in, without substantially reducing   the expected remaining playing time for the player until he fouls out. ",ee22l,2014-05-08 11:33:50
"There are a few others not mentioned. 

1. Fatigue contributes to fouls. 
2. Refs are more likely to swallow their whistle towards the end of a close game than in the middle of the game. 

Both of these considerations make benching your star when in foul trouble in the middle of the game or when fatigued the correct move. ",satnightride,2014-05-08 12:19:41
"Also, refs are a lot more likely to call a foul on someone that recently caught their attention.  How often do you see a player get two quick fouls within ten or so seconds?  Three fouls within a minute are not uncommon.  It is a good idea to take a player out because the refs are so damn baised in basketball.",jen1980,2014-05-08 14:46:24
"This makes so many assumptions.  The assumption of equality in the value of minutes has been well noted, but there is also an assumption that the referees are equally likely to call fouls at any time.  Not true.",Icamehereto__,2014-05-08 12:05:53
"Well, that's disagreeing with the central premise, namely that fouls are time invariant.

I do agree with you that fouls are more or less likely to occur based on time remaining, but that assumption is explicitly addressed, unlike some of the others",IncongruentModulo1,2014-05-08 13:37:02
"Another thing that is not considered is the fact that players don't always play as effectively (especially on defense), if they are trying to to avoid fouls like the plague.",SurfaceThought,2014-05-08 11:38:44
"Except if fouls are time invariant the stars shouldn't be trying to avoid fouls. Go ahead and foul out, just get as many points on the board before you do.

In other words if I have three players to put in:

The Star with a net positive offense/defense differential of +1 pts/min.

The same Star but trying to avoid fouls with a -1 pt/min differential.

The backup with a +0 pt/min differential.

I want the Star to play as long as he can until he fouls out, at which point the backup can start. I don't want to see the star trying to avoid fouls. Que sera, sera.",david55555,2014-05-08 15:14:49
"I think the point is that the star player is going to make his own decision on how to play by looking at the game clock himself, no matter what the coach says to him.  

Coach: ""All right, star, I don't care if you foul out.  Play like you always do."" 

Star: *There's still 2 quarters left.  I want to play for as long as I can.  Screw coach, I'm going to play at -1 pt/min differential for now.*

Later in the game...

Star: *Ah, there's only 5 minutes in the game.*  ***Now*** *I'm going to play the way I'm supposed to, at +1 pt/min differential*.

I.e., it ignores this (common and possibly accurate) belief that players always want to maximize the amount of time they get to play regardless of the pt/min differential they're contributing.",typesoshee,2014-05-08 23:05:40
"Thanks, pretty much exactly what I would have said. 

Sports players are not always acting purely rationally on the court or field, far from it. ",SurfaceThought,2014-05-09 08:34:50
"The author does address it in the second link of [this](http://www.reddit.com/r/statistics/comments/2520v6/why_nba_star_players_in_foul_trouble_should_not/chd5ppm) but it seems like he sort of dismisses it.  

There's a little tiny game theory going on in the star player's mind as well.  Let's say the coach tells him to play just like he would with 0 fouls, even though the star player currently has, say 3 fouls or whatever.  The player can either 1. play as he normally would, risking more fouls and fouling out in the near future or 2. play in a way that avoids fouls, making him a less valuable player.  If he chooses 1, the expectation is that he will play less minutes than if he wasn't in foul trouble since he may foul out before the end of the game.  If he chooses 2, either a. the coach will leave him in anyway (he's still more effective than the bench replacement) or b. take him out because the coach doesn't want a less-than-effective player playing softly and becoming a target on defense (and the bench replacement is better than a psychologically half-assed star player).  In both 1 and 2b, the player plays less than normal.  2a is the best result for the player, so the player will always choose 2 over 1 in order to maximize his playing time.  This assumes that there is a lot of game time left.  In crunch time, most players with a brain will choose 1 because the probability of fouling out is much smaller when there isn't much game time left.  

The only way that a player would naturally choose 1 over 2 is if his natural fouling rate is pretty low (guards seem to be more like this).  If his natural rate is 2 fouls per 48 minutes, Then even if he gets 3 fouls in the first 1 minute, he can still play the way he normally plays because he still wouldn't be expected to foul out.  If your natural fouling rate is higher (forward/centers seem to be more like this), like 5 fouls per 48 minutes, then if you get 3 fouls in the first minute, the star player is immediately going to be more tentative and try to avoid fouls.  Either this is an explanation or an after-the-fact rationalization that coaches already do this - a big man that gets 2 fouls quickly is almost always benched, while a PG that gets 2 fouls is sometimes left in.  Usually I see the PG get benched if he gets 3 fouls early.  Additionally, it's much easier to target an opponent big for defensive fouls by attacking him, while it's harder to target opponent guards in the same way. ",typesoshee,2014-05-09 17:33:29
"Its certainly true that in Basketball there is an emotional bias towards the final moments of the game (this is true of almost any high scoring game). Part of what makes stars ""stars"" is their performance during those final moments. Would we remember so-and-so if not for that fade-away jumper to win the title at the buzzer... maybe not. So would he follow his coaches orders if doing so meant he didn't get the star treatment? Didn't get the shoe deal? Didn't get the $$$? Maybe not, so I agree that asking a player to play unselfishly is a *hard* thing to do. 

That said they are professionals, and the coach *should* be able to get them to play the right way. It does happen in other sports (Soccer for instance) where star players are sometimes not present in the final moments of the game but are expected to give their full effort and then bow out gracefully in the final moments. So its not impossible, just hard.

Honestly, this is an academic exercise. The NBA has no interest in seeing Basketball be played ""optimally"". If the optimal strategy is to have your stars blow the game open in the opening quarters and then limp to the buzzer with the bench warmers... well nobody wants to watch that. The NBA wants it to be a competitive and thrilling game all the way to the end. If that means that refs exhibit biases in their calling which leads to games being closer than they should otherwise, so much the better for the NBA.

So I think all the statements about time invariance and optimal coaching strategy *should* be true, in the academic sense. A super-intelligent logical being whose only knowledge about the game is the rulebook would come to these conclusions, but the stuff not in the rulebook invalidates all of it.",david55555,2014-05-10 16:02:30
"Part of the point of benching players in foul trouble is that your players can commit fouls. If your opponent has a player that doesn't want to commit fouls, you can target that player for an advantage.",ThrustVectoring,2014-05-08 12:58:43
"Right, if you do not watch basketball you may not understand that fouls are not always necessarily to be avoided, they are to be used strategically, and not having anyone on the court that can foul if need be can also be a problem. 

Ugh, honestly, this article annoys me as a Basketball fan who is interested in advanced analysis of basketball. This is the sort of overly simplified model that makes non-statistics people think that people who do statistical analysis of basketball don't understand the game.

Edit:  a word

",SurfaceThought,2014-05-09 08:42:56
"Judging from the comments, this seems to be another example of when probability/statistical models don't quite 'measure up' to the happenings of the real world.  Although I completely agree with the logic of fouls/benched players, I bet any basketball coach would immediately see the flaws in this.  ",widowspeak9,2014-05-08 15:55:25
So is this worth your time if you've already taken an intro statistics course? This seems like it's just intro stats with a focus on how to do everything in R--just want to make sure I'm interpreting the course description correctly.,Eurynom0s,2013-10-28 09:50:32
Basically. I recommend this class if you need a refresher.,,2013-10-28 11:15:50
"From the video:

&gt; We'll start from the basics for people who've never done a data analysis.",tmalsburg2,2013-10-28 13:53:01
"I just completed ""computing for data analysis"" and have enrolled in this. Anyone looking for a study partner PM me. Australian living in Japan.",xkGEB,2013-10-28 08:37:08
Is there a point to having marked assignments? Do you get a certificate at the end of the course or something?,,2013-10-28 18:36:54
Feedback is essential for learning.,RobMagus,2013-10-28 18:49:49
"I just asked because you lose 20% of your grade if you don't review four other submissions. If marks don't mean anything, I don't know if that's much of a disincentive.",,2013-10-28 19:06:54
Pride/self-esteem/competitiveness?,naught101,2013-10-28 21:43:10
"I am giving a talk tomorrow night for managers who work with statistical analysts.  I am taking the opportunity to clear up a few common sources of misunderstanding and miscommunication.  The link is to the slides I am planning to present.

Comments and suggestions from /r/statistics would be welcome!",AllenDowney,2013-09-23 13:16:45
"This is some great stuff. Speaking as a recent hands-off-stats manager, I can say this is highly useful and relevant. ",dogsrock,2013-09-23 23:51:17
"This is great, very well done. Thanks for making this available to all. Actually, thanks for making all of your books openly available.",lenwood,2013-09-24 07:13:17
You're welcome!,AllenDowney,2013-09-24 12:24:17
"You might want to mention being careful about R^2, specifically that higher does not generally imply better.",1337bruin,2013-09-23 22:36:11
"Good stuff.

The key is that the managers have ongoing engagement / support with a stats consultant who can help them sharpen questions for analysts and avoid drilling into stuff that is irrelevant.  If left on their own not much will change, if their managers push them to improve the analytics and fund a consultant then there's hope.",wil_dogg,2013-09-23 17:11:29
"Allen, can you open comments on the doc?",RA_Fisher,2013-09-23 20:27:11
I think only people who can edit can make comments.  Did you have a comment on a specific slide?,AllenDowney,2013-09-24 04:51:06
"Thanks Allen! Had saved it at office earlier and got to see it only now.

Do you mind me asking what visualisation tools you'd recommend? 
I've used SPSS before and am trying to learn R. ",reddituser1357,2013-09-25 08:31:47
"My tool set is mostly Python and matplotlib, but I'm not sure I would recommend it as a general viz platform.  My answer depends on whether you will be using existing visualizations (so you want the richest libraries) or creating new and custom viz (so you want a language/library that supports development and abstraction).  I think Python and its ecosystem is better for the second use case.",AllenDowney,2013-09-26 06:50:37
[deleted],,2013-09-25 09:46:58
"Sadly, no.  But I have proposed doing it as a webcast for O'Reilly.  Will post here if it goes forward.",AllenDowney,2013-09-26 06:47:24
"This is part one, which presents the Bayesian analysis.  Part two, in a few days, will have the optimal bidding strategy.  If you can't wait, the whole thing is at

http://www.greenteapress.com/thinkbayes/html/thinkbayes007.html

As always, comments and suggestions are welcome.",AllenDowney,2013-04-22 11:40:07
"I love these kinds of examples. I'll have to read the rest of your text before I really grasp the material, though. =)",,2013-04-22 19:12:32
Thanks!,AllenDowney,2013-04-23 05:01:44
"Huh, interesting, thanks!

Does anyone know what number would minimize the discrepancy to the 3, i.e. sum |x_i - s|^3 ?

And what would happen for higher powers, 4, 5, etc?",goddammitbutters,2013-03-23 03:56:31
"I don't know of a word for it, but the 3rd power isn't quite as different from the mean as the other two.  sum |x_i - s|^3 = sum |x_i - s|^2 when x_i and s are 1 apart; the sum with the 3rd power is less than with the 2nd when x_i and s are less than 1 apart, and greater when they're farther than 1 apart; likewise between the 4th and 3rd powers.  As you go each power higher, you give further weight to outliers.  

Now you've got me curious about what statistic you get when you consider the infinity norm.  Edit: apparently the [positive infinity norm is the max and the negative infinity norm is the min](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=0CDoQFjAB&amp;url=http%3A%2F%2Fmath.bard.edu%2Fbelk%2Fmath461%2FInequalities.pdf&amp;ei=57GvUayAMcnlyAHd_oDoDg&amp;usg=AFQjCNHqbb3wm_lDdNHf6OmNHQZVK9nzBQ&amp;sig2=DcnYLtD3W7i53VdvxP2ehw&amp;bvm=bv.47380653,d.aWc&amp;cad=rja) [pdf warning].",incredulitor,2013-06-05 14:29:49
That is interesting. ,economicurtis,2013-06-07 12:12:26
That's really interesting! I wonder if it even has an analytic solution because it's nonconvex.,Derpscientist,2013-03-23 08:16:34
"I am pretty far removed from number theory, but doesn't our standard system break down if we allow 0^0 = 0?",makemeking706,2013-03-23 05:44:14
"Just interpret the notation as meaning the limit as we go to zero.

EDIT: Since it was apparently unclear, set d = |x - s| and then let epsilon go to 0 in d^e. This will tend to 1 unless d = 0, in which case it gives 0.",NOTWorthless,2013-03-23 08:12:00
That is quite the short cut. ,makemeking706,2013-03-23 09:14:10
"That's cool, but what's the limiting process? ",turnersr,2013-03-23 13:22:56
http://www.wolframalpha.com/input/?i=graph+lim+x-%3E0%2C+x^0 :o,Pas__,2013-03-23 14:33:33
I think you mean http://www.wolframalpha.com/input/?i=lim+x-%3E0%2C+0%5Ex (the limit from above is the relevant one),kjearns,2013-03-23 15:18:14
See my edited post.,NOTWorthless,2013-03-23 18:01:48
"Its like ""Jesus: how an outsider revolutionized Christianity"" 
",rightname,2013-02-18 08:37:48
"The linked blog is nonsense.  Fisher singlehandly *invented* what is now considered academic statistics.  He gave the mathematical derivation of the t distribution (after Gosset figured it out by curve fitting) and the F distribution.  He invented likelihood, maximum likelihood, Fisher information, consistency, and efficiency.  He was the first to use estimators (MLE) that were not an explicit function of the data (were found by maximization instead and had no closed form expression as a function of the data).  Perusing his papers, you find he was the first in many areas.  For example, the ""bandwagon of the aughts"" is mostly about regression and classification, and Fisher created the modern versions of these problems (regression with t and F) and classification (Fisher's linear discriminant analysis), although, admittedly, Fisher did not think of [regularization](http://en.wikipedia.org/wiki/Regularization_%28mathematics%29).  Savage's paper [On Rereading R. A. Fisher](http://projecteuclid.org/euclid.aos/1176343456) gives the case that he more or less invented mathematical statistics as we know it today.  For example, the whole  idea that the point of statistical inference is to estimate parameters in statistical models is mostly due to Fisher.  He didn't just introduce a lot of new ideas and methods, he fundamentally changed what the world thinks statistics *is*.

tl;dr.  Of course he was an ""outsider"" because statistics didn't really exist as a subject until he created it.

Edit: He was an ""outsider"" in another sense.  He never, even after he got very famous, held a chair in ""statistics"" because he was *even more famous* as the co-creator (with [Wright](http://en.wikipedia.org/wiki/Sewall_Wright) and [Haldane](http://en.wikipedia.org/wiki/J.B.S._Haldane)) of *population genetics* which unified and mathematized Darwin's theory of evolution and Mendelian genetics.  Fisher wrote around 125 statistics papers and around 500 genetics papers.  So he almost singlehandledly created mathematical statistics as we know it and also as part of ""Fisher, Wright, and Haldane"" created mathematical genetics as we know it.  One of the most amazing minds of the twentieth century.",berf,2013-02-18 06:06:45
"He also invented modern experimental design, which I think you missed (and, to put in perspective, would have *on its own* been an enormous historical achievement).",pseut,2013-02-18 21:18:12
"I didn't miss it.  I know that story well and didn't recount it because the post was already long enough.

Fisher did more than invent modern experimental design.  He also created that notion by writing a book titled *Design of Experiments*.  Before that book people interested in statistics (who were not yet ""statisticians"" because ""statistics"" as a dicipline didn't exist yet) had never thought of experimental design as part of statistics.  After that book they did.  This is interesting because most aspects of the design of a good scientific experiment are outside of the purview of statistics and statisticians (even today) have nothing useful to say on the subject.  So the subject is misnamed (due to the title of Fisher's book).  AFAIK Fisher didn't contribute to the literature on [optimal design](http://en.wikipedia.org/wiki/Optimal_design).

More interesting to me is that Fisher and Wright invented random effects models in genetics: [Fisher (1918)](http://digital.library.adelaide.edu.au/dspace/bitstream/2440/15097/1/9.pdf) was the paper that started the reconcilliation of Mendelian genetics and Darwin's theory of evolution and lead to a huge literature on what is now called [quantiative genetics](http://en.wikipedia.org/wiki/Quantitative_genetics) and Wright's invention of what he called [path analysis](http://en.wikipedia.org/wiki/Path_analysis_%28statistics%29) also published in 1918 lead to the areas now called ""structural equation models"" and ""causal inference"".  All of this was in the genetics literature and before statistics became a dicipline.  Later Fisher incorporated analysis of variance (ANOVA) and random effects models in his book *Design of Experiments* thus bringing this material to the attention of statisticians.

But you are right that Fisher did enough to make 10 or 20 people giants in their fields.",berf,2013-02-19 03:32:57
"Thanks for filling in those details -- I was sure you knew about his contributions, just wanted to mention it because you'd focused on his contributions to *mathematical* statistics and genetics.",pseut,2013-02-19 21:03:56
Thanks for the info and links!,amoeba,2013-02-18 15:48:22
"I don't think it's nonsense necessarily though, I found it to be a pretty interesting little set of stories and facts that give me a better idea of who Fisher was and how he was received etc. so I would say it's at the least an objective recount given by an apparently credible source. Not informative to the extent you've shared but still nice to see posted.",SpOoKy_EdGaR,2013-02-18 18:52:07
"I'm not sure how he's the ""ultimate outsider"" because he studied math and statistical mechanics. Statistics didn't exist as an academic discipline at the time, so math and statistical mechanics was about as close as you could possible have gotten. Fisher was an insider, not an outsider.

For comparison, Gosset studied chemistry and math, and Pearson studied math, physics, and metaphysics.",CrazyStatistician,2013-02-18 05:19:31
Non-Bayesian. :-(,Bromskloss,2013-02-18 04:26:09
it's not a race,,2013-02-18 07:21:37
Bromskloss is a little fanatical about Bayesian statistics. The other day he was [complaining about Andrew Gelman not being a true Bayesian](http://www.reddit.com/r/math/comments/183rr0/what_is_the_difference_between_bayesian/c8cf06l).,CrazyStatistician,2013-02-18 09:20:10
Sounds more like a frequentist troll to me.,,2013-02-18 20:35:34
"I see I have you flagged as friend, so be careful. Everything you say about me must clearly apply to you as well then. We're crazy and fanatical about this together, right? Right? :p

I have come to be convinced that the Bayesian view is the correct one. Therefore it nags me when people are doing things wrong. On the other hand, that opens opportunities to outperform them!

Btw, I most certainly pushed the friend button on you because you expressed a Bayesian sentiment.

Some day I'm going to collect evidence and make a case for why I think Gelman deviates from the straight-and-narrow Bayesian way. (Or it might turn out I have to change my mind.) Cheers!",Bromskloss,2013-02-18 13:00:38
The only real analysis that I think would be interesting with regard to masturbation is linking it to whatever else is going on in your life.  If there's some sort of other quantifiable aspect of your life that deals with stress (or whatever) like test scores or hours at work per day I'd include that.  ,FullSharkAlligator,2013-02-16 12:30:30
Blood pressure readings maybe? Cortisol levels?,DoorGuote,2013-02-16 12:31:46
"How about mood, self-esteem, feelings towards significant other/job/friends, social life....

Not necessarily easily quantifiable, but could produce much more interesting results. The main issue here is that you don't have a question you're trying to answer: you're literally just some guy who is planning on keeping a log detailing his masturbation. 

&gt; I plan on summarizing data every 6 months for anyone who's interested.

Unless you come up with an interesting question: no, no one's interested. Please don't keep us updated.",shaggorama,2013-02-16 14:42:40
"You're right, I should be more creative with additional factors. Right now I guess all I have is a baseline for time of day and week conclusions. That's why I'm asking for input....",DoorGuote,2013-02-16 14:45:50
"Social scientist in training here. How about these? Best to assess once towards the end of your workday to capture the overall sentiment of the day you had.

1. Do you feel good or bad? You can use a scale ranging from -5 (bad) through 0 (neutral) to 5 (good)
2. Do you feel activated and energetic? vs perfectly calm? (Use any scale you want. 1-5 is reasonable)
3. Do you feel competent and capable? vs not effective?
4. Do you feel connected with people and part of a valued group? vs feeling isolated and lonely?
5. Do you feel free and in control of your life and work? vs feeling trapped and unable to do things your way?

These few items essentially capture how you feel (mood: arousal and valence) and how satisfied your needs are (self-determination needs: competence, relatedness, and autonomy). There are legit psychological scales for these things if you want to publish your findings, but these one-item scales should suffice for personal use. 

Happy data mining!",schotastic,2013-02-16 19:41:46
"For only one person, a 1-10 scale of mood/social life/whatever may be enough because he isn't trying to compare how good/bad he feels as compared to other people, only as compared to himself. ",KalamMekhar,2013-02-17 10:25:55
[deleted],,2013-02-17 08:15:49
"In an ideal world that's a really good idea, but  measuring the load might affect the volume.  I can't speak for everybody but I know for myself when  I'm blowing a load onto the wood floor standing up, the load is usually much bigger and more powerful than if I do it onto my chest or into my hand.",DoorGuote,2013-02-17 08:42:36
It's also inconvenient to get it out of your SO afterwards. Especially if she swallows.,reallyserious,2013-02-17 10:02:33
A ladle with volumetric measuring capability?,DoorGuote,2013-02-17 10:19:59
"hahahahahaha! I can't believe I just looked at a plot of someone's masturbation data.

I noticed you didn't report that ""Type"" data.",galton,2013-02-16 12:31:50
"It's not too impressive at the moment :(. For science though, [here it is](http://imgur.com/pavQgOG). ""S"" = sex, ""M"" = masturbation.",DoorGuote,2013-02-16 12:33:30
"I can't believe I'm actually giving advice on your masturbation plots, but here goes: you should make the horizontal lines correspond to integers to make the plot easier to decipher.",galton,2013-02-16 12:54:05
1 sex! Nice bro!,towerofterror,2013-02-16 14:37:38
Yep I was so good she was satisfied for weeks...,DoorGuote,2013-02-16 14:44:36
[deleted],,2013-02-16 19:24:35
"Congratulations, but seriously: we don't care. You should also probably not tell the girl you're logging your orgasms, or she might be another.... outlier.",shaggorama,2013-02-16 21:20:12
"She's a stats major, so...",DoorGuote,2013-02-16 21:37:30
Or the opposite.  Maybe she'll want to be a influencing variable in your dataset.  ,madameinsanity,2013-02-16 21:27:50
"Woah dude, no one has said this yet but: if you want to know what useful data you should measure, you need to first clearly elucidate what the purpose of your survey is. Why are you collecting this data?
",Orophin,2013-02-16 16:28:55
"Because I just learned R and feel like collecting data on any and everything. Plus, I'm interested to see how I spend my work days, since i clearly masturbate at odd times of day. Due to other comments, though, it would be interesting to explore other factors like mood and how nc state is performing in basketball, etc.",DoorGuote,2013-02-16 17:09:02
I think this is awesome and there are a lot of interesting ways to take it. ,MZITF,2013-02-16 22:10:37
"Maybe treat it as a repeated time to event analysis? See how long you go from one to the next, on a Kaplan-Meier curve?",Neurokeen,2013-02-16 13:28:18
"Duration?  I've been meaning to [attempt to] stopwatch myself, so thanks for reminding me. 

I don't know your sex - but as a female, I've definitely had orgasms where I've eventually managed to wonder how many seconds they were.",madameinsanity,2013-02-16 13:55:53
"Duration of make orgasm is fairly regular I think, so that would be uninteresting, not to mention the awkward process of stopwatching my load-blowing. Maybe time if total jerk off session would be interesting: how long of a buildup? Was it a quickie at work or a two hour spank sesh?",DoorGuote,2013-02-16 18:32:16
"Fascinating.  I should edit and say I'm a gay female, with zero personal exposure to guy orgasms.  Does orgasm = ejaculating?

",madameinsanity,2013-02-16 21:23:01
"Yep the orgasm lasts only about 15-20 seconds, most of it comprising ejaculation. ",DoorGuote,2013-02-16 21:36:46
[deleted],,2013-02-16 14:50:18
"True, but I'm slightly worried about my middle of the work day fapping...",DoorGuote,2013-02-16 14:52:41
Currently no visuals of time &amp; day correlation.  Maybe a dotplot with a time continuum on the x-axis?,Thebestfrenchie,2013-02-16 14:58:12
"I work with model building.  You could add some predictors and get really crazy.  Collect data on what you think predicts masturbation-working out, what you ate, number of hours of sleep, etc. You can easily do some multivatate analysis, or a path model. Could be interesting. ",in_hell_want_water,2013-02-16 18:32:55
"Ooh agreed. Keep track of your sleep patterns in general. See if masturbation has any significant effects on sleeping, or if sleeping has significant effects on masturbation.

Morning glory, anyone?",statbro,2013-02-16 21:45:45
"Autocorrellation. Weekdays, holidays",Bob_goes_up,2013-02-17 02:51:56
Good question! I would ask /r/nofap.,djsopkin,2013-02-16 12:56:42
I posted there. Got a downvote. Realized what the subreddit was all about...haha.,DoorGuote,2013-02-16 13:27:09
You should try again. I'm sure people would be totally willing to participate.,djsopkin,2013-02-16 14:22:35
Are you Tyrone Slothrop?,TimofeyPnin,2013-02-16 20:20:46
"I would honestly look at the time period in which it occurred the most often (Tukeys test for the days). Then try and decide other variables that may effect your design. These may include hours of recreation, number of sexual innuendos you are exposed to, measurement of social contact, and possibly work hours or school hours. The interaction and correlation of any variable due to your data may be interesting.
",themasont,2013-02-16 23:44:18
"Use a pedometer? 

Sorry, couldn't help it.",griffer00,2013-02-17 09:23:58
"cool use of ggplot.

But I would be much more interested in seeing the correlation between the flesch-kincaid score and 'speech quality'. I find it dubious that there is a connection, and I think that makes this a misleading study.

Also I wonder what role all those speeches making it onto youtube had on the drop in average speech complexity.",DrWyckoff,2012-05-24 13:06:04
"[It turns out that it's kinda bullshit.](http://languagelog.ldc.upenn.edu/nll/?p=3970)
The test they use to figure out the grade level of the speech is 0.39 * (Words/Sentences) + 11.8 * (Syllables/Words) - 15.59.

Like it says in the Language Log article, ""...not only doesn't it pay any attention to the structure or content of sentences, it doesn't even take any account of word frequency.""

EDIT:Spelling mistakes",adamr1337,2012-05-24 15:04:33
it says that stuff in OP too,imh,2012-05-24 19:18:47
"Dan Lungren, you speak so purty!",,2012-05-24 13:07:37
dat Pearson r.,negative_epsilon,2012-05-26 06:19:17
"Pretty badly biased.  First there is the equation that he uses which is essentially meaningless.  It is all about using multisyllable words.  In my experience, people who use ""big words"" to sound smart are usually about average.

Then in his ""cleaned up"" version he crops off someone, most likely someone whose politics he disagrees with.

Lastly I don't think that this metric is even a good idea.  Politicians should be using simple speech so that people can understand them.  He mentions that Rand Paul targets the lowest level.  Well of course, because Ron Paul (by any subjective measure) targets the highest level.  Look how that worked out for Ron.  Rand is taking the opposite approach and bringing it down to a level that regular people can understand.",,2012-05-26 20:06:10
"The Broom package in R seems pretty useful

https://github.com/dgrtwo/broom

Edit. Also, the last point (point 37) he mentions links to a paper from 2006... That was disappointing for me
",I4gotmyothername,2014-12-17 13:48:24
"I've written my own functions for group tests (ttests, chisq, anova), linear regression, logistic reg, and Cox reg to do a whole bunch of univariate tests, pull out relevant output and make one nice table. 

I'd be surprised if more people don't have such functions written

Always thought about putting all of my functions into a package but haven't taken the time to learn how to make packages. ",DontBendYourVita,2014-12-17 15:58:23
There are two links in point 37. One to a paper from 2014 that I think is really good and another to an older paper (2006).,Mockingbird42,2014-12-17 18:11:28
ahaha oh.. Thanks :P Late-night browsing fail,I4gotmyothername,2014-12-17 22:16:58
I like more or less presented by Tim Hartford done by the BBC. It looks at numbers in the media and works out whether they are reliable. Unfortunately it is on a series break at the moment. ,Ajubbajub,2014-10-26 00:32:36
"It is on a break but there's a large amount of archived content that's freely available. 

http://www.bbc.co.uk/podcasts/series/moreorless

Definitely worth a listen.",Boeing,2014-10-26 01:22:57
I love it but most of the stuff is quite topical so it may seem a bit old. ,Ajubbajub,2014-10-26 03:46:57
You have analysis from the BBC too,bigmansam45,2014-10-26 07:53:14
"The Data Skeptic has some interesting stuff sometimes.  

The hour long posts are the best; the 15 minute mini-episodes tend to be pretty basic in terms of stats material (confidence intervals, p-values, etc.).",McHeathen,2014-10-26 08:59:00
The Economist has an audio edition. The finance and economics section is often quite interesting and delves into research. ,,2014-10-25 23:41:13
"I've always loved [Freakonomics](http://freakonomics.com/radio/freakonomics-radio-podcast-archive/), but it's definitely more broad economics than stats.",Foggalong,2014-10-26 08:18:34
"While being a fan of Freakonomics, I have to say that I switched to the Planet Money podcast and find itch better. NPR's Planet Money covers more important economic news of the day, whereas Freakonomics tends to focus on cutesy microeconomic quirk.",selectorate_theory,2014-10-26 09:14:57
"[Simply statistics]( http://www.biostat.jhsph.edu/~rpeng/podcast/simplystatistics_audio.xml) put out a handful of audio podcasts (maybe just 10 epidosdes ?) a few years ago that were worth checking out.  It was good, particularly the seventh epsidoe in which they discuss the Reinhard-Rogoff controversy. I don't know why the podcast fizzled outsid. ",RVAHokie,2014-10-26 11:07:20
"I've never seen this point framed in an explicitly bayesian way, so that was very interesting. Coming from and epidemiology and quasi-experimental design background, I'm used to thinking about cause in a synthesis of potential outcomes (from Rubin's causal model) and Judea Pearl's work on directed acyclic graphs. I agree that correlation provides evidence of causation. In terms of epi language I would describe a correlational estimate as a confounded (probably) causal estimate. More specifically, a given correlational model necessarily implies causation given a set of structural assumptions (as described by Pearl). Whether those structural assumptions are believable is where things gets complicated (and the real work of science gets done). Good study design helps make the argument that the structural assumptions are accurate. There is a nice illustration in Modern Epidemiology (Rothman and Greenland) using Pearl's framework of why the randomized control trial actually provides a causal estimate (the design forces reality to conform to a certain set of structural assumptions that lead to an unconfounded estimate of the causal effect). When we see a correlational model our reaction shouldn't be ""correlation does not imply causation"", but ""under what circumstances would this model provide an unconfounded causal estimate, and are those circumstances believable."" ",Case_Control,2014-02-20 12:04:36
"This is not too different from the standard view in medicine, though usually phrased in more of a discrete ""levels of evidence"" sense than a Bayesian sense. While direct causal evidence is the gold standard in medicine, correlational studies are still taken as providing some evidence that is sometimes worth acting on, in the absence of better evidence. For example, correlations with negative health outcomes are sometimes taken as reasons to issue recommendation to avoid certain behaviors/drugs/foods (pending further data), and unexpected correlations are often taken as good justification for funding further studies into a relationship.",kmjn,2014-02-20 13:56:37
"What OP fails to realize is that causation is not a statistical problem nor can be it solved by statistics. Causation is a methodological problem that is solved by design; most popularly solved by random assignment (among others). 

One of the core requirements of any causal relationship is that A and B are correlated. If they are uncorrelated, they cannot be causally related. This is Research Methods 101.  To propose that a correlation is evidence of a causation is absolutely correct, though it is hardly novel or useful. It doesn't bring you, in any meaningful way, closer to finding a causal link other than ruling out that possibility of a causal relationship. 

The most important thought one can have about causal relationships is whether plausible alternative explanations exist that describe the relationship between A and B. If others exist, causality cannot be established.",Palmsiepoo,2014-02-20 17:33:58
"This! Statistics simply isn't a causal science without rigorous research design and yes, random assignment, no matter how much you want it to be. ",the_spider_knows,2014-02-21 06:22:23
"I don't know what a causal science is, so I don't know whether statistics is one.  All I'm saying is that observing a correlation is evidence for a causal relationship, in the Bayesian sense of ""is evidence for"".  This is an epistemological claim, and if you accept the Bayesian framework, it is provably true.",AllenDowney,2014-02-21 07:06:48
"I don't agree.  Without a model, none of your likelihoods have any meaning. Can you explain why L(D|A) = 1?  Just from correlations in D, it seems that this is impossible to conclude.  It seems to me that you are imposing a model by assuming the way hypothesis A would affect your data.  That is fine, but then your statement should read: correlation and a particular statistical model are evidence of causation.  The pure data correlations are NOT evidence of causation, unless you want to get meta on me and talk about the set of all possible statistical models.  Good luck putting a measure on that to define Bayes rule.  

To put it in Pearl's language, we cannot say anything about causality without first imposing a model on the data.",veryshuai,2014-02-20 13:25:09
"I agree that all of this is mediated by modeling assumptions.  If you want to say anything about the world, there's no getting around that.  The reason I say that L(D|A) is approximately 1 is that if A actually causes B, then the chance of seeing a correlation is high, provided that you have enough data.  I think that's not a controversial claim, so I am a little surprised that you object.",AllenDowney,2014-02-20 14:19:18
Just start from the most general causal model on two continuous variables: a change in X causes a change in Y. Then linearize it to get strong correlation. Why is that such a stretch?,Transceiver,2014-02-20 21:04:06
Maybe it should be framed in the opposite direction.  Lack of correlation is evidence for a lack of causation.  ,crazytimy,2014-02-20 11:58:02
"You could have a situation where there was no observable correlation, but  there is a true causal effect if suppression was present. I don't have a good handle on how commonly that would happen in practice though. Mackinnon has a good article describing suppression, mediation and confounding:


http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2819361/",Case_Control,2014-02-20 13:18:47
The correlation coefficient only measures linear relationships. Any non-linear relationship may lead to a low correlation (like y=1/x).,entylop,2014-02-20 19:40:10
"Yes, that would be another reason for no correlation but a causal relationship.",Case_Control,2014-02-20 19:49:20
"There are a variety of ways in which this is a poor argument. The most important is the meaning of the word ""cause,"" which has not been rigorously specified in this framework. This provides a degree of cover: The four scenarios listed in the post purport to indicate differing models, but in fact, they only do so if we limit ourselves to an entirely categorical and deterministic definition of ""causation."" In practice, almost every scientific model of interest is probabilistic, which muddies the waters considerably. At the very least, the six arrows connecting A, B, and C each need their own parameter, and the alternatives listed by in the original post merely correspond to certain ranges of parameter values, themselves more or less arbitrarily defined.

For example, let us consider the problem of simultaneous feedback. It could easily be the case that A influences B *and* B influences A. Such a feedback model either fits into two of the listed categories or none of them, but would have quite different implications from simple direct causation, or from a model in which C causes B and A (while the two never directly interact). Furthermore, A -&gt; C -&gt; B is *not* the same as A -&gt; B in any model that has a probabilistic element, because of the propagation of error. Mistaking one for the other has dire medical and policy implications, so the glib invocation of transitive arithmetic shouldn't pass muster.

Yet another point is that even if we limit ourselves to the four alternatives listed (which I maintain is at best incomplete), the Bayesian argument is intellectually dishonest unless we consider not just C, but D, E, F, and every other potentially contributing factor. This is why endogeneity is toxic to causal claims: [Correlation alone, as formally defined, won't converge on the true parameter if its assumptions are violated](http://www.youtube.com/watch?v=dLuTjoYmfXs).

Anyone familiar with Granger causality will be familiar with these discussions, which are not especially new. Granger himself has been careful to point out that sloppy attempts to infer causal structure from elaborate patterns of covariance can easily lead to ludicrous claims. Framing the conversation in such as way as to imply to a lay audience that we may treat Pearson's *r* as a signpost towards a true model does more harm than good.",belarius,2014-02-20 14:20:32
"Thanks for this thoughtful comment.  Two points I want to reply to:

1) You say that my analysis assumes that causation is deterministic.  That's not the case.  I didn't say explicitly that I was talking about probabilistic causation, but I did talk about noise and the probability of seeing a correlation if A causes B.  I don't think my argument depends on deterministic causation or categorical data.

2) You are right that I treated the hypotheses as mutually exclusive when in fact they are not.  That was a simplification, but my argument generalizes easily to additional hypotheses like A&amp;B, A&amp;C, etc.  And as I mentioned in the article, it also generalizes to multiple confounding factors.
",AllenDowney,2014-02-20 14:34:25
"As I'm sure you're aware, ""causation"" is a slippery concept, and one that is difficult to define in quantitative terms. It's especially bad when the propagation of error must be taken into account (as Granger causality attempts to do). If there is an aspect of the phrase ""correlation doesn't imply causation,"" it is the word 'causation,' more than the word 'imply,' that gets people in trouble, especially those who are only just beginning to learn statistics. Perhaps a more truthful (but more patronizing) statement for most people would be ""observing a correlation doesn't give you license to claim that whatever you believe 'causation' might mean was what generated that correlation.""

Since we agree that the various models are not genuinely independent, a fruitful line of discussion would then be to define the possible state space within which the models reside. A network of three nodes, each connected to the other by a pair of arrows (representing some functional relationship), is a good place to start (although more work would be needed to rigorously specify time). This would allow us to define more clearly what we mean by a ""cause."" It would also let us evaluate which scenarios are more likely to be true given an observed correlation.

Following that line of logic, I anticipate two results. First, although the subset of models that we might agree can loosely be defined as ""causal"" might yield a correlation, but so too would relationships that are far removed from qualifying as causal. That is, if I have a hypothesis that A -&gt; B, and I observe a correlation, it might in a loose sense be construed as evidence in favor of that causal model, but would *also* be evidence in favor of a very large number of other scenarios. So, ""correlation implies causation as well as a bunch of non-causal things, and never quite rules out accident or data contamination;"" ergo, don't presume causation.

The second result relates to increasing the complexity of the scenario. The problem of evaluating the set of possible models can be likened to the problem of computing scattering amplitudes using Feynman diagrams: the larger the number particles you have interacting, the larger the number of ways that interactions manifest, and the more difficult it is to evaluate the integral. Even give five or six nodes, the state space of possible models becomes too overwhelming to work with in brute force terms, especially given that the functional relationships between most measurements are much less well defined than those used in quantum mechanics. This presents real challenges to model selection schemes that rely on convergence ""across all possible models."" The number of *possible* models grows so fast that the kind of ""evidence"" provided by observing a correlation somewhere in the subspace doesn't make much of a dent. A well-designed experiment is a much more effective method for achieving that kind of reduction of the model space.",belarius,2014-02-20 16:25:03
"Without being rigorous about it at all, this is the way I visualised OP's argument.

Your state space (with however many nodes and arrows) can be divided into those that have some arrows connecting (directly or indirectly) A and B, and those that have no arrows connecting A &amp; B, i.e. no path, in which case any correlation observed in a sample would be due to sampling error.

Say you take a sample, and calculate the correlation between A and B. The further the correlation is from 0, the greater the evidence in favour of the former set of models relative to the latter. Since the former set of models are the only ones which contain a true causation (direct arrow) between A and B, your sample will have provided some evidence of causation.

Forgive me if there are glaring logical gaps in my reasoning, I'm open to being corrected!",red_concrete,2014-02-21 04:11:46
I agree.  Thanks for this explanation.,AllenDowney,2014-02-21 04:44:06
"Oh my.

As we all know,

P(A|B) = P(A&amp;B)/P(B)

So

P(A|D) = P(A&amp;D)/P(D) &gt; P(A) -&gt; P(A&amp;D) &gt; P(A)P(D) which implies the events are not mutually exclusive.

You've used the fact that the sets have mutual information to show causation, when in fact that is only correlation.   Correlation is a measure of mutual information.

None of this implies causation, or is mplied by causation.

To prove causation, you need to show a physical link.  It has nothing to do with statistics, bayesian or otherwise.

This post is abuse of notation.  The correct interpretation is to posit causation a priori based on the physical rules of the system being investigated.  Correlation can then be used as evidence.  Correlation on it's own is not a measure of causation.




",drunken_Mathter,2014-02-20 13:54:11
"In your reply, you used several of the phrases I have tried to clarify.  If by ""imply"" you mean ""mathematically prove"" then I agree with you, but that's what I am calling trivially true, because we can't mathematically prove anything about the world.  You also said that correlation is not a measure of causation.  I'm not sure what you mean by that.  The claim I am making is that correlation is evidence of causation, and I explain in the article what I mean by that.

About the other point you raised, you are right that I am treating the hypotheses as mutually exclusive, when in fact they are not.  This is a simplification, but the argument I made generalizes to the case where we also consider the combinations A&amp;B, A&amp;C, B&amp;C, etc.",AllenDowney,2014-02-20 14:24:28
"Correlation is a mathematical function.  It maps R^n -&gt; R.  That's it.  It is a measure of mutual information, which, by defintion, is the intersection of two sets.  Mutual information is why P(A&amp;B) &gt; P(A)P(B).  This is all rigorous mathematics.  The fact that the sets overlap does not say anything about causation.  Correlation measures the size of the intersection of sets.  That's it.  It cannot be used as evidence of causation.  Period.  Causation can lead to correlation, so if someone proposes a model which establishes causation because they find a correlation, that's fine, but the causation must exist a priori, and must have a reasonable natural basis.  You are implying that correlation, when found, is *evidence*of causation.  This is false. You must apply a reasonable basis. If this is what you intended, then the language you used is quite sloppy, as it's easy to come away with the impression that Bayesian methods allow one to use a correlation to imply a causation.


I appears to me that you define a change in the *estimation* of the correlation as an indication of causation based on an update of information to a naive prior.  This is the fundamental flaw with using priors.  If your prior is too big, you will see a reduction in the estimation.  If it's too small, you will see an increase.  However, the update to the estimation does not indiate any change in the actual size of the union.  It is only a change in the estimation.  It does not indicate a causal link.

Your experiment cannot reject causation, it can only reject your initial estimation of the correlation.  Correlation cannot define dependence, and therefore cannot measure causation.  Only once dependence has been established does the mututal information refer to a causal effect.  Correlation plays no part in this, only the assumption of dependence.


",drunken_Mathter,2014-02-20 15:22:02
"The computation of correlation is a mathematical function, yes, but collecting data is an observation about the world.  If you collect data, and compute a correlation (by computing a mathematical function), I don't see why you can't interpret the result as evidence about the world.

Or are you claiming that any observation process that involves math can't be used as evidence?",AllenDowney,2014-02-21 03:42:50
Suddenly: outrage,derhexer,2014-02-20 12:00:11
"This topic has come up several times on this forum, so this article might not be surprising to people here.  But it came up in class recently, so I wanted to write about it.  And I think the Bayesian framing helps clarify the issue.  Comments welcome, as always.",AllenDowney,2014-02-20 11:10:26
"It's good to get a Bayesian perspective on things, but be careful you don't re-invent the wheel; there's a whole theoretical framework invented by Judea Pearl that formalizes causal relationships and their inference, essentially addressing exactly the same kinds of questions that you addressed. As far as I know, the most important source for causal theory is Pearl's book [Causality: Models, Reasoning, and Inference](http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/0521773628). In my opinion, the best source of information for the practice of conducting causally-informative research is Hernan and Robins' not-yet-published book [Causal Inference](http://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/), which has drafts of the first two thirds put up online, along with syntax in several statistical languages! I love Hernan's book, and have already consulted it in my research.",w1nt3rmut3,2014-02-20 12:11:28
"I emphatically disagree with this. Causation, from a statistical perspective, is based solely on experimental design. That is, an experiment wherein there is a _true_ independent variable. That IV must be manipulated by the researcher to show an effect. If the manipulation shows an effect -- any approach now implies causation, because the researcher designed the experiment to cause an effect. 

If that is done, any statistical approach used—that shows some level of reliable finding—implies causation. 


Causation is independent of measures, metrics, and tests. To present the idea that causation is implied through correlation _via Bayesian methods_ is disingenuous. Causation has _nothing_ to do with the Bayesian vs. Frequentist fight. 

",dearsomething,2014-02-21 05:44:22
"There is one giant glaring hole in your arguement. No offence but I think you made it becuase you don't fully understand the concept of hypothesis testing. Mathematically, in a hypothesis test, all of your hypotheses shoulod together make up your parameter space. In layman's term, that means your hypothesis together should contain all of the possibilities. However, you've failed to give a rigorously definition of the word ""cause"", which leads to ambiguity to what your hypotheses mean, and I doubt you can rigorously define the word ""cause"" in a way as to make your hypotheses work. So basically, what you have done here is akin to asking the question ""Have you stop beating your wife"".",Mabuss,2014-02-20 14:51:44
"Thanks for posting this. I enjoy your blog quite a bit, but I sort of forget to read blogs. So I was happy to see this pop up here. ",WallyMetropolis,2014-02-20 17:54:15
Are you asking for counter examples from the natural sciences?,westurner,2014-02-20 19:02:03
"Here's one:

A: Patient takes heart medication

B: Patient has heart disease

How does correlation imply causation in this counter example?",westurner,2014-02-20 19:53:28
"This is a case where, in my framework, the prior for B-&gt;A is much higher than the prior for A-&gt;B, so even after seeing the correlation, the posterior for B-&gt;A is higher than A-&gt;B.

In fact, if you can rule out A-&gt;B a priori, then the posterior probability is also 0.",AllenDowney,2014-02-21 03:47:08
"http://en.wikipedia.org/wiki/Cox%27s_theorem

Maybe ""Correlation may indicate plausible correlation""?

... There's an assumption of complete and perfect information that is not valid.",westurner,2014-02-21 03:59:07
"I agree with the sense of your friendly amendment, but I would leave the word ""plausible"" out because it could be confused with background information you might have about which hypotheses are more likely a priori.  I chose ""is evidence for"" because in a Bayesian framework it has a specific meaning.",AllenDowney,2014-02-21 04:50:44
"This may be a question for a different avenue: how is that not confirmation bias? (e.g. ""common sense"" from Cox's theorem; and t) Is this a framework for probability logic?",westurner,2014-02-21 12:26:55
"It would be confirmation bias if I used my prior beliefs to select supportive data or exclude contradictory evidence.  But in the example, I am not doing that.  Nevertheless, it is an inevitable feature of Bayesian analysis that the posteriors depend on the priors.  If the priors are subjective, so are the posteriors.  The only hope is that with enough data, the priors are ""swamped,"" so people with different priors would converge to small differences.

I think of Bayesian analysis as a kind of probabilistic logic, but I think it is not officially considered one -- I don't know why not.",AllenDowney,2014-02-21 13:09:33
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Cox's theorem**](http://en.wikipedia.org/wiki/Cox%27s%20theorem): [](#sfw) 

---

&gt;

&gt;**Cox's theorem**, named after the physicist [Richard Threlkeld Cox](http://en.wikipedia.org/wiki/Richard_Threlkeld_Cox), is a derivation of the laws of [probability theory](http://en.wikipedia.org/wiki/Probability_theory) from a certain set of [postulates](http://en.wikipedia.org/wiki/Postulates). This derivation justifies the so-called ""logical"" interpretation of probability. As the laws of probability derived by Cox's theorem are applicable to any proposition, logical probability is a type of [Bayesian probability](http://en.wikipedia.org/wiki/Bayesian_probability). Other forms of Bayesianism, such as the subjective interpretation, are given other justifications.

&gt;

---

^Interesting: [^Richard ^Threlkeld ^Cox](http://en.wikipedia.org/wiki/Richard_Threlkeld_Cox) ^| [^List ^of ^probability ^topics](http://en.wikipedia.org/wiki/List_of_probability_topics) ^| [^Cox ^\(surname)](http://en.wikipedia.org/wiki/Cox_\(surname\)) ^| [^List ^of ^theorems](http://en.wikipedia.org/wiki/List_of_theorems) 

^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfky0q1) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfky0q1)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 65459:%0Ahttp://www.reddit.com/r/statistics/comments/1ygpqa/correlation_is_evidence_of_causation/cfky0ne)",autowikibot,2014-02-21 03:59:16
"Here's another:

A: Patient suffered abuse (physical, verbal, psychological)

B: Patient experienced head injury (TBI, CTE)

C: Patient avoids contraindicated medication

D: Patient has PTSD and depression

E: Patient played sports

I suppose the data here is sufficient.",westurner,2014-02-20 20:03:33
"I see correlation as ""guilt by association"", which is a logical fallacy but has some value just ruling other aspects out. You use a correlation to justify further investigation. Imagine you're investigating a crime, a witness says the suspect is tall. This rules out short people and the footage you have shows quite a few short people ruling them out is good.

Next you learn that the suspect is male, brown haired and white. There are only 4 or 5 people that match this description in the area at the time. Then you find they had a briefcase, you find one person with all of these properties. None of this proves anything in a conclusive sense, but it would give you someone to question further. ",jazimov,2014-02-21 01:18:40
"Guilt by association is a logical fallacy, but a valid statistical inference.  People who associate with criminals are in fact more likely to be criminals, so criminal association is evidence of criminality.  It is not particularly strong evidence, and in the interest of justice we exclude it from criminal proceedings.  But it is evidence in the sense I defined in the article.",AllenDowney,2014-02-21 03:49:36
"Before I'm ready to make inference on causal relationships, I'd like to be convinced that there is such a thing as causality. What if everything is just conditional probabilities?

Could anyone point to literature which establishes that causal relationships are real things? That would be great. I don't mean literature on how to make calculations with causal structures, for example to reject one in favour of another, but literature that will convince me that it is at all sound to talk about causality.",Bromskloss,2014-02-20 17:37:08
"Do you take medicine when you are sick?  If so, you believe causality exists.",AllenDowney,2014-02-21 03:44:53
"Or is my taking of medicine today caused by me getting well tomorrow? The correlation has no direction.

Strictly speaking, this could be what you meant. I just assume you meant it the other way around.",Bromskloss,2014-02-21 03:57:03
"Sorry, my reply was too glib.  I should have explained what I mean by causation.  The definition I had in mind is pragmatic: to say that there is a causal relationship between A and B is to make a prediction that if A changes, you expect to see a change in B.

(There are problems with any definition of causation, but this one does pretty well for most practical purposes.)",AllenDowney,2014-02-21 07:11:07
"&gt; to say that there is a causal relationship between A and B is to make a prediction that if A changes, you expect to see a change in B.

What if A and B do not cause each other, but are both caused by C? A change in A could still be used to predict a change in B. Whenever I look at the issue, causality seems to fade away and all that remains are conditional probabilities like P(A=a|B=b) and P(B=b|A=a).",Bromskloss,2014-02-21 07:20:14
"To be clearer, I should have said an exogenous change in A.  In your example, if C causes A and B, and something comes along and changes A (but not C), we would not expect a change in B.

Again, I know that this is a philosophical problem we could talk about forever.  But for practical decision-making at medium sizes and low velocities, the common sense notion of causation is good enough.",AllenDowney,2014-02-21 07:43:50
"&gt;  But for practical decision-making at medium sizes and low velocities, the common sense notion of causation is good enough.

I would perhaps take that statement further and say that we don't need any notion of causation at all, since it seems, from my perspective, that conditional probabilities are everything we use for prediction in the end anyway. That's also where I begin to question if causation is a thing at all.

Anyway, have a good day!",Bromskloss,2014-02-21 08:09:27
"Ah, now I see what you mean.  That's interesting.  I will have to think about that.",AllenDowney,2014-02-21 08:34:20
"I agree, but that's not causation. I'm sorry, but you are just playing semantics. Unless you can control all your variables in experiment, then all you ever prove is a high degree of coorelation.

My favorite fallacy has to do with low-fat eaters seeming healthier. In many trials they fair better than more moderate eaters. A second one is in the nurses study, where increased intake of estrogen was correlated with decreased heart attack. Lets take the second as an example.

There was a follow up clinical trial, where a controlled group of women took estrogen. It turned out that it actually increased rate of heart attack by over 40%.

So, using your language, we would end off saying something like this: 

1) according to study A - estrogen causes prevention of heart attacks 
2) according to study B - estrogen causes heart attacks

Which, obviously, leaves something to be desired. The current climate requires that causation be demonstrated in controlled scenarios, and it is a reserved word for exactly the purpose of disambiguation above. In general, causation relates to our ""most strong"" empirical theory to date, and not all of them. For very good reason.

To redefine causation even in a strong correlative sense is a serious fallacy.

Source: Researcher in machine learning (!) hey whatap :)",,2014-02-20 15:00:16
"You've missed the point by a lot. There isn't a claim that correlation *proves* causation. Just that it is evidence in favor of causation. That is, if you see correlation, there's more reason to think a causal relationship exists than there was before you knew if there was a correlation. But saying that something is more likely is very different from saying something is true. ",WallyMetropolis,2014-02-20 18:09:54
"Then, yes, I totally agree. lol.",,2014-02-20 18:12:54
"&gt; Source: me!!!

god...",ctphoenix,2014-02-20 19:46:48
so many wise words used to prove nonsense...,jedruch,2014-02-21 04:34:24
"Do you mean me or the guy who just proved that nothing is evidence for anything?

Or do you mean all of us for arguing with people in the Internet?",AllenDowney,2014-02-21 04:52:44
"Considering the number of ""inferior"" languages that are still in widespread use in general, I highly doubt it. When businesses are committed to a specific technology, it's very difficult to kill it broadly.",alwaysonesmaller,2013-05-14 11:50:36
Absolutely. I'd be surprised if I saw a sudden shift in clinical trials away from SAS.,Zeurpiet,2013-05-14 12:01:34
[deleted],,2013-05-14 12:09:49
"digging out 9.3 manuals:

&gt; SAS/STAT software provides Bayesian capabilities in four procedures: GENMOD, LIFEREG, MCMC, and PHREG. The GENMOD, LIFEREG, and PHREG procedures provide Bayesian analysis in addition to the standard frequentist analyses they have always performed. Thus, these procedures provide convenient access to Bayesian modeling and inference for generalized linear models, accelerated life failure models, Cox regression models, and piecewise constant baseline hazard models (also known as piecewise exponential models). The MCMC procedure is a general procedure that fits Bayesian models with arbitrary priors and likelihood functions. 

do clue how well that MCMC is",Zeurpiet,2013-05-14 12:24:55
[deleted],,2013-05-14 12:40:39
I found that so interesting I made it a separate posting. ,Zeurpiet,2013-05-14 13:10:05
"&gt; SAS/STAT software provides Bayesian capabilities in four procedures: GENMOD, LIFEREG, MCMC, and PHREG. 

BUGS-like languages provide Bayesian capabilities in a countably infinite number of procedures. 

To answer you question, SAS implements adaptive metropolis (Haario).",glutamate,2013-05-15 00:25:39
"I know about bugs and jags, not how MCMC compares with it",Zeurpiet,2013-05-15 09:41:30
"25+ year SPSS user here.

I think it's important to distinguish between what is being used by academics who public research in the mainstream statistics / math / ML journals versus those who use a software tool and are publishing outside of that core area.

It is rare in psychology to document the analytical tool you are using -- it's assumed that you are using SPSS or SAS in most cases (although I'm sure R has become more popular, but I've been out of the academy for about 15 years).  And SPSS was and likely still is far more popular in psychology because the academic site license was less expensive, the GUI easier to navigate, and the software overall easier to use.

Outside of academia, IBM is making a big ""vertical integration"" move whereby they sell the hardware, the data management system, the workflow management and document management tools, the analytical tool(s), and consulting services to large companies that are getting tremendous leverage from advanced analytics.

In all that SPSS is called out as the analytical core, but that's just one part of the entire suite of hardware and software, and IBM is an 800 pound gorilla.  They want to dominate and from what I see they have beach-heads in government and industry that allow then to then move in SPSS as needed.

I don't disagree that R is on the up-swing, but for most of what needs to be done in industry, straightforward SPSS analytics gets the job done, and R programming runs the risk of going down a rabbit hole of seeking precision, but not having the ""bells and whistles"" that an executive wants to see when making a strategic decision on what software to deploy.

Also, I do think IBM got a bit aggressive in up-pricing their academic versions and will revert to lower pricing once they have pummelled SAS.",wil_dogg,2013-05-14 11:13:47
"I agree with everything you've said. 

Coming from psych, no one cares what program you use to run a 2x2 ANOVA, because the study design is typically the focus for your paper, not fancy analytics. You don't need to cite SPSS, because there isn't much of a difference between SPSS results and R results. But yeah, most psych papers use SPSS. 

That's because psychology majors aren't statisticians and need something that's easy to use and (more importantly) easy to learn. No one wants to teach psych students stats at the same time as SAS. That's basically a nightmare. ",Iamnotanorange,2013-05-14 13:07:42
"this might explain a lot of the systemic interpretational problems with psychology research.

The field seems to be in an epistemological crisis.",,2013-05-14 20:22:17
What are you talking about?,mountaindrew_,2013-05-15 03:38:22
I am so confused in SPSS but to me R feels really intuative. How weird.,BullNiro,2013-05-14 20:34:23
"Strange, where I did an internship at the University of Tübingen in Germany, they now teach R to all the students and use it almost exclusively, so times are a changing.",,2013-05-15 04:59:02
"I've seen what they're doing with SPSS at IBM. Most of it is a joke. The problem is that the people they are selling SPSS to are even more clueless. They spend thousands of dollars on SPSS specialists(who often don't know statistics) and the software, instead of hiring one qualified individual who can easily plough through their data and give them what they need.",,2013-05-14 19:52:43
"Sounds like you have a lot of experience.  Give me an example where you have ""easily plough(ed) through data and give(n) them what they need"".

Those of us who do this for a living know there is nothing easy about working with industry data, getting the analysis right, automating the process, and setting up a decisioning framework.  You don't hire one qualified individual -- even a small scale deployment requires analysts, programmers, and process managers.

That doesn't mean that IBM is weak when it comes to stats skills of their consultants and front-end sales staff.  I'm just calling your bluff on you thinking you know how easy it is and that hiring one R programmer will solve the problem.",wil_dogg,2013-05-14 20:15:01
So have you actually used SPSS when *you've* worked with industry data?  ,flyingbrotus,2013-05-14 22:50:39
"For about 15 years, yes.  13 years in banking, 2 years in healthcare analytics.",wil_dogg,2013-05-15 04:56:35
"A solid R programmer that knows his statistics can easily beat out a team of 'analysts' that don't know anything beyond pushing buttons. At our company we had a team of four developers and analysts try to solve a user optimization problem and spent three weeks and barely got anywhere. A computer scientist came in and dealt with it in a couple of days. 

I've dealt with SPSS consultants. Few know their basic statistics. Not to mention it doesn't lend itself well to automation and flexibility. You're stuck in their framework. People that know statistics, know R, can work in Linux, know SQL and noSQL, can generally going to be more capable than the consultants.  ",,2013-05-14 20:30:55
"SPSS (as well as SAS, to an extent) are focusing way too much on technical knowledge of their tool rather than on statistical knowledge.  All you have to do is look at their certification practice exams.  It's all about how to navigate the UIs rather than knowing any of the math behind the analyses.",flyingbrotus,2013-05-14 22:47:45
"Just chiming in here: I know statistics, know R, can't use Linux for crap, can scrape by in SQL, haven't a clue how to use noSQL. I work daily with computer scientists who are great in Linux, can program, are great with SQL and noSQL, but are shitty at statistics even with their machine learning experience. I know many people from the behavioral and life sciences that run rings around anyone I've ever met, CS or no, with their stats knowledge. I know some some CS people great with stats too.

It's a diverse world out there. Just as you'll get in trouble blindly trusting SPSS analysts without vetting them, you'll also get in trouble because a Linux-friendly programmer once read HSAUR and think his/her Python skills will be enough to get by. ",HelloMcFly,2013-05-14 20:48:33
"That's an issue of stats knowledge that is independent of the question of whether or not SPSS will increase or decrease market share.

I don't engage SPSS consultants to discuss statistics, my discussions focus on automation, and the new feature that lets SPSS be called from JAVA is one automation feature that is very important for my business.",wil_dogg,2013-05-15 09:21:03
"While SPSS is dominant in psychology, it is generally awful (in my experience) when used in industry.  The GUI is nice when you want to run tests, but if you actually want to operationalize advanced predictive analytics, you need it to be in code.  SPSS provides little-to-no support for their syntax and that is a deal-breaker when it comes to implementing a solution outside of academia.  Just search SPSS on stackoverflow and you'll see how tiny the proportion of users is in comparison to R, Python, or even SAS.  SPSS can do most things that other statistical packages can, but outside of psychology and some market research, it isn't used by anyone other than the IBM consulting team.",flyingbrotus,2013-05-14 22:44:03
"What do you mean ""little or no support for their syntax""?

One click ""PASTE"" puts the GUI-generated code into your open syntax window.

There's plenty of documentation on each command in the PDF syntax guide that is another 2 clicks away from the open GUI window.

Maybe the reason there's not a lot of SPSS chatter on stackoverflow is because it is easy to use and the syntax support is already there, so SPSS users don't have to go to that message board for help.  Also, they may be sticking to the IBM / SPSS message boards

http://www.ibm.com/developerworks/

There's batch submission options as well as (now I haven't tested this yet but it's on the agenda) the ability to make an external call to the SPSS batch process from a JAVA-based data management process.

Also, you are pretty far off base on industry deployments of SPSS.  I worked at a Top 10 bank, and probably the most highly analytical one at that, and although SPSS was not the dominant tool for analytics, it was the dominant tool for predictive model deployment in the operational environment.",wil_dogg,2013-05-15 05:06:28
"I'm a casual user (i.e. a really advanced user... For a biologist) and I almost exclusively use code at this point. I sorta agree: the syntax support for spss is a bit garbage. Passing values from one procedure to another is *way* harder than it needs to be and there's little-to-no documentation on it. This is especially true with their graphics processing, which IMHO is one of the last great strengths of SPSS. eg: if I wanted to run a multilevel linear model and plot a shaded region for the confidence limits I have to either use R called from spss or 1) do my regression 2) pull my values from the table into a new data set with OMS 3) pull all of the variable names into another data set with a transposition and a macro definition and make a procedurally-generated syntax file 4) make a really complex GPL procedure and save that into *another* syntax file and 5) insert that syntax into my main syntax. 

This would take me a day, maybe more, to get running smoothly in a generalizable way. You can't have persistent scratch variables. You can't do math (without ""tricks"") in marcros. You can't use macros with GPL. You can't manipulate variable names without many intermediate steps. You can't easily call and manipulate data labels. Matrix mode documentation is not as complete as it needs to be. GPL transformation is buggy and poorly documented. You get the idea. 


SPSS is still my program of choice, but it could use *a lot* of work in being more generally usable to the basic science crowd. If R had the GUI covering basic stats that SPSS has, I'd see little reason to endorse ~~it~~ SPSS at my level of use.",BillyBuckets,2013-05-15 08:34:47
"When you say ""passing values from one procedure to another.."" is that akin to building a predictive model, and then ensuring that model's score is available on the data set?

That used to be difficult but now the scoring utility makes that pretty easy, and it's syntax-edit friendly.

On the graphics, I agree that is an area where SPSS needs to be more WinTel friendly.  One thing I'm working on with IBM consultants is how to get SPSS stats output into pretty PPT decks and PDF files with as little touch / custom programming as possible.  Given what I'm working on, investing a few weeks to make that work is worth it because of the volume of analytics and the value of those presentations being low-effort so that analysts can focus on things other than cut/paste/edit.",wil_dogg,2013-05-15 09:16:25
"You have ins with IBM? In that case, direct feedback from a group that likely gets little voice:

The chart builder is *the strongest* feature for SPSS to push for scientists, but it needs a heavy update. The only times I've ever had another scientist ask if they can learn my analysis techniques is when they see me go from data to publication-quality graphs (error bars, footnotes, etc) in &lt;1 minute. What I think it needs:

* access to more Syntax commands via the chart builder. Why can't transparency(), split(), shape(), color.interior(), etc. be accessed in the same way color.exterior() can be? I use these all of the time, but I code with syntax. That's a huge barrier for entry to most biologists.

* more flexibility with axis dimensions. Instead of just columns and rows, why not columns * 1 * column from the chart builder? Syntax makes this easy (see above as to why syntax-only charting is hurting SPSS) 

* the templates. oh god, the templates. They are completely undocumented (requiring [reverse-engineering](http://andrewpwheeler.wordpress.com/2012/01/03/hacking-the-default-spss-chart-template/) to customize fully) and some things, such as panel label font specs, don't appear to go into saved templates at all. Additionally, I can't find a way to specify ""default unless otherwise specified"" parameters in templates. If I want my points to be white circles *unless I use GPL to modify them*, I have to have a default chart template to set points to white circles *and disable that template entirely* if I want to make them, say, blue squares. The alternative is make a dummy variable and map that to blue squares, but again this requires a lot more work and syntax coding. Both are barriers to entry.

* multi-panel charting from the chart builder. Again, this is possible with some difficulty via syntax, but only if the default template doesn't override everything.

* placement of legends, footnotes, etc. from the chart builder.

* the ability to generate new syntax from the Modify Chart window. Say I don't know how to plot linear regression lines with GPL, but I can do it from the Modify Chart window. I've wished for a ""paste"" option to regenerate syntax so many times... This would eliminate a lot of the template woes I have as well.

Just for an anecdote: my lab bought a multi-user license for everyone to use when I gave a presentation on how awesome SPSS graphics were compared to Prism and excel. Everyone was pumped to switch. This was nearly four years ago- we've now reduced the license to just one so I can use it, as everyone else couldn't get past the most basic charts and just gave up. Now we're back to excel and prism (but my presentations still look much better).

EDIT: on the passing values point- I was just making up an example and I'm not really talking about predictive models.

Here's an example that I actually encounter: I make confidence bands for nonlinear regression all the time. To do this with the delta method, you need the correlation matrix of the parameters, the gradient of the regression model at every value of your confidence band, the parameters themselves, etc. The derivatives are saved in the dataset, the rest of the info is in various tables in the output window. To pull all of these numbers together and do the math, I had to write a custom dialogue (to build the GPL to graph the confidence bands, since GPL doesn't play nice with macros) that calls a 215 line macro. In MatLab, this would have taken 5-10 lines of code, tops. Matlab (and R, I assume) gives the ability to save and manipulate matrix variables from the output of other procedures. SPSS requires OMS, then processing the OMS result down to just what you want, then Match Files or some other process (e.g., the frustrating MATRIX-END MATRIX procedure) to get the numbers into one place. There might be an easier way to do this, but I have yet to find it. Even the matlab guru sitting next to me rolls his eyes at the hoops I jump through to get SPSS to do what he calls ""basic workflow"".

If SPSS allowed me to create scratch variables of some sort, and manipulate them easily, I'd be a customer for life. 

Instead of:
    
    DATASET DECLARE  Rtable.
    OMS
      /SELECT TABLES
      /IF COMMANDS=['Regression'] SUBTYPES=['Model Summary']
      /DESTINATION FORMAT=SAV NUMBERED=TableNumber_
       OUTFILE='Rtable'.
    REGRESSION
      /MISSING LISTWISE
      /STATISTICS COEFF OUTS R ANOVA
      /CRITERIA=PIN(.05) POUT(.10)
      /NOORIGIN 
      /DEPENDENT Y
      /METHOD=ENTER X.
    OMSEND.
    DATASET ACTIVATE Rtable.
    delete variables  TableNumber_ Command_ Subtype_ Label_ Var1 R AdjustedRSquare Std.ErroroftheEstimate.

I want to be able to just use:

    REGRESSION
      /MISSING LISTWISE
      /STATISTICS COEFF OUTS R ANOVA
      /CRITERIA=PIN(.05) POUT(.10)
      /NOORIGIN 
      /DEPENDENT Y
      /METHOD=ENTER X
      /MATRIX (#regRsq=R(:,3)).

Where the /MATRIX option just defines what parts of what tables you want to save in working memory for use elsewhere.",BillyBuckets,2013-05-15 10:05:11
"Have you ever posted that to IBM to see if they can code that up in REGRESSION?  Seems like a very useful utility.  In fact, I think you've shown me a trick that I can use albeit could be better, but I've neer used the OMS procedure and I've been wanting to create data from output for various uses.",wil_dogg,2013-05-15 11:00:44
"No, I haven't really interacted with IBM or their forums at all. I only get my hands dirty with SPSS when I need to- I'm a benchtop scientist so most of my time is spent moving liquids from one tube to another. I also have no formal statistics or programming training, so I only know what I've needed to learn over the years. This makes discussions with programmers a bit difficult for me; I never learned much computer science or statistics vocabulary.

I'd rather they implemented this matrix ability in *all* outputs. As you'll find, OMS is a pain to use, especially if you want to use MATCH FILES (or now, STAR JOIN I guess) to move the output values back into a pre-existing dataset. All of the properties of the variables need to match and the sorting needs to be just right. You need to turn off value labels in the charts, but bring the value labels from the original dataset into the OMS dataset... this can all be done, but it usually adds 20+ lines of code for something that could potentially be done in 2 or 3 lines using Matlab or R.",BillyBuckets,2013-05-15 11:09:37
"I don't use STAR JOIN -- I revert to syntax for matching files as STAR JOIN seems to take forever (I often am sorting and merging files with 100,000+ records and 1,000+ variables, sometimes as many as 100MM records and 200 variables).

What I'm seeing in SPSS is that they are making some new tools such as saving XML scoring code available across may procedures.  I wonder if they are not doing much graphics innovation within SPSS because they are now selling COGNOS as a business intelligence tool and trying to get people to use that.  My work is focusing on getting SPSS and PPT to talk to each other so that my analysts spend less time writing reports (cut/paste/format/save) and more time interpreting results.",wil_dogg,2013-05-15 11:23:49
"Just out of interest, how are you trying to get SPSS and ppt to work together? I assume its using some intermediate code to manipulate the windows COM system?",bdobba,2013-05-17 01:07:41
"I'm very early in exploring this and relying on IBM to point the way to some examples of where it's been done successfully.  This is one weakness I see in the IBM/SPSS consulting sphere -- they should have these types of examples readily available, some code / data / output examples that people can download and try out.  My hunch is that the focus is on Cognos because that's another tool they can sell, as opposed to coming up with neat Windows interfaces that further Microsoft's utility in the business presentation space.",wil_dogg,2013-05-17 04:16:11
"Maybe for client side, but for people who are consultancy side and are looking for a quick way to get their data into a presentation for their clients I can't really see Cognos being a viable option.",bdobba,2013-05-17 04:25:50
I agree completely.  For small business / small analytics consultancy the link to Microsoft presentation and documentation tools is important.,wil_dogg,2013-05-17 07:30:30
[deleted],,2013-05-14 12:08:11
"Yeah, but SAS is just as expensive as SPSS. Your school probably switched in a concerted effort to choose one over another, rather than a large price differential between the two programs. ",Iamnotanorange,2013-05-14 13:03:13
[deleted],,2013-05-14 15:22:01
It was both a price increase for SPSS and a price decrease for SAS.  My hunch is that SPSS will lower the academic licensing fees to ensure they beat SAS in terms of ease-of-access for undergrads and grad students.,wil_dogg,2013-05-14 20:17:27
What bells and whistles are you referring to?,Ithm,2013-05-14 18:06:19
"An easy-to-use GUI interface, high quality graphics output, and integration with other tools are the kinds of things that people look for in an industry application.  From what I understand R is great in terms of breath of statistical processes that are possible to accomplish, and the open source community innovates on that a lot.  But if the community is not making it talk to Cognos and Powerpoint, then that means there need to be manual-steps in my analytical --&gt; reporting process, and more manual steps means more cost.  Likewise, if R is a programming language and those programming skills are rare relative to someone who can learn basic stats and then use a WinTel GUI interface to do the work, then R is actually a lot more expensive relative to a tool like SPSS.

Keep in mind, there's the ""rack rate"" of what you pay for SPSS, and then there's bulk purchases and timing your purchase to be near the end of an SPSS calendar quarter when their salesforce will give you a steep discount in order to close the deal.

Also, what happens if an R proc has a bug in it, and I use that proc to do an analysis, the solution gets coded and implemented on a bank's system, and 2 years later the bug is discovered, and when the post mortem is done it is found that my analysis was sub-optimal.  The bank paid me $300,000 for that work, and now they are coming at me wanting $10,000,000 because that was the lost opportunity to them due to the suboptimal analysis.  With SAS I am indemnified -- I can point at SAS and tell the bank to go after them.  With R I do not have that air cover.  I know that might seem like an unlikely occurrence but it happens all the time when high-cost analytics is put against big P/L opportunities in banks and other investment houses.

Finally, think about this -- I can get on the phone with IBM, buy a big Netezza box with all the software loaded, get IBM consultants coming to my shop to do the installation, train my staff, and suddenly I'm doing big-time analysis 100-1000X faster than previous to getting that Netezza deployment.  Executives are willing to pay for that because they can then hold IBM accountable for the results.  With R you basically get free software.  Nothing wrong with that, but I'd have to ask myself, am I getting free software, but then the savings are eaten up by the high cost of R programming skills as well as the lack of integration of R procs with other tools that I am implementing, or that I want to help a bank or HMO to implement.

Again, this is not knocking R per se, I'm pretty sure it can whup SPSS when it comes to cutting edge statistical procs, but that's not where a lot of incremental value is created -- incremental value usually comes from speed-to-market, low run-the-engine costs, and ease of integration.  Logistic regression is still the workhorse in a lot of predictive modeling exercises, so long as you can do data reduction, smart transformations, and fit an equation you'll get 95% of the value of a more complex algorithm by simply using logistic regression.
",wil_dogg,2013-05-14 19:33:24
"Very interesting, thanks. I'm curious about the liability part. Is there some precedent to that, or is it mostly speculation?",Ithm,2013-05-14 19:37:14
"It's not speculation.  When I do independent contract consulting some clients require that I carry sufficient liability insurance such that if I make a goof that costs them money, they know they can collect from the insurance company.

In banks, if a data vendor sends the wrong data you can not only get the updated correct date for free, but often charge back to the vendor the lost time you spent working on the wrong data.

Think of it this way -- the bank is the 800 pound gorilla, the data vendor (or the independent analyst like me) wants the bank's business, but the bank also is looking out for its own interests, hence the bank's motivation to litigate when there is a loss event.

Granted, in many cases things may get smoothed over by senior management in both firms -- the business chief in the bank gets the data company's CFO to plead mea culpa and agree to a 10% discount for next year.  But that's still a cost event.",wil_dogg,2013-05-14 20:09:29
"&gt;Logistic regression is still the workhorse in a lot of predictive modeling exercises, so long as you can do data reduction, smart transformations, and fit an equation you'll get 95% of the value of a more complex algorithm by simply using logistic regression.

dammit it really is.  it probably won't win you any kaggle competitions, but it definitely still works.",flyingbrotus,2013-05-15 09:48:02
People tend to over-emphasize the fit function and under-appreciate variable selection / data reduction.  I over-invest in data reduction methods and then go beyond logistic regression when it really matters.,wil_dogg,2013-05-15 10:57:23
"Maybe it's because this is a statistics subreddit and not a programming one, but I'm surprised nobody has mentioned the in-memory issue with R and Python.  As it stands now, SAS is far better at working with massive datasets because it doesn't hog RAM.  I personally prefer working with R, but I have run into countless issues with memory and they can definitely hold back projects.  I think SAS is going to stick around, not only because of their ability to deal with billions and billions of rows, but because they seem dedicated to adding new machine learning functionalities to their arsenal of analytical algorithms.  They may not be the sexy company you hear about in meetups and Ted talks, but they're a name that is trusted both by industry statistical analysts and F500 companies.  R and Python are trusted by the analysts, which will eventually lead to industry CIO acceptance.  Generally, SPSS is only trusted by the companies, so I believe it will fade.",flyingbrotus,2013-05-14 23:00:09
"The R people just argue that they can do everything they possibly want to do with the primitive functions provided by ff, bigmemory, biglm, etc., or just call SQL to do in-database processing. I suppose this might work for some small test problems with considerable effort, but nobody who has ever done serious data processing (terabyte level) with SAS would ever consider trying to do something similar with R.",zip117,2013-05-15 09:27:01
"Unfortunately, you are correct.  While the support for massive datasets has gotten a lot better as of late, it is still nothing compared to SAS.  However, there is always the argument to be made that you don't really need terabytes of data to find a trend.  Good sampling methods can easily lead to very strong predictive models.",flyingbrotus,2013-05-15 09:37:26
"I mean data processing in general, not just predictive analytics. I do economics consulting work and we routinely have to analyze massive insurance claims datasets to estimate future liabilities or conditionally merge+summarize data from different datasets depending on specific attributes.

You could *maybe* do the same stuff in R combined with some complex SQL - I recall doing something a couple months ago where I needed to use the ""OUTER APPLY"" statement in SQL Server. Nothing is as intuitive as the SAS PDV when it comes to sequential data processing tasks which don't easily fit into SQL relational algebra.

Take the common process of estimating quantiles of a population, aside from the edge cases for which R handles with [nine algorithms](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html). Suppose you have a massive, unsorted dataset and you can't afford to iterate through it more than once. SAS offers the validated P^2 (piecewise parabolic) method for approximating quantiles in one pass. If you want to do that in R I guarantee you will end up writing your own algorithm with considerable effort.",zip117,2013-05-15 10:30:54
"You can also use Hadoop with Python for truly large data manipulation. So far R+SQL worked for me though, like bdobba mentioned.",pokie6,2013-05-15 16:06:37
"&gt; Maybe it's because this is a statistics subreddit and not a programming one, but I'm surprised nobody has mentioned the in-memory issue with R and Python. As it stands now, SAS is far better at working with massive datasets because it doesn't hog RAM

That's not such an issue if you use them with a query language like MySQL",bdobba,2013-05-15 03:39:12
I've never done that before.  Is there a way to do it with PostgreSQL?  Do you just call R from your query?,flyingbrotus,2013-05-15 07:14:29
"No, you write a query in your R code wrapped in some syntax - I don't use PostgreSQL but here's a [pdf](http://www.stat.berkeley.edu/~nolan/stat133/Fall05/lectures/SQL-R.pdf) about acessing databases from within R
",bdobba,2013-05-15 07:19:26
"Thank you!  I am going to try setting this up on my machine soon.  Having skimmed the linked document, it doesn't really mention anything about actual analytics.  It really just talks about SELECT statements.  Have you ever used this for things like regression?  I imagine it would be possible to use the dbSendQuery() function to build really large random forests. You could grab small chunks of data and run a decision tree on each piece.  Then you could take the ensemble of each tree.",flyingbrotus,2013-05-15 07:30:24
"no, the only thing embedded queries are going to give you is increased speed during the data manipulation process; you're still going to have to run your regressions in R. This will speed up your scripts considerably, however.

I would say also that  most of the time people complain about R code being slow is down to trying to write code like other languages - e.g. lots of for loops and little to no use of the vectorised operations which speed things up by orders of magnitude.",bdobba,2013-05-16 08:49:49
"I completely agree about the for loops and whatnot, but my complaint with R is more about the fact that you are limited by memory when working with large datasets.",flyingbrotus,2013-05-16 13:31:28
"right, but the point I'm making is that perceived limitation is more often than not people writing code that is not as efficient as It could be, especially with the computing power available nowadays. 

With enough ram and properly vectorised code you shouldnt have a problem with 95% of the large datasets you encounter, though I'll admit you're going to run into a dataset once in a while that causes you a headache.

If you really are dealing with the monumental size datasets that R can't handle even with the right setup, then in my opinion learning clojure or another of the JVM languages is a much better option than hamstringing yourself by using SAS.",bdobba,2013-05-16 14:49:27
Ha!,,2013-05-14 14:49:55
"Isn't the more obvious explanation that people who use SPSS and SAS just aren't reporting the software they use to perform statistical analyses? It's become an unspoken norm and it has no diagnostic utility to the research report. 

I do research in psychology, and it's pretty much a given that whatever you're using, whether it's R, SPSS, Excel, SAS, or SOFAStats, that they're all running the same tests (Regression, ANOVA, t-test) in a virtually identical way.",,2013-05-14 17:57:53
A little competition will be good for SPSS. Maybe IBM will finally get around to improving their product; maybe by replacing the Levene test with one of the more powerful alternatives. Same goes for Mauchley's sphericity test.,,2013-05-14 21:45:06
"IBM doesn't seem to bother with improving their products, even in the face of competition.  Just look at Lotus Notes... They just like to get by on their name.  As the saying goes, ""nobody ever got fired for hiring IBM.""",flyingbrotus,2013-05-14 22:52:30
"There's actually a lot of new features in SPSS that improve the predictive modeling capabilities, as well as bootstrapping features for those who are doing simulations and Stats R&amp;D.",wil_dogg,2013-05-15 09:19:08
"If that is the case, then I stand corrected.  I still don't think IBM's approach to analytics is sustainable given the strength of its competition.  On one side you have SAS who is the high price competitor and who (IMO) has a better product, and on the other side you have the open source options like R, Python, and RapidMiner that can do just as much for little-to-no cost of implementation.  Sure, it's great that SPSS can easily integrate with popular data warehouse appliances like Netezza, but that doesn't always help you out that much in the market.  Teradata has their own data mining tools that don't seem to get used by anyone",flyingbrotus,2013-05-16 13:30:06
Forecasting without error bars or confidence intervals for future predictions? looks like 2014 will also be the beginning of the end for R. ,glutamate,2013-05-15 00:22:36
ggplot2 can easily do all that.  I have no idea why this blog decided to make their visualizations look like they came out of Excel 98.,flyingbrotus,2013-05-15 07:23:36
"Though I used to use (and even teach) both a few decades ago and haven't touched either in 20 years, I seriously doubt it. 

As far as I can tell, they're both heavily entrenched in particular areas and still selling pretty solidly, though if one or more of the various menu-etc-packages for R really got pushed, then SPSS might eventually start to struggle.
",efrique,2013-05-14 19:27:29
"While the book seems pretty awesome, the part of this that got my attention is IPython Notebook.  I did not know this existed, and now my mind is blown.",luxliquidus,2013-03-19 11:53:26
"I'm going to assume you're new to the python scientific computing workbench and welcome you to the party with a few links, just to be safe:

* [numpy](http://www.numpy.org/)
* [scipy](http://www.scipy.org/SciPy)
* [matplotlib](http://matplotlib.sourceforge.net/)
* [pandas](http://pandas.sourceforge.net/)
* [sckit-learn](http://scikit-learn.sourceforge.net/)
* [pytables](http://www.pytables.org/)
* [spyder](http://code.google.com/p/spyderlib/)

I apologize if I'm being presumptuous and hope these links are at least useful to someone if not you. ",shaggorama,2013-03-19 12:52:14
"IPython Notebook is very new too, so I'm sure not many people know about it.",roger_,2013-03-19 13:09:39
"Yeah, that's fair. I guess it is only a year old or so. There's just so much chatter in the python scientific computing community that it seems like if you're in the know on one tool, you probably know about most of em.",shaggorama,2013-03-19 13:13:54
"It's a bit of both.  I've been in and out of Python scientific computing... and I don't think Notebook existed last time I was in.  Still, upvote for the excellent list of tools!",luxliquidus,2013-03-19 13:47:17
"Thx for that list, would you consider the CPHvb or other libs for easier distribution across GPGPUs to be mature?

http://pyvideo.org/video/1209/bringing-high-performance-to-pythonnumpy-without

(I think folks starting into statistics/data analytics are expected to know, or at least be able to read, java, R, python, matlab, and possibly mathematica and C++, which isn't easy.",gtani,2013-03-20 03:56:54
"Haven't played with any of pythons GPU programming libs, sorry.",shaggorama,2013-03-22 09:34:02
"Yep. I think Notebook is now IPython's killer feature since you can  share results really easily and even work completely in the cloud ([Wakari](https://www.wakari.io/), etc.)",roger_,2013-03-19 13:18:34
"This is awesome, guys! Many thanks. Could you upload pdfs of all chapters for those of us lowly readers that would like to start with the pdfs?",golden-boy,2013-03-30 09:40:52
"Hey, contributing author here. Originally, we had plans to create pdfs, but the nbviewer site has been a great platform (much easier to do styling and such). We will probably stick with this for the time being. 

There are tools like [nbconvert](https://github.com/ipython/nbconvert) that allow converting to pdf from notebook.

Similarly, if using chrome to view the books on nbveiwer, you can ""print"" as a pdf.",HootBack,2013-03-30 10:53:08
Thanks a bunch; I'll use the Chrome route.,golden-boy,2013-03-30 15:43:43
"I'm getting a Ph.D. in computer science from a top school.  I write code for my papers all the time.  But here's the deal: most of the time we do the minimum amount of work possible to demonstrate the validity of our work.  This usually means that the software was never designed to be portable, has lots of bugs, there isn't an installer, and just the process of adapting it to a real-world situation would take a ton of time.  Writing high quality code takes more time than we have.  So it's rarely worth sharing the source code -- you would practically have to rewrite it from scratch to make a real product out of it.  See http://matt.might.net/articles/crapl/.

Research should stand on it's own.  Sometimes, like if you've built an implementation of something which you claim to be highly reliable or highly portable, you will need to include the code.  It comes down to whether or not an implementation is necessary for the reviewers to evaluate the idea, and usually it's not.

*Good* research contributes small pieces of very valuable information.  *Excellent* research is also extremely useful to others, so useful that those with resources to build high quality code can use the paper to write their own implementation (possibly incorporating other research papers as well).  And, if the reviewers have done their job right, the paper explains the new ideas clearly enough to facilitate this.

If everyone wrote code good enough to distribute with their paper, we wouldn't ever have time to get real work done.  Anyone who works with software knows that 15% of the time is spent writing it, and 85% is maintaining it.  In the end, the author alludes to, ""In a future post I’ll talk about the new issue I’m struggling with – maintaing all that software I’m creating.""  He has no idea how hard this is, or why it isn't worthwhile to attach code to every paper.",bchurchill,2013-01-26 22:50:49
"&gt; most of the time we do the minimum amount of work possible to demonstrate the validity of our work

Great.  Provide that, and others can also see that your work is valid.

&gt; If everyone wrote code good enough to distribute with their paper

If the code isn't good enough when writing the paper, why are you writing a paper on it?!

&gt; Anyone who works with software knows that 15% of the time is spent writing it, and 85% is maintaining it. 

Who cares about maintaining the code?  The purpose of releasing the code is to back the claims of the paper, not as some grand product.",oursland,2013-01-27 01:22:01
"I am also a PhD student in a computational field and used to to be in a computer science department before switching. I agree with your first and third points and am generally a proponent of publishing code along with papers, but what I disagree with is:

&gt;If the code isn't good enough when writing the paper, why are you writing a paper on it?!

Computer science isn't about coding - that's software engineering. CS is a branch of mathematics and it's looking at analytical/asymptotic (or expected, in the case of randomized algos) properties of algorithms. Hell, there can be theoretical CS papers that have not much to do with algorithms even. I know I am exaggerating and there are plenty of CS papers out there that toss an ad-hoc algorithm on data and see what pops out (deep belief nets anyone?). But even that, I'll argue, is not about good coding - it's a data-driven exploration of those ad-hoc algorithms.",remington_steele,2013-01-27 06:07:16
"&gt; Computer science isn't about coding - that's software engineering.  CS is a branch of mathematics and it's looking at analytical/asymptotic (or expected, in the case of randomized algos) properties of algorithms. ...

You're too focused on the terminology.  The focus should be on doing things right and leaving little to no room for error.",oursland,2013-01-27 11:23:50
"I'm in education/psychology, and there is a growing trend to supply code on your website for others to use.  

I'd like to give a shout out to [Andrew Hayes](http://afhayes.com/spss-sas-and-mplus-macros-and-code.html), [Kristopher Preacher](http://www.quantpsy.org/supp.htm), [Dave MacKinnon](http://www.public.asu.edu/~davidpm/ripl/mediate.htm#download), [Daniel Bauer](http://www.unc.edu/~dbauer/publications.html), and [Dave Kenny](http://davidakenny.net/dtt/datatotext.htm) for saving me a shit-ton of time by providing the code that illustrates their methods.  ",misanthrope237,2013-01-27 09:38:53
"This is stupid.  The point of a paper isn't to produce production software.  I'd rather do my own implementation most of the time than use some shitty broken MATLAB package that a grad student threw together that runs on one dataset, anyways.",necroforest,2013-01-26 12:57:20
"The author agrees with you. Someone writing a paper shouldn't aim for production-quality software, BUT it should provide the code used in their paper, otherwise others can't replicate their work (and that's the basis for science).",zzleeper,2013-01-26 13:17:08
"A paper should always be detailed enough that others can replicate the work without access to the code. That said, providing code should be encouraged, but it shouldn't be necessary.",standard_error,2013-01-26 13:40:46
"&gt; A paper should always be detailed enough that others can replicate the work without access to the code.

What you say is true.  However reproducing another's work can be a laborious process if the source isn't provided.  How can a reviewer approve the paper and it's results, if they cannot reproduce them?  Simply accepting other's findings on faith that they've done their due diligence has resulted in [numerous publications](http://en.wikipedia.org/wiki/Scientific_misconduct#Individual_cases) based on fabrication.  These cases take funding away from valuable research, mislead others causing them to waste their time.

In the field of mathematics, no proofs means no reproducible work and therefore no paper.  Why should those working in software, which is infinitely reproducible, be exempt?",oursland,2013-01-26 19:39:33
"&gt; How can a reviewer approve the paper and it's results, if they cannot reproduce them?  

Do you work in academics?  This is not how the reviewing process works.  Reviewers don't go though and rerun all of the experiments to verify each p-value matches what the authors reported.  They check that the methods used are legitimate, that their results and conclusions are consistent,  and that they have provided enough details so that others could reproduce the work.  Also, I have experience working with code that an author sent me and it can take days or weeks just to get it to run.  There's no way anything would ever get published if this were a requirement.  

You seem to be worried that researchers are making up their results.  This certainly is a valid concern, and does happen, but having access to code does not solve this problem.  If someone provides me with their data and code and I get it to run I still can't be sure that everything is okay.  How do I know their data is legitimate?  Do they need to submit copies of all of their lab notebooks?  If so, how do we know some undergrad research assistant didn't just randomly record numbers because they were lazy?  

&gt; In the field of mathematics, no proofs means no reproducible work and therefore no paper. Why should those working in software, which is infinitely reproducible, be exempt?  

Maybe you are focusing more on computer scientists, but any decent statistics paper that develops new methodology (so something that isn't just data analysis) does have mathematical justification.  Pick any random article in JASA or Annals or whatever and this is clear.  The goal of statistics research is not to produce new software.

Now, that said, I do believe that authors should provide code if asked but I do not believe it should be a requirement for publication.",ApproximatelyNormal,2013-01-27 10:47:56
"I mean, if publishing the code to reproduce experimental results were required, researchers would learn how to write code that easily reproduced results.  Of course, if a paper isn't producing an analysis on a data set, then this obviously is irrelevant.

It's not that fucking hard to write code that others can use to reproduce your own output.  The majority of researchers are much smarter than me, and I could certainly manage it.",rrenaud,2013-01-28 13:29:23
"Academic code is usually so bad that making it work for any specific purpose would probably require reengineering it.  

In mathematics, published proofs are wrong all the time.  There are lots of available statistics on this.  However, math publications are still very useful because they illucidate the proof technique, still convince the readers the things they say are true, and build conceptual frameworks.  

Here, computer scientistis are sharing the crucial concepts behind their work.  They aren't concerned with producing something that people can use, because that takes a ton of work, which frankly isn't research anymore.",bchurchill,2013-01-26 22:55:14
"&gt; Academic code is usually so bad that making it work for any specific purpose would probably require reengineering it.

What about simply ensuring that the author isn't a liar?  That seems to a specific purpose for which producing the code would be particularly suited for.",oursland,2013-01-27 01:14:40
[deleted],,2013-01-27 02:56:31
"&gt; maybe it is a bit cowardly/unethical, but I'm not that keen to start questioning more senior colleagues on publishing possibly flawed results

That **IS** an ethical failing on your part.",oursland,2013-01-27 11:24:21
"I feel as if there's a reasonable timestamp on these kinds of things, though, too. Just like histological samples can't be kept forever, I wouldn't go back to a statistician more than a decade after a paper and expect her to be able to hand off her code.",Neurokeen,2013-01-26 13:29:45
"You can reproduce their results by reading the paper and implementing it.  If you can't reproduce it by reading the paper (i.e., without being spoon fed source code), then it's a shitty paper.",necroforest,2013-01-26 14:15:35
This is also the basis of publications in many fields but this has it's [problems](http://en.wikipedia.org/wiki/Scientific_misconduct#Individual_cases).,oursland,2013-01-26 19:40:35
"One thing that isn't mentioned is intellectual property concerns. I read a paper recently that was a few years old. After digging for some code it turns out it *had* been posted on their website but was taken down and some related patent was pending. I'm not saying this is often the case, but if people want to protect their inventions they should have that right.

Another factor is that people might feel if they release code, they will have to support it and that is a huge downside.",gicstc,2013-01-26 14:38:19
"&gt; After digging for some code it turns out it had been posted on their website but was taken down and some related patent was pending.

If the process is patented, then providing the source does nothing to invalidate the patent.

&gt; but if people want to protect their inventions

That's what patents and copyright are for, not failing to provide the sources for publications.

&gt; Another factor is that people might feel if they release code, they will have to support it and that is a huge downside.

This screams of ""concern trolling.""",oursland,2013-01-26 19:44:26
"Well I'll be damned, that's a word, huh?

Thanks for opening up my eyes.",Qw3rtyP0iuy,2013-02-06 22:50:36
"1. Not every vital parameter follows the Normal.
1. Don't confuse sanity with hivemind. Being outside 2SD doesn't automatically mean you are wrong.

",derwisch,2010-11-04 08:38:37
"You are right about the first point. With most political issues, the distribution of opinion (assuming there was a metric for such a thing) would probably be at least bimodal, due to the fact that many people will form an opinion on an issue as a result of their allegiance with one or more ideology. For example, while there are certainly a lot of people who have ""moderate"" opinions regarding the extent to which the US should provide health care, there are likely to be peaks at both ""universal health care"" and ""no health care at all"".

Your second point is moot: Everyone, presumably, believes that their opinion is the correct one. The alternative is rather paradoxical. 

The point of this analogy is that, although those in the thin tail of the distribution are only ~5% of the the population, their opinion often gather a disproportionate amount of media attention, due in part to their novelty as outliers. 

Regardless of who is correct, the sentiment of this sign, and of the rally, was that the there is a danger our society can be too easily distracted by the barking zealots from beyond 2σ, simply because they make better television.",siddboots,2010-11-27 15:50:50
"As a person who has recently been learning R, I was like ""oh no, I wasted my time""—then once I read the article, it sounds quite exciting. With the easy transition stuff, I don't see why we should be pessimistic? ",,2010-09-13 14:15:28
"Even without the transition stuff, I don't necessarily see the problem.

I've been writing code in one form or other since I was about 12 (in the mid-70s), and I've been regularly doing data analysis on computers since 1981.

In that time, I have used well over a dozen programming languages and well over a dozen data analysis packages (and some where the distinction was unclear) - I was just able to name more than 12 of each right off the top of my head that I have used.  (Perhaps there'd be as many as 40 altogether if I really started stretching the memory).

R will eventually (in 5 years? 10 years? 15 years? - who knows) be surpassed by something else as my data analysis tool of choice. I first encountered S in the 80s, and then on and off through S+ and R subsequently, so I guess I've been using it in some form for a while now, though I am really only now moving out of the ""occasional user"" into something more expert.

Its eventual passing no more makes learning R a waste of time than the dozens of tools I used to love but no longer use (and one or two I *still* love but don't use for one reason or another) were a waste of time. 

They were my tools for a while. That's all they need to be. They were not a waste. 

Was it a waste learning Visicalc? Was it a waste learning Lotus 1-2-3? Was it a waste learning Cobol, Basic Fortran, Pascal? Not at all. I learned important things from from all of them, but most importantly, I used them for what I needed to.

In spite of its problems, I like R (most of its problems don't affect me at present; some will eventually). Whatever turns out to be so good that it *makes me give up R* is going to have to be *great*. 

It *will* come, and I won't be sorry when it does. I'll be too busy learning another new language and doing wonderful things with it.

I suspect the first tool that replaces R will be kind of like R; several of the things I used before R were also kind of like R in some ways. In that case, the transition won't be so hard. But if it's radically different, maybe I will be doing things I can't even dream of doing in R, in which case the transition will still be worth it.

What  I have noticed is that the ability to combine different tools is increasing. 15 years ago I was calling Fortran and C routines from S. Now, R talks to dozens of different things. That will probably continue improving - old languages and packages may not all entirely disappear, many will just become something you talk to through an interface. 
",efrique,2010-09-13 16:14:58
"&gt; They were my tools for a while. That's all they need to be. They were not a waste.

Yes, but one of the reasons I stayed with Linux was ""so I don't have to re-learn my tools too much"". Works perfectly for Emacs and bash, but less so with configuration of interfaces (makes it hard too look behind the scenes). Your knowledge of C was expected to have a longer shelf life than your knowledge of Lotus 1-2-3 or Degas. And I'd hope that R would have some more years. 

The semantic aspects of R can be addressed. ""R gives you everything you want, but never in the data structure you need"" can be overcome. Syntactic sugar like plyr is one method to address this. 

And, of course, the main upside of R is that it is not SAS.

",derwisch,2010-09-14 00:33:36
"Parts will be hard to fix, such as all the fit objects carrying around their data with them. Technically, it's not hard, but socially it will be.

call-by-value is a major headache right now, but that doesn't have to be the case. Maybe the lazy evaluation makes that harder; I'm not sure. All I know is that I've done big data work in a call-by-value language (q) that works, whereas it's a major headache with data.frames in R, to the point where I've switched to storing data in environments.",oddthink,2010-09-14 05:22:46
"Interesting answer. I don't have anything to respond, but I enjoyed reading it.",,2010-09-13 22:51:58
[deleted],,2010-09-13 19:50:35
"Apply constructs aren't any faster then for loops in R currently (I asked Robert Gentalman at a bioconductor course) but code vectorized using matrix math can be which can be a really convenient way to implement things like gibs samplers for multi dimensional paramater spaces but a bit of a brain fuck if your linear algebra isn't great.

Still, I too find myself using python more and more (particularly for string based problems like genetics ...) but still turn to R pretty often for that baked in functionality you mention ... I have trouble getting over how verbose numpy can be.",micro_cam,2010-09-14 09:33:35
"You have a good point about numpy: working with matrices in R is really intuitive. 

I'm currently in a masters bio stat program, and have experience in R and Python. I think as a learning tool, and for future use, I'm going to start duplicating the R functionality my program covers in it's computing course. I think with good use of numpy and matplotlib, one could come up with a decent and easy to use basic tool set. 

",randomsample,2010-09-14 10:07:32
"You should blog your experience and post here ... I'm a former math/stats student working as a software engineer at a bio institute and would be interested to see what you come up with.

Also you might find the sage project interesting ... it claims to use R via python but seems to be aimed more at matlab users currently:

http://www.sagemath.org/",micro_cam,2010-09-14 11:13:54
"This is true. The recommended steps for ""optimizing"" R code is as follows:

* Hash it out.
* If not fast enough, ""vectorize"" (use efficient functions like sweep, rowSums, etc.)
* If not fast enough, link C.

I can see how one would quickly prefer Python.",,2010-09-19 10:35:50
"I think his function is an example of bad R coding and not pitfalls of R. If a program is necessarily complex, it should be scaled onto another language. I frequently use R to shape data matricies and do beefy tasks in a more efficient scalar language like C.",,2010-09-13 10:49:48
Actually the article touches upon topics you've just named: obscure visibility rules make R sub-optimal for big programs and poor interpreter makes it unfit for scalar computations.,vityok,2010-09-13 11:13:44
"Rvalue references, being added to C++, might be part of the solution to the pass-by-value/pass-by-reference efficiency problem. Essentially, the type of the variable knows whether or not it is the last remaining copy of a particular piece of data. Then the compiler can know that it's safe to modify the referenced data without breaking call-by-value semantics. It's called ""move semantics"" instead.",aaronmcdaid,2010-09-13 16:51:54
"&gt;  In the Wilcoxon-Mann-Whitney test, the alternative hypothesis is that the empirical distribution curves are merely shifted one relative to the other.

Well, no, this is wrong.

*IF* you make the a priori assumption that the distributions are the same apart from a possible location shift, *THEN* the alternative is as stated.

But without that assumption, the WMW works perfectly well. It's a test for P(X&lt;Y)=1/2 vs the alternative that it's not 1/2 (i.e. the alternative is just stochastic dominance). The U statistic (when appropriately scaled by the number of pairwise comparisons) is in fact a sample estimate of the probability P(X&lt;Y).

Unfortunately a lot of books get this rather basic fact wrong, so the misinformation has become widespread.

It doesn't change the underlying point the article makes, however.",efrique,2014-09-22 23:10:43
"Thanks for this comment (I am the author of the blog post). I gave it a thought and it now makes perfect sense to me. I will correct it. In the mean time, you can also post this a comment on forum of the post if you wish, that would be useful for everyone. ",gui11aume,2014-09-23 01:16:03
"I just came across the R documentation of [ks.test](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ks.test.html)
&gt; The possible values ""two.sided"", ""less"" and ""greater"" of alternative specify the null hypothesis (...) the statistic in the ""greater"" alternative being D^+ = max[F_x(u) - F_y(u)]. Thus in the two-sample case alternative = ""greater"" includes distributions for which x is stochastically smaller than y (the CDF of x lies above and hence to the left of that for y), in contrast to t.test or wilcox.test.

If even R spreads this idea, that explains how it came to be so popular in biology and genomics.",gui11aume,2014-09-23 03:02:34
"Wow, really well written blog post. Bookmarked.

I try to tell people statistics is more than wagging our fingers and telling them they can't do that.  But sometimes it's needed.  

On the other hand, its possible they were looking for a test that is neither a KS or a WRS test, but something that does not yet exist in the statistics literature that can compare the tails of two samples.  Possibly a good point of research.",Floydthechimp,2014-09-22 16:09:48
"&gt;I try to tell people statistics is more than wagging our fingers and telling them they can't do that. But sometimes it's needed.

Very true. Statistics is such a powerful subject, but ""with great power comes great responsibility"" :-) Seriously though, the problem of p-hacking and mis-using statistical tests probably contributes to a lot of the reproducibility problems we see in science. 

I think this mis-use of the KS test appears so frequently because using the KS test in this way can produce really ridiculously small p-values, and as we all know some editors/reviewers/readers are easily snowed by a very small p-value, even if the effect size is un-important (or worse, the hypothesis supposedly being tested is different than that implied by the underlying statistical test).  ",DrGar,2014-09-22 17:46:39
"Really neat. As a biomechanist I found this really interesting, and thank you for your work.

If you could please humour my ignorance: 

1) your generation for a single potential for each person: are you returning the single lowest factor from the random n number of factors? Or creating a surrogate measure for all the n factors you randomly simulated for that person.

2) how do you suspect weighting the contributions of each factor to total potential would change the likelihood of seeing a record holder over the simulations...for example VO2max could contribute to 60% of your total potential..while tendon stiffness only 4%..yet both are independent factors contributing to potential. So being lucky enough to have tendon stiffness as your limiting factor could still let you potentially break records?

3) Some distances are more likely to have records broken...if I were to have a child today which event do you think he or she should enter to be a world record holder?

Love to learn from your work! Thanks in advance!",AlbertsPoohole,2013-10-02 16:55:36
"Hi.  Thanks for your kind words.  To respond:

1) Yes, the model generates n factors for each person, and then computes the minimum.  So it is a strict ""weakest link"" model.

2) In the model, all factors have equal weight.  If, in reality, some factors are more important than others, the effect would be similar to decreasing n.

3) Some records are lagging behind the long-term trend, so they seem to be ripe for breaking.  Most of the middle distances, from 800m to 2 miles, are in this category.  The sprints are way ahead of schedule (because Usain Bolt) and the marathon is a little ahead of the long-term trend.",AllenDowney,2013-10-03 06:41:17
" To paraphrase George Box..""All models are wrong, but some are useful""... you have provoked my brain..and as a scientist and I appreciate that :) Cheers!",AlbertsPoohole,2013-10-03 09:44:57
"I would surmise that this is because of participation in those events. Marathons and short distance sprints are much more glamorous (and highly paid) races, and attract the athletes with the most natural talent.",StopTheMineshaftGap,2013-10-03 10:40:57
"Unless I'm missing something, he's oversimplified way too far.  By his logic, as long as the population grows exponentially, there is no limit to what humans (not evolved super-humans, people of roughly the same genetic stock as all of us) can achieve, and we will get to any record in a time linear in the marathon runner's speed.  So with a large enough population, a 30mph marathon should not be a problem.  Clearly there is a flaw here.",ajmarks,2013-10-02 20:37:11
"So are you saying that if we have enough babies, one day mankind will be able to run so fast we finish the race before it starts?

[The future is cool.](http://media.salon.com/2012/11/anti_fragile_rect.jpg)",ButUmmLikeYeah,2013-10-02 23:28:10
"I think hardly any linear regression is presented with the claim that it can be extrapolated beyond all limits. I think an extrapolation to 2040 is where I would draw the red line, if you pardon the pun.

",derwisch,2013-10-02 23:16:39
"Why 2040?  Why not 2030?  2050?  2275?  That's the problem with extrapolating beyond the sample data, especially in a scenario *in which you know for a fact it will break down*.",ajmarks,2013-10-03 03:24:23
"Thanks for these great questions.  Here are some additional thoughts:

In general, linear extrapolation of a time series is a dubious business, but in this case I think it is justified:

1) The model does not assume that there is no limit to human performance.  The model includes an upper bound, but it offers a reason to believe that we are not yet approaching this bound.

There’s a theoretical reason to expect the distribution of running speed to be long-tailed, and evidence that it is, so it’s reasonable to believe that there is still room between the fastest human ever born and the fastest possible human.

2) I’m not just fitting a line to arbitrary data; there is theoretical reason to expect the progression of world records to be linear, and since there is no evidence that the curve is starting to roll over, I think it is reasonable to expect it to continue for a while.

3) Finally, I am not extrapolating beyond reasonable human performance.  The target pace for a two-hour marathon is 13.1 mph, which is slower than the current world record for the half marathon (58:23 minutes, or 13.5 mph).  Since we have already seen people who can maintain that pace for an hour, I don’t think it is beyond theoretical human capability to maintain it for two hours.",AllenDowney,2013-10-03 05:21:40
"Great article, thanks for the read.",skuller99,2013-10-02 21:22:19
"Is a least squares fit appropriate when the data selected in the way that the y's are strictly increasing with the x's? I remember tackling the question in an applied course at university, and I included the mile and 1500m results of major sports events into the data set.

    sim &lt;- data.frame(x=rnorm(50), y=rnorm(50))
    k &lt;- rle(sapply(seq(along=sim$y), function(i) max(sim$y[1:i])))
    records &lt;- data.frame(x=cumsum(k$lengths), k$values)
    plot(records, type=""s"")
",derwisch,2013-10-02 23:32:21
"I think your concern is legitimate, in general, but in this case there is a theoretical reason to expect the progression to be linear.  And the distribution of residuals is at least approximately normal.  It looks like there is some serial correlation, but other than that, the assumptions of linear regression are satisfied.",AllenDowney,2013-10-03 05:25:02
"Very interesting, thanks for your analysis.  Some of the big names in the endurance coaching/Ex Phys world have had similar thoughts on the matter, not sure if you are familiar with the work of Dr. Phil Skiba and others; may or may not be useful in your model, just figured I'd pass it along if you had not come across it. 

---

Implications of the critical speed and slow component of Vo2 for the 2-hour marathon
http://www.ncbi.nlm.nih.gov/pubmed/21542150

The two-hour marathon: who and when?
http://jap.physiology.org/content/110/1/275.full.pdf+html",youtri,2013-10-04 10:58:53
Thanks for these references!,AllenDowney,2013-10-04 11:10:02
"I think my favorite is ""There's no better aspirin than Bayer.""  Yeah, but there's no worse either.  Aspirin is aspirin.  Misleading with phraseology, a common unethical statistical move.",Series_of_Accidents,2013-09-11 11:32:56
"That reminds me of a classic graffito:

An ad said ""Nothing works faster than Anadin"", to which someone added ""Then take nothing instead.""",jmmcd,2013-09-11 16:10:20
"To expand on this, puffery in general is always funny to see. ""Best tacos this side of the border"" or something to that effect falls under that same protection of puffery.",Neurokeen,2013-09-11 13:32:44
"So you mean that all the tacos taste the same Or just that the don't know if they're the best?.... I have a degree in tacology, just saying.",arnolzz,2013-09-11 14:40:54
"""clinically tested"" and ""scientifically tested"". But you didn't tell us the outcome?",Manetheran,2013-09-11 17:10:28
"15 minutes could save you 15% or more!

Or, it could not.",oblivious_human,2013-09-11 15:40:25
"I saw that commercial, called geico, told them what I wanted and how much I was paying my current insurer. They offered me the same policy for about $30 more per month. I quoted their slogan and the rep. basically told me that I could either take their price or keep my current policy with another provider.

I tell this story to anyone who tells me that they have gieco and try to convince them to shop around.",brickoftr00th,2013-09-11 16:33:36
"Every insurance company can say this. Since they usually qualify it by saying, ""of people who switch"".",aftersox,2013-09-11 17:48:52
The 'could' covers that nicely already.,naught101,2013-09-12 03:31:59
"My absolute top commercial pet peeve:  ""On average, customers who switch to Allstate save $500 per month!""

That's the average _for the customers who actually switched_, not the average of all potential customers.  I hear it as ""on average, customers require savings of $500 to be enticed to switch to Allstate"" -- a bigger number is a _bad_ thing.  

You could do amazingly well on this metric by charging $100,000/yr for policies, but offering five people the policy for free.  They were paying $600/mo before?  ""Customers who switch to IdiotCare save $600/mo!""

If I were to hear ""our insurance costs the same amount (or more), and customers are flocking to us"", THEN I would be interested.",ctornync,2013-09-11 14:00:38
[deleted],,2013-09-11 17:13:56
"But there are selection issues. Customers who do switch may not be similar to you or even to an average customer. People who switch to your company are going to be a different (e.g., non-random) subset of all people who considered switching to your company. Basically nobody is going to switch if it doesn't save them money. So all of the switchers are saving more by switching than you would expect for a person chosen at random. ",iacobus42,2013-09-11 19:05:12
[deleted],,2013-09-11 21:19:37
"I know, I quoted that in my first sentence.  I'm not saying it's _not true_, I'm saying it's a worse-than-useless fact.",ctornync,2013-09-11 21:58:51
"The point is that the statistic is intentionally stated because it is not useful but the untrained ear assumes it actually means ""the average person pays this much less with our company than they do with other companies.""

Joe and Jane Consumer think ""we are average people! We will probably save this much money too!"" Which is incorrect. ",BillyBuckets,2013-09-12 03:13:36
"But the implication is that if you decide to switch, you too will save that. When in reality, its not the case",Zoraxe,2013-09-12 05:49:10
Often times financial advisors advertise the average annual return as the arithmetic mean rather than the geometric mean.,carmichael561,2013-09-11 13:50:04
"Elaborate, please?",misanthrope237,2013-09-11 15:38:19
"Suppose you start with s_0 in a portfolio and in the i^th year the portfolio increases in value by r_i percent.  after n years the initial sum is worth

(1 + r_1)(1+r_2) ... (1+r_{n+1})s_0.

note that we are *multiplying* the return rates.  as a result, the geometric mean is the appropriate way to represent the average return.  

also, by the [AM-GM inequality](https://en.wikipedia.org/wiki/AM-GM_inequality), the arithmetic mean will be greater than or equal to the geometric mean.  for real world financial data, the arithmetic mean will typically be greater than the geometric mean so it makes the portfolio manager look better.

edit: fixed a typo",carmichael561,2013-09-11 15:53:21
I assume the arithmetic mean ignores compound interest.,TerrySpeed,2013-09-11 15:52:55
correct,skiedAllDay,2013-09-11 16:07:20
"Quick Wikipedia suggests

given a dataset {x,y,z}

their average (arithmetic mean) is (x+y+z) / n, where n is the number of elements in the dataset. For example, the arithmetic mean of 6, 10, and 20 is (6+10+20)/3 = 36/3 = 12. This is the usual ""average"" that everyone learns.

The geometric mean uses multiplication instead of addition. Also, instead of dividing by n, you take the nth root. So (x\*y\*z)^(1/n). The geometric mean of 6, 10, and 20 is (6\*10\*20)^(1/3) = (60\*20)^(1/3) = 1200^(1/3) = 10.627...

I don't know why either is better or worse, but it looks like that is the difference.",userino,2013-09-11 15:53:55
"Thanks for the explanations, all!",misanthrope237,2013-09-11 16:00:35
"Is that not illegal? Annual return is supposed to be defined as the geometric mean, from what I could tell.",Novakog,2013-09-12 11:16:52
"That's probably true, though it depends on the jurisdiction. I've never seen someone do this in writing, but I've heard people ""accidentally"" do it when answering questions during a sales pitch.",carmichael561,2013-09-12 12:17:32
"I don't remember the exact words, but those match.com commercials say something like ""people who join match.com are 3 times more likely to find a relationship"". That's all it says. I'm not a professional statistician, but I have no idea how to interpret that.",tapmron,2013-09-11 14:57:04
"I'll give it a try.

A given person has some probability X of finding a relationship.

Someone who joins match.com has a probability 3*X of finding a relationship. For instance, if (somehow it was determined that) you had a 20% probability of finding a relationship, then joining match.com would give you a 60% probability of finding a relationship.

I have no idea if that's true, though I suspect it isn't. I think that's what they are claiming, though.",userino,2013-09-11 15:47:45
"This is probably what they mean, except there's probably much less of a causal effect (maybe none at all). People who join match.com (or any dating site) are actively seeking a relationship, and so of course they are more likely going to find one then people who are not seeking a relationship.

There's also a lot of room for other bullshit, like including people who are already in a relationship as not having ""found"" one, to deflate the numbers for the comparison group.",BanachSpaced,2013-09-11 17:11:48
Dunno in this case the selection bias would actually be attractive for someone wanting to use this service.,petewilko,2013-09-11 23:28:40
"Multiplicative comparisons of percents like that annoy me.  It should be stated at a 40 percentage point increase in your example.  If the underlying population of non-match.com users have a 50% success rate, clearly the 3x shouldn't imply that 150% of match.com users find dates.",jts5009,2013-09-11 16:56:36
"That's why I chose 20%!

Or, maybe . . . I guess, if I double my chances, did I have a 200% increase? Or maybe a 100% increase? It's all very easy to get mixed up.",userino,2013-09-11 17:01:16
"Yep! I sometimes see things like ""Event X has a 10% chance, but Event Y is 30% more likely"".  Does Event Y have a 13% chance? A 40% chance? Using the ""percentage points"" terminology removes the ambiguity.",jts5009,2013-09-11 17:06:16
"If the base were 50%, then a 3x increase would not be possible. There's nothing confusing about saying the observed effect is a 3x lift. Saying ""a 40 point increase"" is no better either, because in both cases you omit the base rate.",BanachSpaced,2013-09-11 17:07:21
It would make more sense if they were using odds as in P(A)/(1-P (A))  because then the odds can be multiplied by 3 and still have probability between 0 and 1,tapmron,2013-09-12 13:26:33
"Right, so this one irks me so effin much. Not because I think the statistic is wrong - could be right or wrong - but because of the implied casual inference. As if joing match is the  cause of the difference. More likely, people join match represent people who are more actively looking for a relationship and may have found one by some other means. This is so misleading and a little logic reveals that. ",manic_panic,2013-09-11 17:39:39
"One dating site ad claimed that ""55 people find a partner on [datingsite] every day"". But clearly this is impossible - since every match consists of two individuals, the number of individuals who find a partner must be even.",standard_error,2013-09-12 00:13:46
"The classic is the two slashes to mean that the graph axis has been broken, making changes appear larger than they actually are.

I found a neat Wikipedia article on ""misleading graphs"" in general: https://en.wikipedia.org/wiki/Misleading_graph. Some neat tricks, there!",userino,2013-09-11 16:12:20
"""...4 out of 5 dentists recommend....""",max8765,2013-09-11 11:28:23
"Rookie here, just took my first intro-level statistics class. Please explain to me how this statement is misleading and/or doesn't mean anything.",wasabichicken,2013-09-11 23:33:15
"if the advertisers cherry-pick 4 dentists that say their product is best then it could very well be true. the statement is true even if the sample size is only 5, the actual most recommended product might be different when looking at a sample size of 1000 dentists. sorry if i misused terms, i've never done stats before",the_minimalist,2013-09-12 01:18:45
Basically this. 5 dentists is not a large sample. Consider a case where there are only two brands: Brand A or Brand B and pretend that the TRUE recommendation for each brand is 50/50 chance. This is basically a coin flip. If you flipped a coin 5 times and it can up heads 4 times you wouldn't be as impressed as if you flipped a coin 100 times and it came up heads 80 times.,valen089,2013-09-12 05:35:46
"Any and all stats in cosmetics/skincare are never to be trusted.

For example, ""40% reduction in appearance of wrinkles"". This usually means that 40% of the sample in the 'study' stated that they experienced a *perceived* decrease, but isn't tested double-blind or against a placebo/control, and usually isn't tested repeatedly. And those sample sizes are usually tiny, like less than 100. So great, 40 people said that it looked like they might have had fewer wrinkles for a day. Ugh. ",jean-michelasciiart,2013-09-11 20:36:52
"Also their chemistry claims are worse. 

""Amino peptide technology"" was one of my favorites. ",BillyBuckets,2013-09-12 03:19:23
"There is 50% increase in cellular rejuvenation of the hair. 

1) hair doesn't have cells!!!!!
2) how on earth are you measuring rejuvenation?
3) What is your reference group, sample size?",HenryGale52,2013-09-22 10:05:53
"Basically all of them.  I can't name a single time I've seen a commercial reference a number and thought ""Hmm.  This seems accurate.""",beaverteeth92,2013-09-11 12:11:50
"Come on, that's overstating the case. e.g. Subway ad for $5 sandwich = use of a number to communicate true, potentially useful information. ",arvi1000,2013-09-11 12:30:08
"actually, subway was recently sued for advertising footlong sandwiches even though the sandwich is not always literally 12 inches long.

http://www.forbes.com/sites/nadiaarumugam/2013/01/27/why-lawsuits-over-subways-short-footlong-sandwiches-are-baloney/",carmichael561,2013-09-11 15:55:12
Reminds me of changing the 12-pack of 12 oz. beers to 12 bottles of 11 oz. each. How much are you missing there? X-D,userino,2013-09-11 17:42:11
"Yes, but as the Forbes article explains, it isn't quite the same.

edit: swype error",carmichael561,2013-09-11 18:44:47
No 5 sub's at any stores I went to in sfo,jrkotrla,2013-09-11 17:22:56
"yeah, it sucks.  not all stores have to participate in the $5 footlong deal.",carmichael561,2013-09-11 23:06:59
The price of something isn't a statistic.,MipSuperK,2013-09-11 15:49:51
why not? ,Zeurpiet,2013-09-12 09:37:57
"To take the very literal mathematical definition, a statistic is a function that maps from a data (sample) space to a different space (usually smaller space, usually a single number, but it could be a vector). Example : a mean maps data as f(data)=sum(data)/count(data). So a mean statistics maps from the data space to a number.

Announcing the price of something isn't a data sampling, it's just specifying a constant. The price of a sub sandwich in an area could be open for statistical analysis, but the price of a sub sandwich at Subway is a data point.",MipSuperK,2013-09-12 10:48:01
"a data point would be a statistic, although degenerate.",Zeurpiet,2013-09-12 11:24:07
"The statement was:
&gt; I can't name a single time I've seen a commercial reference a number and thought ""Hmm. This seems accurate.""

$5 is a number and that statement _is_ over the top :)",ctornync,2013-09-11 19:27:20
Yet it's $5.35 at the checkout! What gives?,MacBelieve,2013-09-11 13:27:52
$5.41*,secunder,2013-09-11 13:59:54
:-/,userino,2013-09-11 15:58:12
"""Following A""

""Wall street jumped 2% today, **Following An** announcement of strong jobs groth.""

False Implication: the jobs announcement caused the share market increase.

Truth: No one has any idea what caused the share market increase.  News broadcasts use these weasel words to avoid being proven wrong.",kabas,2013-09-12 04:29:54
Economists always know why the stock market did what it did at 4:01 Eastern standard time.,HenryGale52,2013-09-22 10:06:52
"This one's been bugging me lately. My Febreze bottle says it kills 99.99% of odor-causing bacteria. Naively, I read that as saying that if I spray it on my stinky shoes, 99.99% of the odor-causing bacteria should die. Thinking statistically, I think that it really says that it can kill 99.99% of a set of different odor-causing bacteria, but not necessarily 100% of any one type in the set. Or it kills 100% of most odor-causing bacteria, but some strains are immune. Basically, it's not clear what it actually means.",Doctor_Underdunk,2013-09-11 16:19:16
"I don't know the mechanism of action but most things that kill bacteria (that aren't meant to be used inside a body) kill basically all bacteria they touch. For example, bleach just dumps free radicals and kills everything and EtOH dries the cells out. So what I would suspect is that it kills bacteria in general. There isn't really a defined ""set"" of bacteria that cleaning agents are tested on.

Typically, cleaning agents killing power is measured in [log base ten reductions](http://www.ciriscience.org/a_107-What_is_Log_Reduction) Using the agent once leaves 10% of the initial population behind. Applying it again leaves 1%, and again 0.1% and so on. What the label is trying to say is that it has enough agent/power to affect a 3 log reduction (or leave 0.01% of the original population). Hence the 99.99%. 

You will see this on a lot of things (soap, hand sanitizer, bleach) and it always refers to how many log reductions it does. People have an easier time understanding ""kills 99.99%"" then they do ""affects a 3 log reduction.""",iacobus42,2013-09-11 19:15:04
"This is a great explanation. Thanks for sharing. It doesn't change the fact that what the label claims and what the science actually says are different, though. I still really only care about the type of bacteria in my shoes, not all the other types it kills.",Doctor_Underdunk,2013-09-11 20:27:58
"Also, likely measured by applying the product to liquid/agar bacterial cultures. Adding Febreeze to LB media is a different process than spraying Febreeze on your stinky gym bag. ",BillyBuckets,2013-09-12 03:23:01
"&gt; ""affects a 3 log reduction.""

Which base?!

:)",kabas,2013-09-12 04:31:09
"Hopefully .01% wouldn't make a difference, unless that 1-in-10,000 species is really, really stinky.",userino,2013-09-11 16:57:23
"I checked the label and it was actually 99.9%, not 99.99%, so I only need to have a top-1000 microbe... Which is scientifically proven to be 10x more probable than baseline!",Doctor_Underdunk,2013-09-11 20:25:54
99.99%. Great. 0.01% to start exponential growth.,Zeurpiet,2013-09-12 09:41:03
[deleted],,2013-09-11 23:15:52
"Oh yes, unconditional life expectancies.

This one gets played heavy in politics too. You see life expectancy increases and people go crazy about retirement/welfare/etc spending, but neglect that one of the larger drivers of increases in life expectancy tends to be early mortality.

Also missing from a lot of those discussions is the fact that conditional life expectancy gains for aged people aren't necessarily even - people more likely to be drawing welfare have actually seen a flat-line in life expectancy, and the overall gains are mostly attributable to persons in higher economic strata.",Neurokeen,2013-09-12 08:38:58
"Where to start...

1) when anybody mentions a dollar figure without context: ""consumers saved $5B last year because of X! ... It's just incredible, Tom. Back to you Linda."" ... $5B out of how much??? ... if total costs prior to the savings were $6B, then that's effin awesome... if it was $1T, then who the fuck cares. But, the word ""Billion"" is big, so you can definitely sway people into thinking it's important when it very well could not mean a damn thing. It's not morally sound to provide numbers like that without ANY context.

2) similar to (1), but for dollar figures mentioned over time with no indication of having been adjusted for inflation

... basically, any time figures are delivered without context.

More specifically, when BP had it's big oil spill in the Gulf of Mexico, I recall a PR briefing they held where the presenter (a BP rep) showed a graph of ""Cumulative Oil Collection"", stating that ""We (BP) have been ramping up out oil collection efforts"" ... *Cumulative* cannot go anywhere BUT up. That particular statement was a blatant attempt to misconstrue the facts. Usually, these sort of failures are simply careless... BP demonstrated a clear intent to deceive.

EDIT: grammar ... also, just realized I'm not talking about commercials, but commercials pull the same nefarious mumbo jumbo",datalies,2013-09-11 17:31:09
None of this is true at all.,datalieslies,2013-09-12 15:11:05
Clinically proven.,aftersox,2013-09-11 17:45:57
"Let me take a stab at this.

""Kills 99.9% of germs."" People tend to equate this to 100% and round up, when in fact that .1% is as dangerous as the other 99.9. Further, these figures are based on PERFECT laboratory usage. They are also not specifying whether it's 99.9% of known germ species or whether it's 99.9% by cell count.

http://online.wsj.com/article/SB126092257189692937.html

Real-world usage is closer to 50%. Washing your hands kills 50% of the germs on them. Yay.",fogu,2013-09-11 20:25:18
"The word ""virtually"" is heard as ""almost"", leading viewers to believe that the two compared things are ""almost"" identical, when it should actually be interpreted as ""not in fact"".  [VR is not, in fact, reality!] ",bill_tampa,2013-09-12 06:07:18
This isn't about statistic as much as it is about weasel words.,mrpopenfresh,2013-09-12 06:11:01
"""On average, wrinkles decreased by 39.7%""  ",whodai,2013-09-21 20:44:47
"This is a good time for me to double check one of my thoughts about those ""x out of N"" statistics...

Let's take a simple probability, like p=0.25, which lends itself to an interpretation of ""1 out of every 4 people have disease xyz"". However the exact probability is closer to

(4 choose 1) * p * (1-p)^3 = 4 * 0.25 * 0.75^3 =.421.

Is there a hole in my logic, or is it not correct to equate an event which has (binary) probability p to the intuition that out of 100 trials, (p*100) of those events will have the desired event outcome?",greensmurf30,2013-09-11 12:33:49
"If I'm reading that correctly, what you've just calculated is the probability that, out of a random sample of four people, exactly one has the disease xyz.

This is not at all the same as saying ""x out of N"" where x/N is the underlying population proportion.

Edit: Repetition in the last sentence made it confusing.",Neurokeen,2013-09-11 13:34:15
"I think 0.421 is the probability that out of 4 randomly chosen individuals, exactly 1 person is affected.",MacBelieve,2013-09-11 13:32:05
I've never bothered to look into it but I'm suspicious of JD Powers and their ratings. ,RightWingersSuck,2013-09-11 12:09:40
"I studied under a statistician in grad school who did a lot of time series research on climatology.  He proved (using changepoint analysis) that the earth is in fact getting warmer. However, nobody can statistically prove that pollution or people caused global warming. All can be proven is the years of the rise of industry (and pollution) has a direct correlation with the rise of the global temperature. As every elementary statistician knows, correlation does not mean causation.",save_the_platypi,2013-09-12 05:34:24
"It is difficult or impossible to show causation with statistics on a natural experiment (non DOE)- but with chemistry to you have a mechanism and combined with statistics you have a pretty compelling case. Example: A measles vaccination correlates really well with people not developing measles when exposed. But to be statistically fair - could be that being stuck with a needle caused the immunity, could be the vaccine, could be the lollipop afterwords... BUT now, you add to the mix a biologist who can show that immune cells chemically change when exposed to the vaccine and that prevents measles virus from infecting - now you have a mechanism and correlation. ",HenryGale52,2013-09-22 10:13:05
Congrats! Just keep going!,srkiboy83,2013-09-08 02:07:52
I didn't respond to your story but the theme resonates with me - being forced to take on a sales role despite your aptitude in statistics and analysis. All the best to you!,yellowjack,2013-09-08 06:11:32
"This is an article I have been thinking about for a while.  People often come to /r/statistics asking about statistical tests for normality, or more generally whether data come from a particular distribution.

I think this is often not the right question.  This article is my attempt to frame a better question (and answer it).",AllenDowney,2013-08-07 08:25:08
"I don't understand.  Why not go to the next step and perform a Kolmogorov-Smirnov test, etc?  Just looking at plots is good but performing the actual test and looking at plots is better.",FullSharkAlligator,2013-08-07 09:36:18
"The statistical test does not provide any additional information.  Real world data never matches an analytic distribution perfectly, so if you perform a test, there are only two possible outcomes:

1) You have a lot of data, so the p-value is low, so you (correctly) conclude that the data did not really come from the analytic distribution.  But this doesn't tell you how big the discrepancy is, or whether the analytic model would still be good enough.

2) You don't have enough data, so the p-value is high, and you conclude that there is not enough evidence to reject the null hypothesis.  But again, this doesn't tell you whether the model is good enough.  It only tells you that you don't have enough data.

Neither outcome helps with what I think is the real problem: deciding whether a particular model is good enough for the intended purpose.",AllenDowney,2013-08-07 10:17:17
"&gt; The statistical test does not provide any additional information.

It is an objective measure of how much the data does not behave like the theoretical distribution, which to me seems better than just claiming ""the graphs look similar"".  The entire point of hypothesis tests in this context is basically process of elimination, eliminating bad models (can you reject the null?).  Even though real world data doesn't come from a theoretical distribution how comfortable are you assuming that it did, and drawing inferences from that assumption?  A test stat seems like a better way to answer that question.

When you have more data, you can develop better models because you can eliminate bad models through more powerful stat tests.  There's dangers of overfitting etc etc but the fact that more data gives you more powerful stat tests doesn't mean stat tests are useless.",FullSharkAlligator,2013-08-07 10:47:11
"The point is that he doesn't care if the model is ""wrong"" and so there is no point testing it. He knows the model is wrong - the question is if it is useful, and if the fitted cdf doesn't deviate too far from the ecdf then it probably isn't a catastrophe to just use a wrong parametric model. 

He assesses this by eyeballing the plot, but we could be more analytic if we wanted to by looking at (say) total-variation distance or L1 norm and attempting to formally assess the impact of a wrong assumption on our inferences. But if the cdfs match up well, this is evidence that the chosen model is ""good enough,"" although this sort of thinking doesn't always work out well.",NOTWorthless,2013-08-07 11:44:28
"&gt; The point is that he doesn't care if the model is ""wrong"" and so there is no point testing it. He knows the model is wrong - the question is if it is useful, and if **the fitted cdf doesn't deviate too far from the ecdf** then it probably isn't a catastrophe to just use a wrong parametric model.

That's exactly what a KS test tells you though.  Why not use that information?",FullSharkAlligator,2013-08-07 12:17:22
"The KS test is a test for whether the true cdf deviates *at all* from the fitted cdf (i.e. cdf under the null). If the it deviates by a sup of 10^-1000, but you have 10^10^10 data points, the KS test will reject even though the fitted and actual cdf are virtually identical. If you want, you can use the KS test statistic as a measure of distance, and that would be useful, but it isn't the same thing as actually doing the test. 

",NOTWorthless,2013-08-07 12:24:57
"But if you have enough data, you will eliminate all models.  If a statistical test rejects a model, that doesn't mean the model is not good enough for practical purposes.  And if the test fails to reject a model, that doesn't mean it is good enough.  Either way, the test does not answer the modeling question.",AllenDowney,2013-08-07 11:20:23
"&gt; But if you have enough data, you will eliminate all models. 

Wait... I thought some data provably/by-definition come from some distributions. (E.g., maxwell-boltzmann).

Sorry for the tangent, do not mean to derail.",quaternion,2013-08-07 12:34:13
"Yes, good point.  For some simple systems reality is so close to the model that the model shouldn't get rejected even as data size approaches practical limits.

(Although at the risk of lapsing into philosophy, I would argue that we can only prove things about mathematical models; we can't prove anything about the real world.)",AllenDowney,2013-08-07 13:03:45
"You can't eliminate all models, I can't think of a time in which you could.  

Say you find that in your data set height is unlikely to be considered normal from a KS test.  I.e.

Height != mu + epsilon

where epsilon ~ N(0,sigma^2)

Could this model be true though:

Height = mu + Beta(age) + epsilon?

etc.

These test stats are very valuable tools, eliminating them just because you know your model is false (remember all models are false, some are useful) is a mistake.
",FullSharkAlligator,2013-08-07 11:33:38
"If all models are false then what is the point of doing the test in the first place? The formal result of the test becomes completely meaningless. So, don't do the test and instead think about the inference that you want to make and whether the model you are using is wrong *in a way that actually matters* with that inference in mind. 

I could give a personal example, but I think [Andrew Gelman](http://www.stat.columbia.edu/~gelman/research/published/avoiding.pdf) has a better one with his 3x3x16 contingency table example. ",NOTWorthless,2013-08-07 14:48:19
"I mean how do you define ""actually matters.""  An example that I can think of is that people believed stock returns were normally distributed for a long time, and seemed comfortable with that assumption because it was convenient and seemed to fit to the data reasonably well.  Now we know with more data that returns have much fatter tails than the normal distribution and are skewed downward (negative shocks stronger than positive shocks).

A lot of people still build models on the assumption that returns are normally distributed and lose a lot of money, of course a lot of people can still make money as well on that assumption.  Regardless it seems silly to stick to that assumption when a statistical test tells you that it's VERY unlikely that stock returns are normally distributed, and you can use that information to design a better model to potentially make smarter investment decisions.",FullSharkAlligator,2013-08-07 15:18:37
"Not sure I see what point you are trying to make. No one is saying you have to use models that tests reject, just that the fact alone that they were rejected by a test isn't reason itself not to use them. And ""what matters"" in the stock market is relatively easy from a decision theoretic standpoint - the relevant utility function is just some function of your profit/loss. If you want to see if your model is adequate for practical purposes, check its ability to generalize with cross validation or something.

It's interesting that you use the phrase ""VERY unlikely"" when everyone seems to agree that we are already 100% sure the data aren't normally distributed - indeed, 100% sure is ""very unlikely."" The formal result of the test hasn't told you anything. I'll spare you the bonus frequentist nitpicking (""very unlikely"" sounds an awful lot like a probability statement). ",NOTWorthless,2013-08-07 16:02:03
"&gt; Not sure I see what point you are trying to make. No one is saying you have to use models that tests reject, just that the fact alone that they were rejected by a test isn't reason itself not to use them.

Not true, people are basically arguing the tests are pointless and give no information.  I'm arguing they are useful and giving an example for how you can use them to improve your models.  Earlier someone made the point that basically all hypothesis tests are useless because if your data is large enough you'll reject the null.  That's an argument for more data, not fewer hypothesis tests.

&gt; It's interesting that you use the phrase ""VERY unlikely"" 

Ehh I was just covering my butt
",FullSharkAlligator,2013-08-07 16:24:59
what do you think of and objective score to describe the degree of difference from a normal distribution rather than a probability that there's any difference from a normal distribution? I take it you're not arguing for subjectivity per se but rather the p values are telling us something that we don't really need to know anyway,cuginhamer,2013-08-07 21:44:47
"Yes, if you are comparing different models it is useful to compute some metric of the distance between the data and the models -- a K-S statistic, for example.  But I still think it is important to look at the pictures to see what the discrepancies look like, and then choose a distance metric appropriate for the application.  For example, you might be most interested in a model that captures one tail or the other, or you might need to get the central part of the distribution right but don't care about the tails.",AllenDowney,2013-08-08 05:40:49
Isn't this an argument against all hypothesis testing?,hotandtiredanddry,2013-08-07 18:46:40
Fisher made the same critique for basing academic research on p value thresholds back in the early days,cuginhamer,2013-08-07 21:45:57
"Yes, most of my objections about this kind of distribution testing also apply to hypothesis testing in general.

I am picking on this particular application because when people ask about distribution testing, it is almost always the wrong question. What they really want, most of the time, is help making modeling decisions.",AllenDowney,2013-08-07 19:04:14
"&gt; but performing the actual test and looking at plots is better.

Why would you assert that this is better, exactly?

For most purposes, a formal hypothesis test *doesn't answer the right question at all*.",efrique,2013-08-07 15:42:40
"If you are doing something like regression, and have power to detect violations from normality, then you don't need normality! See e.g. http://www.ncbi.nlm.nih.gov/pubmed/11910059

This is one of the most frustrating questions I get as a statistician. Some myth, perpetuated by mathematicians who like pretty math...",RevdWicksCherrycoke,2013-08-07 22:28:22
Whoa.  Really?,flipstables,2013-08-08 16:30:19
"&gt; Many people are more familiar with histograms than CDFs, so they sometimes try to compare histograms or PMFs.  This is a bad idea.  Use CDFs.

What's your reasoning behind that?",banthur,2013-08-07 19:20:55
"I think this will have to be the topic of my nest post, but here's a short answer for now:

1) Histograms and empirical PDFs get messy as the number of possible values increases, so you have to do some kind of smoothing, but getting the smoothing/binning right is tricky.

2) Even when smoothed, PDFs are noisier than CDFs.  If you compare PDFs, you often see differences that are only artifacts, and miss actual differences in the shapes of the distributions.

3) You can't plot more than 2-3 PDFs on the same axes, but you can often plot lots of CDFs.

4) The y-axis of a CDF contains useful information.  The y-axis of a PDF is mostly meaningless.

",AllenDowney,2013-08-08 05:23:17
"Given a parameterized model choice, the next step is to estimate the parameters using something like maximum likelihood. So couldn't one loop over a set of parameterized distributions and form the maximum likelihood of this object?",thant,2013-08-07 14:03:25
"Yes, this is pretty much how Bayesian model selection works (or at least, the same general idea).",AllenDowney,2013-08-07 15:09:35
"Very interesting perspective. Instead of fretting aboug non normality, it is best to try and find the right model.",,2013-08-07 21:51:35
"As you went out of your way to assume data is plural of datum here, even though it's more correct to use data as the singular synonym for ""information"", I'll point out that your post title is asking:

""Are your data points normal?""

When you really mean:

""Are your data points normally *distributed*?""

if you're going to be pedantic, be pedantic.  Each datum cannot be normal, only the collection can be.",drunken_Mathter,2013-08-07 09:42:44
"I think you're wrong. We often use the names of distributions as adjectives describing ""data"": Normal data, Exponential data, Binomial observations. If you can say ""normal data"" (which [you certainly can](https://www.google.com/search?q=""normal+data""), it seems), why shouldn't you be able to ask ""are your data Normal?"", just as you would ask ""is your house big?""",CrazyStatistician,2013-08-07 10:12:46
"it's about gammar.  Yes, you can say normal data, if you are using data to mean ""collection of points"".  Since the author wrote 'data' as plural, normal is describing each points, not the collection. 

If you read my comment carefully, you would have picked up the distinction.  I said it's correct to say the collection is normal, where=as the author used data to mean ""more than one, where each is described as"".

My point is if you go out of your way to use data as plural, you need to agree with the rest of the grammatical structure.",drunken_Mathter,2013-08-07 11:03:28
"I think people is giving you too harsh a treatment with the votes. It seems correct to me too to say that it's the distribution, rather than the data, which is normal. Maybe being precise about that would even aid understanding for a reader who is not so familiar with the topic and is already struggling to understand.

Edit: Oh, maybe I misunderstood you anyway! Three things could be said to be normal: (0) a datum, (1) the data and (2) the distribution. I thought you meant ""don't do (1); do (2)"", but maybe you meant ""don't do (0); do (1)"".",Bromskloss,2013-08-07 11:10:28
"i'm just annoyed that the author put ""data"" as the plural of ""datum"" but didn't correct the predicate.  Why go out of your way to use an arcane (though correct) meaning if you just screw up the rest of the sentence?  Again, I don't mind being technically correct (I'm a prig myself!  obviously!), but if you're going to do it, do it well.",drunken_Mathter,2013-08-07 11:20:14
"So, from what I am understanding, you believe it would be correct to say

&gt; Normal data is [...].

but not

&gt; Normal data are [...].

Is that a fair characterization of your position?",CrazyStatistician,2013-08-07 11:31:20
"you're mixing the two usages of data, that's the problem.

""The cats are blue"" means each cat is blue.

""The data are normal"" means each datum is normal.

""The colection of points is normal"" is correct.  ""The data is normal"" is correct.  ""The data are normal"" is not.",drunken_Mathter,2013-08-07 11:42:17
"I was intentionally ""mixing the two usages of data,"" as you say, because I was trying to make sure I understood the distinction you made.

I still think ""the data are normal"" is perfectly acceptable. But then I've never really adhered to prescriptivist grammar.",CrazyStatistician,2013-08-07 11:56:04
"It seems that you have understood then.  Statistics is language perscriptivism, just as grammar.  Statistics does not arise from mathematics, it is applied mathematics, which requires language to be meaningful.  If you don't say it correctly, it's meaningless, and you come off sounding as someone who cannot understand the concepts.

This is a joke, really.  Yes, let's just all refer to a datum as normal.  It's totally meaningless.  It ignores the specific distinction between a point and a distribution, which is core and central to statistics, which is why, specifically, it does matter.

The author goes out of his way to point out that data is plural (again, correct) but then destroys the statistical content of the title by predicate disagreement.  So why on earth would i take the article seriously?  Poor communication means poor understanding.


I don't really care, but it's just stupid.  Why go out of your way to say data (plural) ?  What's wrong with data is normal?  it's correct.  ""data are normal"" is not.

What's wrong with being correct, exactly?  Why would I take anyone serious who cannot get this simple concept correct?",drunken_Mathter,2013-08-07 13:04:02
"So by this description, it would be incorrect to say that ""The people of Alaska are mostly concentrated around a few cities"" because it would be ascribing a group property (of concentration, suggesting density) to each individual unit (person) that makes up the plural noun?

I can't say that I agree with the prescriptive stance you're advocating, because the sentence I've created here seems to be a perfectly valid construction.

Other examples:

""The billiard balls are spread uniformly across the table.""

""The diatomic chlorine molecules are evenly distributed in this bottle.""

By this prescription, these would have to be revised to read as ""The collection of ... is ..."", which not only makes for a very awkward construction, but seems totally unnecessary.",Neurokeen,2013-08-07 15:19:53
"&gt; ...are concentrated

&gt;... are spread

&gt; ... are ... distributed

I totally agree with you here, you are precisely making the same point I did in my first comment.
&gt; When you really mean:
""Are your data points normally distributed?""
",drunken_Mathter,2013-08-07 16:24:46
"I think you're ultimately conflating collective and plural nouns. 

What you're saying would be the case if the word data were a collective noun. However, this shifting between the singular and plural verb only applies in the case of collective nouns where the speaker wants to emphasize the property as being attached to the units of the whole or as the whole itself. With simply plural nouns, there isn't really a choice.

The word data, to be hyper-correct if there is such a thing, is simply the plural of the latin 2nd declension datum - not necessarily a collective noun describing a grouping of datum.",Neurokeen,2013-08-07 19:24:20
"Actually you have it backwards.  I agree with you noting that data is not collective when used to pluralize datum, which is how the author used it, and precisely why ""are you data normal"" is incorrect.

When used as a collective noun, which is a colloquialism which I accept, it's then appropriate to say ""*is* your data normal"" while using the plural form, it is only appropriate to say ""is your data *normally distributed*"".  ",drunken_Mathter,2013-08-07 20:00:42
"I'm confused then.

If it's not collective (and therefore we're not able to talk about the noun as a singular group), then it's simply plural, and the plural form of the basic copula is ""are"".

""The child's toys *are* strewn about.""

This construction, then, is no different than:

""The data *are* normally distributed.""

I still don't see how you're making a distinction between these two sentences, and if I were to say ""The child's toys *is* strewn about"" it would clearly be incorrect.

Basically, to recap:

1) ""*Data*"", as used here, is a plural noun, without any exception rules (hence the comment about collective nouns, which the word ""data"" is not).

2) The plural copula is ""*are*"".

Therefore, ""*Data are...*"" is a correct construction.

I don't see anything that goes against either of those.

Regarding the earlier point where you suggest that ""normal"" be replaced with ""normally distributed"", I don't see where simply saying ""normal"" suggests any more than ""normal in distribution"" or the word ""exponential"" would simply imply ""exponential in distribution"". Adding the extra two words (""in distribution"") isn't syntactically required.

Going back to the ""strewn about"" example, no, a single toy cannot be strewn about, but many toys can be. This construction is not implying a property inherent to each particular toy.",Neurokeen,2013-08-07 20:12:32
"OK, I think I see.

Look at this:

&gt; Data leads a life of its own quite independent of datum, of which it was originally the plural. It occurs in two constructions: as a plural noun (like earnings), taking a plural verb and plural modifiers (as these, many, a few) but not cardinal numbers, and serving as a referent for plural pronouns (as they, them); and as an abstract mass noun (like information), taking a singular verb and singular modifiers (as this, much, little), and being referred to by a singular pronoun (it). Both constructions are standard. The plural construction is more common in print, evidently because the house style of several publishers mandates it.
[From here](http://www.merriam-webster.com/dictionary/data)

When data is used as the plural of datum, it's like this sentence:
""The data are red"".  Each data point is red, red refers to a characteristic of each data point.

When you say ""The data is red"", it appears to mean the same thing, but it's not correct.  
When using the singular, you are referring to the collection and not each point individually.  The mass of information cannot be red.

To be clear, using ""data are"" and ""data is"" are both correct, but the predicate must agree with the subject and the verb as well.  And here is where ""normal"" cannot be used to describe each individual datum, and so cannot be used as 'The datums are normal"", however, the correct way to say that is ""The datums are normally distributed"".  This is because the datum / data / data points are *distributed*, and normally modifies the type of distribution.




It's quibbling, I know that.  But why bother saying ""data are"" if you aren't going to treat it properly as a plural noun?  

Normal (as opposed to *normally distributed*) doesn't make sense when attempting to deconstruct the sentence and apply the predicate to each subject individually, it can only be applied to the group of points.

Frankly, it's far more correct to say ""The data are normally distributed"" than any of them.  But the author didn't.  My only real point, ignoring all the rest: if you're going to speak about data (plural of datum), they are distributed.  ",drunken_Mathter,2013-08-07 20:35:04
"To my ear, ""data"" sounds plural.  I can't help it.  And according to Grammar Girl, both usages are standard:

http://www.quickanddirtytips.com/education/grammar/data-singular-or-plural?page=all

",AllenDowney,2013-08-07 10:22:49
"I'm not disagreeing if it's plural or not, but if you use data as plural, each point cannot be normal, you must say the multiple points of information (data) are normally distributed.  Each point cannot be normal, though the collection of points can be.
",drunken_Mathter,2013-08-07 11:04:55
Why is digest so popular? ,rm999,2013-06-13 11:15:50
"That's a very good question. From the description it is used to ""Create cryptographic hash digests of R objects"". But I would guess the ""cryptographic"" part isn't why the majority of people use it. Looking at the package page it contains a lot of regular hash functions. Those can be useful to uniquely (for a loose definition of unique) identify an object if there is no other way to get a unique id for it. Getting a unique id is important when using the object in e.g. a lookup table.",reallyserious,2013-06-13 13:27:34
"It's also used by some pretty widespread packages, including ggplot2 and knitr (most likely for precisely the reasons you mentioned), which probably helps.",ThisIsDave,2013-06-13 15:39:15
Good observation. If it's a dependency to other popular packages it will be right up there at the top even if nobody explicitly uses the functionality in their own code. ,reallyserious,2013-06-14 01:26:29
"Who's the joker that fit a quadratic curve to those points?  Anyway, SAS curmudgeon's will argue that the open source routines can ""contain mistakes"" and ""aren't production ready.""    And the R proponents will continue getting work done.",DrNewton,2013-03-20 15:20:13
"I also like how the quadratic curve is centered after zero, like some jerk was going around removing R packages from CRAN. ",rm999,2013-03-20 18:50:55
dollars to donuts says it was Johan Jensen.,zdk,2013-03-20 21:29:14
"I use R almost exclusively but I think there is some merit to the predictable SAS criticism. Many R packages are not high quality and do contain errors. But then so does my own code, which I have to write more of in SAS.",deadsalle,2013-03-20 18:09:28
"Should have used local smoothing.

Also, I hate SAS syntax with a passion, and their graphics while improved over the old dot matrix crap is still light years behind R.",anonemouse2010,2013-03-20 20:02:31
"This is an extremely deceptive analysis because ""number of libraries"" in an opensource toolkit absolutely does not directly map to the ""capability"" of that toolkit. As R gains contributors, more and more people are going to develop libraries that accomplish exactly or nearly the exact same thing. Because SAS is privately developed, you're basically assured that for anything you want to do there's only one way to do it. 

For example, let's say I want to compute the credible interval for a posterior from a bayesian analysis. I could use `HPDinterval()` from the `coda` library, but hey, check this out! There's some guy writing a book called *LearnBayes*, and he has a function for calculating HPD's called `emp.hpd()` in his `TeachingDemos` library. 

Both of these libraries are available from CRAN, but everything in `TeachingDemos` can probably be accomplished with functions from other libraries. I don't really know my way around SAS (yet) but I strongly suspect that there is only one HPD function available.

This is just one, small, isolated example. This analysis assumes that each library and function available in CRAN is unique and provides functionality not available elsewhere. This is clearly a false assumption, and although it is interesting that CRAN has grown so much, I don't think this is a particularly well done or meaningful analysis.

Additionally: how do you account for the parameters different procedures can take? Should we consider the ""capaiblity"" of a single procedure to be 1 single unit, or should it perhaps be some function of the required and optional parameters? I can't think of any good examples at the moment, but surely the ""capability"" of a function that takes a single parameter is less than the ""capability"" of a function that can take up to 20 that modify its behavior and output.",shaggorama,2013-03-21 10:00:02
"You're right, there is one main one main procedure for doing HPDs in SAS called PROC MCMC, which comes with a very detailed 286 page [user's guide](http://support.sas.com/documentation/onlinedoc/stat/121/mcmc.pdf) with lots of examples and graphics. Everything that the procedure does is well documented through equations and references to peer-reviewed literature.

This single procedure, which accounts for most of the bayesian capabilities in SAS, is compared to a single R function in this article. The comparison cannot and should not be made.

I'm tired of these ridiculous adversarial comparisons between these two programs. The SAS system is designed first and foremost for high volume data processing, on zSeries mainframes for instance, and second for business analytics. It is excellent, high-quality software for this purpose and there is nothing comparable. DATA step programming is somewhat unwieldy compared to the R language for general data management, but feels completely natural when you need high-throughput I/O. See: Program Data Vector (PDV).

R is an excellent general technical computing environment. Setting aside the statistics capabilities for a moment, the closest comparison would probably be MATLAB. With few exceptions, it is an in-memory application which gives you a lot more flexibility to manipulate your data with matrix/vector operations, for applications like raster image analysis. One downside is documentation - very few packages come with high-quality cohesive documentation as SAS procedures do. You often only find out how a package works through a mix of reference manuals, vignettes and source code.

Different programs for different purposes. I use both almost every day and gladly pay for both: by contributing packages to CRAN, and by paying license fees to SAS for their expertise in high-throughput data processing.",zip117,2013-03-21 18:31:13
FDA still uses SAS. Big corporate hijacking progress.,,2013-03-20 18:59:39
"Thats not strictly true, they don't require any specific software package be used.

There's an official [R: Regulatory Compliance and Validation Issues
A Guidance Document for the Use of R in Regulated Clinical
Trial Environments](http://www.r-project.org/doc/R-FDA.pdf) from the R site itself which covers aspects of R and the guidelines the FDA and other Good Clinical Practice (GCP) guidelines and so forth.

There is also a [this blog post](http://www.r-bloggers.com/fda-r-ok-for-drug-trials/) about a [poster](http://blog.revolutionanalytics.com/downloads/FDA-Janice-Brodsky-UseR-2012.pdf) presented at the 2012 useR! conference presented by the someone from the FDA saying that submissions could use R.

The FDA themselves set guidelines/standards, others conduct the research in line with these.

What the above analysis doesn't take account of is the redundancy that exists between the many R packages, nor as others point out the quality of the code and whether packages are maintained.",enilkcals,2013-03-21 02:18:55
[deleted],,2013-03-21 08:07:36
"Not preaching/ranting as it sounds as though you're up against this sort of stuff already, just my thoughts on the situation...

I recognise that this is a problem, but personally I'm much more comfortable with the situation with R and its packages than a commercial program for the simple reason that its open-source.

Not only does this mean that in all likelihood more pairs of eyes have looked at the code over time and corrected/improved problems/errors, but also that any corporation wishing to adopt it can sit down and go through the source code with a very fine tooth to ensure that it is as you say ""do[ing] what it says it does AND doesn't do anything else"".

You just can't do that with SPSS/SAS or even a lot of Stata routines that are hard coded rather than written as ado files.  Instead you're paying for a license that the software purports to do what it says its doing, but in reality no software is ever bug free and there are always corrections &amp; bug fixes being incorporated in minor and major version releases.

Its the mindset of industry that is the problem ""someone's got to be held repsonsible for X, Y and Z"".  Take responsibility yourselves, inspect the code and check that its doing what it purports to do, if not correct it, feed it back and everyone benefits, the beauty of open-source.

",enilkcals,2013-03-21 09:09:44
"- the difference between mean estimation and individual prediction (or, what SEM and SD *really* mean)
* the assumptions of parametric tests, how to check for them, and how robust certain tests are to violations of these assumptions
* (for scientists): how the SEM is really just the ~68% confidence interval and is not a particularly useful thing to graph (a pet peeve of mine)
* the general linear model and how it is the basis for: t-tests, ANOVA, MANOVA, ANCOVA, logistic regression, survival analysis, etc. Most statistics a scientist ever deals with are based in the GLM.
* Power, type I and II errors, positive/negative predictive value, sensitivity, specificity, etc. All of the terms that come from combining the different terms of a binary classification matrix (these terms apply to statistics outside of binary classification statistics)
* bayesian stats and how this can give you more information than just ""p=0.05""

let me know if google/wikipedia aren't enough for these and i can help point you in the right direction. Also, I am not a statistician. I am a biomedical scientist who uses and reads a lot about statistics.",BillyBuckets,2012-08-21 10:42:07
"SEM: structural equation modeling? No, you clearly mean standard error of the mean. But means aren't always what you estimate! Just call it a standard error. You can have standard errors of medians, proportions, regression coefficients, odds ratios, etc. Not all are symmetric, not all can be computed just one way, not all exist.",,2012-08-21 14:35:40
"Sorry, I was biasing this toward t-distribution statistics, which are so straightforward but *still* misused by biologists constantly.",BillyBuckets,2012-08-21 17:39:59
"That's fine. I think whenever you can actually calculate the standard error of an effect estimate, it's very helpful to report the estimate and its CI in words. This is why I kind of like the chi-square test versus Fisher's exact test. The former is the score test for the binary adjusted logistic regression model, so the odds ratio and it's CI are useful in that case.",,2012-08-22 08:33:34
"&gt;(for scientists): how the SEM is really just the ~68% confidence interval

Be aware that this is true for normally distributed data. If you have reasons to suspect fat tails,  don't trust this.

But I agree: if you can determine standard deviation, you can as easily calculate confidence intervals and inter quantile ranges which are much more useful. ",calsaverini,2012-08-21 15:52:03
"&gt; (for scientists): how the SEM is really just the ~68% confidence interval and is not a particularly useful thing to graph (a pet peeve of mine)

SEM is useful in showing the general spread of data, or where error tends to spread. However, lately I have been moving away from plotting the SEM to actually plotting the distribution with violin plots or density estimates similar to the [""spaghetti"" plot](http://andrewgelman.com/2012/08/graphs-showing-uncertainty-using-lighter-intensities-for-the-lines-that-go-further-from-the-center-to-de-emphasize-the-edges/), since with today's computers we can plot these things pretty much instantly.

Do you have any references for distribution plotting techniques such as these?",zsakuL,2012-08-21 14:12:26
"Thanks BillyBuckets, that's a pretty good list. Fortunately I know most of those and their mathematical background!

",I_kick_puppies,2012-08-22 04:36:30
"I'm not sure, but understanding 1) regression analysis and 2) exactly what a p-value (and analysis of variance techniques like ANOVA) is and how it works probably wouldn't hurt.",Eist,2012-08-21 10:33:42
"If you're talking general concepts, to me, there are the two crucial ideas:

1. **Randomness and random variability:** The idea that observations vary.  If you take five 5lb bags of flour, they're not going to have exactly the same amount of flour in them.  Same would go for five 20oz bottles of coke, five nurses taking your temperature, etc.  This is all noise, and from this noise you have to find the signal.

2. **Null Hypothesis as the status quo:**  Nobody said it better than Ronald Fisher: ""We may speak of this hypothesis as the 'null hypothesis', and it should be noted that the null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation.""

If you really understand the theory behind these concepts, you can understand hypothesis testing experiments, estimation, and all of the more specific items that /u/billybuckets listed.  ",OhDannyBoy,2012-08-21 12:32:40
I was talking more about concepts that could be applied. But these two points you made seem quite important. I shall add it to the list of things to look over!,I_kick_puppies,2012-08-22 04:41:55
"Bayes' theorem. All different interpretations of it. 

You don't need to go into the details of the controversies behind the interpretations, but learn how to use the theorem in all its different incarnations for useful  data processing.",calsaverini,2012-08-21 15:55:04
"Any good resources for looking more into this? I am versed in stats from a research stand point, I've used bayes theorem but not much more than understanding conditional probabilities. Anything that can help me understand the breadth of its uses and any practical resources would be lovely. ",Palmsiepoo,2012-08-21 20:06:56
"Check out Bayesian logical data analysis by Phil Gregory and Understanding Computational Bayesian Statistics. That's what I am using to get a better understanding of Stats. BTW, I also have a similar background so I think the level of math in these books will be sufficient. ",brownck,2012-08-21 21:21:57
"The interpretation of Bayes' theorem for conditional probabilities is not disputed at all. It is a mathematical consequence of the definition of conditional probability. 

The contentious area starts when you start attaching names such as ""prior"" and ""posterior"" to your conditional distributions. This gives a different meaning to them. You're not anymore conditioning just on events (which is absolutely well accepted by everyone), you are conditioning it on *prior knowledge* or *posterior knowledge*. This is contentious. But is also very useful. It the concept don't offend you, it opens new ways of doing data analysis. 

You may do a (cautious) reading of Edwin Jaynes' ""Probability Theory: the Logic of Science"". The book do a bit of polemics about frequentists and bayesians and etc, but if you ignore the author's ranting (and also some misplaced oddities he says about quantum mechanics), it's an incredibly good book. ",calsaverini,2012-09-01 06:05:47
"1) Linear models. Backwards and forwards
2) Asking statistical questions. As a statistical consultant, this can be the majority of the time taken in consulting.
2) Design of experiments, builds on 1). If I have a question, what data is necessary to gather that is cost effective to answer that specific question. Sample size calculations.
3) Basic testing of hypotheses given appropriate data
4) For a job, basic R and SAS
5) Statistical significance vs. clinical/ practical significance.
6) Tests for distributional assumptions
7) Model selection, and adjustment for multiple testing
8) Statistical graphics: how to tell what directions to go with a dataset, and what you won't be able to do.
9) Bayesian stats
10) I think if you assimilate this stuff, the final key chunk is to be able to tell people when they are doing a legitmate statistical analysis, and when they are just mining for a significant result.

I would recommend Data Analysis using Regression with Multilevel/ Hierarchical models by Gelman and Hill.",aerotuck,2012-08-22 19:44:24
"First and foremost: statistics is not math. Maths is certainly involved, but numbers alone (i.e. summary statistics) generally aren't sufficient to completely describe a dataset appropriately. See [Anscombe's Quartet](http://en.wikipedia.org/wiki/Anscombe%27s_quartet).

Also, most statistics is grounded in probability. If you're thinking of picking up one or the other, I'd start with probability first and then move on to statistics after you've got a feel for probability.",shaggorama,2012-08-21 13:53:24
"Set theory. Permutations, combinations. 

How to use R.",,2012-08-21 21:12:23
"Before knowing about any particular frequentist procedure, you really need to understand the intuition of frequentist inference itself and that requires understanding the sampling distribution of a statistic.  To really understand that, you probably also need to understand the alternative option, the posterior distribution.",equark,2012-08-22 00:39:00
"The truth and only the truth about p-values.  The **DATA** is unlikely if you **ASSUME** a specific model, which you already know is wrong.  It doesn't tell you anything about repeatability, likelihood of the alternative hypothesis, etc.",inspired2apathy,2012-08-22 06:20:13
"Not a statistician but use some in business. Most useful tools are histograms, pareto charts, x &amp; x-bar charts, the difference between correlation and causation, what Statistical Process Control means and its relation to six sigma. ",donotclickjim,2012-08-21 11:12:55
"Not sure why you're getting downvoted, although those things are important, it's tough to provide a real appreciation for them in a business setting or something like six sigma which provides a really surface-level understanding of things. Even something as simple as causation (not even correlation vs. causation) is a topic I have devoted years to understanding and it is one of the most misunderstood concepts in methodology. As for stats, sometimes there is nothing more useful than graphing data (see the above posters reference to ascombe's quartet.)",Palmsiepoo,2012-08-21 20:09:31
"Hey thanks. I noticed the down votes and thought I just wasn't stat geeky enough for /statistics but the truth is most people in the business world have an appreciation for stats and would trust it more if only they knew more. This is why graphs are so important. Most of the population glaze over if you try to explain an equation but show them a graph and the lights come on. I feel this is why there is such a huge disconnect between the science world and the rest of us (people like me). You can ""prove"" the ""right answer"" all day long using stats and math but if we don't get it we won't believe you and suspect BS. I so wish I could stress to academia that it doesn't matter if you understand the material because if you can't explain it in a way that the average person can understand it no one will listen to you.",donotclickjim,2012-08-21 20:32:55
"I deal with this all time. However, just because something is simple doesn't mean it's right. Nor does complexity mean better. The question comes down to: what is more appropriate? If it's more appropriate to show a graph, then use one. But if it's more appropriate to perform some form of inferential statistical test, then that's what you use. The right tool for the right job. 

It absolutely drives me insane when people disregard the optimal method for finding an answer simply because they either don't understand it or don't want to try something new. I get that shit all the time. In the same way statisticians need to break down complex answers into meaningful information for business-people, business people need to trust that if they hire a statistician, methodologist, or researchers, that they will give you the best answer. Unfortunately, in businesses, many times people just want to ask a question that can only give them a positive answer, which is not only unethical, but it's inaccurate. Positing the question: ""did it work?"" is not the same as ""how much did it work?"" where statisticians would ask the former, business people tend to ask the latter.",Palmsiepoo,2012-08-21 20:45:57
"Agreed. I ran a T-test one time to ""prove"" a planned change had the desired results. We spent hours explaining what exactly the results meant. ",donotclickjim,2012-08-21 20:54:59
Cross-validation!,pandemik,2012-08-21 10:36:55
"Gauss Markov assumptions built into linear regression models: what they are, when they're satisfied, what effects violations have on the results.  ",jambarama,2012-08-21 11:50:34
I hope you're referring to the fact that whether or not the residuals are normally distributed doesn't matter a good god damn unless you're doing prediction intervals.,,2012-08-21 14:38:38
Eh.,Eist,2012-08-21 16:26:41
The book [All of Statistics](http://www.amazon.com/All-Statistics-Statistical-Inference-Springer/dp/0387402721) gives a broad but (relatively) quick introduction to modern statistics.,blind_swordsman,2012-08-21 10:56:15
"Wasserman is a bit pretentious with these series. In my program's advanced theory of statistics course, they looked at his ""All of Nonparametric Statistics"" and problems were like, ""Go through the derivations of the author's conclusions on page X, do you agree with him? Why not?"" and ""Prove result Y is wrong.""",,2012-08-21 14:38:01
"I am sure that your mathematical skills are strong enough to do any of the mathematical tasks in statistics.  

 I might suggest a more naive, non-mathy approach of the big picture, and then deepen your understanding with a more mathematical approach.  Look at statistics within a context. Use real data.  

Here are common topics that I think are fundamental: 
1.  With any study, be sure you understand:  what individual units did we collect data on?    What variables were collected on each individual?  You must have these nailed down before you know which statistical tools are appropriate to use. 
2.  Graph your data before looking at any numbers.  Draw a Picture, Draw a Picture, Draw a picture.   See how the data are distributed before  making assumptions based on numbers.   

3.  Understand as best you can the purpose of random assignment in an experiment, and the purpose of  random selection in a sample. If we don't have these, we can't apply mathematics to make inferences from samples.  

4.  Understand the basics of probability.   Understand when a Venn diagram, a Tree diagram,  or a two-way table (contingency table)  can be used to organize what you know.   KNOW YOUR SAMPLE SPACE.   

5.  Key issues in probability  Understand whether you're computing P (A given B)  or the P(B given A)  in context.   Interpret appropriately.  

6.  Sampling Distributions. They form the basis of confidence intervals and hypothesis tests. There are some wonderful Java Applets to help you understand sampling distributions http://onlinestatbook.com/stat_sim/sampling_dist/index.html

7.  What does a p-value mean, in context?     

8.  When you construct a 95% confidence interval, be sure you understand the this represents the chance that in the long run, 95% of all intervals successfully capture the population parameter of interest. 

9.  And ALWAYS understand / check the assumptions &amp; conditions under which a statistical procedure is valid.  

I hope this isn't too simple.  ",stat_geek,2012-09-06 19:59:50
"Here's some R code I wrote that recreates the histograms.  The very last line shows how the density plot would look.

Histogram http://imgur.com/KlcFO

Kernel Density http://imgur.com/bt807

    thedata &lt;- c(3.15,5.46,3.28,4.2,1.98,2.28,3.12,4.1,3.42,3.91,2.06,5.53
    ,5.19,2.39,1.88,3.43,5.5,2.54,3.64,4.33,4.85,5.56,1.89,4.84,5.75,3.22
    ,5.52,1.84,4.31,2,4.01,5.31,2.56,5.11,2.58,4.43,4.96,1.9,5.6,1.92)
    A &lt;- cbind(thedata       , factor('A', levels=c('A','B','C','D')))
    B &lt;- cbind(thedata - 0.25, factor('B', levels=c('A','B','C','D')))
    C &lt;- cbind(thedata - 0.50, factor('C', levels=c('A','B','C','D')))
    D &lt;- cbind(thedata - 0.75, factor('D', levels=c('A','B','C','D')))
    df &lt;- data.frame(rbind(A,B,C,D))
    require(ggplot2)
    qplot(thedata, facets=V2~., geom=c(""histogram""), data=df, binwidth=1)
    qplot(thedata, facets=V2~., geom=c(""density""), data=df)",DrNewton,2012-02-19 07:43:39
"cool; thanks for going to the trouble. 

I note a small discrepancy in the plots; this seems to be caused by a slight difference in the way values on the bin boundaries are placed into bins (the specifics of the circumstances when the intervals include the boundaries to the left or to the right, or some mixture of the two); it appears that Splus (which is what I believe I generated the histograms in back in 2008) might have used a different convention to R. (Edit: or maybe it's a rounding-artifact if the histograms were generated pre-rounding.)

Specifically, if the values that can hit bin boundaries are changed slightly (from 5.50, 5.75 and 2.00 to 5.51, 5.74 and 2.01, say), the histograms will look the same as in the original post:

    histdata &lt;- c(3.15,5.46,3.28,4.2,1.98,2.28,3.12,4.1,3.42,3.91,2.06,5.53
    ,5.19,2.39,1.88,3.43,5.51,2.54,3.64,4.33,4.85,5.56,1.89,4.84,5.74,3.22
    ,5.52,1.84,4.31,2.01,4.01,5.31,2.56,5.11,2.58,4.43,4.96,1.9,5.6,1.92)
    opar&lt;-par()
    par(mfrow=c(2,2))
    hist(histdata,breaks=1:6,main=""Annie"",xlab=""V1"")
    hist(histdata-0.25,breaks=1:6,main=""Brian"",xlab=""V2"")
    hist(histdata-0.5,breaks=1:6,main=""Chris"",xlab=""V3"")
    hist(histdata-0.75,breaks=1:6,main=""Zoe"",xlab=""V4"")
    par(opar)

I will edit the original post (edit: now done) with this slightly modified data that avoids the bin-boundary issue. The changes make essentially no difference to the kernel density estimate. For KDEs I'd suggest a narrower bandwidth (and not cutting off the density estimates at hard boundaries), more like so:

    opar&lt;-par()
    par(mfrow=c(2,2))
    plot(density(histdata,bw=.2),main=""Annie"")
    plot(density(histdata-.25,bw=.2),main=""Brian"")
    plot(density(histdata-.5,bw=.2),main=""Chris"")
    plot(density(histdata-.75,bw=.2),main=""Zoe"")
    par(opar)

(edit: I also added in this code to the post - since I wrote it out, I guess it should be available there. Providing code is a good idea, thanks.)
",efrique,2012-02-19 09:01:54
Thanks for this. I love wheni get to see how others code. ,metamorphaze,2012-02-19 08:11:30
"Statistics are summarizations/compression of data. Any time you throw out information something is lost. It's up to you to understand your datasets before blindly applying statistics to them and interpreting only those statistics. 

Histograms and kernel density plots are great tools to quickly represent an entire distribution, but lots of information is thrown out. I think well-designed histograms work fine the vast majority of the time, but this post is a good example of what can go wrong. ",rm999,2012-02-19 12:47:12
Thanks for this post!,ShannonOh,2012-02-19 03:57:35
"Interesting post... thanks!

You backed off on mentioned other techniques for dealing with these problems, but I think you could have mentioned one simple one: plot cumulative distributions.  Empirical CDFs represent all of the data -- no rounding off, no bin width.

The drawback is that it takes time for students to get used to interpreting them.",AllenDowney,2012-02-21 04:14:10
"&gt; plot cumulative distributions. Empirical CDFs represent all of the data -- no rounding off, no bin width.

I did, however, include R code for computing it at the very end of the post.

You're quite right that I could have - and should have, really - discussed it, but I was primarily interested in discussing displays that directly convey a sense of the shape of the density.",efrique,2012-02-21 23:50:26
"I protest: this is not for dummies!

And I should know...",iwasanewt,2011-09-10 07:54:18
"I'm in the same boat. I think what this might be is a tutorial for someone who already has a measure theory problem, but doesn't have any experience.

Measure Theory for Dummies seems like it would be easier to understand if the motivation were clearer.",TheFrigginArchitect,2011-09-12 08:54:23
"I did something kind of similar in Tableau:

http://public.tableausoftware.com/profile/bdeely#!/vizhome/Piketty-Visualization/Piketty-IncomeNormal",bordumb,2014-06-30 06:56:09
Cool!,t_rex_tullis,2014-06-30 07:05:24
This was something I started to search for when the controversy stated. This will hopefully see the end of closed research and papers.,mtelesha,2014-06-30 17:32:46
"When I read the title my first thought was, this had better be an article about Kolmogorov. ",collynomial,2014-01-09 06:29:41
"My first thought was Bayes, from which you will correctly infer that I am over 200 years old.",jenpalex,2014-01-09 19:33:23
"I know [r-bloggers.com](http://www.r-bloggers.com) aggregates many of these, but I find it overwhelming and use it more for reference than perusing. Some individual blog picks:

* [Andrew Gelman's blog](http://andrewgelman.com/), best overall statistics blog IMO and probably the longest running, everyone reads this

* [Larry Wasserman's blog](http://normaldeviate.wordpress.com/), no longer updated :( but tops in theory discussion, read the archives

* [Simply Statistics](http://simplystatistics.org/), run by several faculty at Hopkins biostat, tends to talk shop and provide career advice more than the others, which I appreciate as a student

* [Karl Broman's blog](http://kbroman.wordpress.com/), also skews shop talky

* [Cosma Shalizi's blog](http://bactra.org/weblog), has as of late been mostly teaching materials, but they are well-written enough that I find myself reading them when that feed pops up

* [Christian Robert's blog](http://xianblog.wordpress.com/) (Xi'an)

* [Allen Downey's blog](http://allendowney.blogspot.com/)

* [Thomas Lumley's blog](http://notstatschat.tumblr.com/), which is newer to me than the rest of these. I also enjoy his department's [Stats Chat](http://www.statschat.org.nz/) blog, though that is not very technical and more general interest statistical material.

**Visualization focused:**

* [Flowing Data](http://flowingdata.com/)

* [Junk Charts](http://junkcharts.typepad.com/junk_charts/)

* [chartsnthings](http://chartsnthings.tumblr.com/), New York Times Graphics Department sketches",normee,2014-01-01 12:56:19
A good list. ,efrique,2014-01-02 05:28:41
http://andrewgelman.com/,ApproximatelyNormal,2014-01-01 12:48:17
http://simplystatistics.org/,,2014-01-01 14:05:05
www.r-bloggers.com will you get started. ,UpDown,2014-01-01 12:13:32
http://www.statsblogs.com/ as a general start next to r-bloggers,wiekvoet,2014-01-01 12:27:09
http://datacolada.org/,yourtrashysister,2014-01-01 16:00:59
http://how2stats.blogspot.com/p/home.html,alphabetcereal,2014-01-01 16:22:37
"This isn't necessarily ""statistics"", but I guess more econometric centric and wonky. Either way, it should interest some people here since there's a lot of data analysis: [Econbrowser](http://www.econbrowser.com/)",beepbeepbeep99,2014-01-01 18:01:47
"krugman.blogs.nytimes.com

Not because he goes into a lot of statistical sophistry, but because he takes situations, looks for data, draws graphs and walks you through interpretations.  ",wil_dogg,2014-01-01 20:34:47
Thanks. you're all awesome. I notice though that no mention was made of [Yhat](http://blog.yhathq.com/). Its one I enjoy reading =),I4gotmyothername,2014-01-03 12:31:11
*Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel*.,naught101,2013-10-14 23:10:25
"In the comments: ""Thanks. I wrote the piece on the Nobel winners that will be published in the Wall Street Journal tomorrow. I wish I had seen this before closing the piece.""

oof.",7h3C47,2013-10-15 08:30:28
"This is not a port of ggplot. Rather it's a wrapper around matpltlib that allows you to use ggplot syntax. 

This is both a fail and a win. First it's a win because it produces what look like high quality figures. It's a fail because the syntax is not pythonic. 

Python needs a plotting library that is beautiful but that plotting lib should also work within the standards the python community has come to know and accept. ",bilateralconfusion,2013-10-15 04:28:16
"Well, those plots are produced by matplotlib, so it's possible to produce nice quality figures in it. Basically it's a matter of re-defining the matplotlib defaults, which could be done in matplotlib's conf file, couldn't it? Has anyone attempted this yet?

BTW, I'm still fairly new to matplotlib, but it also doesn't strike me as particularly pythonic, since it's basically copying a whole lot of syntax from matlab.. ",naught101,2013-10-15 06:51:50
"oooh, looky here: http://tonysyu.github.io/mpltools/auto_examples/style/plot_ggplot.html",naught101,2013-10-15 17:42:36
"Yay! But wtf?

&gt; **Goals**
&gt;
&gt; * never use matplotlib again
&gt;
&gt; **Dependencies**
&gt;
&gt; * matplotlib
",naught101,2013-10-15 06:46:49
You know what they mean.,shaggorama,2013-10-15 09:55:10
[deleted],,2013-10-14 11:44:32
[deleted],,2013-10-14 15:52:37
"""save"" button",dassouki,2013-10-14 15:57:28
Do you write all your notes a publicly shown Reddit comments? ;-),Bromskloss,2013-10-14 16:19:18
"Waaaay too much blinking and echo.
",captain_awesomesauce,2013-09-19 20:51:02
That is one beautiful posterior density.,econometrician,2013-09-09 15:46:02
Solarized dark theme? Have an upvote.,valen089,2013-09-09 08:24:53
You should consider x-posting this in R/RSTATS,Mutica,2013-09-11 06:06:24
thanks. hoping you could you give me some advice? I have been skimming through the pdf and the presentation and was wondering what you think about using your library to do something more like [this](http://camdp.com/blogs/how-solve-price-rights-showdown) originally done in pymc?  honestly i am looking for an alternative as pymc isn't working for me in win7 nor ubuntu. :/,whatyou,2013-04-10 14:08:21
"Good question.  I saw this example previously and liked it.  Yes, I think you could code this up in the Think Bayes framework very quickly.  Or if I get a chance, I will code it up and send it along.  It might make a good case study for the book.",AllenDowney,2013-04-10 15:32:24
"Ok, I had a chance to look at this, and wrote this blog post about it:

http://allendowney.blogspot.com/2013/04/the-price-is-right.html

My results are substantially different from the results in the original article.  Can I ask you to take a look, and let's see if we can figure out what's going on?",AllenDowney,2013-04-11 08:57:09
"hmm. I am learning myself but my guess is the [model](http://pymc-devs.github.io/pymc/modelfitting.html#chap-modelfitting)  since in the original article he uses:

    model = mc.Model([final_price, toronto, snowblower, price_estimate, error ])

whereas 

* price_estimate = snowblower + toronto
* error is defined [here](http://pymc-devs.github.io/pymc/distributions.html?highlight=normal_like#pymc.distributions.normal_like).


i am really unsure how ""Model"" works in pymc I am not understanding what it is doing but he certainly seems to be combining a lot more terms in the model including seperately the snowblower, toronto and then using them combined as price_estimate. would be interesting how to scale his and your methods if one were to say have 10 prizes instead of 2. With that said [section 3.5.1](http://pymc-devs.github.io/pymc/tutorial.html) i think gives some more clues i need to let soak in before I am fall further down the rabbit hole.


...

finally, the sampling being done is markov chain monte carlo whereby they run 220k iterations whereas it does not use the first 180k iterations. code here:

    mcmc.sample( 220000, 180000) 


not sure this helps, but I for one am learning a lot :)",whatyou,2013-04-11 12:02:47
"Thanks for checking this out.  I sent a note to Cameron and he reports that he found an error in his code.  He will correct that and then I will update my blog post when we resolve the discrepancy.

In the meantime, I am working on the optimal bidding strategy.  Results coming soon...",AllenDowney,2013-04-11 14:18:26
great! i haven't had time to dig deeper but will take a look this wekend.,whatyou,2013-04-12 09:01:54
"I see he has updated both his blog and the github book he is working on.

seems in the code that the main difference is a squaring of the uncertainties and the normal_like function whereas the third variable is tau

    tau : Precision of the distribution, which corresponds to 1/σ2 (tau &gt; 0).

that he has changed from 1E-8 to 3E3**2 which I am not understanding as the 1E-8 was essentially 1/7500^2.


although he still mentions 28000 within the text of the blog the github document explains it as less 15000 from 35000; nonetheless both are now showing ~20000 in the graphs.

haven't had a chance to figure out the model function, maybe later this week I can spend more time on it. 




edit.. took a look just now, seems he is simply putting all those terms in the model as nodes in the markov chain. I don't understand why.. do you?

edit2 fixed the 3000 above it was 3000**2",whatyou,2013-04-15 11:56:56
"I finished my analysis of this problem and added it as a new Chapter 6 in Think Bayes:

http://www.greenteapress.com/thinkbayes/html/thinkbayes007.html

One interesting result: there are some cases where the optimal bid is actually higher than the contestant's best guess.",AllenDowney,2013-04-22 13:27:06
"Bookmarked it and will look at it at some later point in time.

But I would like to thank you for your awesome books, of which Think Python is the only one that I have completely worked through. At the time I was really proud of some exercise solutions, so I put them here:
http://en.wikibooks.org/wiki/Think_Python/Answers",koloron,2013-04-10 20:11:56
Very nice!  Thanks for letting me know about that.,AllenDowney,2013-04-11 07:08:07
"I actually went back to Think Python (the current version) and did the exercises for chapter 17. In Exercise 17.8 I'm told to import 'read_colors' from 'color_list.py'. However there currently is no such function in 'color_list.py'. That's why your solution 'color_space.py' is broken too…

You can find my workaround here: \*blush\*

http://en.wikibooks.org/w/index.php?title=Think_Python/Answers&amp;oldid=2512124#Exercise_17.8",koloron,2013-04-11 20:06:38
Thanks!,,2013-04-10 10:31:12
"Wow, thanks! ",mmmmatt,2013-04-10 13:56:14
People might also find some interesting bits of Bayesian computation in here: https://github.com/mattjj/pybasicbayes,just2fatty,2013-04-10 17:18:16
"Looks interesting.  Thanks!
",AllenDowney,2013-04-10 18:56:19
Thanks very much for these!  They look very useful.,jtr99,2013-04-10 07:48:45
"Nice talk. Few suggestions:

Slide 68 has a mistake: .021 is not a probability.

Also, probably shouldn't use the pmf abbreviation for pdfs. You may know what you mean, but this leads to serious confusion among newcomers, e.g. ""how can this probability be greater than 1?"" 

The dice example is a little weird. All the conditional probabilities are flat, but the die with the smallest number of sides has the smallest normalizing constant (and therefore the highest likelihood). This means the smallest die that's larger than the largest observed number will always win out. It illustrates how, all else being equal, Bayes implicitly accounts for parsimony.  It still works, but its a bit of a weird example, it seems to illustrate implicit regularization by parsimony rather than the parameter ""fitting"" the data (there is a bit of that with the ""impossible"" outcomes, but that's much more discontinuous than the typical case).

Still enjoyed it though. I hope more computer scientists get engaged in statistics.",,2013-04-10 17:37:00
"Hi, and thanks for these comments.  I am using discrete PMFs to approximate continuous PDFs, so I really mean PMF when I say PMF, and the value 0.021 really is a probability (not a probability density).

I agree that the result of the dice example is counterintuitive.  As you say, the smallest die that is not ruled out wins, but it only wins in the sense that it is the MLE.  But the posterior distribution contains more information than just the MLE.

I'm not sure I agree with your distinction between implicit regularization and fitting the data.  The hypothesis with the highest posterior probability is the one that yields the highest likelihood of the data.  That seems like a straightforward consequence of Bayes's Theorem.",AllenDowney,2013-04-10 18:54:22
"Fair enough, there's nothing wrong with the dice example, I just found it odd because in most introductory examples I've come across the space of conditional distributions implied by the target parameter have the same domain. Of course, they don't have to and it's a perfectly valid application of BT though.

The posterior on the coin example is a beta distribution (scaled to go to 100 instead of 1), so I wasn't expecting a discrete approximation, but I understand now why you're calling it a pmf.",,2013-04-11 18:47:16
This is great. I really appreciate the work you do! ,,2013-04-11 00:54:43
"The pre-reqs for the MS program are very low. 

NYU doesn't have a stats department, you'd think that they'd just create one and give it a very applied/computational focus.",towerofterror,2013-02-20 00:39:12
What are you crazy? Not enough buzz words in that. ,,2013-02-20 04:51:33
"Do they list the elective courses anywhere?

Also I think the link is broken for the 3 semester option.",,2013-02-19 14:33:11
"I'm at NYU. They're launching a whole suite of new courses, though AFAIK no details have been released yet.",zdk,2013-02-19 17:37:36
"""Data science""? ",Anth741,2013-02-19 13:51:59
"The science of manipulating and mining digital data, usually web data. Why?",justdweezil,2013-02-19 23:02:02
"Because I like Data analysis, and machine learning and it sounded fascinating. ",Anth741,2013-02-20 10:58:21
"You know, as opposed to the kind of science that doesn't use data.",WallyMetropolis,2013-02-19 17:39:39
Pseudo science?,deadsalle,2013-02-19 18:04:05
Thanks for this one! ,axey89,2013-02-19 11:22:57
I might apply to this.,DemonKingWart,2013-02-19 17:35:18
"Can't wait to hear if something like this lands in Canada.  Approaches to big data analytics, big data systems, etc. is not being tailored too in education as much as the industry is demanding it.",a2love,2013-02-19 19:08:04
"rstudio.  you can also fuck with the display settings so it looks sweet.  (it's also easier on the eyes, but who cares?)",thestatsmancan26,2013-01-28 10:16:55
They also now support vim keybindings! If you want to be extra-hacker-ish.,giziti,2013-01-28 11:13:14
There is no other answer if you are learning R. RStudio is very helpful when you are just starting out and very powerful when you know how to use it.,MicturitionSyncope,2013-01-28 11:35:40
How!?! When!?! I've been using the R command-line so that it will obey my ~/.inputrc because I literally cannot write code without vi bindings (I'm old). I couldn't get RStudio to do that. This will change my life...,WhoTookPlasticJesus,2013-01-28 21:37:31
"[You've heard of google, though, right? ;)](http://lmgtfy.com/?q=vim+rstudio)",somehacker,2013-01-28 23:58:59
"The most recent release has it as an option. I haven't tested to see how much of vi is implemented, though. It will be under tools&gt;options&gt;code edition&gt;Enable vim editing mode (it's the very last option)",giziti,2013-01-29 05:05:26
"So... let's get into the distinction to help OP understand why his prof uses VIM. There are two types of programmers: ones who use an IDE and ones who don't. Ones who use IDEs think creatively and responsively but can be more prone to errors. Ones who write batch executable scripts think very deliberately and plan. 

Now, I like an IDE as much as anyone else, but I write more reliably and with better outcomes when I edit in a standalone script editor and then batch execute that. First off, I can utilize a lot more script editor tools in my two favorites (jEdit and emacs with ESS) like macros, split screen, and they have a lower latency so I don't twiddle my thumbs when I write a line of a massive `mapply` loop. I can also use the Bash shell to interact with my environment if I need to find datafiles, lib references, or make calls to SQL server to inspect table elements if I'm remote computing. Lastly, if you have to interface with other software like using R to call SQL, C, SAS, and Bash scripts, then multieditors that handle other types of scripts can be very useful.

**TL;DR** If you want to look like a hacker. Emacs will truly be the most 1337 editor you can use.",,2013-01-28 12:07:30
[real programmers use ed](http://xkcd.com/378/),featherfooted,2013-01-28 12:48:27
[Yes they do](http://www.gnu.org/fun/jokes/ed-msg.html),kiwipete,2013-01-28 16:00:43
"As much as I keep trying to get into vim, I agree with the assertion that it has two modes: shitty text editor, and break everything. The learning curve for it is SO bad. Things that should be simple like highlighting text, then copying and pasting that text takes at least 5 minutes of reading the documentation for me, even though I have been working with it for about 10 years. Maybe I'm just lame. ",somehacker,2013-01-28 23:48:58
"What really reduces the learning curve is grokking the grammar of VIM instead of rote memorizing everything.  That greatly compresses the search space for what sorcery to invoke while working.  The grammar is the primary genius behind VIM.  The canonical synthesis of VIM's design paradigm is here: [Stack Overflow Post on Vim's Grammar](http://stackoverflow.com/a/1220118 )

VIM happens to be very well documented and has many excellent screencasts on the environment.  Granted, not everyone's going to be into what appears esoteric, but I do believe anyone can learn it well if they want to and are exposed to quality information sources on the topic.  If an IDE supports using external editors it's a great marriage, as editing is what VIM excels at and is designed to achieve primarily, but it is suboptimal in all other areas, which luckily are where IDEs shine.


",,2013-02-01 00:49:41
"I gave vim are really good go. After weeks of using it and gathering plugins that sped up my workflow (like nerd tree &amp; clang complete), I realised I was just replicating a poor IDE, so I switched back to a real IDE. Sometimes I miss the speed at which I can make cumbersome edits, but I miss it a lot less than all the stuff that comes with a proper IDE by default.",AxiomL,2013-01-29 00:53:58
[RStudio](http://www.rstudio.com/). Very friendly to use and the integrated plots and help functions are awesome.,Fireflite,2013-01-28 10:17:42
"Also the ability to just click to view variables is very handy. There are small things abour RSudio that were/are frustrating, but two or three projects in and I've found it to be completely worth it. ",RSeafood,2013-01-28 13:29:56
emacs with ess rocks.,thewe,2013-01-28 10:55:04
"Caveat, it seriously takes like 6 months to know what the hell you're doing with it.",,2013-01-28 12:08:00
But other emacs users will know you by the hardened look in your eye. The bonds of elisp parentheses are stronger than the bonds of blood.,kiwipete,2013-01-28 16:03:20
"the learning curve is kind of gentle compared to, say, vi, though because you can start with the menus and gradually work up to half your keystrokes being C- or M- .",giziti,2013-01-29 05:08:57
"Seconding this. If you're not going to do RStudio, Emacs with ESS is where it's at. But I like RStudio a lot.",giziti,2013-01-28 11:12:14
I love emacs and ess but I wouldn't recommend it to start with.,zmjones,2013-01-28 13:46:50
"I hope you don't underestimate yourself like you do other people. If one is competent enough to learn a fairly tough language like R, learning a proper text editor along side isn't an issue. I did it just fine. I would be light-years behind if I had learned a bunch of keyboard shortcuts for some other editor, and decided to switch to emacs later. It's too hard to break those habits. ",thewe,2013-01-29 06:39:50
"I suppose that is somewhat true. Experience has taught me that most people don't care about those type of efficiencies (or programming in elisp) and so trying to get them to start with emacs is a bad idea. 

I agree with the sunk costs though. It is much harder to switch after you become comfortable with a particular environment. I too am glad I started with emacs.",zmjones,2013-01-29 06:46:04
"&gt; I would be light-years behind if I had learned a bunch of keyboard shortcuts for some other editor

Don't underestimate yourself! ;-) I had a decade of unix systems experience before switching careers to a field which has me doing statistical coding work every now and then. My editor of choice as Unix Dude? Vi (actual vi, not ""vi improved"" for a goodly portion of that time).

In the last year or so, the amount of programming I do has increased (as has my appreciation of lisp, including emacs lisp), and so I decided to take the plunge and re-wire my brain into emacs. I can't say I'm an emacs superstar yet, but I long since stopped leaving "":wq"" in text documents.

One thing to recommend emacs + ESS over RStudio, is that emacs an ""all things to most people"" kind of affair. RStudio is great, and I recommend it to people who I don't think will do any coding beyond R, but emacs has solid support for just about any language or task you can imagine.",kiwipete,2013-01-30 08:34:07
Totally.  OP probably has his hands full enough just learning stats &amp; R through.,BillWeld,2013-01-28 12:27:58
"If you want to *use* R, I cannot recommend RStudio enough. I have programmed in many languages and many environments, and RStudio is one of the most well-packaged software suites I've seen for development (certainly rivals some of the best Eclipse setups I've seen).

If you're really concerned about ""looking like a hacker"" when people see you work, you may want to look into [Notepad++](http://notepad-plus-plus.org/), [Sublime](http://www.sublimetext.com/), and handy-dandy emacs and vim.",featherfooted,2013-01-28 10:49:50
"I was just joking about the hacker thing because my professor had vim I believe and it had a dark background and I thought it looked really cool.

Thanks for the recommendation, I have used R Studio but it was very brief.

I would have to give it a second glance.",,2013-01-28 14:51:53
You can really just change the font and background style in any editor. I need a dark text on a darker background or after a few hours my eyes start to bleed.,,2013-01-28 16:54:09
"I use Sublime Text 2 for pretty much everything. There are some plugins to add R specific functionality (ie syntax highlighting, ""run this code in R"") but the cool part about sublime text is that the text editor is done really well from a language agnostic perspective... their are keyboard shortcuts for things like multiple selection, jumping into files and to symbols (with really fast fuzzy matching). I work in a number of languages (r,go,python,bash,C,js,html) and I like being able to use the same set of tools across them. The visual code maps on the side of the screen are also great hackerish eye candy. 

I tried Rstudio and wasn't particularly impressed but I am not an ide person and I'm usually running my analysis in a distributed or remote environment and I prefer to keep the editor decoupled from the shell.",micro_cam,2013-01-28 13:59:21
Do you have a link for the R plugins?,1337bruin,2013-01-29 11:02:43
"I don't actually have any R specific plugins installed at the moment but there are a couple including R-sublime and Rtools. I would give Rtools a shot since it is installed via the Package Control plugin which is awesome:

https://www.google.com/search?q=sublimme+text+R

There is also native code highlighting included with sublime and you can use the general purpose sublimeREPL with R as well though I prefer to just save a file and run it from the comand line (sometimes I work on a dir on a remote server via fuse or synch files up via scp)

",micro_cam,2013-01-29 11:15:57
Emacs with ESS is amazing.,,2013-01-28 13:28:07
[deleted],,2013-01-28 11:44:45
"[You are correct, sir](http://info.revolutionanalytics.com/free-academic.html).",navyjeff,2013-01-28 14:21:05
"If you want to learn vim just type vimtutor into your shell (assuming you're on a *nix based system, ie. Mac, Linux).  The tutorial is easy and vim is extremely powerful. rstudio looks like it has a lot of fans so you should probably roll with that, but learning vim is invaluable if you're going to be become a programmer and vimtutor is the place to start.",p01ym47h,2013-01-28 18:15:08
I like TINN...I can't remember if development support has ceased but its a nice little editor.,nblarson,2013-01-28 10:31:57
"As a pure text editor, Sublime Text 2 works well. If you're on an older mac TextMate is good and TextWrangler (while a little threadbare) is great and free. 

but I second the recommendation to use RStudio. It's a **fantastic** application, free, open source and well supported. Can't recommend it enough to new R users. ",therealprotonk,2013-01-28 16:34:16
I'm a big fan of RKward.,,2013-01-28 21:55:46
"This is actually the only real GUI for R that I know of outher than Revolution R. It has a real data-grid editor just like SPSS, Stata and SAS, and you can make your own menus.

See [here](http://sourceforge.net/projects/rkward/) and [here](http://rkward.sourceforge.net/)",Synes_Godt_Om,2013-01-28 23:41:02
It made my transition from MATLAB easier.  It is quite similar to Spyder if you want to take up Python for math and statistics.  There are some great machine learning libraries in Python that I sometimes need.,,2013-01-28 23:51:32
rkward was one of two reasons I switched to linux. Linux might not have been on my radar without my knowing of rkward.,Synes_Godt_Om,2013-01-29 06:18:44
"My research team was on Linux, so I never really had the option, but finding things like Rkward and Spyder has made the switch so much easier.",,2013-01-29 09:57:28
"Thanks for pointing me to spyder, I had completely forgot about it. I don't do much python but have been meaning to dive into it. I made a command line interface in python to lem (the latent class program) some years ago, that made it possible to batch-run many models and a few other simple tools.

Now I'm planning to take a closer look at pandas and iPython as soon as I get the time.",Synes_Godt_Om,2013-01-29 12:33:44
"If you do Machine Learning, check this out:

http://scikit-learn.org/stable/",,2013-01-29 12:39:58
"I was just getting to love vim, but then [stupid boring story] so I was forced to use windows only, and it's been a big pain to get vim to work on W7, so I'm cutting my time losses and using RStudio for now. It's pretty great for what it is.",bobbyfiend,2013-01-28 17:58:07
"Just curious, what were your troubles with Vim on W7? I've been using in to W8 for a while with only a few issues.",phalangion,2013-01-29 07:08:01
vim installed fine. I couldn't get the r-vim plugin to work :(,bobbyfiend,2013-01-29 18:57:57
"vim with vim-r-plugin is really awesome, it just has a very steep learning curve for the uninitiated which I'm sure is why he told you not to use it.",johnmcdonnell,2013-01-28 20:13:15
NPPtoR works great for me,bstockton,2013-01-28 20:19:26
"As everyone else has suggested, RStudio is great.  I'm not a programmer, so I work with what I know.  When I have to enter a lot of repetitive code with little tweaks (changing subset, variables, etc), I write my code in excel sheets.  It works very well for me, and had no learning curve.  I just set up columns for different parts of the command.  Then tie them together in a single command.  Example; =A2&amp;""&lt;-""&amp;B2&amp;""subset=""&amp;C2
The tricky part is that excel doesn't like to use ""'s and ,'s as text, so sometimes you need to make a column thats "" or , so you can refer to them as cells. ",Tankbean,2013-01-28 21:48:53
"Gonna join the [RStudio](http://www.rstudio.com/) circlejerk here. It really is the bee's knees. Everything you want, nothing you don't, free, light, well-maintained, and cross-platform. ",somehacker,2013-01-28 23:38:57
eclipse by a fat margin,Zeurpiet,2013-01-28 11:02:51
Eclipse for R?,clm100,2013-01-28 11:15:49
[yessir](http://lukemiller.org/index.php/2012/11/the-new-definitive-guide-for-setting-up-eclipse-statet-and-r-on-windows-7/),,2013-01-28 12:10:34
http://www.walware.de/goto/statet,Zeurpiet,2013-01-28 12:09:13
"Don't use a clunky overweight editor, use the *standard* editor: ed! [More info here](http://www.gnu.org/fun/jokes/ed-msg.html).",johnmcdonnell,2013-01-28 23:07:36
I want to point out that there are more detailed reports linked at the end for those who are interested.  What's interesting is that the effect isn't observed when voting machines are not in use.,brolysaurus,2012-10-24 01:50:45
"Not going to read through it all again, but if this were the case, Romney would also score higher overall when there are voting machines in use and i'm sure we would have heard of that.",Tripplethink,2012-10-24 02:33:57
"Aren't we missing the big picture? 
1) The effect only occurs when votes are electronically tabulated and it's a large enough effect to easily swing an election.
2) When it occurs, it only helps a single GOP candidate and hurts all other candidates.
3) It never occurs when no GOP candidates are involved.
4) There is no quality control on electronic voting, AFAIK. No one is responsible for plugging in thousands of votes and checking that they are accurately counted. 
5) Experts insist that it would be easy to tamper with the voting machines. 
6) In 2004 exit polling, which had been the gold standard for detecting fraud, all of a sudden became inaccurate, overestimating Kerry's share of the vote in 26 states by more than one standard error. 

Certainly this should be enough cause to justify a random QC sampling of some machines on the day of the elections. The fact that there is no QC testing is extremely troubling in the first place and should put us on high alert for exactly this type of anomaly. 

Have any states where this effect was seen done a hand recount? 
Is a recount even possible with some machines?",plumcrazy,2012-10-27 04:11:22
[deleted],,2012-10-23 19:41:56
"Probably, some reputable people did analyze this and didn't find anything.",Tripplethink,2012-10-24 02:37:12
"Let me paraphrase what you're saying...""These are not the droids you're looking for""",John1066,2012-10-27 22:13:35
probuboly.,ROBOT-MAN,2012-10-27 14:52:49
"@Romney gaining votes with cumm. precinct size:

The smallest precincts set the starting points. If they are systematically lower than the average percentage of votes, then, as a result, percentages will always increase. I'm not from the US but it seems plausible to me that there is a higher prevalence of ""very christian"" people in the smallest pricincts, since those probably represent small villages, and that those people would not vote for a mormon. Of course, Utah would be an exception, because there are a lot of mormons there. Interestingly enough, they don't find this effect in Utah.

@2008 general elections:

They present isolated counties in which they find an effect. However, there are a lot of counties, so it would actually be very unlikely to not find the effect in any of them. Also, it again seems plausible that there are more conservative and racist and less black people in the smallest precincts.",Tripplethink,2012-10-23 11:37:39
"Basically, it seems as if the analysis hinges on the idea that precinct size is unrelated to underlying demographic differences/population density/what-have-you. The assertion is made that ""It's precinct size that serves as the predictor, and not urban/rural characteristics"" - but the only thing demonstrated is that precinct size is a predictor, and no demonstration of its non-relation to the other possible confounding factors is given.",Neurokeen,2012-10-23 11:48:06
"&gt;no demonstration of its non-relation to the other possible confounding factors is given.

Not true.  I'm not saying the following explanations are good or sufficient, but possible confounding factors are addressed.

&gt;Claims of Effect by External Causes:  Demographics 

&gt;The “demographic argument” is what most people use to try to explain the slopes in the Republican presidential candidate charts. To quell the demographics argument early on, a researcher suggested that demographics be charted directly as function of Cumulative Precinct Vote Tally, since precinct size, measured by vote tally is independent of demographics. The reasoning was that if demographics were relevant as a function of precinct size, they would be indicated as a trend on the cumulative charts. 

&gt;Here are various demographic groups of Republican Males and Females. All show a consistent flat line:

&gt;Other arguments claimed that Mitt Romney received a larger percentage of the vote in larger precincts, because rich people live in large precincts and are more likely to vote for Romney, because they are rich. Besides the premise being false, such a demographic claim was investigated and failed. These chart traces are flat (Figure 4)

&gt;Other demographic arguments were all were rejected as the cause of the unusual slopes favoring candidate Mitt Romney. A mysterious cause could be claimed, but no facts have been presented.",Troybatroy,2012-10-23 12:06:17
"Neurokeen may not be correct in a literal way, but in effect he is because the confounding factors they checked were never very likely to begin with. Why would gender differ between smaller and larger precincts and even if it does, why would it have such an effect? There are other variables that are way more likely to explain what they found but they either didn't check them or chose to not present them.",Tripplethink,2012-10-23 12:37:00
"I admit, I missed that. I was text searching for urban/rural, population density, and race. Even seeing that section, I'm not particularly convinced that they were looking at the right factors.",Neurokeen,2012-10-23 13:31:35
"I agree checking/displaying gender was not helpful.  They did present some economic data afterwards.

&gt;Other arguments claimed that Mitt Romney received a larger percentage of the vote in larger precincts, because rich people live in large precincts and are more likely to vote for Romney, because they are rich. Besides the premise being false, such a demographic claim was investigated and failed. These chart traces are flat (Figure 4)

&gt;Other demographic arguments were all were rejected as the cause of the unusual slopes favoring candidate Mitt Romney. A mysterious cause could be claimed, but no facts have been presented.

I think their method of display is informative, but doing a visual check isn't scientific.  They reference a more thorough analysis, but I don't have time to look it over.",Troybatroy,2012-10-23 12:47:47
"&gt;I'm not from the US but it seems plausible to me that there is a higher prevalence of ""very christian"" people in the smallest pricincts, since those probably represent small villages, and that those people would not vote for a mormon. 

I don't think this could possibly be it.

Go to page 5 on the more recent paper (same thing with more data, you can see the actualy slow calculation on page 25)

http://www.themoneyparty.org/main/wp-content/uploads/2012/10/Republican-Primary-Election-Results-Amazing-Statistical-Anomalies_V2.0.pdf

Why don't we see the same effect for Hunstman?  He too is Mormon?  I specifically picked New Hampshire because he data is so tiny in the other areas, but if your assumption is correct we should see something similiar (albeit smaller) for Hunstman, but we don't his votes go down while Romneys go up.  



",Adtwerk,2012-10-30 12:23:02
"So to refute the ""demographic argument,"" they look at gender and income.  What about urbanicity? Surely that is highly correlated with both precinct size and voting for Romney.  
What about other election data?  Downticket ballots?  ",ICrepeATATs,2012-10-23 17:51:13
"What is urbanicity? It is a categorical variable? How do you find it?

Also, in primaries, the data on downticket ballots is likely to be rather difficult - most are pretty determined, with few useful data points. It might be interesting, but I don't know where to find the data.",davidmanheim,2012-10-23 20:11:41
"Census releases data by 'Voting District,' aka precinct.   One of the variables is 'households living in urban areas' or something similar.  You can construct a continuous variable of urbanicity by dividing 'households living in urban areas' by 'total households in area.'  
I used these data a few months ago, so I'm a bit fuzzy on the names, but confident it's there. ",ICrepeATATs,2012-10-24 04:27:35
"But most precincts are small enough that they are either 100% urban, or not. How are you going to get reliable estimates?",davidmanheim,2012-10-25 14:45:53
"The Census will have that at the VTD level, which is basically precinct. Should be a dichotomous variable in either SF1 or SF2 of the Decennial.",casualfactors,2012-10-23 23:55:41
"We've checked downticket races. They usually flatline, but occasionally when an establishment GOP runs we see some strong slopes.",fchoquette,2012-11-04 17:03:06
"If anyone would like to do their own analysis, here's some data. I reanalyzed Oklahoma (at random) using the data [here](http://www.ok.gov/elections/The_Archives/Election_Results/2012_Election_Result
s/index.html). Here's the OK data [munged](http://textuploader.com/?p=6&amp;id=IZ78O) into the stats they're using. Here's the [python script](http://textuploader.com/?p=6&amp;id=vK98d) I wrote to parse the raw data. I don't know where, if anywhere, to find data on what precincts use electronic voting machines, but an analysis of the whole dataset produces a graph which matches theirs.",eggsyntax,2012-10-30 18:56:52
"One more gripe:  I have a hard time believing the authors are data folks when they draw these graphs as lines rather than scatter plots.  A line indicates a continuous measure, or a measure over time.  Surely there isn't a precinct at every point on the x axis.  So they're 'connecting the dots,' which is a faux pas and hurts their case. ",ICrepeATATs,2012-10-24 04:30:17
"It's a new type of chart that was developed for this project. Scatter charts are fine, but with 10,000 points, it's kinda hard to see the trend. Of course you can put in a straight line trend, but that's a line and again does not fully represent the data. A polynomial is also a fit that does not best represent the data.

The cumulative line is delicate, exact and clearly shows any subtle variations along from the small precincts to the large precincts.",fchoquette,2012-11-04 17:01:04
"What beats me is: why on earth are they using cumulative graphs?? What's the use? If the demonstrated effect is there, surely it would show in a simple scatterplot showing percentages versus precinct size, wouldn't it? That would be way less arbitraty, and more interesting because it gives a better insight in the raw data that's used.",Paltenburg,2012-10-26 07:11:32
the effect would be far more pronounced in a graph as such. it should be a line with a slope twice that of the graphs given,empathica1,2012-10-27 06:57:53
"There are 854 precincts in my county. In my state there are 11,107. That would be a lot of dots on a scatterplot. It's much easier to connect the dots, and nothing is lost by doing so.",themagicpickle,2012-10-31 17:27:59
Cumulative charts are much easier to see than a cloud of points. Try it!,fchoquette,2012-11-04 16:57:31
"Yes, but it is an interpretation of the data, isn't it. In this case I would be more interested how the actual data looks like, that is the relation between county-size and romney votes. For example, a cumulative chart sorted the other way round (so starting with the big counties) is yet a different interpretation, how would that look like?

And about the comment that there's too many dots: make it a density map then.",Paltenburg,2012-11-05 06:13:37
I find it surprising there isn't a discussion of the difference between supervised and unsupervised learning approaches. I would say this article considers only the unsupervised case.,,2011-05-27 12:46:49
What difference would it make? Chompsky's complaint that machine learning isn't identifying core identities or relationships would still hold.,DoorsofPerceptron,2011-05-28 05:49:28
"&gt;This essay discusses what Chomsky said, speculates on what he might have meant, and tries to determine the truth and importance of his claims. 

-Peter Norvig, on why Noam Chomsky is an ignorant attention-whore who knows nothing about what he attempts to critique. ",casualfactors,2011-05-27 17:45:19
This is the never ending debate on the purpose of mathematical models. Pure experimentalists often deride them. I see it often in Biology. The purpose of a model is the same reason why an architect builds a wooden model of a building. To test it out and make some predictions. Pretty simple. ,,2011-05-31 21:10:35
"Your question is too general to answer; however, data scientists mostly clean and organize data.  Analysis is only 10-20% of the job for most people.  ",master_innovator,2015-02-21 09:57:17
"Curious, isn't that what data analysts do? Or database admin? What separates the data scientists from them?",Andican,2015-02-21 11:40:53
"The best description I've heard of the difference is that a data scientist knows more programming than a statistician, but less about programming than a computer scientist.  The core skill, why people get paid 200K plus, is being able to critically think and solve business problems with data, communicate that value to upper management, and train a team to do the same.",master_innovator,2015-02-21 12:08:29
"Great answer. Data people I know that make the most money are really not number geniuses. Sure, they have a PhD and are talented in that arena, but where they really shine is being excellent speakers, communicators, and managers",MZITF,2015-02-21 15:06:15
"Titles are meaningless.  A data scientist at one place could be equivelent to a data analyst somewhere else.  Hell, 2 data scientists could be doing vastly different things.

DBA is a much more technical role that is familiar with vendor databases (e.g. Oracle, MySQL, or whatever NoSQL tech floats your boat).

If you're getting your hands dirty with the data, you absolutely need to clean, restructure, and audit your data.",flipstables,2015-02-21 14:38:22
"&gt; data scientist

Is a marketing term where most of them clean and organize data and spend 10% on analysis.",homercles337,2015-02-21 15:38:33
"it would depend on where you work.

i'm currently at a start-up that uses a LEAN/Six Sigma model, so i'm both analyst and scientist (with the scientist title). i would love to have someone clean and organize my data, however i also find that in cleaning and organizing it i can get insights that ultimately help me solve the problem i'm working on.

i would argue that an analyst is either a junior data scientist (simply lower on the totem pole) and/or has little to no programming experience, however can extract meaning from data using tools like Excel.",kierisi,2015-02-21 12:14:21
"Most of the problems I work on can be modeled pretty well using:

- Bayesian GLM

- Neural Network

- Support Vector Machine

- Random Forest",chrico031,2015-02-21 13:11:28
Why Bayesian GLM rather than just GLM?,nrs02004,2015-02-21 20:35:23
"Honestly?

Because I can typically get better performance out of a Bayesian GLM on the data I model than out of a GLM.",chrico031,2015-02-21 21:04:05
How do you measure performance?,quaternion,2015-02-21 21:35:35
Cross-validation on test datasets.,chrico031,2015-02-21 22:31:32
"I'm just an analyst but I'll chime in. I too would say that 75% of my work consist of data wrangling. In terms in analysis I've done kmeans/hierarchical clustering with a bunch of different distance options. I've done correspondence analysis, PCA, frequent pattern analysis, latent class analysis, and some simple anova testing. My go to model for classification is random forest in R. I'd like to get to know more clustering approaches so if anyone has a few go to models for that please suggest some. ",STUstone,2015-02-21 20:24:31
"You can get a little bit into spectral clustering, as well as Gaussian mixture clustering. The R package 'mclust' has a very well written vignette that will introduce you to the latter.",srkiboy83,2015-02-22 09:04:10
"I like C4.5 (although random forests are better), CART and logistic regression is quite good when you're dealing with LOTS of features. ",watersign,2015-02-23 20:34:09
"I work for a biotech startup. Going through my most recent project, techniques I used included

 - Convolutional neural networks
 - Graph-guided fused lasso
 - Probabilistic PCA
 - Graph lasso 
 - Chow-Liu algorithm
 - Simulated annealing
 - Gaussian process classification
 - Bayesian hierarchical clustering
 - MCMC sampling
 - Logistic regression
 - Linear regression

It was a pretty open-ended problem I was attacking. Lotta dead ends and backtracking.",bluecoffee,2015-02-21 10:03:11
"Hey, I know a couple of those :|

Guess I'll be an analyst forever",Ghostface_Grillah,2015-02-21 10:11:01
"I learnt of almost all of the above from reading [MLAPP](http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation) (except the NNs, which I learnt from [here](http://deeplearning.net/reading-list/)). This is my first job out of uni and I was originally hired (last summer) as a software developer.",bluecoffee,2015-02-21 10:20:22
"Educational background?

MS Econ here. Took a lot of pure math (e.g. Real Analysis) instead of stats so I feel a little off base but this is where I want to be in 3 years.

Whats funny is that I'm effectively 50/50 programming to analyst at work. Got R, Python, SQL nailed down (and Linux) so I feel close...",Ghostface_Grillah,2015-02-21 10:27:24
"MA maths. Started a PhD but quit it a year in. Assuming your linear algebra is rock-solid, best advice I can give is to read MLAPP, get comfortable with your analysis ecosystem of choice (mine's Python), and *do something with it*. Pick a dataset you think's interesting - god knows there are enough of them floating around nowadays - and see what you can find.

Also, consider working for a small company. I don't think I'd have been able to carve out the role I have if the CTO didn't sit next to me.",bluecoffee,2015-02-21 10:33:58
"&gt; Assuming your linear algebra is rock-solid

Yep. Also took upper-level matrix theory after LA.

I'm at a small company now and they're ""getting into analytics"" and want me to help. The tough part, though, is that they (as would be expected) are used to seeing BI so they tend to have this bias that DS is BI.

I'm getting them on board, slowly, with the predictive/inferential side of things since I don't want to just make bar charts all the time. But so far, I've made bar charts and web apps, lol. 

(P.S. I also left a PhD after a year and a half)",Ghostface_Grillah,2015-02-21 10:43:36
"I'm also at a startup, and my problem is that my boss (a very good analyst, btw) keeps insisting that I should build more 'dashboards' for the ML models we use. I'm never sure what to tell him, really.",srkiboy83,2015-02-22 09:07:44
Yep. I've made a couple of those. Its kinda cool the first time but not very intellectually stimulating. ,Ghostface_Grillah,2015-02-22 11:09:34
I've read MLAPP briefly for a class but did not find the linear algebra too be too onerous. But it may just because I have not read it closely. What's a chapter that's really heavy on LA so that I can check whether I need to go back and beef up my LA?,selectorate_theory,2015-02-21 13:48:16
"note that ""rock-solid"" is different to ""advanced"". The LA you need to get through the book is foundational, but if (for example) you've got a solid mental model of how sym pos def operators create a sense of ""similarity"", there's a lot of stuff that turns trivial.",bluecoffee,2015-02-21 15:30:54
"Okay, what you just said I actually don't understand. Indeed, I've found that while I know the basic linear algebra theorems and properties, I don't actually have the (geometric) intuition similar to what you just said. Do you recommend any book / courses that study linear algebra in this way?",selectorate_theory,2015-02-21 16:35:36
"Aight, first a cliffnotes example as to why it's useful for probability:

 - If *A* is symmetric &amp; positive definite, then *x^T A y* defines an inner product - a sense of distance/similarity.
 - In particular, given *A* we can pick a basis such that *A*'s inner product turns into the normal dot product: *x^T A y = x'^T y'*, where *x', y'* are *x, y* in the new basis.
 - And for the case where *x = y*, we get *x^T A x = ||x'||^2*
 - Now take a Gaussian and ignore the normalizing constants. We have *p(x) = exp(1/2 (x - m)^T C^-1 (x - m))*, where *C* is the covariance matrix.
 - But the covariance matrix is symmetric and positive definite, which means its inverse - the precision, *P* - is too. So by picking the right basis, we get *p(x) = exp(1/2 ||x' - m'||^2 )* 
 - Or, in words: the probability of *x* depends solely on its distance from *m* in a space with a certain sense of distance

Anyway the term *x^T A y* for a sym-pos-def *A* turns up all over the place in machine learning, which makes ideas like the above incredibly useful.

As to how to learn it: [this little handout](http://www.math.toronto.edu/jkamnitz/courses/mat247/bilinearforms1.pdf) and [this one](http://www.math.toronto.edu/jkamnitz/courses/mat247/bilinearforms2.pdf) hit some of the key ideas, and you can find more in any linear algebra book with a section on bilinear forms.

If you want to practice the ideas, the best place is probably an intro differential geometry textbook, where they're strongly linked to ""fundamental forms"" and ""metrics"". I learnt from [Gravitation](http://www.amazon.com/Gravitation-Physics-Charles-W-Misner/dp/0716703440/) (which is an amazing book), but for your purposes that'll probably be a few gems in a sea of irrelevant garbage.",bluecoffee,2015-02-22 02:00:53
"Thanks for the quick explanation on this particular topic! Do you pick up these linear algebra over time, or there's a particular book / course that focuses on the type of linear algebra that appears frequent in machine learning?

One example is matrix calculus: used all the time for optimization, but a first course in linear algebra rarely covers it.",selectorate_theory,2015-02-23 20:54:55
"I think you just pick it up as a result of using it in a geometric context. I haven't seen any sources that focus on picking up the intuition without the associated diffgeom/genrel/etc applications, which is a shame because like I've said, it's very useful.",bluecoffee,2015-02-24 04:14:38
"Coursera has a course '[Coding the Matrix: Linear Algebra through Computer Science Applications](https://www.coursera.org/course/matrix)'.
I trust it is exactly what you are looking for.

",dexplorer,2015-02-22 03:25:07
"it isn't. looking at the syllabus, it's LA101 with a few algs thrown in.",bluecoffee,2015-02-22 10:14:41
Taking it right now. It makes the concepts easier to understand/see the applications of.,masasin,2015-02-22 04:17:59
"What's the best way to get comfortable with MLAPP? I recently bought a copy, but I find that the majority of the math seems incomprehensible and pulled out of a hat.

I'm mid-way through a humanities (music theory) PhD and I definitely want out of humanities academia. My pipe dream job would be doing ML for a place like Spotify or Pandora or Echo Nest, but I doubt anyone would hire me for anything interesting with a humanities degree.

My (shaky) math background: calculus+ODEs, one of those awkward ""introduction to analysis"" courses that's proof-based but not at the level of baby Rudin, probability (from Ross's book), linear algebra (also proof-based but not very rigorous -- I can follow what you wrote below about PSD matrices but I'd need MLAPP to have little blurbs explaining all that instead of assuming that I know it). 

I'm currently hopping departments to audit undergrad CS classes while putting the majority of my dissertation work to the side in the hopes of being able to do something both ML and music related once I get my skills up. Last semester I did computer vision and AI. I'm auditing information retrieval and an intro to data science course this semester.

My current goal is exactly what you say: master MLAPP and wrangle some data. Unfortunately, I'm in the ""danger zone"" on that infamous venn diagram -- domain expertise and hacking skills, but no statistical intuition.
",bunchabinchois,2015-02-24 09:41:35
"Honestly, it sounds like you don't need much to be able to handle MLAPP, just some more practice with linalg. To that end, I'd suggest

 - [Linear Algebra Done Right](http://www.amazon.com/Linear-Algebra-Right-Undergraduate-Mathematics/dp/0387982582/). Takes an algebraic approach, explaining linalg mainly in terms of subspaces and invariants. It assumes you've had a first course in matrix math/linalg already, which it sounds like you have.

 - [Linear and Geometric Algebra](http://www.amazon.com/Linear-Geometric-Algebra-Alan-Macdonald/dp/1453854932/) and [Vector and Geometric Calculus](http://www.amazon.com/Vector-Geometric-Calculus-Alan-Macdonald/dp/1480132454/). These two books take a geometric approach and use it to build up a  intuitive view of vector calculus. 

Quite often in maths, getting to grips with a concept means finding it's place in many different contexts. The books above cover the same ground, but from multiple different angles. I suspect that'll help you a lot.

As a final note, these are just the books that worked for me. For more recommendations, check [this site](http://hbpms.blogspot.co.uk/) and test books out on Library Genesis.",bluecoffee,2015-02-25 01:31:12
"honstly have you gotten any use of convolutional neural nets in biotech?

My experience has been that biological data sets, even ""large"" ones, are more of the large-p, small-n variety so methods like that haven't been too useful. Not to mention that there's usually an underlying ""mechanistic"" process which is of interest and not well captured by a distributed representaiton.",klaxion,2015-02-21 13:48:26
"&gt; honstly have you gotten any use of convolutional neural nets in biotech?

Yuup. The data was indeed small-n, but I used artificial deformations to construct a much larger training set. The features it learnt were substantially better than the hand-built ones that had been used previously.

e: The problem was to do with extracting information from biotech-sourced imagery. ",bluecoffee,2015-02-21 15:25:59
What are artificial deformations? do you have a good reference?,klaxion,2015-02-21 15:39:19
"It's not definitive, but try Section 4.1 of the [Alexnet paper](http://research.microsoft.com/pubs/68920/icdar03.pdf). It's a simple idea: artificially enlarge the training set by applying label-preserving transformations to the source images. Standard ones are scaling, cropping, mirroring, colour shifts, random noise and displacement fields.",bluecoffee,2015-02-21 15:51:02
"Hey I was checking out a couple of your replies talking about your background. I was wondering if you had any colleagues that work in the same role as you do, but had backgrounds that were more biology/biomedical based?

Just curious as I come from a biology/biotech education, do benchwork, but am currently getting into statistics/programming.  Is there an advantage to having a biological intuition that someone who is obviously very skilled in the math department (such as yourself) might not automatically have?",Maksimilian,2015-02-22 13:05:24
"&gt;  I was wondering if you had any colleagues that work in the same role as you do, but had backgrounds that were more biology/biomedical based?

Yup - the CTO is a bioscience/stats guy. There's a tradeoff though, in that while I know nothing whatsoever about the biology, I'm a much stronger mathematician than he is.

Outside the company, I've heard several biologists say that there is an *immense* number of low-hanging fruit for anyone with biology  knowledge and an understanding of programming + stats/ML. Same in sociology and anthropology and all the other traditionally-qualitative sciences. Simple fact is that 90% of people with a mathematical inclination end up in mathematics, engineering, computer science or physics. Every other discipline is starved.

[This guy's publication history](https://danlwarren.wordpress.com/science/) is a good example of how you can build a career out of that kind of thing.",bluecoffee,2015-02-23 03:14:10
"Cool, your comment is encouraging. And out of curiosity is your CTO a PhD (I'm assuming yes)? have you seen people reach high levels without a PhD? with maybe a masters degree in each discipline (bio + stats or bio +comp sci)?",Maksimilian,2015-02-23 07:13:44
"He's a PhD, I'm not. The limited experience I've had is that with ML people so scarce, results matter an awful lot more than qualifications. I think the main advantage of a PhD is that it gives you several years to build an impressive, public-domain body of work. Doing the same while in full-time employment isn't impossible, but it's a lot more difficult.",bluecoffee,2015-02-23 07:53:23
"Wow, u am stuper smrt!  Huh u geet so smrt?",homercles337,2015-02-21 15:41:08
"* Risk models (probability of default, loss given default, etc.)
* Response models (probability of responding to ads)
* Content discovery (recommendation engines, collaborative filtering, similarity)
* Search (autocomplete/autosuggest, query rewriting)",dimview,2015-02-21 12:41:41
"Professional carpenters of Reddit, what type of chairs/shelves do you do?",daidoji70,2015-02-21 17:52:50
how many years of experience would you say you have with brown?,nrs02004,2015-02-21 20:34:53
"Wrong subreddit buddy, try /r/carpentry ?",radarsat1,2015-02-22 02:40:31
He is making a point on the ambiguity of op's inquisition by equating it to asking a Carpenter what type of chairs/shelves they make.,Autotoxin,2015-02-22 12:47:34
"Thank you, Capn'",radarsat1,2015-02-22 14:20:34
"I work in model validation at a bank, so I don't actually build models, just critique what other people build. The most common thing I'm seeing right now are scorecards built for rank ordering accounts on their risk (think FICO scores). I've seen many models using multiple linear regression. One thing to note is that the basic methods get used much more than you'd think, especially in banks where explainability of the model is of utmost importance. ",tacojohn48,2015-02-21 21:55:48
"not a statistician or a data scientist, but i use decision trees and logistic regression quite a bit!",watersign,2015-02-23 20:33:26
pricing cell phone apps,DemonKingWart,2015-02-21 13:08:50
"http://en.wikipedia.org/wiki/Anscombe's_quartet

Not a traditional ""dataset"" but shows the problems with point estimates.",Nimish,2014-11-12 02:15:41
Came here to post exactly this :D,tpn86,2014-11-12 04:40:01
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Anscombe's quartet**](https://en.wikipedia.org/wiki/Anscombe's%20quartet): [](#sfw) 

---

&gt;

&gt;__Anscombe's quartet__ comprises four [datasets](https://en.wikipedia.org/wiki/Dataset) that have nearly identical simple statistical properties, yet appear very different when graphed. Each dataset consists of eleven (*x*,*y*) points. They were constructed in 1973 by the [statistician](https://en.wikipedia.org/wiki/Statistician) [Francis Anscombe](https://en.wikipedia.org/wiki/Francis_Anscombe) to demonstrate both the importance of graphing data before analyzing it and the effect of [outliers](https://en.wikipedia.org/wiki/Outlier) on statistical properties. 

&gt;For all four datasets:

&gt;The first [scatter plot](https://en.wikipedia.org/wiki/Scatter_plot) (top left) appears to be a simple linear relationship, corresponding to two variables correlated and following the assumption of normality. The second graph (top right) is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear, and the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is not relevant. In the third graph (bottom left), the distribution is linear, but with a different regression line, which is offset by the one outlier which exerts enough influence to alter the regression line and lower the correlation coefficient from 1 to 0.816. Finally, the fourth graph (bottom right) shows an example when one [outlier](https://en.wikipedia.org/wiki/Outlier) is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear.

&gt;====

&gt;[**Image from article**](https://i.imgur.com/gH4QtU9.png) [^(i)](https://commons.wikimedia.org/wiki/File:Anscombe%27s_quartet_3.svg)

---

^Interesting: [^Frank ^Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) ^| [^Linear ^regression](https://en.wikipedia.org/wiki/Linear_regression) ^| [^Regression ^model ^validation](https://en.wikipedia.org/wiki/Regression_model_validation) ^| [^Correlation ^and ^dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence) 

^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cm09fj2) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cm09fj2)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2014-11-12 02:16:03
"Similarly, this is a fun activity where students can plot their own points and throw in an outlier to see the affect on r and the regression line:

http://illuminations.nctm.org/Lesson.aspx?id=5726",jwardnw,2014-11-12 14:52:58
"Fisher's iris data set is the standard, so that.",beaverteeth92,2014-11-12 06:51:20
"Sepal Length. Sepal Width. Even now when I learn new methods, it is still comforting to know what dataset I am looking at.",Mockingbird42,2014-11-12 14:57:38
"I like [Prussian Horse Kick Data](http://www.math.uah.edu/stat/data/HorseKicks.html) to demonstrate the Poisson distribution. For those who don't know, it's the number of Prussian calvary killed by horse kicks to the head between 1875-1894",Distance_Runner,2014-11-12 11:03:02
And the reason why the Poisson distribution was invented in the first place!,beaverteeth92,2014-11-13 13:10:36
"I like to use Galton's height dataset.  I think, it has about 700 people in it, and the people come from families and are coded so that you know the parents of each of the children, and spouses of each parent.

The main benefit of this dataset is that height for men and women is a distribution that people are very familiar with in an informal way.  So, they, for the most part can reason about height and connect it directly to statistical tests, and vice versa.

1) height for each gender is almost perfectly normally distributed

2) the male and female height distributions overlap somewhat, so you can calculate odds ratios for gender if you know height

3) you can calculate height heritability by regressing male and female height on mid-parental height

4) you can calculate correlations between the heights of each of married couples

5) a surprising number of people believe unsupported things about height, like that either the father or mother is more important in passing on height, or that boys take after fathers, and girls take after mothers.  These can be tested and disproven.

6) you can calculate regression toward the mean.",maxwell_smart_jr,2014-11-12 18:48:01
"This one professor had a class where the first thing everyone had to do was find a data set and clean it. A lot of the people in the class were older, and had either already be in the working world, or were still in it. They all chose something they were already interested in, that they had some working knowledge of. It took a few weeks to get everything in working order so they could start covering the material in the book. Obviously they didn't get as far in the text as some of the other stats classes, but the students had a *really* strong understanding of the material they did cover. It was impressive.",Secret_Identity_,2014-11-12 07:05:16
I feel like this would never work in an undergraduate course. But it is a great idea though.,teleporterdown,2014-11-12 08:03:00
"Well, you can't just say ""venture forth and acquire data!"" You have to help them a bit--showing them potential sources of data, explaining the types of things that need to be present in the data, etc. You also need to have hard deadlines for things with clearly-stated consequences of not meeting the deadlines, to prevent too many slackers/whiners. 

But it is certainly doable.  

It's all in how you teach, in my opinion.",forever_erratic,2014-11-12 08:40:58
Really? Which part do you think would be hard to sell?,Secret_Identity_,2014-11-12 08:29:26
"I'm just thinking that there would be a small number of students who could really get into it, but the majority would consider it a chore. It all depends on the level of class and how you present it though. ",teleporterdown,2014-11-12 12:05:34
"I'm generally more optimistic about what students are willing to do. Especially the young ones. They're all secretly (or not so secretly) looking for a cause to die for, something to throw themselves into. 

I do think it would be easy to lose them. It would have to be a really well structured course.",Secret_Identity_,2014-11-12 12:19:18
"Well data cleaning is a chore, but it's a chore that you really have to know how to do.",beaverteeth92,2014-11-13 13:13:23
I love this idea. When I am a more experienced teacher and I can spend more time handling the bizarre situations that will inevitably crop up I may try this.,Degovx,2014-11-12 23:31:46
"I like the [General Social Survey data](http://sda.berkeley.edu/archive.htm).  The second link on that page only has a few variables, but it's easier for students to point and click and get tables and stacked bar charts in seconds.  The first link has a much longer list of variables - both categorical and a few quantitative - and more options for the output.  So it takes longer, but it's a great source for real data for test questions and homework questions.

Gallup and Pew Research also publish many of their findings online.  In most articles, you can find the sampling method and the margin of error at the bottom of the article - again, they're great for teaching!  :)",kestrel2,2014-11-12 11:47:17
I like [this one](http://lib.stat.cmu.edu/DASL/Stories/stateexpend.html) because many students blindly run linear regression without doing any graphing. It helps them learn to look at their data and consider whether their model is appropriate. ,dmlane,2014-11-12 16:14:25
"Sometimes I run a social network analysis workshop, and my favorite data set is the Dolphin Social Network. http://networkdata.ics.uci.edu/data.php?id=6 Mostly because of their names, like Upbang.",alexleavitt,2014-11-12 08:16:48
"There are some interesting sets regarding Benford's law.. I am on a phone so I'm not sure I can find it now, but there's one about most common iPhone pass codes, one about Twitter I think... They are all on a blog or website about Benford's law. Anyway it was probably one of the most engaging examples I used.",agent229,2014-11-12 17:53:26
"I sometimes like to tell a story of skin cancer statistics. If the only thing you know about a person is that he/she has skin cancer, then the life expectancy of that person is higher than if the only thing you know about he/she was that they did not have skin cancer.

Why ? : Because more young people get it, and its not that dangerous all things considered.",tpn86,2014-11-12 04:42:03
got a source for that?,Sir_Peng,2014-11-12 09:58:48
"Nope, its gone from the place i found it :( ",tpn86,2014-11-12 15:54:23
"Following up on my old comment, I thought you might enjoy this:

https://en.wikipedia.org/wiki/Low_birth-weight_paradox",tpn86,2014-11-24 23:28:49
"#####&amp;#009;

######&amp;#009;

####&amp;#009;
 [**Low birth-weight paradox**](https://en.wikipedia.org/wiki/Low%20birth-weight%20paradox): [](#sfw) 

---

&gt;

&gt;The __low birth-weight paradox__ is an apparently [paradoxical](https://en.wikipedia.org/wiki/Paradox) observation relating to the [birth weights](https://en.wikipedia.org/wiki/Birth_weight) and [mortality rate](https://en.wikipedia.org/wiki/Mortality_rate) of children born to [tobacco smoking](https://en.wikipedia.org/wiki/Tobacco_smoking) mothers. [Low birth-weight](https://en.wikipedia.org/wiki/Low_birth-weight) children born to [smoking mothers](https://en.wikipedia.org/wiki/Smoking_and_pregnancy) have a *lower* [infant mortality](https://en.wikipedia.org/wiki/Infant_mortality) rate than the low birth weight children of non-smokers. It is an example of [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox).

&gt;

---

^Interesting: [^Simpson's ^paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) ^| [^Birth ^weight](https://en.wikipedia.org/wiki/Birth_weight) ^| [^Mexican ^paradox](https://en.wikipedia.org/wiki/Mexican_paradox) ^| [^Outline ^of ^children](https://en.wikipedia.org/wiki/Outline_of_children) 

^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cmcf4dm) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cmcf4dm)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)",autowikibot,2014-11-24 23:28:59
"Good ideas! I *wish* I could teach R in my classes. 

I've looked at the correlation between time spend doing homework and final grade in the class (I use MyMathLab for algebra homework). 

In large classes I have the students take a survey with a variety of questions. I like the question about the number of US states visited and comparing the results from one school to the other.",jwardnw,2014-11-12 14:50:58
It was surprisingly easy for me to arrange. One of my grad classes got cancelled and I said can I teach r programming instead and it was approved after less than 10 minutes of discussion!,Degovx,2014-11-12 23:33:09
"[Moment is a general term](http://en.wikipedia.org/wiki/Moment_\(mathematics\)), with the mean and variance being two special cases (of an infinite number of possibilities).

The calculation of the nth moment of random variable X about the point z is given by E[ (X-z)^n ]. So the variance is the second moment of X about the mean, i.e.,  E[ (X-E[X])^2 ]. The mean is the first moment of X about the origin, i.e., E[ (X-0)^1 ] = E[X]. ",DrGar,2014-11-04 10:58:53
"Thanks that helps, but what does ""the first moment of X about the origin"" actually mean?  I'm having a hard time interpreting/internalizing the meaning of this phrase.",buttfoot,2014-11-04 11:04:20
"It's remarkably similar to the notion of moment in physics. http://en.m.wikipedia.org/wiki/Moment_(physics)

Except e.g. instead of mass, you have probability mass, and instead of distance, you have the X axis of your distribution.",jkff,2014-11-04 12:24:54
"Hmm...I think you are perhaps ascribing too much significance to that phrase. To me is is just a very precise phrase that tells you how to plug into E[ (X-z)^n ]. In general, we can have expected values of any function f(X), which would be given by E[f(X)]. The function f(X)=(X-z)^n is a particularly common and useful one for various values of z and n, and in order to specify what the values of z and n take, we have to use phrases like ""the nth moment of X about z"", or ""the nth central moment of X"" which is shorthand for ""the nth moment of X about E[X]"", or ""the nth non-central moment of X"" which is shorthand for ""the nth moment of X about 0"". ",DrGar,2014-11-04 11:12:12
"Got it, this makes more sense now. We never went over the general expression E[ (X-z)^n ] in class, so seeing that together with the phrase ""the nth moment of X about z"" makes it all click.
",buttfoot,2014-11-04 11:24:52
Do mention to your professor that it would have helped to have this explained. Hopefully it will prevent him from assuming too much about your background knowledge. Remember both of you want everyone to learn as much as possible!,againstinterp,2014-11-04 18:10:56
Glad it helped.,DrGar,2014-11-04 11:27:56
"I've always thought about it from a practical point of view. Say you want to find the expected value of a random variable following some distribution. For some distributions, the expected value is readily apparent (uniform, bernouli) , but for other distributions it isn't at all. A moment generating function (MGF) gives a concrete method to find the expected value, varience (,kurtosis,....) of a distribution that you may be unfamiliar with, given that these quantities exist. Also, if you want to make a statement about *all* or a family of distribution's expected values/variances/ect you probably have to appeal to MGFs in your argument.

A moment is an abstraction of expected value. Equivalently, expected value is a special kind of moment. As is often the case when abstracting a concrete concept, the intuition goes away. A moment by itself may have little meaning apart from it's algebraic existence. So, the [49th](http://www.wolframalpha.com/input/?i=49th+moment+of+a+normal+distribution) moment of a normal distribution isn't going to be particularly important to anyone, but it's nice to be able to not only be able to guarantee its existence but produce it as well. ",webbed_feets,2014-11-04 13:29:47
"You know, actually going to Wolfram and entering 1st moment, then 2nd moment, then 3rd moment gave me a very clear image of what a moment is. ",xithy,2014-11-04 16:43:32
"Other people have already described what moments are in general, but I thought I'd take an aside to discuss how the word moment is another one of those lexically ambiguous words that have a very precise meaning in math/statistics and a completely different meaning to the average person. Think words like integral, derivative, normal, that have a quite different meaning in a mathematical sense than in the usual sense.

There was a great talk about lexical ambiguity at one of the educational sessions at last year’s JSM about how when teaching or consulting with people who know little or are just learning statistics, it is important to make sure that the terminology we use is understood by the audience we are communicating with. It's pretty easy for a lot of people who don't have a huge statistics background to get confused or ""think they understand"" the definition of the words you're using, just because the words are familiar to them.

As a further point to this, I've noticed that when I'm writing statistical papers - I do the opposite of what I said above. I make a conscious effort to avoid using any statistical words in the layman's sense to avoid confusing the statistician reading the paper, e.g. never using the word efficient unless I'm talking about efficiency, and if you use the word derivative you better be talking about a rate of change.",statisticalhornist,2014-11-04 20:27:29
"µ = E(X) = first moment -- the mean

E(X^(k)) == raw k-th moment or k-th moment about the origin

E[(X-µ)^(k)] = central k-th moment, or k-th moment about the mean

http://en.wikipedia.org/wiki/Moment_%28mathematics%29

more generally E[(X-c)^(k)] =  k-th moment about c

",efrique,2014-11-04 17:21:28
"If you don't know what ""moment"" means I promise you there are at least a dozen other students in your class that don't know either. Nobody's going to think you're a dumbass for asking for clarification from the prof. But /r/statistics has your back anyhow.:-)",BobBeaney,2014-11-04 22:56:10
